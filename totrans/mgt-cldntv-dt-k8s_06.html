<html><head></head><body><section data-pdf-bookmark="Chapter 5. Automating Database Management on Kubernetes with Operators" data-type="chapter" epub:type="chapter"><div class="chapter" id="automating_database_management_on_kuber">&#13;
<h1><span class="label">Chapter 5. </span>Automating Database Management <span class="keep-together">on Kubernetes with Operators</span></h1>&#13;
<p><a contenteditable="false" data-primary="automation" data-secondary="of database management with operators" data-type="indexterm" id="auto_op"/><a contenteditable="false" data-primary="databases" data-secondary="automating management with operators" data-type="indexterm" id="db_op"/><a contenteditable="false" data-primary="operators" data-secondary="automating database management with" data-type="indexterm" id="op_db"/>In this chapter, we’ll continue our exploration of running databases on Kubernetes, but shift our focus from installation to operations. It’s not enough just to know how the elements of a database application map onto the primitives provided by Kubernetes for an initial deployment. You also need to know how to maintain that infrastructure over time in order to support your business-critical applications. In this chapter, we’ll take a look at the Kubernetes approach to operations so that you can keep databases running effectively.</p>&#13;
<p>Operations for databases and other data infrastructure consist of a common list of “day two” tasks, including the following:</p>&#13;
<ul>&#13;
<li><p>Scaling capacity up and down, including reallocating workload across resized clusters</p></li>&#13;
<li><p>Monitoring database health and replacing failed (or failing) instances</p></li>&#13;
<li><p>Performing routine maintenance tasks, such as repair operations in Apache Cassandra</p></li>&#13;
<li><p>Updating and patching software</p></li>&#13;
<li><p>Maintaining secure access keys and other credentials that may expire over time</p></li>&#13;
<li><p>Performing backups, and using them to restore data in disaster recovery</p></li>&#13;
</ul>&#13;
<p>While the details of how these tasks are performed may vary among technologies, the common concern is how we can use automation to reduce the workload on human operators and enable us to operate infrastructure at larger and larger scales. How can we incorporate the knowledge that human operators have built up around these tasks? While traditional cloud operations have used scripting tools that run externally to your cloud infrastructure, a more cloud native approach is to have this database control logic running directly within your Kubernetes clusters. The question we’ll explore in this chapter is: what is the Kubernetes-friendly way to represent this control logic?</p>&#13;
<section data-pdf-bookmark="Extending the Kubernetes Control Plane" data-type="sect1"><div class="sect1" id="extending_the_kubernetes_control_plane">&#13;
<h1>Extending the Kubernetes Control Plane</h1>&#13;
<p><a contenteditable="false" data-primary="control plane" data-secondary="about" data-type="indexterm" id="cp_abo"/><a contenteditable="false" data-primary="Hightower, Kelsey" data-type="indexterm" id="idm46183198035776"/>The good news is that the designers of Kubernetes aren’t surprised at all by this question. In fact, the Kubernetes control plane and API are designed to be extensible. Kelsey Hightower and others have referred to Kubernetes as <a href="https://oreil.ly/pFRDO">“a platform for building platforms”</a>.</p>&#13;
<p><a contenteditable="false" data-primary="extension points" data-secondary="about" data-type="indexterm" id="idm46183197865040"/>Kubernetes provides multiple extension points, primarily related to its control plane. <a data-type="xref" href="#kubernetes_control_plane_and_extension">Figure 5-1</a> includes the <a href="https://oreil.ly/hsxFY">Kubernetes core components</a> such as the API server, scheduler, Kubelet and <code>kubectl</code>, along with indications of the <a href="https://oreil.ly/UXbo0">extension points</a> they support.</p>&#13;
<figure><div class="figure" id="kubernetes_control_plane_and_extension">&#13;
<img alt="Kubernetes control plane and extension points" src="assets/mcdk_0501.png"/>&#13;
<h6><span class="label">Figure 5-1. </span>Kubernetes control plane and extension points</h6>&#13;
</div></figure>&#13;
<p>Now let’s examine the details of extending the Kubernetes control plane, starting with components on your local client and those within the Kubernetes cluster. Many of these extension points are relevant to databases and data infrastructure.</p>&#13;
<section data-pdf-bookmark="Extending Kubernetes Clients" data-type="sect2"><div class="sect2" id="extending_kubernetes_clients">&#13;
<h2>Extending Kubernetes Clients</h2>&#13;
<p><a contenteditable="false" data-primary="Krew" data-type="indexterm" id="idm46183197574912"/><a contenteditable="false" data-primary="kubectl command" data-type="indexterm" id="idm46183197576656"/>The <code>kubectl</code> command-line tool is the primary interface for many users for interacting with Kubernetes. You can extend <code>kubectl</code> with <a href="https://oreil.ly/kbh4u">plug-ins</a> that you download and make available on your system’s <code>PATH</code>, or use <a href="https://krew.dev">Krew</a>, a package manager that maintains a <a href="https://oreil.ly/iw93T">list of <code>kubectl</code> plug-ins</a>. Plug-ins perform tasks such as bulk actions across <a href="https://oreil.ly/AllQX">multiple resources</a> or even <a href="https://oreil.ly/3Bsj2">multiple clusters</a>, or assessing the state of a cluster and making <a href="https://oreil.ly/BWbpn">security</a> or <a href="https://oreil.ly/Exxjp">cost</a> recommendations. More particularly to our focus in this chapter, several plug-ins are available to manage operators and custom resources.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Extending Kubernetes Control Plane Components" data-type="sect2"><div class="sect2" id="extending_kubernetes_control_plane_comp">&#13;
<h2>Extending Kubernetes Control Plane Components</h2>&#13;
<p><a contenteditable="false" data-primary="control plane" data-secondary="extending components of" data-type="indexterm" id="idm46183197426576"/><a contenteditable="false" data-primary="extension points" data-secondary="for components of control plane" data-type="indexterm" id="idm46183197425008"/>The core of the Kubernetes control plane consists of several <a href="https://oreil.ly/ZnxfB">control plane components</a> including the API server, scheduler, controller manager, Cloud Controller Manager, and etcd. While these components can be run on any node within a Kubernetes cluster, they are typically assigned to a dedicated node which does not run any user application Pods. The components are as follows:</p>&#13;
<dl>&#13;
<dt>API server</dt>&#13;
<dd><a contenteditable="false" data-primary="API server" data-type="indexterm" id="idm46183197820128"/><a contenteditable="false" data-primary="clients, extending" data-type="indexterm" id="idm46183197811296"/><a contenteditable="false" data-primary="CRD (Custom Resource Definition)" data-type="indexterm" id="idm46183197419296"/><a contenteditable="false" data-primary="extension points" data-secondary="for clients" data-type="indexterm" id="idm46183197418320"/>This is the primary interface for external and internal clients of a Kubernetes cluster. It exposes RESTful interfaces via an HTTP API. The API server performs a coordination role, routing requests from clients to other components to implement imperative and declarative instructions. The API server supports two types of extensions: custom resources and API aggregation. CRDs allow you to add new types of resources and are managed through <code>kubectl</code> without further extension. API aggregation allows you to extend the Kubernetes API with additional REST endpoints, which the API server will delegate to a separate API server provided as a plug-in. Custom resources are the more commonly used extension mechanism and will be a major focus throughout the remainder of the book.</dd>&#13;
<dt>Scheduler</dt>&#13;
<dd><a contenteditable="false" data-primary="scheduler" data-type="indexterm" id="idm46183197416304"/><a contenteditable="false" data-primary="scheduling plug-ins" data-type="indexterm" id="idm46183198103616"/><a contenteditable="false" data-primary="plug-ins" data-type="indexterm" id="idm46183197607408"/><a contenteditable="false" data-primary="binding plug-ins" data-type="indexterm" id="idm46183197411296"/>This determines the assignment of Pods to Worker Nodes, considering factors including the load on each Worker Node, as well as affinity rules, taints, and tolerations (as discussed in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a>). The scheduler can be extended with plug-ins that override default behavior at multiple points in its decision-making process. For example, a <em>scheduling plug-in</em> could filter out nodes for a specific type of Pod or set the relative priority of nodes by assigning a score. <em>Binding plug-ins</em> can customize the logic that prepares a node for running a scheduled Pod, such as mounting a network volume the Pod needs. Data infrastructure such as Apache Spark that relies on running a lot of short-lived tasks may benefit from this ability to exercise more fine-grained control over scheduling decisions, as we’ll discuss in  <a data-type="xref" href="ch09.html#alternative_schedulers_for_kubernetes">“Alternative Schedulers for Kubernetes”</a>.</dd>&#13;
<dt>etcd</dt>&#13;
<dd><a contenteditable="false" data-primary="etcd" data-type="indexterm" id="idm46183197427824"/>This distributed key-value store is used by the API server to persist information about the cluster’s configuration and status. As resources are added, removed and updated, the API server updates the metadata in etcd accordingly, so that if the API server crashes or needs to be restarted, it can easily recover its state. As a strongly consistent data store that supports high availability, etcd is frequently used by other data infrastructure that runs on Kubernetes, as we’ll see frequently throughout the book.</dd>&#13;
<dt>Controller manager and Cloud Controller Manager</dt>&#13;
<dd><a contenteditable="false" data-primary="Cloud Controller Manager" data-type="indexterm" id="idm46183197874176"/><a contenteditable="false" data-primary="controller manager" data-type="indexterm" id="idm46183197461264"/>The controller manager and Cloud Controller Manager incorporate multiple control loops called <em>controllers</em>. These managers contain multiple logically separate controllers compiled into a single executable to simplify the ability of Kubernetes to manage itself. The controller manager includes controllers which manage built-in resource types such as Pods, StatefulSets, and more. The Cloud Controller Manager includes controllers that differ among Kubernetes providers to enable the management of platform-specific resources such as load balancers or VMs.</dd>&#13;
</dl>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Extending Kubernetes Worker Node Components" data-type="sect2"><div class="sect2" id="extending_kubernetes_worker_node_compon">&#13;
<h2>Extending Kubernetes Worker Node Components</h2>&#13;
<p><a contenteditable="false" data-primary="extension points" data-secondary="for Kubernetes Worker Node components" data-type="indexterm" id="idm46183197415696"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183197414640"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183197413648"/>Some elements of the Kubernetes control plane run on every node in the cluster. These <a href="https://oreil.ly/KmmkS">Worker Node components</a> include the Kubelet, kube-proxy, and container runtime:</p>&#13;
<dl>&#13;
<dt>Kubelet</dt>&#13;
<dd><a contenteditable="false" data-primary="Kubelet" data-type="indexterm" id="idm46183197394176"/>This manages the Pods running on a node assigned by the scheduler, including the containers that run within a Pod. The Kubelet restarts containers when needed, provides access to container logs, and more.</dd>&#13;
<dt>Compute, network, and storage plug-ins</dt>&#13;
<dd><a contenteditable="false" data-primary="CSI (Container Storage Interface)" data-secondary="about" data-type="indexterm" id="idm46183198099568"/><a contenteditable="false" data-primary="compute" data-type="indexterm" id="idm46183198099920"/><a contenteditable="false" data-primary="networks" data-secondary="plug-ins for" data-type="indexterm" id="idm46183197386992"/><a contenteditable="false" data-primary="device plug-ins" data-type="indexterm" id="idm46183197385616"/><a contenteditable="false" data-primary="CNI (Container Network Interface)" data-type="indexterm" id="idm46183197384512"/><a contenteditable="false" data-primary="FPGA (field-programmable gate arrays)" data-type="indexterm" id="idm46183197383440"/><a contenteditable="false" data-primary="GPUs (graphics processing units)" data-type="indexterm" id="idm46183197382368"/>The Kubelet can be extended with plug-ins that take advantage of unique compute, networking, and storage capabilities provided by the underlying environment on which it is running. Compute plug-ins include container runtimes, and <a href="https://oreil.ly/cdqrT">device plug-ins</a> that expose specialized hardware capabilities such as GPUs or field-programmable gate arrays (FPGA). <a href="https://oreil.ly/aMdKH">Network plug-ins</a>, including those that comply with the Container Network Interface (CNI), can provide features beyond Kubernetes built-in networking, such as bandwidth management or network policy management. We’ve previously discussed storage plug-ins in  <a data-type="xref" href="ch02.html#kubernetes_storage_architecture">“Kubernetes Storage Architecture”</a>, including those that conform to the CSI.</dd>&#13;
<dt>Kube-proxy</dt>&#13;
<dd><a contenteditable="false" data-primary="Kube-proxy" data-type="indexterm" id="idm46183197578800"/>This maintains network routing for the Pods running on a Worker Node so that they can communicate with other Pods running inside your Kubernetes cluster, or clients and services running outside of the cluster. Kube-proxy is part of the implementation of Kubernetes Services, providing the mapping of virtual IPs to individual Pods on a Worker Node.</dd>&#13;
<dt>Container runtime</dt>&#13;
<dd><a contenteditable="false" data-primary="container runtime" data-type="indexterm" id="idm46183197514416"/>The Kubelet uses the container runtime to execute containers on the worker’s operating system. Supported container runtimes for Linux include <a href="https://oreil.ly/6ylhH">containerd</a> and <a href="https://oreil.ly/8fk2x">CRI-O</a>. <a href="https://oreil.ly/col9R">Docker</a> runtime support was deprecated in Kubernetes 1.20 and removed entirely in 1.24.</dd>&#13;
<dt>Custom controllers and operators</dt>&#13;
<dd><a contenteditable="false" data-primary="custom controllers and operators" data-type="indexterm" id="idm46183197370064"/><a contenteditable="false" data-primary="controllers" data-secondary="custom" data-type="indexterm" id="idm46183197369088"/><a contenteditable="false" data-primary="operators" data-secondary="custom" data-type="indexterm" id="idm46183197367872"/><a contenteditable="false" data-primary="" data-startref="cp_abo" data-type="indexterm" id="idm46183197366656"/>These controllers are responsible for managing applications installed on a Kubernetes cluster using custom resources. Although these controllers are extensions to the Kubernetes control plane, they can run on any Worker Node.</dd>&#13;
</dl>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Operator Pattern" data-type="sect1"><div class="sect1" id="the_operator_pattern">&#13;
<h1>The Operator Pattern</h1>&#13;
<p><a contenteditable="false" data-primary="operator pattern" data-secondary="about" data-type="indexterm" id="idm46183197379648"/>With this context, we’re ready to examine one of the most common patterns for extending Kubernetes: the <em>operator pattern</em>. This pattern combines custom resources with controllers that operate on those resources. Let’s examine each of these concepts in more detail to see how they apply to data infrastructure, and then you’ll be ready to dig into an example operator for MySQL.</p>&#13;
<section data-pdf-bookmark="Controllers" data-type="sect2"><div class="sect2" id="controllers">&#13;
<h2>Controllers</h2>&#13;
<p><a contenteditable="false" data-primary="operator pattern" data-secondary="controllers" data-type="indexterm" id="idm46183198740208"/>The concept of a controller originates from the domain of electronics and electrical engineering, in which a controller is a device that operates in a continuous loop. On each iteration through the loop, the device receives an input signal, compares that with a set point value, and generates an output signal intended to produce a change in the environment that can be detected in future inputs. A simple example is a thermostat, which powers up your air conditioner or heater when the temperature in a space is too high or low.</p>&#13;
<p>A <em>Kubernetes controller</em> implements a similar <a href="https://oreil.ly/LKefh">control loop</a>, consisting of the following steps:</p>&#13;
<ol class="less_space pagebreak-before">&#13;
<li><p>Reading the current state of resources</p></li>&#13;
<li><p>Making changes to the state of resources</p></li>&#13;
<li><p>Updating the status of resources</p></li>&#13;
<li><p>Repeat</p></li>&#13;
</ol>&#13;
<p>These steps are embodied both by Kubernetes built-in controllers that run in the controller manager and Cloud Controller Manager, as well as <em>custom controllers</em> that are provided to run applications on top of Kubernetes. Let’s look at some examples of what these steps might entail for controllers that manage data infrastructure:</p>&#13;
<dl>&#13;
<dt>Reading the current state of resources</dt>&#13;
<dd><p><a contenteditable="false" data-primary="resources" data-secondary="reading current state of" data-type="indexterm" id="idm46183197551312"/><a contenteditable="false" data-primary="watch events" data-type="indexterm" id="idm46183197354176"/>A controller tracks the state of one or more resource types, including built-in resources like Pods, PersistentVolumes, and Services, as well as custom resources (which we discuss in the next section). Controllers are driven asynchronously by notification from the API server. The API server sends <a href="https://oreil.ly/UvOJY"><em>watch events</em></a> to controllers to notify them of changes in state for resource types for which they have registered interest, such as the creation or deletion of a resource, or an event occurring on the resource.</p>&#13;
<p>For data infrastructure, these changes could include a change in the number of requested replicas for a cluster, or a notification that a Pod containing a database replica has died. Because many such updates could be occurring in a large cluster, controllers frequently use caching.</p></dd>&#13;
<dt>Making changes to the state of resources</dt>&#13;
<dd><p><a contenteditable="false" data-primary="resources" data-secondary="making changes to" data-type="indexterm" id="idm46183197421840"/>This is the core business logic of a controller—comparing the state of resources to their desired state and executing actions to change the state to the desired state. In the Kubernetes API, the current state is captured in <code>.status</code> fields of resources, and the desired state is expressed in terms of the <code>.spec</code> field. The changes could include invocations of the Kubernetes API to modify other resources, administrative actions on the application being managed, or even interactions outside of the Kubernetes cluster.</p>&#13;
<p>For example, consider a controller managing a distributed database with multiple replicas. When the database controller receives a notification that the desired number of replicas has increased, the controller could scale an underlying Deployment or StatefulSet that it is using to manage replicas. Later, when receiving a notification that a Pod has been created to host a new replica, the controller could initiate an action on one or more replicas in order to rebalance the workload across those replicas.</p></dd>&#13;
<dt>Updating the status of resources</dt>&#13;
<dd><a contenteditable="false" data-primary="resources" data-secondary="updating status of" data-type="indexterm" id="idm46183197389920"/><a contenteditable="false" data-primary="kubectl describe command" data-type="indexterm" id="idm46183197390256"/><a contenteditable="false" data-primary="kubectl get command" data-type="indexterm" id="idm46183197401552"/>In the final step of the control loop, the controller updates the <code>.status</code> fields of the resource using the API server, which in turn updates that state in etcd. You’ve viewed the status of resources like Pods and PersistentVolumes in previous chapters using the <code>kubectl get</code> and <code>kubectl describe</code> commands. For example, the status of a Pod includes its overall state (<code>Pending</code>, <code>Running</code>, <code>Succeeded</code>, <code>Failed</code>, etc.), the most recent time at which various conditions were noted (<code>PodScheduled</code>, <code>ContainersReady</code>, <code>Initialized</code>, <code>Ready</code>), as well as the state of each of its containers (<code>Waiting</code>, <code>Running</code>, <code>Terminated</code>). Custom resources can define their own status fields as well. For example, a custom resource representing a cluster might have status values reflecting the overall availability of the cluster and its current topology.</dd>&#13;
</dl>&#13;
<section data-pdf-bookmark="Events" data-type="sect3"><div class="sect3" id="events">&#13;
<h3>Events</h3>&#13;
<p><a contenteditable="false" data-primary="kubectl describe pod command" data-type="indexterm" id="idm46183198172384"/><a contenteditable="false" data-primary="events, controllers and" data-type="indexterm" id="idm46183197336800"/><a contenteditable="false" data-primary="controllers" data-secondary="events and" data-type="indexterm" id="idm46183197335824"/>A controller can also produce <em>events</em> via the Kubernetes API for consumption by human operators or other applications. These are distinct from the watcher events described previously that the Kubernetes API uses to notify controllers of changes, which are not exposed to other clients.</p> &#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Writing a Custom Controller</h1>&#13;
<p><a contenteditable="false" data-primary="controllers" data-secondary="writing custom" data-type="indexterm" id="idm46183198741408"/><a contenteditable="false" data-primary="Programming Kubernetes (Hausenblas and Schimanski)" data-type="indexterm" id="idm46183198741760"/><a contenteditable="false" data-primary="Hausenblas, Michael, Programming Kubernetes" data-type="indexterm" id="idm46183197329360"/><a contenteditable="false" data-primary="Schimanski, Stefan, Programming Kubernetes" data-type="indexterm" id="idm46183197328288"/><a contenteditable="false" data-primary="controller-runtime project" data-type="indexterm" id="idm46183197327216"/><a contenteditable="false" data-primary="Go" data-type="indexterm" id="idm46183197326144"/><a contenteditable="false" data-primary="Google" data-type="indexterm" id="idm46183197325040"/><a contenteditable="false" data-primary="Borg" data-type="indexterm" id="idm46183197323936"/>While you may not ever need to write your own controller, being familiar with the concepts involved is helpful. <a class="orm:hideurl" href="https://oreil.ly/Ad4Ga"><em>Programming Kubernetes</em></a> is a great resource for those interested in digging deeper.</p>&#13;
<p>The <a href="https://oreil.ly/VjP9w">controller-runtime project</a> provides a common set of libraries to help aid the process of writing controllers, including registering for notifications from the API server, caching resource status, implementing reconciliation loops, and more. Controller-runtime libraries are implemented in the Go programming language, so it’s no surprise that most controllers are implemented in Go.</p>&#13;
<p><a href="https://go.dev">Go</a> was first developed at Google in 2007 and used in many cloud native applications including Borg, the predecessor to Kubernetes, and then in Kubernetes itself. Go is a strongly typed, compiled language (as opposed to interpreted languages like Java and JavaScript) with a high value on usability and developer productivity (in reaction to the higher learning curve of C/C++).</p>&#13;
</div>&#13;
&#13;
<p>If you’ve ever misconfigured a Pod specification and observed a <code>CrashLoopBackOff</code> status, you may have encountered events. Using the <code>kubectl describe pod</code> command, you can observe events such as a container being started and failing, followed by a backoff period, followed by the container restarting. Events expire from the API server in an hour, but common Kubernetes monitoring tools provide capabilities to track them. Controllers can also create events for custom resources.</p>&#13;
&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Custom Resources" data-type="sect2"><div class="sect2" id="custom_resources">&#13;
<h2>Custom Resources</h2>&#13;
<p><a contenteditable="false" data-primary="custom resources" data-type="indexterm" id="cr_ab"/><a contenteditable="false" data-primary="operator pattern" data-secondary="custom resources" data-type="indexterm" id="op_cr"/><a contenteditable="false" data-primary="resources" data-secondary="custom" data-type="indexterm" id="re_cu"/>As we’ve discussed, controllers can operate on built-in Kubernetes resources as well as <a href="https://oreil.ly/62uQj">custom resources</a>. We’ve briefly mentioned this concept, but let’s take this opportunity to define what custom resources are and how they extend the Kubernetes API.</p>&#13;
<p><a contenteditable="false" data-primary="CRD (Custom Resource Definition)" data-type="indexterm" id="idm46183197357088"/>Fundamentally, a <em>custom resource</em> is a piece of configuration data that Kubernetes recognizes as part of its API. While a custom resource is similar to a ConfigMap, it has a structure similar to built-in resources: metadata, specification, and status. The specific attributes of a particular custom resource type are defined in a CRD. A CRD is itself a Kubernetes resource that is used to describe a custom resource.</p>&#13;
<p>In this book, we’ve been discussing how Kubernetes enables you to move beyond managing VMs and containers to managing virtual datacenters. CRDs provide the flexibility that helps make this a practical reality. Instead of being limited to the resources that Kubernetes provides off the shelf, you can create additional abstractions to extend Kubernetes for your own purposes. This is a critical component in a fast-moving ecosystem.</p>&#13;
<p>Let’s see what you can learn about CRDs from the command line. Use <code>kubectl api-resources</code> to get a listing of all of the resources defined in your cluster:</p>&#13;
<pre data-type="programlisting"><strong>kubectl api-resources</strong>&#13;
NAME               SHORTNAMES  APIVERSION  NAMESPACED  KIND&#13;
bindings                       v1          true        Binding&#13;
componentstatuses  cs          v1          false       ComponentStatus&#13;
configmaps         cm          v1          true        ConfigMap&#13;
...</pre>&#13;
<p>As you look through the output, you’ll see many resource types introduced in previous chapters, along with their short names: StorageClass (<code>sc</code>), PersistentVolumes (<code>pv</code>), Pods (<code>po</code>), StatefulSets (<code>sts</code>), and so on. The API versions provide some clues as to the origins of each resource type. For example, resources with version <code>v1</code> are core Kubernetes resources. Other versions such as <code>apps/v1</code>, <code>networking.k8s.io/v1</code>, or <code>storage.k8s.io/v1</code> indicate resources that are defined by various Kubernetes SIGs.</p>&#13;
<p><a contenteditable="false" data-primary="kubectl apt-resources command" data-type="indexterm" id="idm46183197291808"/>Depending on the configuration of the Kubernetes cluster you are using, you may have some CRDs defined already. If any are present, they will appear in the output of the <code>kubectl api-resources</code> command. They’ll stand out by their API version, which will typically include a path other than <code>k8s.io</code>.</p>&#13;
<p><a contenteditable="false" data-primary="kubectl get crd command" data-type="indexterm" id="idm46183197288208"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="about" data-type="indexterm" id="idm46183197287568"/>Since a CRD is itself a Kubernetes resource, you can also use the command <code>kubectl get crd</code> to list custom resources installed in your Kubernetes cluster. For example, after installing the Vitess Operator referenced in the following section, you would see several CRDs:</p>&#13;
<pre data-type="programlisting"><strong>kubectl get crd</strong>&#13;
NAME                                   CREATED AT&#13;
etcdlockservers.planetscale.com        2021-11-21T22:06:04Z&#13;
vitessbackups.planetscale.com          2021-11-21T22:06:04Z&#13;
vitessbackupstorages.planetscale.com   2021-11-21T22:06:04Z&#13;
vitesscells.planetscale.com            2021-11-21T22:06:04Z&#13;
vitessclusters.planetscale.com         2021-11-21T22:06:04Z&#13;
vitesskeyspaces.planetscale.com        2021-11-21T22:06:04Z&#13;
vitessshards.planetscale.com           2021-11-21T22:06:04Z</pre>&#13;
<p><a contenteditable="false" data-primary="kubectl describe crd command" data-type="indexterm" id="idm46183197289920"/>We’ll introduce the usage of these custom resources later, but for now let’s focus on the mechanics of a specific CRD to see how it extends Kubernetes. You use the <code>kubectl describe crd</code> or <code>kubectl get crd</code> commands to see the definition of a CRD. For example, to get a YAML-formatted description for the <code>vitesskeyspace</code> custom resource, you could run this:</p>&#13;
<pre data-type="programlisting"><strong>kubectl get crd vitesskeyspaces.planetscale.com -o yaml</strong>&#13;
<strong>...</strong></pre>&#13;
<p>Looking at the <a href="https://oreil.ly/5ml3q">original YAML configuration</a> for this CRD, you’ll see something like this:</p>&#13;
<pre data-type="programlisting">apiVersion: apiextensions.k8s.io/v1beta1&#13;
kind: CustomResourceDefinition&#13;
metadata:&#13;
  annotations:&#13;
    controller-gen.kubebuilder.io/version: v0.3.0&#13;
  creationTimestamp: null&#13;
  name: vitesskeyspaces.planetscale.com&#13;
spec:&#13;
  group: planetscale.com&#13;
  names:&#13;
    kind: VitessKeyspace&#13;
    listKind: VitessKeyspaceList&#13;
    plural: vitesskeyspaces&#13;
    shortNames:&#13;
    - vtk&#13;
    singular: vitesskeyspace&#13;
  scope: Namespaced&#13;
  subresources:&#13;
    status: {}&#13;
  validation:&#13;
    openAPIV3Schema:&#13;
      properties:&#13;
        ...</pre>&#13;
<p>From this part of the definition, you can see the declaration of the custom resource’s name or kind and <code>shortName</code>. The <code>scope</code> designation of <code>Namespaced</code> means that custom resources of this type are confined to a single Namespace.</p>&#13;
<p><a contenteditable="false" data-primary="OpenAPI v3 schema" data-type="indexterm" id="idm46183197275808"/><a contenteditable="false" data-primary="JSON schema" data-type="indexterm" id="idm46183197275168"/>The longest part of the definition is the <code>validation</code> section, which we’ve omitted due to its considerable size. Kubernetes supports the definition of attributes within custom resource types, and the ability to define legal values for these types using the <a href="https://oreil.ly/b13qP">OpenAPI v3 schema</a> (which is used to document RESTful APIs, which in turn uses <a href="http://json-schema.org">JSON schema</a> to describe rules used to validate JSON objects). Validation rules ensure that when you create or update custom resources, the definitions of the objects are valid and can be understood by the Kubernetes control plane. The validation rules are used to generate the documentation you use as you define instances of these custom resources in your application.</p>&#13;
<p><a contenteditable="false" data-primary="kubectl get vitesskeyspaces command" data-type="indexterm" id="idm46183197313024"/>Once a CRD has been installed in your Kubernetes cluster, you can create and interact with the resources using <code>kubectl</code>. For example, <code>kubectl get vitesskeyspaces</code> will return a list of Vitess keyspaces. You create an instance of a Vitess keyspace by providing a compliant YAML definition to the <code>kubectl apply</code> command<a contenteditable="false" data-primary="" data-startref="cr_ab" data-type="indexterm" id="idm46183197301312"/><a contenteditable="false" data-primary="" data-startref="op_cr" data-type="indexterm" id="idm46183197272256"/><a contenteditable="false" data-primary="" data-startref="re_cu" data-type="indexterm" id="idm46183197271040"/>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Operators" data-type="sect2"><div class="sect2" id="operators">&#13;
<h2>Operators</h2>&#13;
<p><a contenteditable="false" data-primary="controllers" data-secondary="operators vs." data-type="indexterm" id="idm46183197268160"/><a contenteditable="false" data-primary="operator pattern" data-secondary="operators" data-type="indexterm" id="idm46183197266784"/><a contenteditable="false" data-primary="operators" data-secondary="about" data-type="indexterm" id="idm46183197265408"/><a contenteditable="false" data-primary="operators" data-secondary="controllers vs." data-type="indexterm" id="idm46183197264032"/>Now that you’ve learned about custom controllers and custom resources, let’s tie these threads back together. An <a href="https://oreil.ly/BXc35"><em>operator</em></a> is a combination of custom resources and custom controllers that maintain the state of those resources and manage an application (or <em>operand)</em> in Kubernetes.</p>&#13;
<p>As we’ll see in examples throughout the rest of the book, this simple definition can cover a pretty wide range of implementations. The recommended pattern is to provide a custom controller for each custom resource, but beyond that, the details vary. A simple operator might consist of a single resource and controller, while a more complex operator might have multiple resources and controllers. Those multiple controllers might run in the same process space or be broken into separate Pods.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Controllers Versus Operators</h1>&#13;
<p>While technically operators and controllers are distinct concepts in Kubernetes, the terms are frequently used interchangeably. It’s common to refer to a deployed controller or collection of controllers as “the operator,” and you’ll see this usage reflected both in this book and the community in general.</p>&#13;
</div>&#13;
<p>To unpack this pattern and see how the different elements of an operator and the Kubernetes control plane work together, let’s consider the interactions of a notional operator, the DBCluster operator, as shown in <a data-type="xref" href="#interaction_between_kubernetes_controll">Figure 5-2</a>.</p>&#13;
&#13;
<p>After an administrator installs the DBCluster operator and <code>db-cluster</code> custom resource in the cluster, users can then create instances of the <code>db-cluster</code> resource using <code>kubectl</code> (1), which registers the resource with the API server (2), which in turns stores the state in etcd (3) to ensure high availability (other interactions with etcd are omitted from this sequence for brevity).</p>&#13;
&#13;
<figure><div class="figure" id="interaction_between_kubernetes_controll">&#13;
<img alt="Interaction between Kubernetes controllers and operators" src="assets/mcdk_0502.png"/>&#13;
<h6><span class="label">Figure 5-2. </span>Interaction between Kubernetes controllers and operators</h6>&#13;
</div></figure>&#13;
&#13;
<p><a contenteditable="false" data-primary="DBCluster operator" data-type="indexterm" id="idm46183197496672"/>The DBCluster controller (part of the operator) is notified of the new <code>db-cluster</code> resource (4) and creates additional Kubernetes resources using the API server (5), which could include StatefulSets, Services, PersistentVolumes, PersistentVolumeClaims, and more, as we’ve seen in previous examples of deploying databases on Kubernetes.</p>&#13;
<p>Focusing on the StatefulSet path, the StatefulSet controller running as part of the Kubernetes controller manager is notified of a new StatefulSet (6) and creates new Pod resources (7). The API server asks the scheduler to assign each Pod to a Worker Node (8) and communicates with the Kubelet on the chosen Worker Nodes (9) to start each of the required Pods (10).</p>&#13;
<p>As you see, creating a <code>db-cluster</code> resource sets off a chain of interactions as various controllers are notified of changes to Kubernetes resources and initiate changes to bring the state of the cluster in line with the desired state. The sequence of interactions appears complex from a user perspective, but the design demonstrates strong encapsulation: the responsibilities of each controller are well bounded and independent of other controllers. This separation of concerns is what makes the Kubernetes control plane so extensible.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Managing MySQL in Kubernetes Using the Vitess Operator" data-type="sect1"><div class="sect1" id="managing_mysql_in_kubernetes_using_the">&#13;
<h1>Managing MySQL in Kubernetes Using the Vitess Operator</h1>&#13;
<p><a contenteditable="false" data-primary="MySQL" data-secondary="managing using Vitess Operator" data-type="indexterm" id="mysql_vit"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="managing MySQL using" data-type="indexterm" id="vit_mysql"/>Now that you understand how operators, custom controllers, and custom resources work, it’s time to get some hands-on experience with an operator for the database we’ve been using as our primary relational database example: MySQL. MySQL examples in previous chapters were confined to simple deployments of a single primary replica and a couple of secondary replicas. While this could provide a sufficient amount of storage for many cloud applications, managing a larger cluster can quickly become quite complex, whether it runs on bare-metal servers or as a containerized application in Kubernetes.</p>&#13;
<section data-pdf-bookmark="Vitess Overview" data-type="sect2"><div class="sect2" id="vitess_overview">&#13;
<h2>Vitess Overview</h2>&#13;
<p><a contenteditable="false" data-primary="Google" data-type="indexterm" id="idm46183197560672"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="about" data-type="indexterm" id="idm46183197555904"/><a href="https://oreil.ly/7I0vO"><em>Vitess</em></a> is an open source project started at YouTube in 2010. Before the company was acquired by Google, YouTube was running on MySQL, and as YouTube scaled up, it reached a point of daily outages. Vitess was created as a layer to abstract application access to databases by making multiple instances appear to be a single database, routing application requests to the appropriate instances using a sharding approach. Before we explore deploying Vitess on Kubernetes, let’s take some time to explore its architecture. We’ll start with the high-level concepts shown in <a data-type="xref" href="#vitess_cluster_topology_cellscomma_keys">Figure 5-3</a>: cells, keyspaces, shards, and primary and replica tablets.</p>&#13;
<figure><div class="figure" id="vitess_cluster_topology_cellscomma_keys">&#13;
<img alt="Vitess cluster topology: cells, keyspaces, and shards" src="assets/mcdk_0503.png"/>&#13;
<h6><span class="label">Figure 5-3. </span>Vitess cluster topology: cells, keyspaces, and shards</h6>&#13;
</div></figure>&#13;
<p>At a high level, a Vitess cluster consists of multiple MySQL instances called <em>tablets</em> which may be spread across multiple datacenters, or <em>cells</em>. Each MySQL instance takes on a role as either a primary or replica, and may be dedicated to a specific slice of a database known as a <em>shard</em>. Let’s consider the implications of each of these concepts for reading and writing data in Vitess:</p>&#13;
<dl>&#13;
<dt>Cell</dt>&#13;
<dd><a contenteditable="false" data-primary="replicas" data-type="indexterm" id="idm46183197973472"/><a contenteditable="false" data-primary="cells, in Vitess Operator" data-type="indexterm" id="idm46183197230272"/>A typical production deployment of Vitess is spread across multiple failure domains in order to provide high availability. Vitess refers to each of these failure domains as a <a href="https://oreil.ly/2VDke"><em>cell</em></a>. The recommended topology is a cell per datacenter or cloud provider zone. While writes and replication involve communication across cell boundaries, Vitess reads are confined to the local cell to optimize performance.</dd>&#13;
<dt>Keyspace</dt>&#13;
<dd><a contenteditable="false" data-primary="keyspace, in Vitess Operator" data-type="indexterm" id="idm46183197338880"/><a contenteditable="false" data-primary="sharded keyspace" data-type="indexterm" id="idm46183197339232"/><a contenteditable="false" data-primary="unsharded keyspace" data-type="indexterm" id="idm46183197225888"/>This is a logical database consisting of one or more tables. Each keyspace in a cluster can be <em>sharded</em> or <em>unsharded</em>. An unsharded keyspace has a primary cell where a MySQL instance designated as the <em>primary</em> will reside, while other cells will contain <em>replicas</em>. In the unsharded keyspace shown on the left side of <a data-type="xref" href="#vitess_cluster_topology_cellscomma_keys">Figure 5-3</a>, writes from client applications are routed to the primary and replicated to the replica nodes in the background. Reads can be served from the primary or replica nodes.</dd>&#13;
<dt>Shard</dt>&#13;
<dd><p><a contenteditable="false" data-primary="shard, in Vitess Operator" data-type="indexterm" id="idm46183197228992"/>The real power of Vitess comes from its ability to scale by spreading the contents of a keyspace across multiple replicated MySQL databases known as <em>shards</em>, while providing the abstraction of a single database to client applications. The client on the right side of <a data-type="xref" href="#vitess_cluster_topology_cellscomma_keys">Figure 5-3</a> is not aware of how data is sharded. On writes, Vitess determines what shards are involved and then routes the data to the appropriate primary instances. On reads, Vitess gathers data from primary or replica nodes in the local cell.</p>&#13;
<p><a contenteditable="false" data-primary="VSchema (Vitess Schema)" data-type="indexterm" id="idm46183197220960"/><a contenteditable="false" data-primary="VIndex" data-type="indexterm" id="idm46183197377792"/>The sharding rules for a keyspace are specified in a <a href="https://oreil.ly/wDmQa">Vitess Schema (VSchema)</a>, an object that contains the sharding key (known in Vitess as the <em>keyspace ID</em>) used for each table. To provide maximum flexibility over the way data is sharded, Vitess allows you to specify which columns in a table are used to calculate the keyspace ID, as well as the algorithm (or <em>VIndex</em>) used to make the calculation. Tables can also have secondary VIndexes to support more-efficient queries across multiple keyspace IDs.</p></dd>&#13;
</dl>&#13;
<p>To understand how Vitess manages shards and how it routes queries to the various MySQL instances, you’ll want to get to know the components of a Vitess cluster shown in <a data-type="xref" href="#vitess_architecture_including_vtgatecom">Figure 5-4</a>, including VTGate, VTTablet, and the Topology Service.</p>&#13;
<figure><div class="figure" id="vitess_architecture_including_vtgatecom">&#13;
<img alt="Vitess architecture including VTGate, VTTablets, Topology Service" src="assets/mcdk_0504.png"/>&#13;
<h6><span class="label">Figure 5-4. </span>Vitess architecture including VTGate, VTTablets, and the Topology Service</h6>&#13;
</div></figure>&#13;
<p>Let’s walk through these components to learn what they do and how they interact:</p>&#13;
<dl>&#13;
<dt>VTGate</dt>&#13;
<dd><a contenteditable="false" data-primary="VTGate (Vitess gateway)" data-type="indexterm" id="idm46183197208224"/>A Vitess gateway (VTGate) is a proxy server that provides the SQL binary endpoint used by client applications, making the Vitess cluster appear as a single database. Vitess clients generally connect to a VTGate running in the same cell (datacenter). The VTGate parses each incoming read or write query and uses its knowledge of the VSchema and cluster topology to create a query execution plan. The VTGate executes queries for each shard, assembles the result set, and returns it to the client. The VTGate can detect and limit queries that will impact memory or CPU utilization, providing high reliability and helping to ensure consistent performance. Although VTGate instances do cache cluster metadata, they are stateless, so you can increase the reliability and scalability of your cluster by running multiple VTGate instances per cell.</dd>&#13;
<dt>VTTablet</dt>&#13;
<dd><a contenteditable="false" data-primary="replicas" data-type="indexterm" id="idm46183197592480"/><a contenteditable="false" data-primary="VTTablet (Vitess tablet)" data-type="indexterm" id="idm46183197204832"/>A Vitess tablet (VTTablet) is an agent that runs on the same compute instance as a single MySQL database, managing access to it and monitoring its health. Each VTTablet takes on a specific role in the cluster, such as the primary for a shard, or one of its replicas. There are two types of replica: those that can be promoted to replace a primary and those that cannot. The latter are typically used to provide additional capacity for read-intensive use cases such as analytics. The VTTablet exposes a gRPC interface, which the VTGate uses to send queries and control commands that the VTTablet then turns into SQL commands on the MySQL instance. VTTablets maintain a pool of long-lived connections to the MySQL node, leading to improved throughput, reduced latency, and reduced memory pressure.</dd>&#13;
<dt>Topology Service</dt>&#13;
<dd><a contenteditable="false" data-primary="Topology Service" data-type="indexterm" id="idm46183197191920"/>Vitess requires a strongly consistent data store to maintain a small amount of metadata describing the cluster topology, including the definition of keyspaces and their VSchema, what VTTablets exist for each shard, and which VTTablet is the primary. Vitess uses a pluggable interface called the Topology Service, with three implementations provided by the project: etcd (the default), ZooKeeper, and Consul.  VTGates and VTTablets interface with the Topology Service in the background in order to maintain awareness of the topology, and do not interact with the Topology Service on the query path to avoid performance impact. For multicell clusters, Vitess incorporates both cell-local Topology Services and a global Topology Service with instances in multiple cells that maintains knowledge of the entire cluster. This design provides high availability of topology information across the cluster.</dd>&#13;
<dt><code>vtctld</code> and <code>vtctlclient</code></dt>&#13;
<dd><a contenteditable="false" data-primary="vtctlclient" data-type="indexterm" id="idm46183197812656"/><a contenteditable="false" data-primary="vtctld" data-type="indexterm" id="idm46183197813008"/>The Vitess control daemon <code>vtctld</code> and its client, <code>vtctlclient</code>, provide the control plane used to configure and manage Vitess clusters. <code>vtctld</code> is deployed on one or more of the cells in the cluster, while <code>vtctlclient</code> is deployed on the client machine of the user administering the cluster. <code>vtctld</code> uses a declarative approach similar to Kubernetes to perform its work: it updates the cluster metadata in the Topology Service, and the VTGates and VTTablets pick up changes and respond accordingly.</dd>&#13;
</dl>&#13;
<p>Now that you understand the Vitess architecture and basic concepts, let’s discuss how they are mapped into a Kubernetes environment. This is an important consideration for any application, but especially for a complex piece of data infrastructure like Vitess.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="PlanetScale Vitess Operator" data-type="sect2"><div class="sect2" id="planetscale_vitess_operator">&#13;
<h2>PlanetScale Vitess Operator</h2>&#13;
<p><a contenteditable="false" data-primary="PlanetScale Vitess Operator" data-type="indexterm" id="psc_ab"/><a contenteditable="false" data-primary="Percona" data-type="indexterm" id="idm46183197446960"/>Over time, Vitess has evolved in a couple of key aspects. First, it can now run additional MySQL-compatible database engines such as Percona. Second, and more important for our investigations, PlanetScale has packaged Vitess as a containerized application that can be deployed to Kubernetes.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Evolving Options For Running Vitess in Kubernetes</h1>&#13;
<p>The state of the art for running Vitess in Kubernetes has evolved over time. While Vitess once included a Helm chart, this was <a href="https://oreil.ly/xhUt4">deprecated in the 7.0 release</a> in mid-2020. The Vitess project also hosted an operator which was <a href="https://oreil.ly/4RPMj">deprecated</a> around the same time. Both of these options were retired in favor of the PlanetScale operator we examine in this section.</p>&#13;
</div>&#13;
<p>Let’s see how easy it is to deploy a multinode MySQL cluster using the <a href="https://oreil.ly/W5Dc2">PlanetScale Vitess Operator</a>. Since the Vitess project has adopted the PlanetScale Vitess Operator as its officially supported operator, you can reference the <a href="https://oreil.ly/Nl7e2">Get Started guide</a> in the Vitess project documentation. We’ll walk through a portion of this guide here to get an understanding of the operator’s contents and how it works.</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Examples Require Kubernetes Clusters with More Resources</h1>&#13;
<p>The examples in previous chapters have not required a large amount of compute resources, and we encouraged you to run them on local distributions such as kind or K3s. Beginning in this chapter, the examples become more complex and may require more resources than you have available on your desktop or laptop. For these cases, we will provide references to documentation or scripts for creating Kubernetes clusters with sufficient resources.</p>&#13;
</div>&#13;
<section data-pdf-bookmark="Installing the Vitess Operator" data-type="sect3"><div class="sect3" id="installing_the_vitess_operator">&#13;
<h3>Installing the Vitess Operator</h3>&#13;
<p><a contenteditable="false" data-primary="installing" data-secondary="Vitess Operator" data-type="indexterm" id="idm46183197397488"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="installing" data-type="indexterm" id="idm46183197396048"/>You can find the source code used in this section in <a href="https://github.com/data-on-k8s-book/examples">this book’s code repository</a>. The files are copied for convenience from their original source in the <a href="https://oreil.ly/Kq7dm">Vitess GitHub repo</a>. First, install the operator using the provided configuration file:</p>&#13;
<pre data-type="programlisting">&#13;
<strong>set GH_LINK=https://raw.githubusercontent.com&#13;
kubectl apply -f \&#13;
  $GH_LINK/vitessio/vitess/main/examples/operator/operator.yaml</strong>&#13;
customresourcedefinition.apiextensions.k8s.io/&#13;
  etcdlockservers.planetscale.com created&#13;
...</pre>&#13;
<p>As you’ll see in the output of the <code>kubectl apply</code> command, this configuration creates several CRDs, as well as a Deployment managing a single instance of the operator. <a data-type="xref" href="#vitess_operator_and_custom_resource_def">Figure 5-5</a> shows many of the elements you’ve just installed, in order to highlight a few interesting details that will not be obvious at first glance:</p>&#13;
<ul class="pagebreak-before">&#13;
<li><p>The operator contains a controller corresponding to each CRD. If you’re interested in seeing what this looks like in the operator source code in Go, compare the <a href="https://oreil.ly/ABID9">controller implementations</a> with the <a href="https://oreil.ly/tUD9z">custom resource specifications</a> that are used to generate the CRD configurations introduced in <a data-type="xref" href="#building_operators">“Building Operators”</a>.</p></li>&#13;
<li><p>The figure depicts a hierarchy of CRDs representing their relationships and intended usage, as described in the operator’s <a href="https://oreil.ly/25qhN">API reference</a>. To use the Vitess Operator, you define a VitessCluster resource which contains the definitions of VitessCells and VitessKeyspaces. VitessKeyspaces, in turn, contain definitions of VitessShards. While you can view the status of each VitessCell, VitessKeyspace, and VitessShard independently, you must update them in the context of the parent VitessCluster resource.</p></li>&#13;
<li><p>Currently, the Vitess Operator supports only etcd as the Topology Service implementation. The EtcdLockserver CRD is used to configure these etcd clusters.</p></li>&#13;
</ul>&#13;
<figure><div class="figure" id="vitess_operator_and_custom_resource_def">&#13;
<img alt="Vitess Operator and Custom Resource Definitions" src="assets/mcdk_0505.png"/>&#13;
<h6><span class="label">Figure 5-5. </span>Vitess Operator and Custom Resource Definitions</h6>&#13;
</div></figure>&#13;
<section data-pdf-bookmark="Roles and RoleBindings" data-type="sect4"><div class="sect4" id="roles_and_rolebindings">&#13;
<h4>Roles and RoleBindings</h4>&#13;
<p><a contenteditable="false" data-primary="RoleBinding (Vitess Operator)" data-type="indexterm" id="idm46183197141760"/><a contenteditable="false" data-primary="Roles (Vitess Operator)" data-type="indexterm" id="idm46183197142112"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="Roles" data-type="indexterm" id="idm46183197139744"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="RoleBinding" data-type="indexterm" id="idm46183197138368"/>As shown toward the bottom of <a data-type="xref" href="#vitess_operator_and_custom_resource_def">Figure 5-5</a>, installing the operator caused the creation of a ServiceAccount, along with two new resources we have not discussed previously: a Role and a RoleBinding. These additional resources allow the ServiceAccount to access specific resources on the Kubernetes API. First, examine the configuration of the <code>vitess-operator</code> Role from the file that you used to <a href="https://oreil.ly/q52Iq">install the operator</a> (you can search for <code>kind: Role</code> to locate the pertinent code):</p>&#13;
<pre data-type="programlisting">apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: Role&#13;
metadata:&#13;
  name: vitess-operator&#13;
rules:&#13;
- apiGroups:&#13;
  - ""&#13;
  resources:&#13;
  - pods&#13;
  - services&#13;
  - endpoints&#13;
  - persistentvolumeclaims&#13;
  - events&#13;
  - configmaps&#13;
  - secrets&#13;
  verbs:&#13;
  - '*'&#13;
...</pre>&#13;
<p>This first portion of the Role definition identifies resources that are part of the core Kubernetes distribution, which may be designated by passing the empty string as the <code>apiGroup</code> instead of <code>k8s.io</code>. The <code>verbs</code> correspond to operations the Kubernetes API provides on resources, including <code>get</code>, <code>list</code>, <code>watch</code>, <code>create</code>, <code>update</code>, <code>patch</code>, and <code>delete</code>. This Role is given access to all operations using the wildcard <code>*</code>. If you follow the URL in the example and examine more of the code, you’ll also see how the Role is given access to other resources, including Deployments and ReplicaSets, and resources in the <code>apiGroup planetscale.com</code>.</p>&#13;
<p>The RoleBinding associates the ServiceAccount with the Role:</p>&#13;
<pre data-type="programlisting">apiVersion: rbac.authorization.k8s.io/v1&#13;
kind: RoleBinding&#13;
metadata:&#13;
  name: vitess-operator&#13;
roleRef:&#13;
  apiGroup: rbac.authorization.k8s.io&#13;
  kind: Role&#13;
  name: vitess-operator&#13;
subjects:&#13;
- kind: ServiceAccount&#13;
  name: vitess-operator</pre>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Least Privilege for Operators</h1>&#13;
<p><a contenteditable="false" data-primary="least privilege, for operators" data-type="indexterm" id="idm46183197251424"/><a contenteditable="false" data-primary="operators" data-secondary="least privilege for" data-type="indexterm" id="idm46183197250288"/>As a creator or consumer of operators, exercise care in choosing which permissions are granted to operators, and be conscious of the implications for what an operator is allowed to do.</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="PriorityClasses" data-type="sect4"><div class="sect4" id="priorityclasses">&#13;
<h4>PriorityClasses</h4>&#13;
<p><a contenteditable="false" data-primary="PriorityClasses (Vitess Operator)" data-type="indexterm" id="idm46183197244176"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="PriorityClasses" data-type="indexterm" id="idm46183197244352"/>Another detail is not depicted in <a data-type="xref" href="#vitess_architecture_including_vtgatecom">Figure 5-4</a>: installing the operator created two PriorityClass resources. <a href="https://oreil.ly/dlkRe">PriorityClasses</a> provide input to the Kubernetes scheduler to indicate the relative priority of Pods. The priority is an integer value, where higher values indicate higher priority. Whenever a Pod resource is created and is ready to be assigned to a Worker Node, the Scheduler takes the Pod’s priority into account as part of its decisions. When multiple Pods are awaiting scheduling, higher-priority Pods are assigned before lower-priority Pods. When a cluster’s nodes are running low on compute resources, lower-priority Pods may be stopped or <em>evicted</em> in order to make room for higher-priority Pods, a process known as <em>preemption</em>.</p>&#13;
<p>A PriorityClass is a convenient way to set a priority value referenced by multiple Pods or other workload resources such as  Deployments and StatefulSets. The Vitess Operator creates two PriorityClasses: <code>vitess-operator-control-plane</code> defines a higher priority used for the operator and <code>vtctld</code> Deployments, while the <code>vitess</code> class is used for the data plane components such as the VTGate and VTTablet Deployments.</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Kubernetes Scheduling Complexity</h1>&#13;
<p><a contenteditable="false" data-primary="scheduling complexity" data-type="indexterm" id="idm46183197357984"/>Kubernetes provides multiple constraints that influence Pod scheduling, including prioritization and preemption, affinity and anti-affinity, and scheduler extensions, as discussed in <a data-type="xref" href="#extending_kubernetes_clients">“Extending Kubernetes Clients”</a>. The interaction of these constraints may not be predictable, especially in large clusters shared across multiple teams. As resources in a cluster become scarce, Pods can be preempted or fail to be scheduled in ways you don’t expect. It’s a best practice to maintain awareness of the various scheduling needs and constraints across the workloads in your cluster to avoid surprises.</p>&#13;
</div>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Creating a VitessCluster" data-type="sect3"><div class="sect3" id="creating_a_vitesscluster">&#13;
<h3>Creating a VitessCluster</h3>&#13;
<p><a contenteditable="false" data-primary="Vitess Operator" data-secondary="creating VitessCluster" data-type="indexterm" id="vo_vc"/><a contenteditable="false" data-primary="VitessCluster, creating" data-type="indexterm" id="vc_cr"/>Now let’s create a VitessCluster and put the operator to work. The code sample contains a configuration file defining a very simple cluster named <code>example</code>, with a VitessCell <code>zone1</code>, keyspace <code>commerce</code>, and single shard, which the operator gives the name <code>x-x</code>:</p>&#13;
<pre data-type="programlisting"><strong>kubectl apply -f 101_initial_cluster.yaml</strong>&#13;
vitesscluster.planetscale.com/example created&#13;
secret/example-cluster-config created</pre>&#13;
<p>The output of the command indicates a couple of items that are created directly. But more is going on behind the scenes, as the operator detects the creation of the VitessCluster and begins provisioning other resources, as summarized in <a data-type="xref" href="#resources_managed_by_the_vitesscluster">Figure 5-6</a>.</p>&#13;
<figure><div class="figure" id="resources_managed_by_the_vitesscluster">&#13;
<img alt="Resources managed by the VitessCluster example" src="assets/mcdk_0506.png"/>&#13;
<h6><span class="label">Figure 5-6. </span>Resources managed by the VitessCluster <code>example</code></h6>&#13;
</div></figure>&#13;
<p>By comparing the configuration script with <a data-type="xref" href="#resources_managed_by_the_vitesscluster">Figure 5-6</a>, you can make several observations about this simple VitessCluster. First, the top-level configuration allows you to specify the name of the cluster and the container images that will be used for the various components:</p>&#13;
<pre data-type="programlisting">apiVersion: planetscale.com/v2&#13;
kind: VitessCluster&#13;
metadata:&#13;
  name: example&#13;
spec:&#13;
  images:&#13;
    vtctld: vitess/lite:v12.0.0&#13;
    ...</pre>&#13;
<p>Next, the VitessCluster configuration provides a definition of the VitessCell <code>zone1</code>. The values provided for <code>gateway</code> specify a single VTGate instance to be allocated for this cell, with specific compute resource limits:</p>&#13;
<pre data-type="programlisting">  cells:&#13;
  - name: zone1&#13;
    gateway:&#13;
      authentication:&#13;
        static:&#13;
          secret:&#13;
            name: example-cluster-config&#13;
            key: users.json&#13;
      replicas: 1&#13;
      resources:&#13;
        ...</pre>&#13;
<p>The Vitess Operator uses this information to create a VTGate Deployment prefixed with <code>example-zone1-vtgate</code> containing a single replica, and a Service that provides access. The access credentials for the VTGate instance are provided in the <code>example-cluster-config</code> Secret. This Secret is used to secure other configuration values, as you’ll see.</p>&#13;
<p>The next section of the VitessCluster configuration specifies the creation of a single <code>vtctld</code> instance (a <em>dashboard</em>) with permission to control <code>zone1</code>. The Vitess Operator uses this information to create a Deployment to manage the dashboard using the specified resource limits, and a Service to provide access to the VTGate:</p>&#13;
<pre data-type="programlisting">  vitessDashboard:&#13;
    cells:&#13;
    - zone1&#13;
    extraFlags:&#13;
      security_policy: read-only&#13;
    replicas: 1&#13;
    resources:&#13;
      ...</pre>&#13;
<p>The VitessCluster also defines the <code>commerce</code> keyspace, which contains a single shard (essentially, an unsharded keyspace). This single shard has a pool of two VTTablets in the cell <code>zone1</code>, each of which will be allocated 10 GB of storage:</p>&#13;
<pre data-type="programlisting">  keyspaces:&#13;
  - name: commerce&#13;
    turndownPolicy: Immediate&#13;
    partitionings:&#13;
    - equal:&#13;
        parts: 1&#13;
        shardTemplate:&#13;
          databaseInitScriptSecret:&#13;
            name: example-cluster-config&#13;
            key: init_db.sql&#13;
          replication:&#13;
            enforceSemiSync: false&#13;
          tabletPools:&#13;
          - cell: zone1&#13;
            type: replica&#13;
            replicas: 2&#13;
            vttablet:&#13;
              ...&#13;
            mysqld:&#13;
              ...&#13;
            dataVolumeClaimTemplate:&#13;
              accessModes: ["ReadWriteOnce"]&#13;
              resources:&#13;
                requests:&#13;
                  storage: 10Gi</pre>&#13;
<p>As shown in <a data-type="xref" href="#resources_managed_by_the_vitesscluster">Figure 5-6</a>, the Vitess Operator manages a Pod for each VTTablet and creates a Service to manage access across the tablets. The operator does not use a StatefulSet because the VTTablets have distinct roles, with one as the primary and the other as a replica. Each VTTablet Pod contains multiple containers, including the <code>vttablet</code> sidecar which configures and controls the <code>mysql</code> container. The <code>vttablet</code> sidecar initializes the <code>mysql</code> instance using a script contained in the <code>example-cluster-config</code> Secret.</p>&#13;
<p>While this configuration doesn’t specifically include details about etcd, the Vitess Operator uses its default settings to create a three-node etcd cluster to serve as the Topology Service for the VitessCluster. Because of the shortcomings of the StatefulSets, the operator manages each Pod and PersistentVolumeClaim individually. This points to the possibility for future improvements as Kubernetes and the operator mature; perhaps the Kubernetes API server can one day serve the role of the Topology Service in the Vitess architecture.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="Tezuysal, Alkin, “Vitess Operator for Kubernetes” blog post" data-type="indexterm" id="idm46183197076304"/><a contenteditable="false" data-primary="“Vitess Operator for Kubernetes” blog post (Tezuysal)" data-primary-sortas="vitess" data-type="indexterm" id="idm46183197075504"/>At this point, you have a VitessCluster with all of its infrastructure provisioned in Kubernetes. The next steps are to create the database schema and configure your applications to access the cluster using the VTGate Service. You can follow the steps in Alkin Tezuysal’s 2020 blog post <a href="https://oreil.ly/543Y8">“Vitess Operator for Kubernetes”</a>, which also describes other use cases for managing a Vitess installation on Kubernetes, including schema migration, backup, and restore.</p>&#13;
<p>The backup/restore capabilities leverage VitessBackupStorage and VitessBackup CRDs, which you may have noticed during installation. VitessBackupStorage resources represent locations where backups can be stored. After you configure the backup section of a VitessCluster and point to a backup location, the operator creates VitessBackup resources as a record of each backup it performs. When you add additional replicas to a VitessCluster, the operator initializes their data by performing a restore from the most recent backup.</p>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Visualizing Larger Kubernetes Applications</h1>&#13;
<p><a contenteditable="false" data-primary="applications" data-secondary="visualizing larger Kubernetes" data-type="indexterm" id="idm46183197167792"/><a contenteditable="false" data-primary="Lens tool" data-type="indexterm" id="idm46183197168144"/><a contenteditable="false" data-primary="K9s" data-type="indexterm" id="idm46183197116928"/>It’s a good exercise to use the <code>kubectl get</code> and <code>kubectl</code> describe commands to explore all of the resources that were created when you installed the operator and created a cluster. However, you may find it easier to use a tool such as <a href="https://github.com/lensapp/lens">Lens</a>, which offers a friendly graphical interface enabling you to click through the resources more quickly, or <a href="https://k9scli.io">K9s</a>, which provides a  command-line interface.</p>&#13;
</div>&#13;
<p><a contenteditable="false" data-primary="resharding" data-type="indexterm" id="idm46183197083008"/><a contenteditable="false" data-primary="vtctlclient" data-type="indexterm" id="idm46183197167968"/><a contenteditable="false" data-primary="" data-startref="psc_ab" data-type="indexterm" id="idm46183197067328"/><a contenteditable="false" data-primary="" data-startref="vo_vc" data-type="indexterm" id="idm46183197065856"/><a contenteditable="false" data-primary="" data-startref="vc_cr" data-type="indexterm" id="idm46183197064480"/>Resharding is another interesting use case, which you might need to perform when a cluster becomes unbalanced and one or more shards run out of capacity more quickly than others. You’ll need to modify the VSchema using <code>vtctlclient</code>, and then <a href="https://oreil.ly/i0n5S">update the VitessCluster resource</a> with additional VitessShards so that the operator provisions the required infrastructure. This highlights the division of responsibility: the Vitess Operator manages Kubernetes resources, while the Vitess control daemon (<code>vtctld</code>) provides more application-specific behavior.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="what_we_learned_building_the_vitess_ope">&#13;
<h5>What We Learned Building the Vitess Operator</h5>&#13;
<p><em>With Deepthi Sigireddi, Software Engineer, PlanetScale</em></p>&#13;
<p><a contenteditable="false" data-primary="Sigireddi, Deepthi" data-type="indexterm" id="idm46183198095408"/><a contenteditable="false" data-primary="Vitess Operator" data-secondary="building" data-type="indexterm" id="idm46183197062096"/>Vitess can be described simply as a scaling infrastructure for MySQL. Vitess started at YouTube in 2010 when the team was struggling with daily outages due to MySQL. A few people got together and decided that rather than fighting fires every day, they would solve their problem from the ground up. Initially, Vitess was very customized to YouTube’s environment. Applications were segmented into groups to run against one database or another, with a layer in between to route queries to the right backing database. Over time, the internal architecture became more complex—but simpler from the application’s point of view. Vitess started with custom sharding, which required the application to know which database to query against. Now, the <span class="keep-together">application</span> doesn’t need to know whether there are 10 MySQL databases or 100, or 1,000. As far as the application layer is concerned, it looks like a single database.</p>&#13;
<p><a contenteditable="false" data-primary="Google" data-type="indexterm" id="idm46183197109712"/><a contenteditable="false" data-primary="Borg" data-type="indexterm" id="idm46183197164512"/>The move toward Kubernetes started when YouTube was acquired by Google. The mandate to use Google infrastructure included adapting Vitess to run on Borg, the precursor to Kubernetes. With Borg, the applications had to be tolerant to being restarted anytime, since Borg frequently reschedules components to run on different machines, but that wasn’t something supported by MySQL. The team built in tolerance for this type of automation as features in Vitess, and that’s how Vitess became cloud native. All this sounds familiar to us now because that’s how Kubernetes operates. When the team members at YouTube decided to make Vitess run on Kubernetes, they were able to do the work without a lot of changes.</p>&#13;
<p><a contenteditable="false" data-primary="CNCF (Cloud Native Computing Foundation)" data-type="indexterm" id="idm46183197044880"/><a contenteditable="false" data-primary="Metacontroller" data-type="indexterm" id="idm46183197319536"/><a contenteditable="false" data-primary="Operator SDK" data-type="indexterm" id="idm46183197102752"/>Before Vitess was donated to the CNCF in January 2018, there was a project called <a href="https://oreil.ly/5Ynco">Metacontroller</a>, which predated the Operator SDK (see <a data-type="xref" href="#building_operators">“Building Operators”</a>). This was used to get Vitess working on Kubernetes, independent of Google’s infrastructure. It seemed intuitive that you’d want to run Vitess using an operator, since there was already a community-contributed Helm chart and we saw the movement in the community toward operators.</p>&#13;
<p>There was an early community effort by an individual Vitess contributor to write a Kubernetes operator, but it was a pretty complex undertaking to take on alone, so it didn’t go far. Other Vitess users such as HubSpot have built their own custom operators, which are private since they are quite specific to their own deployments. PlanetScale started building a Kubernetes operator for Vitess to run as a cloud service, and once it matured, we released 90% of that code as an open source Vitess Operator.</p>&#13;
<p>To write an operator for an application, you need to understand both Kubernetes and the application really well. Kubernetes moves fast, with new releases every four months. Many features that were in alpha when we first started building our operator are now a part of Kubernetes. Meanwhile, MySQL continues to evolve and add new query constructs. In MySQL 8.0, a lot of new syntax was added, and maintaining an operator requires keeping up with those changes.</p>&#13;
<p>To run a service in Kubernetes, you have to know the important lifecycle events and how those disrupt availability. Vitess achieves automatic failure detection and failover through a mixture of approaches. If your primary MySQL node is running with a PersistentVolume that goes down, Kubernetes will restart it with a downtime of 20 or 30 seconds. This is pretty fast—maybe more than what some applications can tolerate. We are building into Vitess the ability to detect and fail over much faster than a Kubernetes hot restart. Vitess will detect that the primary has gone down and will fail over to a replica that has kept up with the primary within 5 or 10 seconds. This will greatly improve reliability.</p>&#13;
<p>Another area of improvement we are focused on is speeding up startup and shutdown. Network constraints like TCP/IP timeouts limit how quickly you can detect failure, but MySQL startup and shutdown are not yet at the point of hitting that lower bound. The first operator we built at PlanetScale took 10 or 20 minutes to bring up a cluster. This was partly due to inefficiencies in the Operator SDK, and partly because we had written a single controller with a gigantic reconcile loop. We rewrote the operator to use a newer version of the Operator SDK and to have a separate controller for each resource. This made our startups and shutdowns 20 times faster, which was a hard requirement for providing a cloud service. Clients expect those operations to take 10 or 15 seconds, not 2 or 3 minutes.</p>&#13;
<p>We also need more primitives from Kubernetes in order to continue to mature database operators. While Kubernetes provides Deployments, ReplicaSets, and StatefulSets, it doesn’t yet support the concept of primary and replicas as MySQL needs. Imagine if you could configure Kubernetes to designate a primary, and specify an action to perform if the primary is restarted. A lot of the error-handling code included in Vitess would actually not be required. While Kubernetes has a leader election module, there’s no clear way to leverage this for an operator that already has the concept of primaries and replicas. This leads to more duplicated code.</p>&#13;
<p>One final area of improvement is data locality. Application developers are looking for more control over where their data is stored, and easy ways to ingest or load data. Every organization that provides a database solution on Kubernetes should consider providing it as a service to make it easier for developers to consume. Today if a developer is running an application in AWS and a particular data service is not available there, they have to consider using another cloud or building the capability themselves. It should be really easy to create and populate a data source for an application no matter where you run it.</p>&#13;
<p><a contenteditable="false" data-primary="" data-startref="mysql_vit" data-type="indexterm" id="idm46183197183424"/><a contenteditable="false" data-primary="" data-startref="vit_mysql" data-type="indexterm" id="idm46183197186880"/>Infrastructure provisioning is getting easier and easier, and long may that trend continue. Even so, there is a lot more work to do. Those of us who get paid to work on open source are fortunate because many developers aren’t compensated for their open source contributions. Let’s continue to champion the benefits of working on open source software in our organizations so we can continue to grow as a community.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="A Growing Ecosystem of Operators" data-type="sect1"><div class="sect1" id="a_growing_ecosystem_of_operators">&#13;
<h1>A Growing Ecosystem of Operators</h1>&#13;
<p><a contenteditable="false" data-primary="Operator Framework" data-type="indexterm" id="of_ab"/><a contenteditable="false" data-primary="operators" data-secondary="ecosystem of" data-type="indexterm" id="ops_eco"/>The operator pattern has become quite popular in the Kubernetes community, aided in part by the development of the <a href="https://operatorframework.io">Operator Framework</a>, an ecosystem for creating and distributing operators. In this section, we’ll examine the Operator Framework and related open source projects.</p>&#13;
<section data-pdf-bookmark="Choosing Operators" data-type="sect2"><div class="sect2" id="choosing_operators">&#13;
<h2>Choosing Operators</h2>&#13;
<p><a contenteditable="false" data-primary="operators" data-secondary="choosing" data-type="indexterm" id="op_cho"/>While we’ve focused in this chapter on Vitess as an example database operator, operators are clearly relevant to all of the elements of your data stack. In all aspects of cloud native data, we see a growing number of maturing, open source operators to use in your deployments, and we’ll be looking at additional operators as we examine how to run different types of data infrastructure on Kubernetes in upcoming chapters.</p>&#13;
<p><a contenteditable="false" data-primary="Operator Hub" data-type="indexterm" id="idm46183197005008"/>You should consider multiple aspects in choosing an operator. What are its features? How much does it automate? How well supported is it? Is it proprietary or open source? The Operator Framework provides a great resource, the <a href="https://operatorhub.io">Operator Hub</a>, which you should consider as your first stop when looking for an operator. Operator Hub is a well-organized list of various operators that cover every aspect of cloud native software. It does rely on maintainers to submit their operators for listing, which means that many existing operators may not be listed.</p>&#13;
<p><a contenteditable="false" data-primary="OLM (Operator Lifecycle Manager)" data-type="indexterm" id="idm46183198094736"/>The Operator Framework also contains the Operator Lifecycle Manager (OLM), an operator for installing and managing other operators in your cluster. You can curate your own custom catalog of operators that are permitted in your environment, or use catalogs provided by others. For example, Operator Hub can itself be <a href="https://oreil.ly/ble8P">treated as a catalog</a>.</p>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="Operator Capability Model and" data-type="indexterm" id="idm46183198094192"/><a contenteditable="false" data-primary="Operator Capability Model" data-type="indexterm" id="idm46183196994880"/><a contenteditable="false" data-primary="Ansible, Operator Capability Model and" data-type="indexterm" id="idm46183196991520"/><a contenteditable="false" data-primary="Go" data-type="indexterm" id="idm46183196990544"/>Part of the curation the Operator Hub provides is rating the capability of each operator according to the <a href="https://oreil.ly/eYVEA">Operator Capability Model</a>. The levels in this capability model are summarized in <a data-type="xref" href="#operator_capability_levels_applied_to_d">Table 5-1</a>, with additional commentary we’ve added to highlight considerations for database operators. The examples are not prescriptive but indicate the type of capabilities expected at each level.</p>&#13;
<table class="border" id="operator_capability_levels_applied_to_d">&#13;
<caption><span class="label">Table 5-1. </span>Operator capability levels applied to databases</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Capability level</th>&#13;
<th>Characteristics</th>&#13;
<th>Database operator examples</th>&#13;
<th>Tools</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>Level 1: <span class="keep-together">Basic install</span></td>&#13;
<td>Installation and configuration of Kubernetes and workloads</td>&#13;
<td>The operator uses custom resources to provide a central point of configuration for a database cluster.<br/>The operator deploys the database by creating resources such as Deployments, ServiceAccounts, RoleBindings, PersistentVolumeClaims, and Secrets, and helps initialize the database schema.</td>&#13;
<td>Helm, Ansible, Go</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Level 2: Seamless updates</td>&#13;
<td>Upgrade of the managed workload and operator</td>&#13;
<td>The operator can update an existing database to a newer version without data loss (or, hopefully, downtime). <br/>The operator can be replaced with a newer version of itself.</td>&#13;
<td>Helm, Ansible, Go</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Level 3:<br/>Full lifecycle</td>&#13;
<td>Ability to create and restore from backups, ability to fail over or replace portions of a clustered application, ability to scale the application</td>&#13;
<td>The operator provides a way to create a consistent backup across multiple data nodes and the ability to use those backups to restore or replace failed database nodes.<br/>The operator can respond to a configuration change to add or remove database nodes or perhaps even datacenters.</td>&#13;
<td>Ansible, Go</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Level 4: <span class="keep-together">Deep insights</span></td>&#13;
<td>Providing capabilities including alerting, monitoring, events, or metering</td>&#13;
<td>The operator monitors metrics and logging output by the database software and uses this information to implement health and readiness checks. <br/>The operator pushes metrics and alerts to other infrastructure.</td>&#13;
<td>Ansible, Go</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Level 5: <span class="keep-together">Auto-pilot</span></td>&#13;
<td>Providing capabilities including auto-scaling, auto-healing, <span class="keep-together">auto-tuning</span></td>&#13;
<td>The operator auto-scales the number of database nodes in the cluster up or down to meet performance requirements. The operator might also dynamically resize PVs or change the StorageClass used for various database nodes.<br/>The operator automatically performs database maintenance such as rebuilding indexes to improve slow response times. <br/>The operator detects abnormal workload patterns and takes action such as resharding to balance workloads.</td>&#13;
<td>Ansible, Go</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<p>These levels are useful both for evaluating operators you might want to use, and for providing targets for operator developers to aim for. They also provide an opinionated view on what Helm-based operators can accomplish, limiting them to Level 2. For full lifecycle management and automation, more direct involvement with the Kubernetes control plane is needed. For a Level 5 operator, the goal is a complete hands-off Deployment.</p>&#13;
<p>Let’s take a quick look at a few of the available operators for popular open source databases:</p>&#13;
<dl>&#13;
<dt>Cass Operator</dt>&#13;
<dd><a contenteditable="false" data-primary="Cass Operator" data-secondary="about" data-type="indexterm" id="idm46183197523088"/><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="about" data-type="indexterm" id="idm46183197003696"/><a contenteditable="false" data-primary="DataStax" data-type="indexterm" id="idm46183196964112"/><a contenteditable="false" data-primary="DBaaS (database as a service)" data-type="indexterm" id="idm46183196963360"/>In 2021, several companies in the Cassandra community that had developed their own operators <a href="https://oreil.ly/AS16G">came together</a> in support of an operator built by DataStax, known primarily by its nickname: <a href="https://oreil.ly/ueyGZ">Cass Operator</a>. Cass Operator was inspired by the best features of the community operators as well as DataStax experience running Astra, a Cassandra-based database as a service (DBaaS). The operator has been donated to the <a href="https://k8ssandra.io">K8ssandra project</a>, where it is part of a larger ecosystem for deploying Cassandra on Kubernetes. We’ll take a deeper look at K8ssandra and Cass Operator in <a data-type="xref" href="ch07.html#the_kubernetes_native_database">Chapter 7</a>.</dd>&#13;
<dt>PostgreSQL operators</dt>&#13;
<dd><a contenteditable="false" data-primary="PostgreSQL operators" data-type="indexterm" id="idm46183196957184"/><a contenteditable="false" data-primary="Zalando Postgres Operator" data-type="indexterm" id="idm46183196957536"/><a contenteditable="false" data-primary="PGO (Postgres Operator)" data-type="indexterm" id="idm46183196955584"/><a contenteditable="false" data-primary="Bogdanov, Nikolay, “Computing Kubernetes operators for PostgreSQL” blog post" data-type="indexterm" id="idm46183196954608"/><a contenteditable="false" data-primary="“Computing Kubernetes operators for PostgreSQL” blog post (Bogdanov)" data-primary-sortas="computing" data-type="indexterm" id="idm46183196953632"/>Several operators are available for PostgreSQL, which is not surprising given that it is the second most popular open source database after MySQL. Two of the most popular operators are the <a href="https://oreil.ly/i6Us5">Zalando Postgres Operator</a>, and <a href="https://oreil.ly/A40Qf">PGO</a> (which stands for Postgres Operator) from Crunchy Data. Read Nikolay Bogdanov’s blog post  <a href="https://oreil.ly/AQcvB">“Comparing Kubernetes Operators for PostgreSQL”</a> for a helpful comparison of these and other operators.</dd>&#13;
<dt>MongoDB Kubernetes Operator</dt>&#13;
<dd><a contenteditable="false" data-primary="MongoDB" data-type="indexterm" id="idm46183196948256"/>MongoDB is the most popular document database, beloved by developers for its ease of use. The MongoDB Community Kubernetes Operator provides basic support for creating and managing MongoDB ReplicaSets, scaling up and down, and upgrades. This operator is available on <a href="https://oreil.ly/dAhuV">GitHub</a> but not yet listed on Operator Hub, possibly because MongoDB also offers a separate operator for its enterprise version.</dd>&#13;
<dt>Redis Operator</dt>&#13;
<dd><a contenteditable="false" data-primary="Redis Operator" data-type="indexterm" id="idm46183197532096"/>Redis is an in-memory key-value store that has a broad set of use cases. Application developers typically use Redis as an adjunct to other data infrastructure when ultra-low latency is required. It excels at caching, counting, and shared data structures. The <a href="https://oreil.ly/SKLSz">Redis Operator</a> covers the basic install and upgrade but also manages harder operations such as cluster failover and recovery.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="" data-startref="op_cho" data-type="indexterm" id="idm46183197343632"/>As you can see, operators are available for many popular open source databases, although it’s unfortunate that some vendors have tended to think of Kubernetes operators primarily as a feature differentiator for paid enterprise versions.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Building Operators" data-type="sect2"><div class="sect2" id="building_operators">&#13;
<h2>Building Operators</h2>&#13;
<p><a contenteditable="false" data-primary="Branscombe, Mary, “When to Use, and When to Avoid, the Operator Pattern” blog post" data-type="indexterm" id="idm46183197096240"/><a contenteditable="false" data-primary="“When to Use, and When to Avoid, the Operator Pattern” blog post (Branscombe)" data-primary-sortas="when" data-type="indexterm" id="idm46183197095216"/><a contenteditable="false" data-primary="operators" data-secondary="building" data-type="indexterm" id="idm46183197042208"/>While there is broad consensus in the Kubernetes community that you should <em>use</em> operators for distributed data infrastructure whenever possible, there are a variety of opinions about who exactly should be <em>building</em> operators. If you don’t happen to work for a data infrastructure vendor, this can be a challenging question. Mary Branscombe’s blog post <a href="https://oreil.ly/hd8FE">“When to Use, and When to Avoid, the Operator Pattern”</a> provides some excellent questions to consider, which we’ll summarize here:</p>&#13;
<ul>&#13;
<li><p>What is the scale of the deployment? If you’re deploying only a single instance of the database application, building and maintaining an operator might not be cost-effective.</p></li>&#13;
<li><p>Do you have the expertise in the database? The best operators tend to be built by companies running databases at scale in production, including vendors that are providing DBaaS solutions.</p></li>&#13;
<li><p>Do you need higher levels of application awareness and automation, or would deployment with a Helm chart and standard Kubernetes resources be sufficient?</p></li>&#13;
<li><p>Are you trying to make the operator manage resources that are external to Kubernetes? Consider a solution that runs closer to the resources being managed with an API you can access from your Kubernetes application.</p></li>&#13;
<li><p>Have you considered security implications? Since operators are extensions of the Kubernetes control plane, you’ll want to carefully manage what resources your operator can access.</p></li>&#13;
</ul>&#13;
<p>If you decide to write an operator, several great tools and resources are available:</p>&#13;
<dl>&#13;
<dt><a href="https://oreil.ly/HtSZt">Operator SDK</a></dt>&#13;
<dd><a contenteditable="false" data-primary="Operator SDK" data-type="indexterm" id="idm46183197234576"/>This software development kit, included in the Operator Framework, contains tools to build, test, and package operators. Operator SDK uses templates to autogenerate new operator projects and provides APIs and abstractions to simplify common aspects of building operators, especially interactions with the Kubernetes API. The SDK supports the creation of operators using Go, Ansible, or Helm.</dd>&#13;
<dt><a href="https://book.kubebuilder.io">Kubebuilder</a></dt>&#13;
<dd><a contenteditable="false" data-primary="Kubebuilder" data-type="indexterm" id="idm46183197105712"/><a contenteditable="false" data-primary="Tei, Wei, “Kubebuilder vs Operator SDK” blog post" data-type="indexterm" id="idm46183197554896"/><a contenteditable="false" data-primary="“Kubebuilder vs Operator SDK” blog post (Tei)" data-primary-sortas="kubebuilder" data-type="indexterm" id="idm46183197130896"/>This toolkit for building operators is managed by the Kubernetes API Machinery SIG. Similarly to Operator SDK, Kubebuilder provides tools for project generation, testing, and publishing controllers and operators. Both Kubebuilder and Operator SDK are built on the Kubernetes <a href="https://oreil.ly/XR9y4">controller-runtime</a>, a set of Go libraries for building controllers. Wei Tei’s blog post <a href="https://oreil.ly/NKC8d">“Kubebuilder vs. Operator SDK”</a> provides a concise summary of the differences between these toolkits.</dd>&#13;
<dt><a href="https://kudo.dev">Kubernetes Universal Declarative Operator (KUDO)</a></dt>&#13;
<dd><a contenteditable="false" data-primary="KUDO (Kubernetes Universal Declarative Operator)" data-type="indexterm" id="idm46183196945680"/><a contenteditable="false" data-primary="“How to deploy your first app with Kudo operator on K8S” blog post (Vedetskyi)" data-primary-sortas="how" data-type="indexterm" id="idm46183196929648"/><a contenteditable="false" data-primary="Vedetskyi, Dmytro, “How to deploy your first app with Kudo operator on K8S” blog post" data-primary-sortas="how" data-type="indexterm" id="idm46183196928368"/>This operator allows you to create operators declaratively using YAML files. This is an attractive approach for some developers as it eliminates the need to write Go. Dmytro Vedetskyi’s  blog post <a href="https://oreil.ly/K71fK">“How to Deploy Your First App with Kudo Operator on K8S”</a> provides a helpful introduction to using KUDO and discusses some of the pros and cons of the declarative approach.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="Kubernetes Operators (Dobies and Wood)" data-type="indexterm" id="idm46183197183728"/><a contenteditable="false" data-primary="Dobies, Jason, Kubernetes Operators" data-type="indexterm" id="idm46183197184368"/><a contenteditable="false" data-primary="Wood, Joshua, Kubernetes Operators" data-type="indexterm" id="idm46183197070048"/><a contenteditable="false" data-primary="Hausenblas, Michael, Programming Kubernetes" data-type="indexterm" id="idm46183196926096"/><a contenteditable="false" data-primary="Programming Kubernetes (Hausenblas and Schimanski)" data-type="indexterm" id="idm46183196925120"/><a contenteditable="false" data-primary="Schimanski, Stefan, Programming Kubernetes" data-type="indexterm" id="idm46183196924144"/>Finally, the O’Reilly books <a href="https://oreil.ly/rWIM0"><em>Kubernetes Operators</em></a> by Jason Dobies and Joshua Wood and <a href="https://oreil.ly/iczyv"><em>Programming Kubernetes</em></a> are great resources for understanding the operator ecosystem and getting into the details of writing operators and controllers in Go.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="can_one_operator_rule_them_allquestion">&#13;
<h5>Can One Operator Rule Them All?</h5>&#13;
<p><em>With Umair Mufti, Product Manager, Pure Storage</em></p>&#13;
<p><a contenteditable="false" data-primary="Mufti, Umair" data-type="indexterm" id="idm46183196919072"/><a contenteditable="false" data-primary="operators" data-secondary="about" data-type="indexterm" id="idm46183196917936"/>As discussed in this chapter, the number of Kubernetes operators for databases has been continuously growing. Database developers want their databases to run on Kubernetes, so projects like Vitess are stepping up and developing operators to make it easy for others. This initiative is great, but one potential drawback is that everyone is building operators their own way and solving similar problems with different implementations. As a result, there is no uniformity among operators for stateful workloads.</p>&#13;
<p>The question those who are developing operators have to reckon with is how specialized to expect end users to be. Because of the popularity of cloud native, microservice architectures, application developers now expect polyglot persistence: to run a relational database in addition to a graph database or a key-value store. This forces cluster administrators to provide different types of databases while maintaining the operational simplicity of a single platform.</p>&#13;
<p>No Kubernetes admin wants to maintain 10 or 15 operators on their cluster. The point of Kubernetes is the ease of operations when deploying applications, monitoring them on day two, and making lifecycle management simpler. As soon as you have the maintenance overhead of managing multiple operator lifecycles, you’ve already lost. Multiply that 10 or 15 times over, and you are completely at odds with the value Kubernetes provides. The only way out of this situation is to reduce the number of operators. Could there be a single operator for all our databases or stateful workloads? Let’s explore.</p>&#13;
<p><a contenteditable="false" data-primary="MVC (model-view-controller) framework" data-type="indexterm" id="idm46183198123712"/>The operator pattern is simply a design pattern for running stateful workloads in Kubernetes, just as the model–view–controller (MVC) framework is a pattern for user-facing applications. Various web frameworks such as Angular, Vue, and React use the MVC pattern, but they all implement the pattern in different ways, and your code will vary based on the implementation you use. This is a familiar experience for developers using operators today. Each operator solves the problem of running a stateful workload in Kubernetes in a unique way, and it requires specialization to become proficient with each operator. The irony is that if you’re running Cassandra, Redis, or Postgres, a lot of the problems being addressed are very much the same: cluster membership, failure detection, backup and restore, and more.</p>&#13;
<p>Could we actually build “one operator to rule them all”? Maybe. But  perhaps what we need is not literally one single operator, but a collection of higher-level interfaces that operators should adhere to, so they work with multiple data service types. This would enable administrators to choose an operator based on factors other than the vendor or project that created it. What if you could use an operator that would manage your Cassandra,  Elasticsearch, and Kafka clusters? This is what we need to reduce the burden on operations teams and fully realize the benefits of managing stateful workloads on Kubernetes.</p>&#13;
<p>We need to build another layer of abstraction on top of the operator pattern. As a community, we can develop a common set of custom resources, and each controller can manage them in their own way. For example, we might define a <code>TopologyAwareStatefulSet</code> as a new CRD, or a <code>ClusterMembership</code> CRD that describes how a node joins a cluster. Instead of Elasticsearch developers and Cassandra developers creating separate definitions of a server group or topology, we could all agree that a distributed database has a concept of topology, agree on a CRD, and controllers can implement the specified behavior as needed.</p>&#13;
<p>The ideal end-state is a world with multiple implementations that adhere to a common standard. Kubernetes itself has a specification, and each Kubernetes distribution has to provide certain APIs to be considered a valid distribution. Users can choose which operators to use in the same way, knowing that they can expect a baseline standard while applying other criteria.</p>&#13;
<p>Kubernetes still shows signs that it was born of a stateless world, but there’s an exciting future for stateful workloads on Kubernetes. We are very much in that “Crossing the Chasm” moment and still just hitting the inflection point with stateful workloads. With more advanced operators, we’ll no longer be working in silos, solving the same problems over and over again. Then we can use our collective talents and skills to solve bigger and higher-level problems.</p>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="" data-startref="of_ab" data-type="indexterm" id="idm46183196939520"/><a contenteditable="false" data-primary="" data-startref="ops_eco" data-type="indexterm" id="idm46183197146016"/>As you can see, the state of the art in Kubernetes operators is continuing to mature. Whether the goal is to build a unified operator or just to make it easier to build database-specific operators, it’s clear that great progress can be made as multiple communities begin to collaborate on common CRDs to address problems like cluster membership, topology awareness, and leader election<a contenteditable="false" data-primary="" data-startref="auto_op" data-type="indexterm" id="idm46183197764432"/><a contenteditable="false" data-primary="" data-startref="db_op" data-type="indexterm" id="idm46183197553872"/><a contenteditable="false" data-primary="" data-startref="op_db" data-type="indexterm" id="idm46183196936192"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000004">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, you’ve learned about several ways of extending the Kubernetes control plane, especially operators and custom resources. The operator pattern provides the critical breakthrough that enables us to simplify database operations in Kubernetes through automation. While you should definitely be using operators to run distributed databases in Kubernetes, think carefully before starting to write your own operator. If building an operator is the right course for you, there are plenty of resources and frameworks to help you along the way. There are certainly ways in which Kubernetes itself could improve to make writing operators easier, as you’ve learned from the experts we spoke to in this chapter.</p>&#13;
<p>While we’ve spent the past couple of chapters focusing primarily on running databases on Kubernetes, let’s expand our focus to consider how those databases interact with other infrastructure.</p>&#13;
</div></section>&#13;
</div></section></body></html>