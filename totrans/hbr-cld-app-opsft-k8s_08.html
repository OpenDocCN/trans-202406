<html><head></head><body><section data-pdf-bookmark="Chapter 7. Multicluster Policy Configuration" data-type="chapter" epub:type="chapter"><div class="chapter" id="multicluster_policy_configuration">&#13;
<h1><span class="label">Chapter 7. </span>Multicluster Policy Configuration</h1>&#13;
<p>A key aspect of Kubernetes that we’ve already seen is how it is a declarative, <a contenteditable="false" data-primary="configuring clusters with operators" data-seealso="policies" data-type="indexterm" id="ch07-conclz"/><a contenteditable="false" data-primary="clusters" data-secondary="configuring with operators" data-seealso="policies" data-type="indexterm" id="ch7-concl"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="about" data-tertiary="API-driven system" data-type="indexterm" id="idm45358189903320"/><a contenteditable="false" data-primary="multicluster management" data-secondary="policies" data-see="policies" data-type="indexterm" id="idm45358189901672"/>API-driven system. Initial support for orchestration focused purely on containers and their required support services, such as network services, <code>PersistentVolumeClaim</code>s, and administrative policies. Now we will look at how we can generalize the underlying pattern that Kubernetes API controllers follow. It turns out, declarative management of applications is also a great way to operate the Kubernetes cluster itself. In this chapter, we will discuss the concept of an operator and how we can use operators to simplify the management of our clusters.</p>&#13;
<section data-pdf-bookmark="Configuring Your Cluster with Operators" data-type="sect1"><div class="sect1" id="configuring_your_cluster_with_operators">&#13;
<h1>Configuring Your Cluster with Operators</h1>&#13;
<p>Let’s talk a bit about how the Kubernetes system works and how you can extend the system to meet your needs.</p>&#13;
<section data-pdf-bookmark="Understanding Operators" data-type="sect2"><div class="sect2" id="understanding_operators">&#13;
<h2>Understanding Operators</h2>&#13;
<p>Each API provider includes a balancing loop (pictured in <a data-type="xref" href="#a_balancing_loop_reconciles_the_observed">Figure 7-1</a>): observe actual system state, <a contenteditable="false" data-primary="operators" data-secondary="operator construct" data-tertiary="operators explained" data-type="indexterm" id="idm45358189893656"/><a contenteditable="false" data-primary="policies" data-secondary="operators for configuring clusters" data-tertiary="operators explained" data-type="indexterm" id="idm45358189892040"/><a contenteditable="false" data-primary="controllers" data-secondary="API providers explained" data-type="indexterm" id="idm45358189890328"/><a contenteditable="false" data-primary="operators" data-secondary="about operators" data-type="indexterm" id="idm45358189888952"/>reconcile with desired system state, apply changes, and report status.</p>&#13;
<p>Such a powerful pattern led to more orchestration providers and ultimately was generalized to allow the creation of new CRDs and their responsible controllers in Kubernetes 1.16. <a contenteditable="false" data-primary="controllers" data-secondary="custom controllers" data-type="indexterm" id="idm45358189886760"/><a contenteditable="false" data-primary="custom resource definition (CRD)" data-secondary="controllers" data-type="indexterm" id="idm45358189885384"/>Controllers react to the presence of new custom resources (that are instances of CRDs) or updates to existing custom resources. Controllers also interact with objects under management, traditionally containers or pods, but now the vocabulary exposed by CRDs allows for the management of stateful workloads that require additional orchestration behaviors, of VMs, and even of cloud-based services (such as object store, database as a service, etc.). The net is that you can use the Kubernetes API paradigm to program a broad array of infrastructure and application services using a consistent approach and a source-controllable representation of the declarative API.</p>&#13;
<figure><div class="figure" id="a_balancing_loop_reconciles_the_observed">&#13;
<img src="assets/hcok_0701.png"/>&#13;
<h6><span class="label">Figure 7-1. </span>A balancing loop reconciles the observed differences between the desired state and current state of the system</h6>&#13;
</div></figure>&#13;
<p>Red Hat helped catalyze the community around the operator framework, or a way of applying Kubernetes controllers to manage software life cycles, including deployment, configuration, upgrade, and decommissioning. <a contenteditable="false" data-primary="OperatorHub" data-type="indexterm" id="idm45358189880616"/><a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-type="indexterm" id="idm45358189879512"/>You can leverage these operators on <a href="https://operatorhub.io">OperatorHub.io</a> to more easily manage the containerized software running on your cluster. We will see an example of how to deploy an operator in the next section.</p>&#13;
<p>Red Hat OpenShift applies operators to the management of the entire platform, from initiating a cluster and its supporting infrastructure-provisioning life cycle to configuring DNS routes for cluster services to configuring identity providers. In essence, virtually all aspects of cluster state are specified through declarative APIs (via CRDs), and operators implement the intelligent life cycle of reconciliation (e.g., the balancing loop pattern discussed previously).</p>&#13;
<p>What distinguishes a controller and an operator? This is largely <a contenteditable="false" data-primary="operators" data-secondary="controllers versus" data-type="indexterm" id="idm45358189875848"/><a contenteditable="false" data-primary="controllers" data-secondary="operators versus" data-type="indexterm" id="idm45358189874392"/>subjective, and discussion ranges from the intent of the orchestrator (e.g., operators are for the management of software life cycles running in containers) to how you might deploy the orchestrator to your cluster (a simple Kubernetes deployment versus an operator life cycle bundle). Subjective discussion aside, we will talk about operators as custom Kubernetes CRDs and controllers created by the <code>operator-sdk</code>.</p>&#13;
<p>In <a data-type="xref" href="#the_operator_catalog_in_red_hat_openshif">Figure 7-2</a>, we can see the operators <a contenteditable="false" data-primary="operators" data-secondary="available operators" data-type="indexterm" id="idm45358189870536"/>available that are built and published by Red Hat alongside the operators that are built and supported by the community.</p>&#13;
<figure><div class="figure" id="the_operator_catalog_in_red_hat_openshif">&#13;
<img src="assets/hcok_0702.png"/>&#13;
<h6><span class="label">Figure 7-2. </span>The Operator Catalog in Red Hat OpenShift</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Example: Container Security Operator" data-type="sect2"><div class="sect2" id="example_container_security_operator">&#13;
<h2>Example: Container Security Operator</h2>&#13;
<p>You can consume operators in OpenShift in two steps. <a contenteditable="false" data-primary="operators" data-secondary="example: Container Security operator" data-type="indexterm" id="ch07-conseop4"/><a contenteditable="false" data-primary="clusters" data-secondary="example: Container Security operator" data-type="indexterm" id="ch07-conseop"/><a contenteditable="false" data-primary="policies" data-secondary="operators for configuring clusters" data-tertiary="example: Container Security operator" data-type="indexterm" id="ch07-conseop2"/><a contenteditable="false" data-primary="operators" data-secondary="about deploying" data-type="indexterm" id="idm45358189859832"/><a contenteditable="false" data-primary="security" data-secondary="example: Container Security operator" data-type="indexterm" id="ch07-conseop5"/><a contenteditable="false" data-primary="Container Security operator example" data-type="indexterm" id="ch07-conseop6"/>First, deploy the operator itself. Second, deploy one or more resources managed by the operator. Let’s practice deploying an operator that will help with image security scans in your cluster.</p>&#13;
<p>The following example demonstrates how to deploy an example operator, the Container Security operator. You will need an OpenShift Container Platform 4.4+ cluster to follow the demonstration on your own. Follow these steps:</p>&#13;
<ol>&#13;
<li><p><strong>Open the OperatorHub catalog and find the Container Security operator.</strong> <a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="Container Security operator" data-type="indexterm" id="idm45358189853032"/><a contenteditable="false" data-primary="OperatorHub" data-secondary="Container Security operator" data-type="indexterm" id="idm45358189851288"/>From the web console, open the Operators &gt; OperatorHub section and enter “container security” in the filter text box. A tile named “Container Security” should remain in the list.</p></li>&#13;
<li><p><strong>Open the Container Security operator overview</strong> (as shown in <a data-type="xref" href="#the_container_security_operator_overview">Figure 7-3</a>). Click the tile. <a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="about Community operator support" data-type="indexterm" id="idm45358189847736"/><a contenteditable="false" data-primary="OperatorHub" data-secondary="about Community operator support" data-type="indexterm" id="idm45358189845928"/>If a message about Community operators appears, review the message and click Continue. Community operators do not qualify for official support guarantees and SLAs from Red Hat and are often supported by community contributors in the upstream.</p>&#13;
<figure><div class="figure" id="the_container_security_operator_overview">&#13;
<img src="assets/hcok_0703.png"/>&#13;
<h6><span class="label">Figure 7-3. </span>The Container Security operator overview page</h6>&#13;
</div></figure></li>&#13;
<li><p><strong>Review the summary and click Install.</strong> The Summary page describes the operator and other supporting details. <a contenteditable="false" data-primary="OperatorHub" data-secondary="Capability Level and Provider Type attributes" data-type="indexterm" id="idm45358189841528"/><a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="Capability Level and Provider Type attributes" data-type="indexterm" id="idm45358189840008"/>Pay attention to the Capability Level and Provider Type attributes, as these are critical to understanding whether you should adopt a particular operator for your needs.</p></li>&#13;
<li><p><strong>Configure the operator <a contenteditable="false" data-primary="Operator Lifecycle Manager (OLM)" data-type="indexterm" id="idm45358189837304"/><a contenteditable="false" data-primary="OLM (Operator Lifecycle Manager)" data-type="indexterm" id="idm45358189836008"/>subscription</strong> (as shown in <a data-type="xref" href="#configuring_the_operator_lifecycle_manag">Figure 7-4</a>). Review your desired choices for the Update Channel and Approval Strategy, then click <span class="keep-together">Subscribe</span>.</p>&#13;
</li>&#13;
</ol>&#13;
<figure><div class="figure" id="configuring_the_operator_lifecycle_manag">&#13;
<img src="assets/hcok_0704.png"/>&#13;
<h6><span class="label">Figure 7-4. </span>Configuring the Operator Lifecycle Manager (OLM) subscription to install the Container Security operator</h6>&#13;
</div></figure>&#13;
<p>Subscriptions enable an operator to consume updates as <a contenteditable="false" data-primary="OperatorHub" data-secondary="subscriptions" data-type="indexterm" id="idm45358189830216"/><a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="subscriptions" data-type="indexterm" id="idm45358189828840"/>needed, either with manual or automatic approval. For each operator, you must decide what choice to make for the Update Channel and the Approval Strategy.</p>&#13;
<p>The <em>Update Channel</em> for each operator can be different and<a contenteditable="false" data-primary="OperatorHub" data-secondary="Update Channel" data-type="indexterm" id="idm45358189826024"/><a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="Update Channel" data-type="indexterm" id="idm45358189824648"/> reflects how often updates to the operator behavior will be made available and how stable the updates are. Channels might reflect stable, released, supported versions of the operator or early-access developer previews. The channel you pick will depend on the operator and your specific needs and risk tolerance.</p>&#13;
<p>The <em>Approval Strategy</em> allows you to inject a manual <a contenteditable="false" data-primary="operators" data-secondary="OperatorHub" data-tertiary="Approval Strategy" data-type="indexterm" id="idm45358189821624"/><a contenteditable="false" data-primary="OperatorHub" data-secondary="Approval Strategy" data-type="indexterm" id="idm45358189819976"/>decision prior to upgrades. Operators that follow best practices should be able to apply updates without causing an outage in supported services. The best practice, where possible, is to allow upgrades to proceed automatically, applying the latest security and function fixes that are <span class="keep-together">available</span>.</p>&#13;
<p>Now that we have applied an example operator, let’s take a look at the new behavior available to our cluster. The act of subscribing to the new operator created two resources on your cluster: <code>ClusterServiceVersion</code> and <code>Subscription</code>.</p>&#13;
<p>The <code>ClusterServiceVersion</code> resource provides metadata <a contenteditable="false" data-primary="ClusterServiceVersion resource" data-type="indexterm" id="idm45358189815016"/><a contenteditable="false" data-primary="operators" data-secondary="ClusterServiceVersion resource" data-type="indexterm" id="idm45358189813848"/>about the operator, including details like display names and a description, as well as operational metadata about the types of custom resources that the operator can manage and the types of RBAC required to support the operator’s behavior on the cluster.</p>&#13;
<p>The following example provides a list of the <code>ClusterServiceVersion</code> resources defined in the cluster:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get clusterserviceversion</strong>&#13;
NAME                                  DISPLAY                                      &#13;
VERSION   REPLACES   PHASE&#13;
advanced-cluster-management.v1.0.0    Advanced Cluster Management for Kubernetes   &#13;
1.0.0                Succeeded&#13;
container-security-operator.v1.0.5    Container Security                           &#13;
1.0.5                Succeeded&#13;
etcdoperator.v0.9.4                   etcd                                         &#13;
0.9.4                Succeeded&#13;
openshift-pipelines-operator.v1.0.1   OpenShift Pipelines Operator                 &#13;
1.0.1                Succeeded</pre>&#13;
<p>Each of these resources defines metadata about an operator that is installed on the cluster and <a contenteditable="false" data-primary="custom resource definition (CRD)" data-secondary="operators for custom resource creation" data-type="indexterm" id="idm45358189808888"/>defines information about what the operator contributes to the cluster, including the CRDs (remember, this is the Kubernetes way of adding new types to the API model).</p>&#13;
<p>The <code>Subscription</code> resource declares the intent to apply the <a contenteditable="false" data-primary="Subscription resource" data-type="indexterm" id="idm45358189806008"/><a contenteditable="false" data-primary="operators" data-secondary="Subscription resource" data-type="indexterm" id="idm45358189804872"/>operator to the cluster. <a contenteditable="false" data-primary="Operator Lifecycle Manager (OLM)" data-type="indexterm" id="idm45358189803368"/><a contenteditable="false" data-primary="OLM (Operator Lifecycle Manager)" data-type="indexterm" id="idm45358189802248"/>The OLM will apply resources, including roles, <code>RoleBinding</code>s, a service account, the CRDs acted upon by the operator, and of course the operator’s deployment. After all, the operator is just another Kubernetes controller, ever reconciling desired state and actual state.</p>&#13;
<p>In the following example, we see which operators are deployed in the cluster based on the subscription API provided by the OLM framework:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get subscriptions.operators.coreos.com -n openshift-operators</strong>&#13;
NAME                              PACKAGE                           SOURCE&#13;
                CHANNEL&#13;
container-security-operator       container-security-operator       community-&#13;
operators       alpha</pre>&#13;
<p>The OLM handles the installation, upgrade, and removal of operators on your cluster. For OpenShift Container Platform clusters, the OLM is deployed out of the box. You can optionally deploy OLM against any compliant Kubernetes cluster as well.</p>&#13;
<p>Each subscription results in a set of installed CRDs and running pods that act on those custom types.<a contenteditable="false" data-primary="" data-startref="ch07-conseop" data-type="indexterm" id="idm45358189797240"/><a contenteditable="false" data-primary="" data-startref="ch07-conseop2" data-type="indexterm" id="idm45358189795864"/><a contenteditable="false" data-primary="" data-startref="ch07-conseop3" data-type="indexterm" id="idm45358189794488"/><a contenteditable="false" data-primary="" data-startref="ch07-conseop5" data-type="indexterm" id="idm45358189793112"/><a contenteditable="false" data-primary="" data-startref="ch07-conseop6" data-type="indexterm" id="idm45358189791736"/><a contenteditable="false" data-primary="" data-startref="ch07-conseop4" data-type="indexterm" id="idm45358189790360"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Using Cluster Operators to Manage OpenShift" data-type="sect2"><div class="sect2" id="using_cluster_operators_to_manage_opensh">&#13;
<h2>Using Cluster Operators to Manage OpenShift</h2>&#13;
<p>As we saw with the provisioning life cycle of OpenShift 4.x clusters, <a contenteditable="false" data-primary="operators" data-secondary="cluster operators to manage OpenShift" data-type="indexterm" id="idm45358189787208"/><a contenteditable="false" data-primary="policies" data-secondary="operators for configuring clusters" data-tertiary="cluster operators to manage OpenShift" data-type="indexterm" id="idm45358189785784"/>operators play a central role in how the cluster is configured and continuously reconciled. We can introspect the special operators that manage underlying cluster behavior using the following command:<a contenteditable="false" data-primary="get command" data-secondary="get clusteroperators" data-type="indexterm" id="idm45358189783752"/><a contenteditable="false" data-primary="operators" data-secondary="oc get clusteroperators" data-type="indexterm" id="idm45358189782376"/></p>&#13;
<pre data-type="programlisting">$ <strong>oc get clusteroperators</strong>&#13;
NAME                                       VERSION   AVAILABLE   PROGRESSING   &#13;
DEGRADED   SINCE&#13;
authentication                             4.4.3     True        False         &#13;
False      48d&#13;
cloud-credential                           4.4.3     True        False         &#13;
False      48d&#13;
cluster-autoscaler                         4.4.3     True        False         &#13;
False      48d&#13;
console                                    4.4.3     True        False         &#13;
False      48d&#13;
csi-snapshot-controller                    4.4.3     True        False         &#13;
False      48d&#13;
dns                                        4.4.3     True        False         &#13;
False      48d&#13;
etcd                                       4.4.3     True        False         &#13;
False      28h&#13;
image-registry                             4.4.3     True        False         &#13;
False      48d&#13;
ingress                                    4.4.3     True        False         &#13;
False      48d&#13;
insights                                   4.4.3     True        False         &#13;
False      48d&#13;
kube-apiserver                             4.4.3     True        False         &#13;
False      48d&#13;
kube-controller-manager                    4.4.3     True        False         &#13;
False      48d&#13;
kube-scheduler                             4.4.3     True        False         &#13;
False      48d&#13;
kube-storage-version-migrator              4.4.3     True        False         &#13;
False      48d&#13;
machine-api                                4.4.3     True        False         &#13;
False      48d&#13;
machine-config                             4.4.3     True        False         &#13;
False      48d&#13;
marketplace                                4.4.3     True        False         &#13;
False      48d&#13;
monitoring                                 4.4.3     True        False         &#13;
False      41h&#13;
network                                    4.4.3     True        False         &#13;
False      48d&#13;
node-tuning                                4.4.3     True        False         &#13;
False      48d&#13;
openshift-apiserver                        4.4.3     True        False         &#13;
False      38d&#13;
openshift-controller-manager               4.4.3     True        False         &#13;
False      48d&#13;
openshift-samples                          4.4.3     True        False         &#13;
False      48d&#13;
operator-lifecycle-manager                 4.4.3     True        False         &#13;
False      48d&#13;
operator-lifecycle-manager-catalog         4.4.3     True        False         &#13;
False      48d&#13;
operator-lifecycle-manager-packageserver   4.4.3     True        False         &#13;
False      28h&#13;
service-ca                                 4.4.3     True        False         &#13;
False      48d&#13;
service-catalog-apiserver                  4.4.3     True        False         &#13;
False      48d&#13;
service-catalog-controller-manager         4.4.3     True        False         &#13;
False      48d&#13;
storage                                    4.4.3     True        False         &#13;
False      48d</pre>&#13;
<p>For each of these operators, there will be one or more CRDs that define an API you can configure or query to understand the state of the cluster. What is extremely powerful is that, regardless of the infrastructure substrate supporting the OpenShift 4.x cluster, the API and operators function in a consistent manner.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Example: Configuring the Authentication Operator" data-type="sect2"><div class="sect2" id="example_configuring_the_authentication_o">&#13;
<h2>Example: Configuring the Authentication Operator</h2>&#13;
<p>Access control is obviously critical for any organization. To support <a contenteditable="false" data-primary="policies" data-secondary="operators for configuring clusters" data-tertiary="example: configuring the authentication operator" data-type="indexterm" id="ch07-autop"/><a contenteditable="false" data-primary="operators" data-secondary="cluster operators to manage OpenShift" data-tertiary="example: configuring authentication operator" data-type="indexterm" id="ch07-autop2"/><a contenteditable="false" data-primary="authentication" data-secondary="example: configuring authentication operator" data-type="indexterm" id="ch07-autop3"/><a contenteditable="false" data-primary="security" data-secondary="authentication" data-tertiary="example: configuring authentication operator" data-type="indexterm" id="ch07-autop4"/><a contenteditable="false" data-primary="authentication" data-secondary="identities" data-type="indexterm" id="idm45358189766552"/><a contenteditable="false" data-primary="identities" data-type="indexterm" id="idm45358189765176"/>authentication, a Kubernetes provider must establish the identity of a given user or service account. Identity can be established by means of traditional user credentials, <em>Transport Layer Security</em> (TLS) certificates, or security tokens, as in the case of service accounts. <a contenteditable="false" data-primary="identities" data-secondary="system:anonymous" data-type="indexterm" id="idm45358189763336"/><a contenteditable="false" data-primary="authentication" data-secondary="identities" data-tertiary="system:anonymous" data-type="indexterm" id="idm45358189761896"/><a contenteditable="false" data-primary="security" data-secondary="authentication" data-tertiary="system:anonymous" data-type="indexterm" id="idm45358189760248"/>When no identity information is presented in an API, the user identity is treated as the identity <code>system:anonymous</code>.</p>&#13;
<p>The Kubernetes API server is configured with options for how to trust information <a contenteditable="false" data-primary="kube-apiserver" data-secondary="identities for API requests" data-type="indexterm" id="idm45358189757656"/>about identity that is provided for each API request. Information about the identity is then asserted for each API request and validated by the Kubernetes API server prior to allowing the request to proceed.</p>&#13;
<p>A few special identities beyond typical users to be aware of are listed <a contenteditable="false" data-primary="identities" data-secondary="beyond typical users" data-type="indexterm" id="idm45358189755224"/><a contenteditable="false" data-primary="authentication" data-secondary="identities" data-tertiary="beyond typical users" data-type="indexterm" id="idm45358189753752"/>in <a data-type="xref" href="#example_identities_available_within_a_ku">Table 7-1</a>.</p>&#13;
<table class="border" id="example_identities_available_within_a_ku">&#13;
<caption><span class="label">Table 7-1. </span>Example identities available within a Kubernetes cluster to specify as the subject of a <code>RoleBinding</code> or <code>ClusterRoleBinding</code></caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Identity</th>&#13;
<th>Description</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td>Anonymous requests without an explicitly provided identity:<br/>User: <code>system:anonymous</code><br/>Group: <code>system:unauthenticated</code></td>&#13;
<td>Provides a default identity for any incoming API requests that have not provided identity information. You can establish what (if any) default API resources are available to anonymous requests by way of Kubernetes RBAC.</td>&#13;
</tr>&#13;
<tr>&#13;
<td>All authenticated requests:<br/>User: <code>*</code><br/>Group: <code>system:authenticated</code></td>&#13;
<td>All authenticated requests will include the group <span class="keep-together"><code>system:authenticated</code></span> <a contenteditable="false" data-primary="* user for default permissions" data-primary-sortas="# user" data-type="indexterm" id="idm45358189740328"/><a contenteditable="false" data-primary="user identities" data-see="identities" data-type="indexterm" id="idm45358189738936"/>to support establishing default permissions across all users.</td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Any request using <em>Mutual Transport Layer Security</em> (mTLS) authentication by X.509 certificates:<br/>User: Common Name specified in the certificate (e.g., CN=username)<br/>Group: Organizations specified in the certificate (e.g., O=group1/O=group2)<br/>To create an example X.509 certificate:<br/>&#13;
<code><strong>$ openssl req -new -key username.pem -out username-csr.pem \</strong></code><br/>&#13;
<code><strong>-subj "/CN=username/O=group1/O=group2"</strong></code></p>&#13;
</td>&#13;
<td>Any API request made with mTLS authentication from an X.509 <a contenteditable="false" data-primary="Mutual Transport Layer Security (mTLS)" data-type="indexterm" id="idm45358189732536"/><a contenteditable="false" data-primary="mTLS (Mutual Transport Layer Security)" data-type="indexterm" id="idm45358189731384"/><a contenteditable="false" data-primary="X.509 certificates" data-type="indexterm" id="idm45358189730264"/>certificate that is signed either by the cluster Certificate Authority (CA) or the <span class="keep-together"><code>--client-ca-file=SOMEFILE</code></span> command option provided to the Kubernetes API server will authenticate as <code>username</code> with groups <code>group1</code> and <code>group2</code>.</td>&#13;
</tr>&#13;
<tr>&#13;
<td>Any request made by a service account:<br/>User: <code>system:serviceaccount:&lt;<em>namespace</em>&gt;:&lt;<em>serviceaccount</em>&gt;</code><br/>Group: <code>system:serviceaccounts, system:serviceaccounts:&lt;<em>namespace</em>&gt;</code></td>&#13;
<td>A service account is an identity created by the API server typically for <a contenteditable="false" data-primary="service accounts" data-secondary="identity" data-type="indexterm" id="idm45358189723016"/>nonhuman actors like bots or long-running jobs. Service accounts establish a token that can be used externally or internally by pods that work with the Kubernetes API server. Service accounts work like any other identity and can be referenced by Kubernetes RBAC to associate permissions.</td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Service accounts can be a simple and enticing way to set up automation <a contenteditable="false" data-primary="service accounts" data-secondary="automation against clusters via" data-type="indexterm" id="idm45358189719784"/><a contenteditable="false" data-primary="Mutual Transport Layer Security (mTLS)" data-secondary="automation against clusters via" data-type="indexterm" id="idm45358189718312"/>against your clusters, but take note that service account tokens are long lived and do not automatically rotate or expire. If you have the need for long-running automated access, consider whether to use mTLS certificates (which can have set expiration dates and are resistant to some kinds of security infiltration attacks). <a contenteditable="false" data-primary="resource requests" data-secondary="service account request performance" data-type="indexterm" id="idm45358189716440"/><a contenteditable="false" data-primary="service accounts" data-secondary="request performance bottlenecks" data-type="indexterm" id="idm45358189715048"/>Performance of requests using service accounts can also be limited, as for every request made, the kube-apiserver will make a call to itself to retrieve the service account token and validate it. At extremely high API request rates, this can become a bottleneck.</p>&#13;
</div>&#13;
<p>OpenShift simplifies the management of identity by way of <a contenteditable="false" data-primary="identity providers (IdPs)" data-type="indexterm" id="idm45358189712744"/><a contenteditable="false" data-primary="identities" data-secondary="identity providers" data-type="indexterm" id="idm45358189711768"/><a contenteditable="false" data-primary="authentication" data-secondary="identities" data-tertiary="identity providers" data-type="indexterm" id="ch07_idp"/><em>identity providers</em> (IdPs) that are configured through operators on the platform. When a user or service account authenticates, typically an IdP accepts and validates credential information for the requestor. Through command options to the Kubernetes API server, trust is established between the IdP and the Kubernetes API server for incoming requests and used for authorization in the Kubernetes API server. Later, we’ll see how the identity is then authorized using Kubernetes RBAC.</p>&#13;
<p>A number of IdPs are available in OpenShift. Here, we will discuss a subset of the most commonly used IdPs. In addition, we will review how managed Kubernetes as a Service providers associate identity in relation to the cloud provider’s <em>identity and access management</em> (IAM).</p>&#13;
<p>The <code>OAuth</code> kind in the API group config.openshift.io <a contenteditable="false" data-primary="OAuth resource" data-secondary="control over IdPs for OpenShift" data-type="indexterm" id="idm45358189705480"/><a contenteditable="false" data-primary="identities" data-secondary="identity providers" data-tertiary="OAuth control over" data-type="indexterm" id="idm45358189704040"/><a contenteditable="false" data-primary="identity providers (IdPs)" data-secondary="OAuth control over" data-type="indexterm" id="idm45358189702392"/>with version v1 is a cluster-scoped resource that allows complete control over the available IdPs for OpenShift. The canonical name for this resource is <code>cluster</code>:</p>&#13;
<pre data-type="programlisting">apiVersion: config.openshift.io/v1&#13;
kind: OAuth&#13;
metadata:&#13;
 name: cluster&#13;
spec:&#13;
 identityProviders: []&#13;
 tokenConfig:&#13;
   accessTokenMaxAgeSeconds: 86400</pre>&#13;
<p>When configured, the authentication cluster operator will respond to configure the specified IdPs or report the status if there is a problem.<a contenteditable="false" data-primary="" data-startref="ch07-autop" data-type="indexterm" id="idm45358189698824"/><a contenteditable="false" data-primary="" data-startref="ch07-autop2" data-type="indexterm" id="idm45358189697448"/><a contenteditable="false" data-primary="" data-startref="ch07-autop3" data-type="indexterm" id="idm45358189696072"/><a contenteditable="false" data-primary="" data-startref="ch07-autop4" data-type="indexterm" id="idm45358189694696"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="OpenShift htpasswd Identity Provider" data-type="sect2"><div class="sect2" id="openshift_htpasswd_identity_provider">&#13;
<h2>OpenShift htpasswd Identity Provider</h2>&#13;
<p>The <code>htpasswd</code> IdP is about as simple as it comes. The provider <a contenteditable="false" data-primary="identity providers (IdPs)" data-secondary="htpasswd" data-type="indexterm" id="idm45358189690744"/><a contenteditable="false" data-primary="authentication" data-secondary="identities" data-startref="ch07_idp" data-tertiary="identity providers" data-type="indexterm" id="idm45358189689368"/><a contenteditable="false" data-primary="identities" data-secondary="identity providers" data-tertiary="htpasswd" data-type="indexterm" id="idm45358189687448"/><a contenteditable="false" data-primary="htpasswd identity provider" data-type="indexterm" id="idm45358189685800"/><a contenteditable="false" data-primary="secrets" data-secondary="htpasswd identity provider" data-type="indexterm" id="idm45358189684680"/>establishes identities and credentials by the standard <em>.htpasswd</em> file format, loaded into a secret. If you have a cluster shared by a small group (such as a “department” or “project” scope), consider whether this approach allows you to assign and maintain identities for your needs.</p>&#13;
<p>Follow these steps to configure the <code>htpasswd</code> IdP:<a contenteditable="false" data-primary="htpasswd identity provider" data-secondary="configuring" data-type="indexterm" id="idm45358189681736"/></p>&#13;
<ol>&#13;
<li><p>Create or update an <em>.htpasswd</em> file with your users.</p></li>&#13;
<li><p>Create or update the <code>htpasswd</code> secret in openshift-config.</p></li>&#13;
<li><p>Update the <code>cluster OAuth</code> resource to list the <code>htpasswd</code> IdP and reference the <code>htpasswd</code> secret in the <code>openshift-config</code> namespace.</p></li>&#13;
</ol>&#13;
<p>Use <code>htpasswd</code> to generate or update an existing file:</p>&#13;
<pre data-type="programlisting">$ <strong>htpasswd -c -b -B .htpasswd username password</strong></pre>&#13;
<p>Then create or apply updates from this file to the secret maintained in <code>openshift-config</code>:</p>&#13;
<pre data-type="programlisting">$ <strong>oc create secret generic htpass-secret --from-file=htpasswd=.htpasswd -n </strong>&#13;
<strong>openshift-config</strong></pre>&#13;
<p>Finally, update the cluster <code>OAuth</code> API resource as follows:<a contenteditable="false" data-primary="" data-startref="ch7-concl" data-type="indexterm" id="idm45358189670200"/><a contenteditable="false" data-primary="" data-startref="ch07-conclz" data-type="indexterm" id="idm45358189668824"/></p>&#13;
<pre data-type="programlisting">apiVersion: config.openshift.io/v1&#13;
kind: OAuth&#13;
metadata:&#13;
 name: cluster&#13;
spec:&#13;
 identityProviders:&#13;
 - htpasswd:&#13;
     fileData:&#13;
       name: htpass-secret&#13;
   mappingMethod: claim&#13;
   name: htpasswdidp&#13;
   type: htpasswd&#13;
 tokenConfig:&#13;
   accessTokenMaxAgeSeconds: 86400</pre>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Policy and Compliance Across Multiple Clusters" data-type="sect1"><div class="sect1" id="policy_and_compliance_across_multiple_cl">&#13;
<h1>Policy and Compliance Across Multiple Clusters</h1>&#13;
<p>Now let’s take a look at how we can declaratively manage the desired configuration<a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="declarative management" data-type="indexterm" id="idm45358189664504"/> after the cluster is provisioned. For this topic, we will introduce additional aspects of the <a href="https://oreil.ly/3J1SW">Open Cluster Management project</a> and use RHACM, which provides a supported distribution of this open source project.</p>&#13;
<p>First, we need a way to describe policies for our cluster fleet. <a contenteditable="false" data-primary="Open Cluster Management project" data-secondary="policies assigned via" data-tertiary="Policy API" data-type="indexterm" id="idm45358189661352"/><a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="Policy API for configuration" data-type="indexterm" id="idm45358189659512"/>The Open Cluster Management project introduces the <code>Policy</code> (API group: policy.open-cluster-management.io) API to describe the desired configuration, along with rules about whether the expected configuration “must” exist or “must not” exist. Policies can also be configured to audit a cluster and report if the cluster configuration is compliant. Alternatively, policies can be enforced, ensuring that the cluster matches the desired configuration and is considered compliant as a result.</p>&#13;
<p>We will express a <code>Policy</code> resource <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="Policy resource" data-type="indexterm" id="idm45358189655736"/><a contenteditable="false" data-primary="ConfigurationPolicy" data-type="indexterm" id="idm45358189653960"/>that acts as an envelope of one or more (1..*) <span class="keep-together"><code>ConfigurationPolicies</code></span> (API group: policy.open-cluster-management.io). Each <code>ConfigurationPolicy</code> is then able to describe one or more (1..*) configurations as a rule set. The policy can be enforced as required, in which case changes are made to ensure compliance, or audit only to report compliance of the fleet (see <a data-type="xref" href="#visual_representation_of_the_policy_api">Figure 7-5</a>). Since we’re talking about Kubernetes, each policy is continuously reconciled against fleet members, even when a specific fleet member may lose connectivity to the control plane hub cluster.</p>&#13;
<figure class="width-75"><div class="figure" id="visual_representation_of_the_policy_api">&#13;
<img src="assets/hcok_0705.png"/>&#13;
<h6><span class="label">Figure 7-5. </span>Visual representation of the <code>Policy</code> API from the Open Cluster Management Project</h6>&#13;
</div></figure>&#13;
<section data-pdf-bookmark="Policy Example: Federate a Project Across Your Fleet" data-type="sect2"><div class="sect2" id="policy_example_federate_a_project_across">&#13;
<h2>Policy Example: Federate a Project Across Your Fleet</h2>&#13;
<p>Let’s start with a simple example where we want to federate a specific <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="example: federating a project across fleet" data-type="indexterm" id="idm45358189645256"/><a contenteditable="false" data-primary="fleets of cluster groups" data-secondary="federating a project across fleet examples" data-type="indexterm" id="idm45358189643480"/><a contenteditable="false" data-primary="multicluster management" data-secondary="federating project across fleet" data-type="indexterm" id="idm45358189642056"/>project across all clusters in the fleet. We have a single <code>Policy</code> envelope that contains a single <span class="keep-together"><code>ConfigurationPolicy</code></span> for our desired project, named <code>frontend-app-prod</code>, along with a required <code>LimitRange</code> configuration to ensure that all pods deployed in our project describe memory requests. This example shows a <code>Policy</code> envelope with nested configurations for a project and a <code>LimitRange</code> for that project:</p>&#13;
<pre data-type="programlisting">apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 name: policy-project-frontend-app-prod&#13;
 namespace: open-cluster-management-policies&#13;
 annotations:&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline configuration&#13;
spec:&#13;
 disabled: false&#13;
 remediationAction: enforce&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: policy-project-frontend-app-prod&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
         - frontend-app-prod&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: project.openshift.io/v1&#13;
           kind: Project&#13;
           metadata:&#13;
             name: frontend-app-prod&#13;
         status:&#13;
           Validity: {}&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: v1&#13;
           kind: LimitRange&#13;
           metadata:&#13;
             name: mem-limit-range&#13;
             namespace: frontend-app-prod&#13;
           spec:&#13;
             limits:&#13;
               - default:&#13;
                   memory: 512Mi&#13;
                 defaultRequest:&#13;
                   memory: 256Mi&#13;
                 type: Container&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: PlacementBinding&#13;
metadata:&#13;
 name: binding-policy-project-frontend-app-prod&#13;
 namespace: open-cluster-management-policies&#13;
placementRef:&#13;
 name: production-clusters&#13;
 kind: PlacementRule&#13;
 apiGroup: apps.open-cluster-management.io&#13;
subjects:&#13;
- name: policy-project-frontend-app-prod&#13;
 kind: Policy&#13;
 apiGroup: policy.open-cluster-management.io&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: placement-policy-project-frontend-app-prod&#13;
 namespace: open-cluster-management-policies&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
   type: ManagedClusterConditionAvailable&#13;
 clusterSelector:&#13;
   matchExpressions: []</pre>&#13;
<p>Take note of the following important aspects of the previous example:</p>&#13;
<ul>&#13;
<li><p>The <code>Policy</code> describes categorization that allows an auditor <a contenteditable="false" data-primary="debugging" data-secondary="noncompliant technical controls" data-type="indexterm" id="idm45358189632376"/><a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="auditing policy enforcement" data-type="indexterm" id="idm45358189630984"/>to quickly understand which technical controls are noncompliant if the given <code>Policy</code> cannot be properly enforced.</p></li>&#13;
<li><p>The <code>ConfigurationPolicy</code> specifies namespaces that are <a contenteditable="false" data-primary="namespaces" data-secondary="ConfigurationPolicy governing" data-type="indexterm" id="idm45358189627368"/>governed by the <span class="keep-together"><code>ConfigurationPolicy</code></span> and can optionally specify an overriding value for <span class="keep-together"><code>remediationAction</code></span> if the list of <code>ConfigurationPolicies</code> within a single envelope has a mix of enforcement and auditing behavior. Further, the severity of a given <code>ConfigurationPolicy</code> can provide hints to an operator on how to prioritize compliance remediation if violations are detected.</p></li>&#13;
<li><p>The <code>object-templates</code> specify a desired template (either an entire API object or a patch) and a <code>complianceType</code> rule of whether the desired configuration should exist, should not exist, or should be unique.</p></li>&#13;
<li><p>The <code>PlacementRule</code> describes how to match a <code>Policy</code> against a set of available clusters. We’ll talk more about how these work shortly.</p></li>&#13;
<li><p>The <code>PlacementBinding</code> links the <code>Policy</code> to a <code>PlacementRule</code>. A given <code>Policy</code> can be bound to multiple <code>PlacementRule</code>s. Each <code>PlacementRule</code> can match zero or more clusters.</p></li>&#13;
</ul>&#13;
<p>The annotations applied to the <code>Policy</code> allow users to organize violations according to their own technical standards. In the previous example, <a contenteditable="false" data-primary="National Institute of Standards and Technology Cybersecurity Framework (NIST-CSF)" data-type="indexterm" id="idm45358189615912"/><a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="National Institute of Standards and Technology Cybersecurity Framework" data-type="indexterm" id="idm45358189614840"/><a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="noncompliance" data-type="indexterm" id="idm45358189612952"/>the annotations link this <code>Policy</code> to the National Institute of Standards and Technology Cybersecurity Framework (NIST-CSF) standard under the “Information Protection Processes and Procedures” category as part of the “Baseline configuration” controls:</p>&#13;
<pre data-type="programlisting">policy.open-cluster-management.io/standards: NIST-CSF&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline configuration</pre>&#13;
<p>An <a href="https://oreil.ly/hdqzx">open library for policies</a> that are categorized according to the NIST-053 is <span class="keep-together">available</span>.<a contenteditable="false" data-primary="policies" data-secondary="open library for policies" data-type="indexterm" id="idm45358189607704"/><a contenteditable="false" data-primary="online resources" data-secondary="policies open library" data-type="indexterm" id="idm45358189606232"/></p>&#13;
<p>The <code>complianceType</code> allows you to define how the<a contenteditable="false" data-primary="complianceType" data-type="indexterm" id="idm45358189603992"/> <code>Policy</code> engine will address a noncompliant rule:</p>&#13;
<pre data-type="programlisting">object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: project.openshift.io/v1&#13;
           kind: Project&#13;
           metadata:&#13;
             name: frontend-app-prod&#13;
         status:&#13;
           Validity: {}</pre>&#13;
<p>The enumerated values for <code>complianceType</code> include:</p>&#13;
<dl>&#13;
<dt><code>musthave</code></dt>&#13;
<dd>More than one of the desired configurations may exist, but at least <a contenteditable="false" data-primary="policies" data-secondary="musthave, mustonlyhave, mustnothave" data-type="indexterm" id="idm45358189598616"/><a contenteditable="false" data-primary="musthave complianceType" data-type="indexterm" id="idm45358189597192"/>one of the desired configurations must match the given <code>objectDefinition</code> as specified. When the <code>remediationAction</code> is <code>enforce</code>, the relevant <code>objectDefinition</code>s will be applied to the cluster. If the <code>objectDefinition</code>s cannot be applied, the status of the policy will note the ongoing violation.</dd>&#13;
<dt><code>mustonlyhave</code></dt>&#13;
<dd>Exactly one matching configuration must be present and no more. <a contenteditable="false" data-primary="mustonlyhave complianceType" data-type="indexterm" id="idm45358189592728"/>This is particularly useful when expressing desired configurations for RBAC or security context constraints. If more than one applicable configuration is found, then the status of the policy will note the ongoing violation.</dd>&#13;
<dt><code>mustnothave</code></dt>&#13;
<dd>If a matching configuration is found that conforms to the <a contenteditable="false" data-primary="mustnothave complianceType" data-type="indexterm" id="idm45358189590168"/><code>objectDefinition</code>, then a violation has been detected. If a violation is found, then the status of the policy will note the ongoing violation.</dd>&#13;
</dl>&#13;
</div></section>&#13;
<section data-pdf-bookmark="PlacementRules to Assign Content to ManagedClusters" data-type="sect2"><div class="sect2" id="placementrules_to_assign_content_to_mana">&#13;
<h2>PlacementRules to Assign Content to ManagedClusters</h2>&#13;
<p><code>PlacementRule</code>s (API group: apps.open-cluster-management.io) are a <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="PlacementRules for cluster matching" data-type="indexterm" id="idm45358189585896"/><a contenteditable="false" data-primary="PlacementRules" data-secondary="cluster matched to configurations" data-type="indexterm" id="idm45358189584184"/><a contenteditable="false" data-primary="ManagedCluster API object" data-secondary="PlacementRules" data-type="indexterm" id="idm45358189582840"/><a contenteditable="false" data-primary="labels to organize resources" data-secondary="selecting a cluster" data-type="indexterm" id="idm45358189581448"/><a contenteditable="false" data-primary="matchLabels field in YAML files" data-type="indexterm" id="idm45358189580056"/>powerful mechanism used by Open Cluster Management to assign content to clusters. Each <code>PlacementRule</code> allows you to specify a <code>clusterSelector</code> and <code>clusterConditions</code> to match available <code>ManagedCluster</code>s. Remember that <code>ManagedCluster</code>s define labels like any Kubernetes object. The <code>clusterSelector</code> defines a combination of <code>match​Exp⁠res⁠sion</code> or <code>matchLabel</code>s that is evaluated against available <code>ManagedCluster</code>s.</p>&#13;
<p>The <code>matchExpression</code>s clause can use binary <a contenteditable="false" data-primary="matchExpressions field in YAML files" data-type="indexterm" id="idm45358189573768"/><a contenteditable="false" data-primary="YAML files" data-secondary="matchLabels field" data-type="indexterm" id="idm45358189572616"/><a contenteditable="false" data-primary="YAML files" data-secondary="matchExpression field" data-type="indexterm" id="idm45358189571240"/><a contenteditable="false" data-primary="binary operators" data-type="indexterm" id="idm45358189569864"/><a contenteditable="false" data-primary="NotIn binary operator" data-type="indexterm" id="idm45358189568760"/><a contenteditable="false" data-primary="In binary operator" data-type="indexterm" id="idm45358189567656"/><a contenteditable="false" data-primary="Exists binary operator" data-type="indexterm" id="idm45358189566552"/>operators defined by Kubernetes, including <code>In</code>, <code>NotIn</code>, and <code>Exists</code>. For example, if you wanted to match a <code>Managed​Clus⁠ter</code> with the labels <code>apps/pacman=deployed</code>, <code>region=us-east</code>, <span class="keep-together"><code>env=development</code></span>, and <code>authenticationProfile=htpasswd</code>, the following combination would work:</p>&#13;
<pre data-type="programlisting">matchLabels:&#13;
    apps/pacman: deployed&#13;
  matchExpressions:&#13;
    - {key: region, operator: In, values: [us-east, us-west]}&#13;
    - {key: env, operator: NotIn, values: [development]}&#13;
    - {key: authenticationProfile, operator: Exists}</pre>&#13;
<p>In addition to the <code>clusterSelector</code>, <code>PlacementRule</code>s can evaluate the conditions that a <code>ManagedCluster</code> has reported in its status. Three status conditions are available as of the time of this writing that can be used:</p>&#13;
<dl>&#13;
<dt><code>HubAcceptedManagedCluster</code></dt>&#13;
<dd>Indicates that the <code>ManagedCluster</code> was allowed to join by a cluster-manager-admin user<a contenteditable="false" data-primary="HubAcceptedManagedCluster status" data-type="indexterm" id="idm45358189556696"/></dd>&#13;
<dt><code>ManagedClusterConditionAvailable</code></dt>&#13;
<dd>Indicates that the <code>ManagedCluster</code> has updated its lease reservation within the required period of time and is considered available by the hub<a contenteditable="false" data-primary="ManagedClusterConditionAvailable status" data-type="indexterm" id="idm45358189554040"/></dd>&#13;
<dt><code>ManagedClusterJoined</code></dt>&#13;
<dd>Indicates that the <code>ManagedCluster</code> has completed the registration protocol and has established a valid connection with the control plane hub cluster<a contenteditable="false" data-primary="ManagedClusterJoined status" data-type="indexterm" id="idm45358189551304"/></dd>&#13;
</dl>&#13;
<p>RBAC is important to consider when creating a <code>PlacementRule</code>. <a contenteditable="false" data-primary="role-based access control (RBAC)" data-secondary="PlacementRule creation" data-type="indexterm" id="idm45358189549064"/><a contenteditable="false" data-primary="access control" data-secondary="role-based" data-tertiary="PlacementRule creation" data-type="indexterm" id="idm45358189547688"/>When a user creates a <code>PlacementRule</code>, only <code>ManagedCluster</code>s that the user has access to are available to be evaluated for set inclusion. Hence, the user or role that is responsible for defining <code>PlacementRule</code>s has a lot of control over how broadly content can be applied to the entire fleet.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Policy Example: Managing etcd Encryption Within ManagedClusters" data-type="sect2"><div class="sect2" id="policy_example_managing_etcd_encryption">&#13;
<h2>Policy Example: Managing etcd Encryption Within ManagedClusters</h2>&#13;
<p>The <code>APIServer</code> kind within an OpenShift cluster can be configured to <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="example: managing etcd encryption" data-type="indexterm" id="idm45358189541944"/><a contenteditable="false" data-primary="etcd" data-secondary="policy example: managing encryption" data-type="indexterm" id="idm45358189540200"/><a contenteditable="false" data-primary="encryption of etcd policy example" data-type="indexterm" id="idm45358189538856"/>encrypt any sensitive resources, including secrets, <code>ConfigMaps</code>, routes, OAuth access tokens, and OAuth authorization tokens. To ensure that all clusters in the fleet use encryption, we can define a policy that will update all cluster <code>APIServer</code> kinds to specify what type of encryption should be used. Using <code>PlacementRules</code>, we can match this policy against our desired clusters.</p>&#13;
<p>In the following example, we see a policy that is ensuring that the <code>APIServer</code> CRD, managed by OpenShift, is configured to enforce application-level encryption of sensitive data within etcd:</p>&#13;
<pre data-type="programlisting">apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 name: policy-etcdencryption&#13;
 annotations:&#13;
   policy.open-cluster-management.io/standards: NIST SP 800-53&#13;
   policy.open-cluster-management.io/categories: CM Configuration Management&#13;
   policy.open-cluster-management.io/controls: CM-2 Baseline Configuration&#13;
spec:&#13;
 remediationAction: enforce&#13;
 disabled: false&#13;
 policy-templates:&#13;
   - objectDefinition:&#13;
       apiVersion: policy.open-cluster-management.io/v1&#13;
       kind: ConfigurationPolicy&#13;
       metadata:&#13;
         name: enable-etcd-encryption&#13;
       spec:&#13;
         severity: high&#13;
         namespaceSelector:&#13;
           exclude:&#13;
             - kube-*&#13;
           include:&#13;
             - default&#13;
         object-templates:&#13;
           - complianceType: musthave&#13;
             objectDefinition:&#13;
               apiVersion: config.openshift.io/v1&#13;
               kind: APIServer&#13;
               metadata:&#13;
                 name: cluster&#13;
               spec:&#13;
                 encryption:&#13;
                   type: aescbc&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: PlacementBinding&#13;
metadata:&#13;
 name: binding-policy-etcdencryption&#13;
placementRef:&#13;
 name: placement-policy-etcdencryption&#13;
 kind: PlacementRule&#13;
 apiGroup: apps.open-cluster-management.io&#13;
subjects:&#13;
- name: policy-etcdencryption&#13;
 kind: Policy&#13;
 apiGroup: policy.open-cluster-management.io&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: placement-policy-etcdencryption&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
   type: ManagedClusterConditionAvailable&#13;
 clusterSelector:&#13;
   matchExpressions:&#13;
     - {key: environment, operator: In, values: ["dev", “prod”]}</pre>&#13;
<p>After applying this policy, any <code>ManagedCluster</code> with the label <code>environment=dev</code> or <code>environment=prod</code> will be updated to configure the <code>APIServer</code> (API group: config.openshift.io) to use encryption for any sensitive resources. You can connect directly to the managed cluster to review the progress of encrypting the etcd datastore as described in the OpenShift product documentation. In this example, the command is retrieving status conditions from the <code>OpenShiftAPIServer</code> object and printing information about the current progress:</p>&#13;
<pre data-type="programlisting">$ <strong>oc get openshiftapiserver \&#13;
  -o=jsonpath='{range.items[0].status.conditions[?(@.type=="Encrypted")]}&#13;
  {.reason}{"\n"}{.message}{"\n"}'</strong>&#13;
 &#13;
EncryptionInProgress&#13;
Resource routes.route.openshift.io is not encrypted&#13;
…&#13;
 &#13;
$ <strong>oc get openshiftapiserver \&#13;
  -o=jsonpath='{range.items[0].status.conditions[?(@.type=="Encrypted")]}&#13;
  {.reason}{"\n"}{.message}{"\n"}'</strong>&#13;
 &#13;
EncryptionInProgress&#13;
Resource routes.route.openshift.io is being encrypted&#13;
…&#13;
$ <strong>oc get openshiftapiserver \&#13;
  -o=jsonpath='{range.items[0].status.conditions[?(@.type=="Encrypted")]}&#13;
  {.reason}{"\n"}{.message}{"\n"}'</strong>&#13;
 &#13;
EncryptionCompleted&#13;
All resources encrypted: routes.route.openshift.io, &#13;
oauthaccesstokens.oauth.openshift.io, oauthauthorizetokens.oauth.openshift.io</pre>&#13;
<p>If you simply wanted to audit for whether sensitive resources managed in the etcd datastore were encrypted, you can change the <code>remediationAction</code> to <code>inform</code>, and the policy will report any <code>ManagedCluster</code> that is not encrypting its API state as <span class="keep-together">noncompliant</span>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Policy Example: Managing RBAC Within ManagedClusters" data-type="sect2"><div class="sect2" id="policy_example_managing_rbac_within_mana">&#13;
<h2>Policy Example: Managing RBAC Within ManagedClusters</h2>&#13;
<p>One of the benefits of OpenShift’s consistency is that you have the <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="example: managing role-based access control" data-type="indexterm" id="ch07-rbac"/><a contenteditable="false" data-primary="role-based access control (RBAC)" data-secondary="policy example: managing" data-type="indexterm" id="ch07-rbac2"/><a contenteditable="false" data-primary="ClusterRole object" data-type="indexterm" id="idm45358189518344"/><a contenteditable="false" data-primary="access control" data-secondary="role-based" data-tertiary="example: managing" data-type="indexterm" id="ch07-rbac3"/>same set of <span class="keep-together"><code>ClusterRole</code>s</span> and roles out of the box across all platforms. You may choose to add additional roles to meet your specific organization’s needs.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6> &#13;
<p>Of course, modifying the out-of-the-box roles is strongly discouraged because that can be reverted or prevent successful upgrades as you adopt maintenance updates. That said, you can always create new <code>ClusterRole</code>s or roles to meet your organization’s standards.</p>&#13;
</div>&#13;
<p>The following example defines two roles: <code>developer-read</code> and <code>developer-write</code>. These are cluster-scoped roles. <a contenteditable="false" data-primary="ClusterRoleBinding" data-type="indexterm" id="idm45358189510600"/>If you create a <code>ClusterRoleBinding</code> to a user or group, then you are granting cluster-wide access to the resources enumerated in the <code>ClusterRole</code>. If you create a <code>RoleBinding</code> within a specific namespace or project, then the user will have access to the enumerated resources only within that specific project.</p>&#13;
<p>In the following code example, a policy with category <code>AC Access Control</code> under the control <code>AC-3 Access Enforcement</code> is defined that will ensure that any matching cluster has the <code>ClusterRole</code>s specified within object-templates defined:</p>&#13;
<pre data-type="programlisting">apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 name: policy-role-developer&#13;
 annotations:&#13;
   policy.open-cluster-management.io/standards: NIST SP 800-53&#13;
   policy.open-cluster-management.io/categories: AC Access Control&#13;
   policy.open-cluster-management.io/controls: AC-3 Access Enforcement&#13;
spec:&#13;
 remediationAction: enforce&#13;
 disabled: false&#13;
 policy-templates:&#13;
   - objectDefinition:&#13;
       apiVersion: policy.open-cluster-management.io/v1&#13;
       kind: ConfigurationPolicy&#13;
       metadata:&#13;
         name: policy-role-developer&#13;
       spec:&#13;
         remediationAction: enforce &#13;
          severity: high&#13;
         namespaceSelector:&#13;
           exclude: ["kube-*"]&#13;
           include: ["default"]&#13;
         object-templates:&#13;
           - complianceType: mustonlyhave # role definition should exact match&#13;
             objectDefinition:&#13;
               apiVersion: rbac.authorization.k8s.io/v1&#13;
               kind: ClusterRole&#13;
               metadata:&#13;
                 name: developer-read&#13;
               rules:&#13;
                 - apiGroups: ["*"]&#13;
                   resources: ["deployments", "configmaps", "services", "secrets"]&#13;
                   verbs: ["get", "list", "watch"]&#13;
           - complianceType: mustonlyhave # role definition should exact match&#13;
             objectDefinition:&#13;
               apiVersion: rbac.authorization.k8s.io/v1&#13;
               kind: ClusterRole&#13;
               metadata:&#13;
                 name: developer-write&#13;
               rules:&#13;
                 - apiGroups: ["*"]&#13;
                   resources: ["deployments", "configmaps", "services", "secrets"]&#13;
                   verbs: ["create", "delete", "patch", "update"]&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: PlacementBinding&#13;
metadata:&#13;
 name: binding-policy-role-developer&#13;
placementRef:&#13;
 name: placement-policy-role-developer&#13;
 kind: PlacementRule&#13;
 apiGroup: apps.open-cluster-management.io&#13;
subjects:&#13;
- name: policy-role-developer&#13;
 kind: Policy&#13;
 apiGroup: policy.open-cluster-management.io&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: placement-policy-role-developer&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
   type: ManagedClusterConditionAvailable&#13;
 clusterSelector:&#13;
   matchExpressions:&#13;
     - {key: environment, operator: In, values: ["dev"]}</pre>&#13;
<p>If you think about how your security protocols provide for separation of duty, you can follow the same pattern to enforce your anticipated permissions for each logical security role provided for your users.</p>&#13;
<p>A second policy ensures that any project named <code>pacman-app</code> contains a <code>RoleBinding</code> for a specific group named game-developers. The group would be defined by your IdP for the cluster (e.g., a Lightweight Directory Access Protocol service). This shows an example to configure the user group game-developers to have a binding to the <code>ClusterRole</code>s <code>developer-read</code> and <code>developer-write</code>:</p>&#13;
<pre data-type="programlisting">apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 name: policy-role-developer-binding&#13;
 annotations:&#13;
   policy.open-cluster-management.io/standards: NIST SP 800-53&#13;
   policy.open-cluster-management.io/categories: AC Access Control&#13;
   policy.open-cluster-management.io/controls: AC-3 Access Enforcement&#13;
spec:&#13;
 remediationAction: enforce&#13;
 disabled: false&#13;
 policy-templates:&#13;
   - objectDefinition:&#13;
       apiVersion: policy.open-cluster-management.io/v1&#13;
       kind: ConfigurationPolicy&#13;
       metadata:&#13;
         name: policy-role-developer-binding&#13;
       spec:&#13;
         remediationAction: enforce &#13;
         severity: high&#13;
         namespaceSelector:&#13;
           exclude: ["kube-*"]&#13;
           include: ["default"]&#13;
         object-templates:&#13;
           - complianceType: mustonlyhave # role definition should exact match&#13;
             objectDefinition:&#13;
               apiVersion: rbac.authorization.k8s.io/v1&#13;
               kind: RoleBinding&#13;
               metadata:&#13;
                 name: role-developer-read-binding&#13;
                 namespace: game-app&#13;
               roleRef:&#13;
                 apiGroup: rbac.authorization.k8s.io&#13;
                 kind: ClusterRole&#13;
                 name: developer-read&#13;
               subjects:&#13;
               - apiGroup: rbac.authorization.k8s.io&#13;
                 kind: Group&#13;
                 name: "game-developers"&#13;
           - complianceType: mustonlyhave # role definition should exact match&#13;
             objectDefinition:&#13;
               apiVersion: rbac.authorization.k8s.io/v1&#13;
               kind: RoleBinding&#13;
               metadata:&#13;
                 name: role-developer-read-binding&#13;
                 namespace: game-app&#13;
               roleRef:&#13;
                 apiGroup: rbac.authorization.k8s.io&#13;
                 kind: ClusterRole&#13;
                 name: developer-write&#13;
               subjects:&#13;
               - apiGroup: rbac.authorization.k8s.io&#13;
                 kind: Group&#13;
                 name: "game-developers"&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: PlacementBinding&#13;
metadata:&#13;
 name: binding-policy-role-developer-binding&#13;
placementRef:&#13;
 name: placement-policy-role-developer-binding&#13;
 kind: PlacementRule&#13;
 apiGroup: apps.open-cluster-management.io&#13;
subjects:&#13;
- name: policy-role-developer-binding&#13;
 kind: Policy&#13;
 apiGroup: policy.open-cluster-management.io&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: placement-policy-role-developer-binding&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
   type: ManagedClusterConditionAvailable&#13;
 clusterSelector:&#13;
   matchExpressions:&#13;
     - {key: environment, operator: In, values: ["dev"]}</pre>&#13;
<p>If either policy in these two samples were to be in violation, then you would have direct feedback that your access control protocols may be incorrectly configured, either granting certain users too much control or other users not enough control.<a contenteditable="false" data-primary="" data-startref="ch07-rbac" data-type="indexterm" id="idm45358189495544"/><a contenteditable="false" data-primary="" data-startref="ch07-rbac2" data-type="indexterm" id="idm45358189494072"/><a contenteditable="false" data-primary="" data-startref="ch07-rbac3" data-type="indexterm" id="idm45358189492664"/></p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Policy Example: Managing IdPs Within ManagedClusters" data-type="sect2"><div class="sect2" id="policy_example_managing_idps_within_mana">&#13;
<h2>Policy Example: Managing IdPs Within ManagedClusters</h2>&#13;
<p>For every cluster that you provision, you are going to want to define <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="example: managing identity providers" data-type="indexterm" id="idm45358189489224"/><a contenteditable="false" data-primary="identities" data-secondary="identity providers" data-tertiary="example: managing" data-type="indexterm" id="idm45358189487432"/><a contenteditable="false" data-primary="authentication" data-secondary="identities" data-tertiary="identity providers" data-type="indexterm" id="idm45358189485784"/><a contenteditable="false" data-primary="identity providers (IdPs)" data-secondary="example: managing" data-type="indexterm" id="idm45358189484136"/><a contenteditable="false" data-primary="htpasswd identity provider" data-secondary="example: managing identity providers" data-type="indexterm" id="idm45358189482792"/><a contenteditable="false" data-primary="secrets" data-secondary="htpasswd identity provider" data-tertiary="example: managing identity providers" data-type="indexterm" id="idm45358189481480"/>how the cluster recognizes identities. OpenShift has an authentication operator that allows you to define how the cluster validates an identity provided by user credentials. One of the most straightforward IdPs uses an <code>htpasswd</code>-formatted credential file, defined by a secret in a particular namespace (<code>openshift-config</code>). Here we define a policy that configures the <code>OAuth</code> API object on the cluster to respect an IdP of type <code>htpasswd</code>:</p>&#13;
<pre data-type="programlisting">apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 name: policy-htpasswd-auth-provider&#13;
 namespace: open-cluster-management-policies&#13;
 annotations:&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline configuration&#13;
spec:&#13;
 complianceType: mustonlyhave&#13;
 remediationAction: enforce&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: policy-htpasswd-auth-provider&#13;
     spec:&#13;
       object-templates:&#13;
       - complianceType: mustonlyhave&#13;
         objectDefinition:&#13;
           apiVersion: config.openshift.io/v1&#13;
           kind: OAuth&#13;
           metadata:&#13;
             name: cluster&#13;
           spec:&#13;
             identityProviders:&#13;
             - htpasswd:&#13;
                 fileData:&#13;
                   name: htpass-secret&#13;
               mappingMethod: claim&#13;
               name: htpasswdidp&#13;
               type: htpasswd&#13;
             tokenConfig:&#13;
               accessTokenMaxAgeSeconds: 7776000&#13;
       - complianceType: mustonlyhave&#13;
         objectDefinition:&#13;
           apiVersion: v1&#13;
           data:&#13;
             htpasswd: ""&#13;
           kind: Secret&#13;
           metadata:&#13;
             name: htpass-secret&#13;
             namespace: openshift-config&#13;
           type: Opaque&#13;
       # - complianceType: musthave&#13;
       #   objectDefinition:&#13;
       #     kind: Identity&#13;
       #     apiVersion: user.openshift.io/v1&#13;
       #     metadata:&#13;
       #       name: 'htpassidp:johndoe'&#13;
       #     providerName: htpassidp&#13;
       #     providerUserName: johndoe&#13;
       #     user:&#13;
       #       name: johndoe&#13;
       #       uid: e4d768dd-a6b5-489c-8900-2c18a160d76f&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: PlacementBinding&#13;
metadata:&#13;
 name: binding-policy-htpasswd-auth-provider&#13;
 namespace: open-cluster-management-policies&#13;
placementRef:&#13;
 name: placement-policy-oauth-provider&#13;
 kind: PlacementRule&#13;
 apiGroup: apps.open-cluster-management.io&#13;
subjects:&#13;
- name: policy-htpasswd-auth-provider&#13;
 kind: Policy&#13;
 apiGroup: policy.open-cluster-management.io&#13;
---&#13;
apiVersion: apps.open-cluster-management.io/v1&#13;
kind: PlacementRule&#13;
metadata:&#13;
 name: placement-policy-oauth-provider&#13;
 namespace: open-cluster-management-policies&#13;
spec:&#13;
 clusterConditions:&#13;
 - status: "True"&#13;
   type: ManagedClusterConditionAvailable&#13;
 clusterSelector:&#13;
   matchExpressions:&#13;
   - key: authenticationProfile&#13;
     operator: In&#13;
     values:&#13;
     - htpasswd&#13;
   matchLabels: {}</pre>&#13;
<p>In the previous example, we see that the following rules are enforced:</p>&#13;
<ul>&#13;
<li><p>The <code>OAuth</code> API kind is configured by this policy to use a custom identity <span class="keep-together">provider</span>.<a contenteditable="false" data-primary="OAuth resource" data-secondary="example: managing identity providers" data-type="indexterm" id="idm45358189471624"/></p></li>&#13;
<li><p>The <code>htpasswd</code> contents must be updated for this policy to be valid.</p></li>&#13;
</ul>&#13;
<p>To create the contents of the <code>htpasswd</code> secret, use the <code>htpasswd</code> command:<a contenteditable="false" data-primary="htpasswd identity provider" data-secondary="htpasswd secret contents creation" data-type="indexterm" id="idm45358189467736"/><a contenteditable="false" data-primary="secrets" data-secondary="HTPasswd identity provider" data-tertiary="creating secrets content" data-type="indexterm" id="idm45358189466296"/></p>&#13;
<pre data-type="programlisting">$ <strong>touch htpasswd.txt</strong>&#13;
htpasswd -b -B htpasswd.txt username password</pre>&#13;
<p>Then you can create a secret from the generated file to specify as another <code>object​Defi⁠nition</code> in the policy.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Policy Example: Managing Upgrades with Policy Across ManagedClusters" data-type="sect2"><div class="sect2" id="policy_example_managing_upgrades_with_po">&#13;
<h2>Policy Example: Managing Upgrades with Policy Across ManagedClusters</h2>&#13;
<p>Keeping your cluster fleet up to date will undoubtedly be a core <a contenteditable="false" data-primary="policies" data-secondary="policy and compliance across multiple clusters" data-tertiary="example: managing upgrades with policy" data-type="indexterm" id="ch07-pamc"/><a contenteditable="false" data-primary="upgrades" data-secondary="example: managing upgrades with policy" data-type="indexterm" id="ch07-pamc2"/><a contenteditable="false" data-primary="security" data-secondary="example: upgrades managed with policy" data-type="indexterm" id="ch07-pamc3"/><a contenteditable="false" data-primary="fleets of cluster groups" data-secondary="example: upgrades managed with policy" data-type="indexterm" id="ch07-pamc4"/>concern for any enterprise that is serious about security and maintenance updates. That’s the kind of sentence that just writes itself—of course you want to keep your fleet up to date, and we’re going to look at how you can do this with policies across the entire fleet.</p>&#13;
<p>As we have already discussed in this chapter, the OpenShift Container Platform encapsulates a substantial amount of in-cluster life-cycle management to upgrade the cluster in the CVO. We’ve also seen how RHACM can invoke upgrades for one or more clusters from the web console.</p>&#13;
<p>How do we accomplish the same thing declaratively? As you’ve seen, policies are going to play a big role here. To declaratively manage cluster versions, we need to specify the desired state of the <code>ClusterVersion</code> API object on a <code>ManagedCluster</code>.  Here is an example policy that does exactly that:</p>&#13;
<pre data-type="programlisting"># Upgrade Policy to select known desired version from public connected registry&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 annotations:&#13;
   policy.open-cluster-management.io/categories: CM Configuration Management&#13;
   policy.open-cluster-management.io/controls: CM-2 Baseline Configuration&#13;
   policy.open-cluster-management.io/standards: NIST SP 800-53&#13;
 name: upgrade-cluster&#13;
 namespace: upgrade-policies&#13;
spec:&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: upgrade-cluster&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - '*'&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: config.openshift.io/v1&#13;
           kind: ClusterVersion&#13;
           metadata:&#13;
             name: version&#13;
           spec:&#13;
             desiredUpdate:&#13;
               force: false&#13;
               image: ""&#13;
               version: 4.5.9&#13;
       remediationAction: enforce&#13;
       severity: high&#13;
 remediationAction: enforce&#13;
status: &#13;
 # Note that the associated PlacementRules are omitted for this example&#13;
 placement:&#13;
 - placementBinding: binding-upgrade-cluster&#13;
   placementRule: placement-upgrade-cluster&#13;
 status:&#13;
 - clustername: east1&#13;
   clusternamespace: east1&#13;
   compliant: Compliant</pre>&#13;
<p>The heart of this policy is the specification of the <code>ClusterVersion</code>:</p>&#13;
<pre data-type="programlisting">apiVersion: config.openshift.io/v1&#13;
kind: ClusterVersion&#13;
metadata:&#13;
  name: version&#13;
spec:&#13;
  desiredUpdate:&#13;
    force: false&#13;
    image: ""&#13;
    version: 4.5.9</pre>&#13;
<p>Here, the desired version (4.5.9) must be available from the associated channel that was configured for the cluster. Additionally, the assumption is that this cluster used the connected install method and remains connected to the public image registries for OpenShift.</p>&#13;
<p>What if your cluster cannot be created from the connected install method? OpenShift makes a disconnected install method available. <a contenteditable="false" data-primary="image registries" data-secondary="configuring disconnected image registry" data-type="indexterm" id="idm45358189445624"/>To configure a disconnected image registry, please refer to the <a href="https://oreil.ly/rodMz">product documentation</a>. The next example assumes that you have already configured a disconnected registry and that all clusters in the fleet are able to consume images from that registry.</p>&#13;
<p>For an OpenShift cluster to consume updates from a disconnected registry, the cluster must have a running instance of the OSUS operator deployed and configured. We can define a policy to configure this operator for any cluster in the fleet as follows:</p>&#13;
<pre data-type="programlisting"># Configure the OpenShift Update Service (OSUS) also known informally as &#13;
"Cincinnati".&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 annotations:&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes&#13;
     and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline Configuration&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
 name: policy-cincinatti-operator&#13;
 namespace: upgrade-policies&#13;
spec:&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: cincinatti-policy-prod&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
         apiVersion: cincinnati.openshift.io/v1beta1&#13;
         kind: Cincinnati&#13;
         metadata:&#13;
           name: example-cincinnati&#13;
         spec:&#13;
           registry: quay.io&#13;
           replicas: 1&#13;
           repository: openshift-release-dev/ocp-release&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: cincinatti-policy-subscription&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: operators.coreos.com/v1alpha1&#13;
           kind: Subscription&#13;
           metadata:&#13;
             name: cincinnati-subscription&#13;
             namespace: cincinnati-operator&#13;
           spec:&#13;
             channel: alpha&#13;
             installPlanApproval: Automatic&#13;
             name: cincinnati-operator&#13;
             source: redhat-operators&#13;
             sourceNamespace: openshift-marketplace&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: cincinatti-policy-operatorgroup&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: operators.coreos.com/v1&#13;
           kind: OperatorGroup&#13;
           metadata:&#13;
             name: cincinnati-operatorgroup&#13;
             namespace: cincinnati-operator&#13;
           spec:&#13;
             targetNamespaces:&#13;
             - cincinnati-operator&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 remediationAction: enforce&#13;
status:&#13;
 placement:&#13;
 - placementBinding: binding-policy-cincinatti-operator&#13;
   placementRule: placement-policy-cincinatti-operator&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 annotations:&#13;
   policy.open-cluster-management.io/categories: &#13;
     PR.IP Information Protection &#13;
Processes&#13;
     and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline Configuration&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
 name: policy-config-imageconfig&#13;
 namespace: upgrade-policies&#13;
spec:&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: policy-config-imageconfig-prod&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: config.openshift.io/v1&#13;
           kind: Image&#13;
           metadata:&#13;
             name: cluster&#13;
           spec:&#13;
             additionalTrustedCA:&#13;
               name: trusted-ca&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 remediationAction: enforce&#13;
status:&#13;
 placement:&#13;
 - placementBinding: binding-policy-config-imageconfig&#13;
   placementRule: placement-policy-config-imageconfig&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 annotations:&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes&#13;
     and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline Configuration&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
 name: policy-configmap-ca&#13;
 namespace: upgrade-policies&#13;
spec:&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: configmapca&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: v1&#13;
           data:&#13;
             cincinnati-registry: |-&#13;
               -----BEGIN CERTIFICATE-----&#13;
               YOUR_DISCONNECTED_REGISTRY_CERTIFICATE&#13;
               -----END CERTIFICATE-----&#13;
           kind: ConfigMap&#13;
           metadata:&#13;
             name: trusted-ca&#13;
             namespace: openshift-config&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 remediationAction: enforce&#13;
status:&#13;
 placement:&#13;
 - placementBinding: binding-policy-configmap-ca&#13;
   placementRule: placement-policy-configmap-ca&#13;
---&#13;
apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 annotations:&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes&#13;
     and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline Configuration&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
 name: policy-namespace-operatorgroup&#13;
 namespace: upgrade-policies&#13;
spec:&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: policy-namespace-operatorgroup-prod&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
       - complianceType: musthave&#13;
         objectDefinition:&#13;
           apiVersion: v1&#13;
           kind: Namespace&#13;
           metadata:&#13;
             name: cincinnati-operator&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 remediationAction: enforce&#13;
status:&#13;
 placement:&#13;
 - placementBinding: binding-policy-namespace-operatorgroup&#13;
   placementRule: placement-policy-namespace-operatorgroup&#13;
# END Policies for cincinnati-operator</pre>&#13;
<p>The previous example policy configures an operator that will support our disconnected installation by providing a valid “update graph” used to calculate how the cluster should proceed from its current version to a new desired version.  The following kinds are required for this particular operator to be deployed and configured <span class="keep-together">correctly</span>:</p>&#13;
<dl>&#13;
<dt>kind: <code>Cincinnati</code></dt>&#13;
<dd>The configuration of the Cincinnati operator. The OSUS operator defines a CRD that deploys additional pods on the cluster to help calculate the update graphs available to be consumed. A <code>ClusterVersion</code> API object will reference the update graph to know which versions are available to be upgraded. Information about available update graphs is stored within images that are mirrored as part of the disconnected registry that you define for disconnected installs.</dd>&#13;
<dt>kind: <code>Subscription</code></dt>&#13;
<dd>The OLM subscription to install the OSUS operator on the cluster.</dd>&#13;
<dt>kind: <code>OperatorGroup</code></dt>&#13;
<dd>The <code>OperatorGroup</code> is a prerequisite for all operators to be installed.</dd>&#13;
<dt>kind: <code>Image</code></dt>&#13;
<dd>The <code>Image</code> configuration for a given OpenShift cluster must be told about the CA if you are using a self-signed certificate for your disconnected registry.</dd>&#13;
<dt>kind: <code>ConfigMap</code></dt>&#13;
<dd>The <code>trusted-ca</code> referenced from the <code>Image objectDefinition</code> is defined here to include the certificate for the registry. If your disconnected registry uses a certificate that is trusted by one of the global CAs, you may not require this part.</dd>&#13;
<dt>kind: <code>Namespace</code></dt>&#13;
<dd>Like all pods in Kubernetes, a namespace must exist on the cluster to run the pods for the OSUS operator.</dd>&#13;
</dl>&#13;
<p>To consume available updates from your disconnected registry, you would define a policy for your <code>ClusterVersion</code> to reference the available update graphs that are made known to the cluster via the OSUS operator:</p>&#13;
<pre data-type="programlisting">apiVersion: policy.open-cluster-management.io/v1&#13;
kind: Policy&#13;
metadata:&#13;
 annotations:&#13;
   policy.open-cluster-management.io/categories: PR.IP Information Protection &#13;
Processes&#13;
     and Procedures&#13;
   policy.open-cluster-management.io/controls: PR.IP-1 Baseline Configuration&#13;
   policy.open-cluster-management.io/standards: NIST-CSF&#13;
 name: policy-cincinatti-clusterversion&#13;
 namespace: upgrade-policies&#13;
spec:&#13;
 disabled: false&#13;
 policy-templates:&#13;
 - objectDefinition:&#13;
     apiVersion: policy.open-cluster-management.io/v1&#13;
     kind: ConfigurationPolicy&#13;
     metadata:&#13;
       name: policy-cluster-version&#13;
     spec:&#13;
       namespaceSelector:&#13;
         exclude:&#13;
         - kube-*&#13;
         include:&#13;
         - default&#13;
       object-templates:&#13;
       - apiVersion: config.openshift.io/v1&#13;
         complianceType: musthave&#13;
         kind: ClusterVersion&#13;
         metadata:&#13;
           name: version&#13;
         objectDefinition:&#13;
           spec:&#13;
             channel: stable-4.5&#13;
             upstream: http://disconnected-cincinnati-policy-engine-route-&#13;
cincinnati-&#13;
operator.apps.YOUR_CLUSTER_NAME.YOUR_BASE_DOMAIN/api/upgrades_info/v1/graph&#13;
       remediationAction: inform&#13;
       severity: low&#13;
 remediationAction: inform&#13;
status:&#13;
 placement:&#13;
 - placementBinding: binding-policy-cincinatti-clusterversion&#13;
   placementRule: placement-policy-cincinatti-clusterversion</pre>&#13;
<p>The <code>ClusterVersion</code> object must reference the update graph made available by the OSUS operator. The upstream field in the preceding <code>ClusterVersion</code> spec should reference the route exposed by the OSUS operator.<a contenteditable="false" data-primary="" data-startref="ch07-pamc" data-type="indexterm" id="idm45358189418536"/><a contenteditable="false" data-primary="" data-startref="ch07-pamc2" data-type="indexterm" id="idm45358189417080"/><a contenteditable="false" data-primary="" data-startref="ch07-pamc3" data-type="indexterm" id="idm45358189415704"/><a contenteditable="false" data-primary="" data-startref="ch07-pamc4" data-type="indexterm" id="idm45358189414328"/></p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id00013">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, we discussed how operators expose additional APIs that provide for the configuration of a Kubernetes or OpenShift cluster. We walked through deploying an example operator that continuously scans running pods for references to container images that have known vulnerabilities.</p>&#13;
<p>We then examined how operators support the configuration of all manner of behavior within the cluster and looked at an example of configuring authentication for the Kubernetes API layer by customizing the <code>OAuth</code> CRD that is acted upon by the authentication operator within the cluster.</p>&#13;
<p>Next we looked at how the rich API surface provided by operators allows you to control all aspects of your cluster and how the Open Cluster Management policy framework allows you to customize policies that define configuration—or indeed even deploy new operators via the OLM framework on all of your clusters. The last policy demonstrates a powerful means for driving the entire upgrade configuration and even triggering active upgrades across the entire collection of managed clusters. You should be able to adapt any one of these working examples to form the foundation of your policies for your multicluster fleet.</p>&#13;
<p>In the next chapter, we will look at an example that takes advantage of many of these building blocks to distribute an entire application across multiple clusters that could run across multiple cloud providers.</p>&#13;
</div></section>&#13;
</div></section></body></html>