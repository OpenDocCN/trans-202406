["```\n./bin/docker-image-tool.sh -r <repo> -t <tag> build\n```", "```\n./bin/docker-image-tool.sh -r <repo> -t <tag> push\n```", "```\n./bin/spark-submit \\\n    --master k8s://https://<k8s-apiserver-host>:<k8s-apiserver-port> \\\n    --deploy-mode cluster \\\n    --name <application-name> \\\n    --class <fully-qualified-class-name> \\\n    --conf spark.executor.instances=<instance-number> \\\n    --conf spark.kubernetes.container.image=<spark-image> \\\n    local:///path/to/application.jar\n```", "```\nhelm repo add spark-operator \\\n https://googlecloudplatform.github.io/spark-on-k8s-operator\n\nhelm install my-release spark-operator/spark-operator \\\n --namespace spark-operator --create-namespace\n```", "```\napiVersion: \"sparkoperator.k8s.io/v1beta2\"\nkind: SparkApplication\nmetadata:\n  name: spark-pi\n  namespace: default\nspec:\n  type: Scala\n  mode: cluster\n  image: \"gcr.io/spark-operator/spark:v3.1.1\"\n  imagePullPolicy: Always\n  mainClass: org.apache.spark.examples.SparkPi\n  mainApplicationFile: \n    \"local:///opt/spark/examples/jars/spark-examples_2.12-3.1.1.jar\"\n  sparkVersion: \"3.1.1\"\n  restartPolicy:\n    type: Never\n  volumes:\n    - name: \"test-volume\"\n      hostPath:\n        path: \"/tmp\"\n        type: Directory\n  driver:\n    cores: 1\n    coreLimit: \"1200m\"\n    memory: \"512m\"\n    labels:\n      version: 3.1.1\n    serviceAccount: spark\n    volumeMounts:\n      - name: \"test-volume\"\n        mountPath: \"/tmp\"\n  executor:\n    cores: 1\n    instances: 1\n    memory: \"512m\"\n    labels:\n      version: 3.1.1\n    volumeMounts:\n      - name: \"test-volume\"\n        mountPath: \"/tmp\"\n```", "```\nspec:\nschedulerName: yunikorn\n```", "```\nspec:\nbatchScheduler: \"volcano\"\n```", "```\nhelm repo add dask https://helm.dask.org/\nhelm repo update\nhelm install my-dask dask/dask\n```", "```\nhelm install my-dask dask/daskhub\n```", "```\nfrom dask_kubernetes import HelmCluster\nfrom dask.distributed import Client\n\n# Connect to the name of the helm installation\ncluster = HelmCluster(release_name=\"my-dask\")\n\n# specify the number of workers(pods) explicitly\ncluster.scale(10)\n\n# or dynamically scale based on current workload\ncluster.adapt(minimum=1, maximum=100)\n\n# Your Python code here\n```", "```\nfrom dask.distributed import Client\nfrom dask_kubernetes import KubeCluster, make_pod_spec\n\npod_spec = make_pod_spec(image='daskdev/dask:latest',\n                         memory_limit='4G', memory_request='4G',\n                         cpu_limit=1, cpu_request=1)\n\ncluster = KubeCluster(pod_spec)\n\n# specify the number of workers(pods) explicitly\ncluster.scale(10)\n\n# or dynamically scale based on current workload\ncluster.adapt(minimum=1, maximum=100)\n\n# Connect Dask to the cluster\nclient = Client(cluster)\n\n# Your Python code here\n```", "```\n# By adding the `@ray.remote` decorator, a regular Python function\n# becomes a Ray remote function.\n@ray.remote\ndef my_function():\n    return 1\n```", "```\ncd ray/deploy/charts\nhelm -n ray install example-cluster --create-namespace ./ray\n```", "```\nimport ray\n\nray.init(\"ray://<host>:<port>\")\n\n@ray.remote\ndef my_function():\n    return 1\n```", "```\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ray-test-job\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n        - name: ray\n          image: rayproject/ray:latest\n          imagePullPolicy: Always\n          command: [ \"/bin/bash\", \"-c\", \"--\" ]\n          args:\n            - \"wget <URL>/job_example.py &&\n              python job_example.py\"\n          resources:\n            requests:\n              cpu: 100m\n              memory: 512Mi\n```", "```\nkubectl -n ray create -f job-example.yaml\n```", "```\nray.init(\"ray://example-cluster-ray-head:10001\")\n```"]