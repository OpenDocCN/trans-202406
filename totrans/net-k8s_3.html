<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Container Networking Basics"><div class="chapter" id="container_networking_basics">
<h1><span class="label">Chapter 3. </span>Container Networking Basics</h1>


<p>Now that we’ve discussed networking basics and Linux networking, we’ll discuss how networking is implemented in
containers. Like networking, <a data-type="indexterm" data-primary="containers" data-secondary="history of" id="ch3_term3"/>containers have a long history. This chapter will review the history,
discuss various options for running containers, and explore the networking setup available. The industry, for now, has settled on Docker as the container runtime standard. Thus, we’ll dive into the Docker networking
model, explain how the CNI differs from the Docker network model, and end the chapter with
examples of networking modes with Docker containers.</p>






<section data-type="sect1" data-pdf-bookmark="Introduction to Containers"><div class="sect1" id="idm46219942844904">
<h1>Introduction to Containers</h1>

<p>In this section, we will discuss the evolution of running applications that has led us to containers. Some,
rightfully, will talk about <a data-type="indexterm" data-primary="containers" data-secondary="as abstraction" data-secondary-sortas="abstraction" id="idm46219942843256"/>containers as not being real. They are yet another abstraction of the underlying technology
in the OS kernel. Being technically right misses the point of the technology and leads us nowhere down the road of solving the hard problem that is application management and deployment.</p>








<section data-type="sect2" data-pdf-bookmark="Applications"><div class="sect2" id="idm46219942841464">
<h2>Applications</h2>

<p>Running <a data-type="indexterm" data-primary="application deployment" data-secondary="challenges of" id="ch3_term1"/><a data-type="indexterm" data-primary="system administrators" data-secondary="and application developers" data-secondary-sortas="application developers" id="ch3_term2"/><a data-type="indexterm" data-primary="developers" id="ch3_term64"/>applications has always had its challenges. There are many ways to serve applications nowadays: in the cloud,
on-prem, and, of course, with containers. Application developers and system administrators face many issues, such as dealing with different versions of libraries, knowing how to complete deployments, and having old versions of the application itself. For the longest time, developers of applications had to deal with
these issues. Bash scripts and deployment tools all have their drawbacks and issues. Every new company has its way of
deploying applications, so every new developer has to learn these techniques. Separation of duties, permissions controls,
and maintaining system stability require system administrators to limit access to developers for deployments.
Sysadmins also manage multiple applications on the same host machine to drive up that machine’s
efficiency, thus creating contention between developers wanting to deploy new features and system administrators
wanting to maintain the whole ecosystem’s stability.</p>

<p>A general-purpose <a data-type="indexterm" data-primary="operating systems and networking stack issues" id="ch3_term4"/>OS supports as many types of applications as possible, so its kernel includes all
kinds of drivers, protocol libraries, and schedulers. <a data-type="xref" href="#img-begining">Figure 3-1</a> shows one machine, with one operating system, but there
are many ways to deploy an application to that host. Application deployment is a problem all organizations must solve.</p>

<figure><div id="img-begining" class="figure">
<img src="Images/neku_0301.png" alt="neku 0301" width="672" height="423"/>
<h6><span class="label">Figure 3-1. </span>Application server</h6>
</div></figure>

<p>From a networking <a data-type="indexterm" data-primary="network engineers" id="idm46219942829880"/>perspective, with one operating system, there is one TCP/IP stack. That single stack creates issues
with port conflicts on the host machine. System administrators host multiple applications on the same machine to
increase the machine’s utilization, and each application will have to run on its port. So now, the system
administrators, the application developers, and the network engineers have to coordinate all of this together. More tasks to add to the deployment checklist are creating troubleshooting guides and dealing with all the IT requests.  Hypervisors are a
way to increase one host machine’s efficiency and remove the one operating system/networking
stack issues.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hypervisor"><div class="sect2" id="idm46219942828200">
<h2>Hypervisor</h2>

<p>A hypervisor <a data-type="indexterm" data-primary="hypervisors" id="idm46219942826936"/>emulates hardware resources, CPU, and memory from a host machine to create guest operating systems or
virtual machines. In 2001, VMware released its x86 hypervisor; earlier versions included IBM’s z/Architecture and
FreeBSD jails. The year 2003 saw the <a data-type="indexterm" data-primary="open source projects" id="idm46219942825800"/>release of Xen, the first open source hypervisor, and in 2006 Kernel-based Virtual
Machine (KVM) was released. A hypervisor allows system administrators to share the underlying hardware with multiple
guest operating systems; <a data-type="xref" href="#img-hypervisor">Figure 3-2</a> demonstrates this. This resource sharing increases the host machine’s efficiency,
alleviating one of the sysadmins issues.</p>

<p>Hypervisors also <a data-type="indexterm" data-primary="ports" data-secondary="in shared systems" data-secondary-sortas="shared systems" id="idm46219942823432"/>gave each application development team a separate networking stack, removing the port conflict
issues on shared systems. For example, team A’s Tomcat application can run on port 8080, while team B’s can also run on port
8080 since each application can now have its guest operating system with a separate network stack. Library
versions, deployment, and other issues remain for the application developer. How can they package and
deploy everything their application needs while maintaining the efficiency introduced by the hypervisor and virtual
machines? This concern led to the development of <a data-type="indexterm" data-startref="ch3_term1" id="idm46219942821512"/><a data-type="indexterm" data-startref="ch3_term2" id="idm46219942820808"/><a data-type="indexterm" data-startref="ch3_term64" id="idm46219942820136"/>containers.</p>

<figure><div id="img-hypervisor" class="figure">
<img src="Images/neku_0302.png" alt="neku 0302" width="772" height="669"/>
<h6><span class="label">Figure 3-2. </span>Hypervisor</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Containers"><div class="sect2" id="idm46219942817016">
<h2>Containers</h2>

<p>In <a data-type="xref" href="#img-containers">Figure 3-3</a>, we see the <a data-type="indexterm" data-primary="application deployment" data-secondary="benefits of containers for" id="idm46219942814456"/><a data-type="indexterm" data-primary="containers" data-secondary="benefits of" id="idm46219942813464"/>benefits of the containerization of applications; each container is independent.
Application developers can use whatever they need to run their application without relying on underlying libraries or host
operating systems. Each container also has its own network stack. The container allows developers to package and
deploy applications while maintaining efficiencies for the host machine.</p>

<figure><div id="img-containers" class="figure">
<img src="Images/neku_0303.png" alt="neku 0303" width="772" height="533"/>
<h6><span class="label">Figure 3-3. </span>Containers running on host OS</h6>
</div></figure>

<p>With any technology comes a history of changes, competitors, and innovations, and containers are no <a data-type="indexterm" data-startref="ch3_term3" id="idm46219942809528"/><a data-type="indexterm" data-startref="ch3_term4" id="idm46219942808824"/>different.
The following is a <a data-type="indexterm" data-primary="containers" data-secondary="terminology for" id="ch3_term5"/>list of terms that can be confusing when learning about containers. First,
we list the distinction between container runtimes, discuss each runtime’s functionality, and show
how they relate to Kubernetes.  The functionality of container runtimes breaks down to “high level” and
“low level”:</p>
<dl>
<dt>Container</dt>
<dd>
<p>A running <a data-type="indexterm" data-primary="containers" data-secondary="defined" id="idm46219942804136"/>container image.</p>
</dd>
<dt>Image</dt>
<dd>
<p>A container <a data-type="indexterm" data-primary="images, container" data-secondary="defined" id="idm46219942801688"/>image is the file that  is pulled down from a registry server and used locally as a mount point when
starting a container.</p>
</dd>
<dt>Container engine</dt>
<dd>
<p>A container <a data-type="indexterm" data-primary="container engine" id="idm46219942799128"/>engine accepts user requests via command-line options to pull images and run a container.</p>
</dd>
<dt>Container runtime</dt>
<dd>
<p>The container <a data-type="indexterm" data-primary="container runtime" id="idm46219942796872"/>runtime is the low-level piece of software in a container engine that deals with
running a container.</p>
</dd>
<dt>Base image</dt>
<dd>
<p>A starting <a data-type="indexterm" data-primary="base image of containers" id="idm46219942794616"/>point for container images; to reduce build image sizes and complexity, users can start with a
base image and make incremental changes on top of it.</p>
</dd>
<dt>Image layer</dt>
<dd>
<p>Repositories <a data-type="indexterm" data-primary="images, container" data-secondary="layers of" id="idm46219942792296"/>are often referred to as images or container images, but actually they are made up of one
or more layers. Image layers in a repository are connected in a parent-child relationship. Each image layer
represents changes between itself and the parent layer.</p>
</dd>
<dt>Image format</dt>
<dd>
<p>Container engines have their own container image format, such as LXD, RKT, and Docker.</p>
</dd>
<dt>Registry</dt>
<dd>
<p>A <a data-type="indexterm" data-primary="registry, container" id="idm46219942788296"/>registry stores container images and allows for users to upload, download, and update container images.</p>
</dd>
<dt>Repository</dt>
<dd>
<p>Repositories <a data-type="indexterm" data-primary="repository, container" id="idm46219942786040"/>can be equivalent to a container image. The important distinction is that repositories
are made up of layers and  metadata about the image; this is the manifest.</p>
</dd>
<dt>Tag</dt>
<dd>
<p>A tag <a data-type="indexterm" data-primary="tag, container" id="idm46219942783720"/>is a user-defined name for different versions of a container image.</p>
</dd>
<dt>Container host</dt>
<dd>
<p>The <a data-type="indexterm" data-primary="container host" id="idm46219942781496"/>container host is the system that runs the container with a container engine.</p>
</dd>
<dt>Container orchestration</dt>
<dd>
<p>This is <a data-type="indexterm" data-primary="container orchestration" id="idm46219942779256"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="container orchestration by" id="idm46219942778520"/>what Kubernetes does! It dynamically schedules container workloads for a cluster of
container hosts.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Cgroups and <a data-type="indexterm" data-primary="cgroups (control groups)" id="idm46219942776744"/><a data-type="indexterm" data-primary="containers" data-secondary="cgroups (control groups) and" id="idm46219942775992"/><a data-type="indexterm" data-primary="Linux networking" data-secondary="primitives" id="idm46219942775032"/><a data-type="indexterm" data-primary="namespaces, network" data-secondary="as Linux primitives" data-secondary-sortas="Linux primitives" id="idm46219942774088"/><a data-type="indexterm" data-primary="primitives, Linux" id="idm46219942772872"/><a data-type="indexterm" data-primary="primitives, Linux" data-secondary="cgroups (control groups)" id="idm46219942772200"/><a data-type="indexterm" data-primary="primitives, Linux" data-secondary="namespaces" id="idm46219942771240"/>namespaces are Linux primitives to create containers; they are discussed in the next section.</p>
</div>

<p>An example <a data-type="indexterm" data-primary="containers" data-secondary="functionality of" id="ch3_term6"/><a data-type="indexterm" data-primary="containers" data-secondary="runtimes of" id="ch3_term7"/><a data-type="indexterm" data-primary="high-level container runtimes and functionality" id="ch3_term8"/><a data-type="indexterm" data-primary="low-level container runtimes and functionality" id="ch3_term9"/><a data-type="indexterm" data-primary="runtimes, container" id="ch3_term10"/>of “low-level” functionality is creating cgroups and namespaces for containers, the bare minimum to run one.
Developers require more than that when working with containers. They need to build and
test containers and deploy them; these are considered a “high-level” functionality. Each container runtime
offers various levels of functionality. The following is a list of high and low functionality:</p>
<dl>
<dt>Low-level container runtime functionality</dt>
<dd>

<ul>
<li>
<p>Creating containers</p>
</li>
<li>
<p>Running containers</p>
</li>
</ul>
</dd>
<dt>High-level container runtime functionality</dt>
<dd>

<ul>
<li>
<p>Formatting container images</p>
</li>
<li>
<p>Building container images</p>
</li>
<li>
<p>Managing container images</p>
</li>
<li>
<p>Managing instances of containers</p>
</li>
<li>
<p>Sharing container images</p>
</li>
</ul>
</dd>
</dl>

<p>Over the next few pages, we will discuss runtimes that implement the previous functionality. Each of the following projects has its
strengths and weaknesses to provide high- and low-level functionality. Some are good to know about for historical
reasons but no longer exist or have merged with other <a data-type="indexterm" data-startref="ch3_term5" id="idm46219942753096"/><a data-type="indexterm" data-primary="containers" data-secondary="runC routines for" id="idm46219942752360"/><a data-type="indexterm" data-primary="runC routines, container" id="idm46219942751416"/>projects:</p>
<dl>
<dt>Low-level container runtimes</dt>
<dd>
<dl>
<dt>LXC</dt>
<dd>
<p>C API for creating Linux containers</p>
</dd>
<dt>runC</dt>
<dd>
<p>CLI for OCI-compliant containers</p>
</dd>
</dl>
</dd>
<dt>High-level container runtimes</dt>
<dd>
<dl>
<dt>containerd</dt>
<dd>
<p>Container runtime split off from Docker, a graduated CNCF project</p>
</dd>
<dt>CRI-O</dt>
<dd>
<p>Container runtime interface using the Open Container Initiative (OCI) specification, an incubating CNCF
project</p>
</dd>
<dt>Docker</dt>
<dd>
<p>Open source container platform</p>
</dd>
<dt>lmctfy</dt>
<dd>
<p>Google containerization platform</p>
</dd>
<dt>rkt</dt>
<dd>
<p>CoreOS container specification</p>
</dd>
</dl>
</dd>
</dl>










<section data-type="sect3" data-pdf-bookmark="OCI"><div class="sect3" id="idm46219942737640">
<h3>OCI</h3>

<p>OCI promotes <a data-type="indexterm" data-startref="ch3_term6" id="idm46219942736200"/><a data-type="indexterm" data-startref="ch3_term7" id="idm46219942735464"/><a data-type="indexterm" data-startref="ch3_term8" id="idm46219942734792"/><a data-type="indexterm" data-startref="ch3_term9" id="idm46219942734120"/><a data-type="indexterm" data-startref="ch3_term10" id="idm46219942733448"/><a data-type="indexterm" data-primary="containers" data-secondary="OCI specifications for" id="idm46219942732776"/><a data-type="indexterm" data-primary="OCI specifications for containers" id="idm46219942731832"/>common, minimal, open standards, and specifications for container technology.</p>

<p>The idea for creating a formal specification for container image formats and runtimes allows a
container to be portable across all major operating systems and platforms to ensure no undue technical
barriers. The three values guiding the OCI project are as follows:</p>
<dl>
<dt>Composable</dt>
<dd>
<p>Tools for managing containers should have clean interfaces. They should also not be bound to specific
projects, clients, or frameworks and should work across all platforms.</p>
</dd>
<dt>Decentralized</dt>
<dd>
<p>The format and runtime should be well specified and developed by the community, not one organization. Another goal of the OCI project is independent implementations of tools to run the same container.</p>
</dd>
<dt>Minimalist</dt>
<dd>
<p>The OCI spec strives to do several things well, be minimal and stable, and enable innovation and
experimentation.</p>
</dd>
</dl>

<p>Docker <a data-type="indexterm" data-primary="Docker container technology" data-secondary="OCI specifications and" id="idm46219942724856"/>donated a draft for the base format and runtime. It also donated code for a reference implementation to the OCI. Docker took the contents of the libcontainer project, made it run independently of Docker, and donated it to the OCI project. That codebase is <a data-type="indexterm" data-primary="runC routines, container" id="idm46219942723480"/><a data-type="indexterm" data-primary="containers" data-secondary="runC routines for" id="idm46219942722840"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="runC architecture for" id="idm46219942721896"/>runC, which can be found on <a href="https://oreil.ly/A49v0">GitHub</a>.</p>

<p>Let’s discuss <a data-type="indexterm" data-primary="containers" data-secondary="initiatives for" id="ch3_term11"/>several early container initiatives and their capabilities. This section will end with where Kubernetes
is with container runtimes and how they work together.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="LXC"><div class="sect3" id="idm46219942717992">
<h3>LXC</h3>

<p>Linux <a data-type="indexterm" data-primary="Linux Containers (LXC)" id="idm46219942716728"/><a data-type="indexterm" data-primary="LXC (Linux Containers)" id="idm46219942715992"/>Containers, LXC, was created in 2008. LXC combines cgroups and namespaces to provide an isolated environment for
running applications. LXC’s goal is to create an environment as close as possible to a standard Linux without the
need for a separate kernel. LXC has separate components: the <code>liblxc</code> library, several programming language bindings,
Python versions 2 and 3, Lua, Go, Ruby, Haskell, a set of standard tools, and container templates.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="runC"><div class="sect3" id="idm46219942714088">
<h3>runC</h3>

<p>runC is the most widely used container runtime developed initially as part of Docker and
was later extracted as a separate tool and library. runC is a command-line tool for running applications
packaged according to the OCI format and is a compliant implementation of the OCI spec.
runC <a data-type="indexterm" data-primary="libcontainer" id="idm46219942712456"/>uses <code>libcontainer</code>, which is the same container library powering a Docker engine installation. Before
version 1.11, the Docker engine was used to manage volumes, networks, containers, images, etc. Now, the Docker
architecture has several components, and the runC features include the 
<span class="keep-together">following:</span></p>

<ul>
<li>
<p>Full support for Linux namespaces, including user namespaces</p>
</li>
<li>
<p>Native support for all security<a data-type="indexterm" data-primary="Linux networking" data-secondary="security systems for" id="idm46219942708136"/><a data-type="indexterm" data-primary="security" data-secondary="Linux features for" id="idm46219942707160"/> features available in Linux</p>

<ul>
<li>
<p>SELinux, AppArmor, seccomp, control groups, capability drop, <code>pivot_root</code>, UID/GID dropping, etc.</p>
</li>
</ul>
</li>
<li>
<p>Native support of Windows 10 containers</p>
</li>
<li>
<p>Planned native support for the entire hardware manufacturer’s ecosystem</p>
</li>
<li>
<p>A formally specified configuration format, governed by the OCI under the Linux Foundation</p>
</li>
</ul>
</div></section>













<section data-type="sect3" data-pdf-bookmark="containerd"><div class="sect3" id="idm46219942700888">
<h3>containerd</h3>

<p>containerd is <a data-type="indexterm" data-primary="containerd service" id="idm46219942699560"/>a high-level runtime that was split off from Docker. containerd is a background service that
acts as an API facade for various container runtimes and OSs. containerd has various components that provide it with
high-level functionality. containerd is a service for Linux and Windows that manages its host system’s complete
container life cycle, image transfer, storage, container execution, and network attachment. containerd’s client CLI
tool is <code>ctr</code>, and it is for development and debugging purposes for direct communication with <a data-type="indexterm" data-primary="containerd-shim" id="idm46219942697848"/>containerd.
containerd-shim is the component that allows for daemonless containers. It resides as the parent of the container’s
process to facilitate a few things. <a data-type="indexterm" data-primary="containers" data-secondary="runtimes of" id="ch3_term12"/><a data-type="indexterm" data-primary="runtimes, container" id="ch3_term13"/>containerd allows the runtimes, i.e., runC, to exit after it starts the container.
This way, we do not need the long-running runtime processes for containers. It also keeps the standard I/O and
other file descriptors open for the container if containerd and Docker die. If the shim does not run, then the pipe’s
parent side would be closed, and the container would exit. containerd-shim also allows the container’s exit status to
be reported back to a higher-level tool like Docker without having the container process’s actual parent do it.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="lmctfy"><div class="sect3" id="idm46219942693736">
<h3>lmctfy</h3>

<p>Google started <a data-type="indexterm" data-primary="Imctfy container technology" id="idm46219942692440"/><a data-type="indexterm" data-primary="open source projects" id="idm46219942691688"/>lmctfy as its open source Linux container technology in 2013. lmctfy is a high-level
container runtime that provides the ability to create and delete containers but is no longer actively maintained and was
porting over to libcontainer, which is now containerd. lmctfy provided an API-driven configuration without
developers worrying about the details of cgroups and namespace internals.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="rkt"><div class="sect3" id="idm46219942690232">
<h3>rkt</h3>

<p>rkt started <a data-type="indexterm" data-primary="rkt container project" id="idm46219942688936"/>at CoreOS as an alternative to Docker in 2014. It is written in Go, uses pods as its basic compute unit,
and allows for a self-contained environment for applications. rkt’s native image format was the App Container
Image (ACI), defined in the App Container spec; this was deprecated in favor of the OCI format and specification
support. It supports the CNI specification and can run Docker images and OCI images. The rkt
project was archived in February 2020 by the maintainers.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Docker"><div class="sect3" id="idm46219942687352">
<h3>Docker</h3>

<p>Docker, released in 2013, solved <a data-type="indexterm" data-primary="Docker container technology" data-secondary="architecture and components of" id="ch3_term14"/><a data-type="indexterm" data-primary="application deployment" data-secondary="challenges of" id="idm46219942684792"/>many of the problems that developers had running containers
end to end. It has all this functionality for developers to create, maintain, and deploy containers:</p>

<ul>
<li>
<p>Formatting container images</p>
</li>
<li>
<p>Building container images</p>
</li>
<li>
<p>Managing container images</p>
</li>
<li>
<p>Managing instances of containers</p>
</li>
<li>
<p>Sharing container images</p>
</li>
<li>
<p>Running containers</p>
</li>
</ul>

<p><a data-type="xref" href="#img-docker-eng">Figure 3-4</a> shows us the architecture of the Docker engine and its various components. <a data-type="indexterm" data-primary="Docker container technology" data-secondary="engine of" id="idm46219942676264"/>Docker began as a monolith
application, building all the previous functionality into a single binary known as the <em>Docker engine</em>. The engine
contained the <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Docker" data-secondary-sortas="Docker" id="idm46219942674664"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="CLI (client) in" id="idm46219942673352"/>Docker client or CLI that allows developers to build, run, and push containers and images. The Docker server
runs as a daemon to manage the data volumes and networks for running containers. The client communicates to the
server through the Docker API. It uses <a data-type="indexterm" data-primary="containerd service" id="idm46219942671992"/>containerd to manage the container life cycle, and it <a data-type="indexterm" data-primary="containers" data-secondary="runC routines for" id="idm46219942671128"/><a data-type="indexterm" data-primary="runC routines, container" id="idm46219942670184"/>uses runC to spawn the container
process.</p>

<figure><div id="img-docker-eng" class="figure">
<img src="Images/neku_0304.png" alt="neku 0304" width="1102" height="778"/>
<h6><span class="label">Figure 3-4. </span>Docker engine</h6>
</div></figure>

<p>In the past few years, Docker has broken apart this monolith into separate components. To run a container,
the Docker engine creates the image and passes it to containerd. <a data-type="indexterm" data-primary="containerd-shim" id="idm46219942666728"/>containerd calls containerd-shim, which uses runC to run
the container. Then, containerd-shim allows the runtime (runC in this case) to exit after it starts the container. This way, we
can run daemonless containers because we do not need the long-running runtime processes for containers.</p>

<p>Docker provides a <a data-type="indexterm" data-primary="system administrators" data-secondary="and application developers" data-secondary-sortas="application developers" id="idm46219942665208"/><a data-type="indexterm" data-primary="developers" id="idm46219942663960"/>separation of concerns for application developers and system administrators. It allows the developers
to focus on building their apps, and system admins focus on deployment. Docker provides a fast development cycle; to
test new versions of Golang for our web app, we can update the base image and run tests against it.
Docker provides application portability between running on-premise, in the cloud, or in any other data center. Its
motto is to build, ship, and run anywhere. A new container can quickly be provisioned for scalability and run
more apps on one host machine, increasing that machine’s efficiency.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="CRI-O"><div class="sect3" id="idm46219942662408">
<h3>CRI-O</h3>

<p>CRI-O is an O<a data-type="indexterm" data-primary="containers" data-secondary="CRI-O runtime with" id="idm46219942660744"/><a data-type="indexterm" data-primary="CRI-O runtime" id="idm46219942659768"/><a data-type="indexterm" data-primary="CRI runtime" id="idm46219942659064"/>CI-based implementation of the Kubernetes CRI, while the OCI is a set of
specifications that container runtime engines must implement. Red Hat started the CRI
project in 2016 and in 2019 contributed it to the CNCF. <a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="CRI interface and" id="idm46219942658040"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="CRI-O runtime in" id="idm46219942657096"/>CRI is a plugin interface that enables Kubernetes, via <code>Kubelet</code>, to communicate with any container runtime that satisfies the CRI interface. CRI-O development began in 2016
after the 
<span class="keep-together">Kubernetes</span> project introduced CRI, and CRI-O 1.0 was released in 2017.  The <a data-type="indexterm" data-primary="high-level container runtimes and functionality" id="idm46219942654744"/>CRI-O is a lightweight CRI
runtime made as a Kubernetes-specific high-level runtime built on gRPC and Protobuf over a UNIX socket. <a data-type="xref" href="#cri-place">Figure 3-5</a>
points out where the CRI fits into the whole picture with the Kubernetes architecture. CRI-O provides stability in
the Kubernetes project, with a commitment to passing Kubernetes tests.</p>

<figure><div id="cri-place" class="figure">
<img src="Images/neku_0305.png" alt="neku 0305" width="1136" height="999"/>
<h6><span class="label">Figure 3-5. </span>CRI about Kubernetes</h6>
</div></figure>

<p>There have been many companies, technologies, and innovations in the container space. This section has been a brief history
of that. The industry has landed on making sure the container landscape remains an open OCI project for all to use across
various ways to run containers. Kubernetes has helped shaped this effort as well with the adaption of the
CRI-O interface. Understanding the components of the container is vital to all administrators of container
deployments and developers using containers. A recent example of this importance is in Kubernetes 1.20, where dockershim
support will be deprecated. The Docker runtime utilizing the dockershim for administrators is deprecated, but developers
can still use Docker to build OCI-compliant containers to <a data-type="indexterm" data-startref="ch3_term11" id="idm46219942649592"/><a data-type="indexterm" data-startref="ch3_term12" id="idm46219942648888"/><a data-type="indexterm" data-startref="ch3_term13" id="idm46219942648216"/><a data-type="indexterm" data-startref="ch3_term14" id="idm46219942647544"/>run.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The first <a data-type="indexterm" data-primary="dockershim" id="idm46219942645736"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="engine of" id="idm46219942645000"/>CRI implementation was the dockershim, which provided a layer of abstraction in front of the
Docker engine.</p>
</div>

<p>Now we will dive deeper into the container technology that powers them.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Container Primitives"><div class="sect1" id="idm46219942816360">
<h1>Container Primitives</h1>

<p>No matter if you are using Docker or containerd, <a data-type="indexterm" data-primary="containers" data-secondary="runC routines for" id="idm46219942641576"/><a data-type="indexterm" data-primary="runC routines, container" id="idm46219942640600"/>runC starts and manages the actual containers for them. In this
section, we will review what runC takes care of for developers from a container perspective. <a data-type="indexterm" data-primary="namespaces, network" data-secondary="as Linux primitives" data-secondary-sortas="Linux primitives" id="idm46219942639656"/><a data-type="indexterm" data-primary="primitives, Linux" data-secondary="namespaces" id="idm46219942638440"/><a data-type="indexterm" data-primary="cgroups (control groups)" id="ch3_term15"/><a data-type="indexterm" data-primary="containers" data-secondary="cgroups (control groups) and" id="ch3_term16"/><a data-type="indexterm" data-primary="primitives, Linux" data-secondary="cgroups (control groups)" id="ch3_term17"/><a data-type="indexterm" data-primary="Linux networking" data-secondary="primitives" id="ch3_term18"/><a data-type="indexterm" data-primary="primitives, Linux" id="ch3_term19"/>Each of our
containers has Linux primitives known as <em>control groups</em> and <em>namespaces</em>. <a data-type="xref" href="#namespaces">Figure 3-6</a> shows an example of what
this looks like; cgroups control access to resources in the kernel for our containers, and namespaces are individual
slices of resources to manage separately from the root namespaces, i.e., the host.</p>

<figure><div id="namespaces" class="figure">
<img src="Images/neku_0306.png" alt="neku 0306" width="584" height="620"/>
<h6><span class="label">Figure 3-6. </span>Namespaces and control groups</h6>
</div></figure>

<p>To help solidify these concepts, let’s dig into control groups and namespaces a bit 
<span class="keep-together">further.</span></p>








<section data-type="sect2" data-pdf-bookmark="Control Groups"><div class="sect2" id="idm46219942626952">
<h2>Control Groups</h2>

<p>In short, a <a data-type="indexterm" data-primary="kernel, cgroups in Linux" id="idm46219942625448"/>cgroup is a Linux kernel feature that limits, accounts for, and isolates resource usage.
Initially released in Linux 2.6.24, cgroups allow administrators to control different CPU
systems and memory for particulate processes. Cgroups are provided through pseudofilesystems and are maintained by the
core kernel code in cgroups. These separate subsystems maintain various cgroups in the kernel:</p>
<dl>
<dt>CPU</dt>
<dd>
<p>The process can be guaranteed a minimum number of CPU shares.</p>
</dd>
<dt>Memory</dt>
<dd>
<p>These set up memory limits for a process.</p>
</dd>
<dt>Disk I/O</dt>
<dd>
<p>This and other devices are controlled via the device’s cgroup subsystem.</p>
</dd>
<dt>Network</dt>
<dd>
<p>This is maintained by the <code>net_cls</code> and marks packets leaving the cgroup.</p>
</dd>
</dl>

<p><code>lscgroup</code> is a <a data-type="indexterm" data-primary="lscgroup tool, Linux" id="idm46219942616888"/>command-line tool that lists all the cgroups currently in the system.</p>

<p>runC will <a data-type="indexterm" data-primary="containers" data-secondary="runC routines for" id="idm46219942615560"/><a data-type="indexterm" data-primary="runC routines, container" id="idm46219942614552"/>create the cgroups for the container at creation time. A cgroup controls how much of a resource a container
can use, while <a data-type="indexterm" data-startref="ch3_term15" id="idm46219942613592"/><a data-type="indexterm" data-startref="ch3_term16" id="idm46219942612920"/><a data-type="indexterm" data-startref="ch3_term17" id="idm46219942612248"/>namespaces control what processes inside the container can see.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Namespaces"><div class="sect2" id="idm46219942611320">
<h2>Namespaces</h2>

<p>Namespaces are <a data-type="indexterm" data-primary="namespaces, network" data-secondary="processes of" id="ch3_term20"/><a data-type="indexterm" data-primary="primitives, Linux" data-secondary="namespaces" id="ch3_term21"/>features of the Linux kernel that isolate and virtualize system resources of a collection of
processes. Here are examples of virtualized resources:</p>
<dl>
<dt>PID namespace</dt>
<dd>
<p>Processes <a data-type="indexterm" data-primary="PID namespace, Linux" id="idm46219942605160"/>ID, for process isolation</p>
</dd>
<dt>Network namespace</dt>
<dd>
<p>Manages network interfaces and a separate networking stack</p>
</dd>
<dt>IPC namespace</dt>
<dd>
<p>Manages <a data-type="indexterm" data-primary="IPC namespace, Linux" id="idm46219942601624"/>access to interprocess communication (IPC) resources</p>
</dd>
<dt>Mount namespace</dt>
<dd>
<p>Manages <a data-type="indexterm" data-primary="Mount namespace, Linux" id="idm46219942599416"/>filesystem mount points</p>
</dd>
<dt>UTS namespace</dt>
<dd>
<p>UNIX <a data-type="indexterm" data-primary="UTS namespace, Linux" id="idm46219942597240"/>time-sharing; allows single hosts to have different host and domain names for different
processes</p>
</dd>
<dt>UID namespaces</dt>
<dd>
<p>User <a data-type="indexterm" data-primary="UID namespaces, Linux" id="idm46219942594984"/>ID; isolates process ownership with separate user and group assignments</p>
</dd>
</dl>

<p>A process’s user and group IDs can be different inside and outside a user’s namespace.  A process can
have an unprivileged user ID outside a user namespace while at the same time having a user ID of 0 inside the
container user namespace. The process has root privileges for execution inside the user namespace but is unprivileged
for operations outside the namespace.</p>

<p><a data-type="xref" href="#namespace_single">Example 3-1</a> is an example of how to inspect the namespaces for a process. All information for a process is on
the <code>/proc</code> filesystem in Linux. PID 1’s PID namespace is <code>4026531836</code>, and listing all the namespaces shows that
the PID namespace IDs match.</p>
<div id="namespace_single" data-type="example">
<h5><span class="label">Example 3-1. </span>Namespaces of a single process</h5>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ps -p <code class="m">1</code> -o pid,pidns
  PID      PIDNS
    <code class="m">1</code> 4026531836

vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ls -l /proc/1/ns
total 0
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 cgroup -&gt; cgroup:<code class="o">[</code>4026531835<code class="o">]</code>
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 ipc -&gt; ipc:<code class="o">[</code>4026531839<code class="o">]</code>
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 mnt -&gt; mnt:<code class="o">[</code>4026531840<code class="o">]</code>
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 net -&gt; net:<code class="o">[</code>4026531957<code class="o">]</code>
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 pid -&gt; pid:<code class="o">[</code>4026531836<code class="o">]</code>
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 user -&gt; user:<code class="o">[</code>4026531837<code class="o">]</code>
lrwxrwxrwx <code class="m">1</code> root root <code class="m">0</code> Dec <code class="m">12</code> 20:41 uts -&gt; uts:<code class="o">[</code>4026531838<code class="o">]</code></pre></div>

<p><a data-type="xref" href="#containers">Figure 3-7</a> shows <a data-type="indexterm" data-primary="cgroups (control groups)" id="idm46219942544584"/><a data-type="indexterm" data-primary="containers" data-secondary="cgroups (control groups) and" id="idm46219942543976"/><a data-type="indexterm" data-primary="primitives, Linux" data-secondary="cgroups (control groups)" id="idm46219942543096"/>that effectively these two Linux primitives allow application developers to control and manage
their applications separate from the hosts and other applications either in containers or by running natively on the host.</p>

<figure><div id="containers" class="figure">
<img src="Images/neku_0307.png" alt="neku 0307" width="774" height="666"/>
<h6><span class="label">Figure 3-7. </span>Cgroups and namespaces powers combined</h6>
</div></figure>

<p>The following <a data-type="indexterm" data-startref="ch3_term20" id="idm46219942539464"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="building" id="idm46219942538728"/><a data-type="indexterm" data-primary="Ubuntu" id="ch3_term22"/>examples use Ubuntu 16.04 LTS Xenial Xerus. If you want to follow along on your system, more
information can be found in this book’s code repo. The repo contains the tools and configurations for building the
Ubuntu VM and Docker containers. Let’s get started with setting up and testing our namespaces.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Setting Up Namespaces"><div class="sect2" id="idm46219942536200">
<h2>Setting Up Namespaces</h2>

<p><a data-type="xref" href="#root-container-namespaces">Figure 3-8</a> outlines a basic container <a data-type="indexterm" data-primary="packets" data-secondary="Linux kernel management of" id="ch3_term23"/><a data-type="indexterm" data-primary="namespaces, network" data-secondary="creating" id="ch3_term24"/><a data-type="indexterm" data-primary="containers" data-secondary="creating" id="ch3_term25"/>network setup. In the following pages, we will walk through all the Linux
commands that the low-level runtimes complete for container network creation.</p>

<figure><div id="root-container-namespaces" class="figure">
<img src="Images/neku_0308.png" alt="neku 0308" width="705" height="802"/>
<h6><span class="label">Figure 3-8. </span>Root network namespace and container network namespace</h6>
</div></figure>

<p>The following <a data-type="indexterm" data-primary="bridge interface" id="idm46219942526184"/><a data-type="indexterm" data-primary="eth0 device and bridge interface" id="idm46219942525448"/><a data-type="indexterm" data-primary="root network namespaces" id="idm46219942524808"/><a data-type="indexterm" data-primary="veth (virtual Ethernet) pairs" data-secondary="bridge interface for" id="idm46219942524136"/><a data-type="indexterm" data-primary="veth (virtual Ethernet) pairs" data-secondary="creation of" id="idm46219942523176"/><a data-type="indexterm" data-primary="interfaces, network" id="ch3_term26"/><a data-type="indexterm" data-primary="Linux networking" data-secondary="network interfaces for" id="ch3_term27"/><a data-type="indexterm" data-primary="network interfaces" id="ch3_term28"/>steps show how to create the networking setup shown in <a data-type="xref" href="#root-container-namespaces">Figure 3-8</a>:</p>
<ol>
<li>
<p>Create a host with a root network namespace.</p>
</li>
<li>
<p>Create a new network namespace.</p>
</li>
<li>
<p>Create a veth pair.</p>
</li>
<li>
<p>Move one side of the veth pair into a new network namespace.</p>
</li>
<li>
<p>Address side of the veth pair inside the new network namespace.</p>
</li>
<li>
<p>Create a bridge interface.</p>
</li>
<li>
<p>Address the bridge interface.</p>
</li>
<li>
<p>Attach the bridge to the host interface.</p>
</li>
<li>
<p>Attach one side of the veth pair to the bridge interface.</p>
</li>
<li>
<p>Profit.</p>
</li>

</ol>

<p>The following are all the Linux commands needed to create the network namespace, bridge, and veth pairs and wire them together:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code><code class="nb">echo </code><code class="m">1</code> &gt; /proc/sys/net/ipv4/ip_forward
<code class="nv">$ </code>sudo ip netns add net1
<code class="nv">$ </code>sudo ip link add veth0 <code class="nb">type </code>veth peer name veth1
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>veth1 netns net1
<code class="nv">$ </code>sudo ip link add veth0 <code class="nb">type </code>veth peer name veth1
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip addr add 192.168.1.101/24 dev veth1
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip link <code class="nb">set </code>dev veth1 up
<code class="nv">$ </code>sudo ip link add br0 <code class="nb">type </code>bridge
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>dev br0 up
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>enp0s3 master br0
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>veth0 master br0
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1  ip route add default via 192.168.1.100</pre>

<p>Let’s dive into an example and outline each command.</p>

<p>The <code>ip</code> Linux command sets up and controls the network namespaces.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can find more information about <code>ip</code> on its <a href="https://oreil.ly/jBKL7">man page</a>.</p>
</div>

<p>In <a data-type="xref" href="#example0302">Example 3-2</a>, we have used <a data-type="indexterm" data-primary="Vagrant/Vagrantfile" id="ch3_term29"/>Vagrant and <a data-type="indexterm" data-primary="VirtualBox interface" id="idm46219942436776"/>VirtualBox to create a fresh installation of Ubuntu for our testing purposes.</p>
<div id="example0302" data-type="example">
<h5><span class="label">Example 3-2. </span>Ubuntu testing virtual machine</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>vagrant up
Bringing machine <code class="s1">'default'</code> up with <code class="s1">'virtualbox'</code> provider...
<code class="o">==</code>&gt; default: Importing base box <code class="s1">'ubuntu/xenial64'</code>...
<code class="o">==</code>&gt; default: Matching MAC address <code class="k">for</code> NAT networking...
<code class="o">==</code>&gt; default: Checking <code class="k">if</code> box <code class="s1">'ubuntu/xenial64'</code> version <code class="s1">'20200904.0.0'</code> is up to date...
<code class="o">==</code>&gt; default: Setting the name of the VM:
<code class="nv">advanced_networking_code_examples_default_1600085275588_55198</code>
<code class="o">==</code>&gt; default: Clearing any previously <code class="nb">set </code>network interfaces...
<code class="o">==</code>&gt; default: Available bridged network interfaces:
1<code class="o">)</code> en12: USB 10/100 /1000LAN
2<code class="o">)</code> en5: USB Ethernet<code class="o">(</code>?<code class="o">)</code>
3<code class="o">)</code> en0: Wi-Fi <code class="o">(</code>Wireless<code class="o">)</code>
4<code class="o">)</code> llw0
5<code class="o">)</code> en11: USB 10/100/1000 LAN 2
6<code class="o">)</code> en4: Thunderbolt 4
7<code class="o">)</code> en1: Thunderbolt 1
8<code class="o">)</code> en2: Thunderbolt 2
9<code class="o">)</code> en3: Thunderbolt <code class="nv">3</code>
<code class="o">==</code>&gt; default: When choosing an interface, it is usually the one that <code class="nv">is</code>
<code class="o">==</code>&gt; default: being used to connect to the internet.
<code class="o">==</code>&gt; default:
    default: Which interface should the network bridge to? <code class="nv">1</code>
<code class="o">==</code>&gt; default: Preparing network interfaces based on configuration...
    default: Adapter 1: nat
    default: Adapter 2: <code class="nv">bridged</code>
<code class="o">==</code>&gt; default: Forwarding ports...
    default: <code class="m">22</code> <code class="o">(</code>guest<code class="o">)</code> <code class="o">=</code>&gt; <code class="m">2222</code> <code class="o">(</code>host<code class="o">)</code> <code class="o">(</code>adapter 1<code class="o">)</code>
<code class="o">==</code>&gt; default: Running <code class="s1">'pre-boot'</code> VM customizations...
<code class="o">==</code>&gt; default: Booting VM...
<code class="o">==</code>&gt; default: Waiting <code class="k">for</code> machine to boot. This may take a few minutes...
    default: SSH address: 127.0.0.1:2222
    default: SSH username: vagrant
    default: SSH auth method: private key
    default: Warning: Connection reset. Retrying...
    default:
    default: Vagrant insecure key detected. Vagrant will automatically replace
    default: this with a newly generated keypair <code class="k">for</code> better security.
    default:
    default: Inserting generated public key within guest...
    default: Removing insecure key from the guest <code class="k">if</code> it<code class="err">'</code>s present...
    default: Key inserted! Disconnecting and reconnecting using new SSH key...
<code class="o">==</code>&gt; default: Machine booted and ready!
<code class="o">==</code>&gt; default: Checking <code class="k">for</code> guest additions in VM...
<code class="o">==</code>&gt; default: Configuring and enabling network interfaces...
<code class="o">==</code>&gt; default: Mounting shared folders...
    default: /vagrant <code class="o">=</code>&gt;
    /Users/strongjz/Documents/code/advanced_networking_code_examples</pre></div>

<p>Refer to the book
repo for the Vagrantfile to reproduce this.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a href="https://oreil.ly/o8Qo0">Vagrant</a> is a local virtual machine manager created by HashiCorp.</p>
</div>

<p>After Vagrant boots our virtual machine, we can use <a data-type="indexterm" data-primary="ssh command, Linux" id="idm46219942315288"/>Vagrant to <code>ssh</code> into this VM:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$±</code> <code class="p">|</code>master U:2 ?:2 ✗<code class="p">|</code> → vagrant ssh
Welcome to Ubuntu 16.04.7 LTS <code class="o">(</code>GNU/Linux 4.4.0-189-generic x86_64<code class="o">)</code>

vagrant@ubuntu-xenial:~<code class="err">$</code></pre>

<p><em>IP forwarding</em> <a data-type="indexterm" data-primary="forwarding" data-secondary="with Linux IP" data-secondary-sortas="Linux IP" id="idm46219942311096"/><a data-type="indexterm" data-primary="IP forwarding, Linux" id="idm46219942309912"/>is an operating system’s ability to accept incoming network packets on one interface, recognize them for
another, and pass them on to that network accordingly. When enabled, IP forwarding allows a Linux machine to receive
incoming packets and forward them. A Linux machine acting as an ordinary host would not need to have IP forwarding
enabled because it generates and receives IP traffic for its purposes. By default, it is turned off; let’s
enable it on our Ubuntu instance:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sysctl net.ipv4.ip_forward
net.ipv4.ip_forward <code class="o">=</code> 0
vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo <code class="nb">echo </code><code class="m">1</code> &gt; /proc/sys/net/ipv4/ip_forward
vagrant@ubuntu-xenial:~<code class="nv">$ </code> sysctl net.ipv4.ip_forward
net.ipv4.ip_forward <code class="o">=</code> 1</pre>

<p>With our install of the Ubuntu instance, we can see that we do not have any additional network namespaces, so let’s create one:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns list</pre>

<p><code>ip netns</code> allows <a data-type="indexterm" data-primary="netns, namespaces and" id="idm46219942292600"/>us to control the namespaces on the server. Creating one is as easy as typing <code>ip netns add net1</code>:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns add net1</pre>

<p>As we work through this example, we can see the network namespace we just created:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns list
net1</pre>

<p>Now that we have a new network namespace for our container, we will need a veth pair for communication between the <a data-type="indexterm" data-primary="root network namespaces" id="idm46219942188120"/>root
network namespace and the container network namespace <code>net1</code>.</p>

<p><code>ip</code> again allows administrators to create the veth pairs with a straightforward command. Remember from <a data-type="xref" href="ch02.xhtml#linux_networking">Chapter 2</a> that
veth comes in pairs and acts as a conduit between network namespaces, so packets from one end are automatically forwarded
to the other.</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link add veth0 <code class="nb">type </code>veth peer name veth1</pre>
<div data-type="tip"><h6>Tip</h6>
<p>Interfaces 4 and 5 are the veth pairs in the command output. We can also see which are paired with each other,
<code>veth1@veth0</code> and <code>veth0@veth1</code>.</p>
</div>

<p>The <code>ip link list</code> command <a data-type="indexterm" data-primary="veth (virtual Ethernet) pairs" data-secondary="creation of" id="ch3_term31"/>verifies the veth pair creation:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>ip link list
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue state
UNKNOWN mode DEFAULT group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc
pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 02:8f:67:5f:07:a5 brd ff:ff:ff:ff:ff:ff
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc
pfifo_fast state UP mode DEFAULT group default qlen 1000
    link/ether 08:00:27:0f:4e:0d brd ff:ff:ff:ff:ff:ff
4: veth1@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu <code class="m">1500</code> qdisc
noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 72:e4:03:03:c1:96 brd ff:ff:ff:ff:ff:ff
5: veth0@veth1: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu <code class="m">1500</code> qdisc
noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 26:1a:7f:2c:d4:48 brd ff:ff:ff:ff:ff:ff
vagrant@ubuntu-xenial:~<code class="err">$</code></pre>

<p>Now let’s move <code>veth1</code> into the new network namespace created previously:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link <code class="nb">set </code>veth1 netns net1</pre>

<p><code>ip netns exec</code> allows us to verify the network
namespace’s configuration. The output verifies that <code>veth1</code> is now in the network namespace <code>net</code>:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip link list
4: veth1@if5: &lt;BROADCAST,MULTICAST&gt; mtu <code class="m">1500</code> qdisc noop state
DOWN mode DEFAULT group default qlen 1000
    link/ether 72:e4:03:03:c1:96 brd ff:ff:ff:ff:ff:ff link-netnsid 0</pre>

<p>Network namespaces are entirely separate TCP/IP stacks in the Linux kernel. Being a new interface and in a new network
namespace, the <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="of veth interface" data-secondary-sortas="veth interface" id="idm46219942114088"/>veth interface will need IP addressing in order to carry packets from the <code>net1</code> namespace to the root
namespace and beyond the host:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns <code class="nb">exec</code>
net1 ip addr add 192.168.1.100/24 dev veth1</pre>

<p>As with host networking interfaces, they will need to be “turned on”:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip link <code class="nb">set </code>dev veth1 up</pre>

<p>The state has now transitioned to <code>LOWERLAYERDOWN</code>. The status <code>NO-CARRIER</code> points in the right direction. Ethernet
needs a cable to be connected; our upstream veth pair is not on yet either. The <code>veth1</code> interface
is up and addressed but effectively still “unplugged”:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip link list veth1
4: veth1@if5: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500
qdisc noqueue state LOWERLAYERDOWN mode DEFAULT
group default qlen <code class="m">1000</code> link/ether 72:e4:03:03:c1:96
brd ff:ff:ff:ff:ff:ff link-netnsid 0</pre>

<p>Let’s turn up the <code>veth0</code> side of the pair now:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link <code class="nb">set </code>dev veth0 up
vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link list
5: veth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500
qdisc noqueue state UP mode DEFAULT group default qlen 1000
link/ether 26:1a:7f:2c:d4:48 brd ff:ff:ff:ff:ff:ff link-netnsid 0</pre>

<p>Now the veth pair inside the <code>net1</code> namespace is <code>UP</code>:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip link list
4: veth1@if5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500
qdisc noqueue state UP mode DEFAULT group default qlen 1000
link/ether 72:e4:03:03:c1:96 brd ff:ff:ff:ff:ff:ff link-netnsid 0</pre>

<p>Both sides of the veth pair report up; we <a data-type="indexterm" data-primary="bridge interface" id="idm46219942017672"/><a data-type="indexterm" data-primary="veth (virtual Ethernet) pairs" data-secondary="bridge interface for" id="idm46219942017064"/>need to connect the root namespace veth side to the bridge interface. Make
sure to select the interface you’re working with, in this case <code>enp0s8</code>; it may be different for others:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link add br0 <code class="nb">type </code>bridge
vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link <code class="nb">set </code>dev br0 up
vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link <code class="nb">set </code>enp0s8 master br0
vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip link <code class="nb">set </code>veth0 master br0</pre>

<p>We can see that the enp0s8 and veth0 report are <a data-type="indexterm" data-startref="ch3_term31" id="idm46219941968776"/>part of the bridge <code>br0</code> interface, <code>master br0 state up</code>.</p>

<p>Next, let’s <a data-type="indexterm" data-primary="testing for network connections" id="idm46219941966760"/>test connectivity to our network namespace:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>ping 192.168.1.100 -c 4
PING 192.168.1.100 <code class="o">(</code>192.168.1.100<code class="o">)</code> 56<code class="o">(</code>84<code class="o">)</code> bytes of data.
From 192.168.1.10 <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">1</code> Destination Host Unreachable
From 192.168.1.10 <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">2</code> Destination Host Unreachable
From 192.168.1.10 <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">3</code> Destination Host Unreachable
From 192.168.1.10 <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">4</code> Destination Host Unreachable

--- 192.168.1.100 ping statistics ---
<code class="m">4</code> packets transmitted, <code class="m">0</code> received, +4 errors, 100% packet loss, <code class="nb">time </code>6043ms</pre>

<p>Our new <a data-type="indexterm" data-startref="ch3_term29" id="idm46219941916312"/>network namespace does not have a default route, so it does <a data-type="indexterm" data-primary="ping network utility" data-secondary="routing for" id="idm46219941915576"/>not know where to route our packets for the <code>ping</code>
requests:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1
ip route add default via 192.168.1.100
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip r
default via 192.168.1.100 dev veth1
192.168.1.0/24 dev veth1  proto kernel  scope link  src 192.168.1.100</pre>

<p>Let’s try that again:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>ping 192.168.2.100 -c 4
PING 192.168.2.100 <code class="o">(</code>192.168.2.100<code class="o">)</code> 56<code class="o">(</code>84<code class="o">)</code> bytes of data.
<code class="m">64</code> bytes from 192.168.2.100: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">1</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.018 ms
<code class="m">64</code> bytes from 192.168.2.100: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">2</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.028 ms
<code class="m">64</code> bytes from 192.168.2.100: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">3</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.036 ms
<code class="m">64</code> bytes from 192.168.2.100: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">4</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.043 ms

--- 192.168.2.100 ping statistics ---
<code class="m">4</code> packets transmitted, <code class="m">4</code> received, 0% packet loss, <code class="nb">time </code>2997ms</pre>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>ping 192.168.2.101 -c 4
PING 192.168.2.101 <code class="o">(</code>192.168.2.101<code class="o">)</code> 56<code class="o">(</code>84<code class="o">)</code> bytes of data.
<code class="m">64</code> bytes from 192.168.2.101: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">1</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.016 ms
<code class="m">64</code> bytes from 192.168.2.101: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">2</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.017 ms
<code class="m">64</code> bytes from 192.168.2.101: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">3</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.016 ms
<code class="m">64</code> bytes from 192.168.2.101: <code class="nv">icmp_seq</code><code class="o">=</code><code class="m">4</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">64</code> <code class="nb">time</code><code class="o">=</code>0.021 ms

--- 192.168.2.101 ping statistics ---
<code class="m">4</code> packets transmitted, <code class="m">4</code> received, 0% packet loss, <code class="nb">time </code>2997ms
rtt min/avg/max/mdev <code class="o">=</code> 0.016/0.017/0.021/0.004 ms</pre>

<p>Success! We have created the bridge interface and veth pairs, migrated one to the new network namespace, and tested
connectivity. <a data-type="xref" href="#Namespace-Recap">Example 3-3</a> is a recap of all the commands we ran to accomplish that.</p>
<div id="Namespace-Recap" data-type="example">
<h5><span class="label">Example 3-3. </span>Recap network namespace creation</h5>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code><code class="nb">echo </code><code class="m">1</code> &gt; /proc/sys/net/ipv4/ip_forward
<code class="nv">$ </code>sudo ip netns add net1
<code class="nv">$ </code>sudo ip link add veth0 <code class="nb">type </code>veth peer name veth1
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>veth1 netns net1
<code class="nv">$ </code>sudo ip link add veth0 <code class="nb">type </code>veth peer name veth1
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip addr add 192.168.1.101/24 dev veth1
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1 ip link <code class="nb">set </code>dev veth1 up
<code class="nv">$ </code>sudo ip link add br0 <code class="nb">type </code>bridge
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>dev br0 up
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>enp0s3 master br0
<code class="nv">$ </code>sudo ip link <code class="nb">set </code>veth0 master br0
<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>net1  ip route add default via 192.168.1.100</pre></div>

<p>For a developer <a data-type="indexterm" data-primary="system administrators" data-secondary="and application developers" data-secondary-sortas="application developers" id="idm46219941704328"/><a data-type="indexterm" data-primary="developers" id="idm46219941672664"/>not familiar with all these commands, that is a lot to remember and very easy to bork up! If the
bridge information is incorrect, it could take down an entire part of the network with network loops. These issues
are ones that system administrators would like to avoid, so they prevent developers from making those types of
networking changes on the system. <a data-type="indexterm" data-primary="application deployment" data-secondary="benefits of containers for" id="idm46219941671560"/><a data-type="indexterm" data-primary="containers" data-secondary="benefits of" id="idm46219941670648"/>Fortunately, containers help remove the developers’ strain to remember all
these commands and alleviate system admins’ fear of giving devs access to run those commands.</p>

<p>These  commands  are  all  needed  just  for  the  network  namespace  for  <em>every</em>  container  creation  and  deletion.  The 
namespace  creation  in  <a data-type="xref" href="#Namespace-Recap">Example 3-3</a> is  the  container runtime’s job. Docker manages this for us, in its way. The <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="standardization with" id="idm46219941667416"/>CNI project standardizes the network creation for all systems. The CNI, much like the
OCI, is a way for developers to standardize and prioritize specific tasks for managing parts of the container’s life
cycle.  In <a data-type="indexterm" data-startref="ch3_term18" id="idm46219941666136"/><a data-type="indexterm" data-startref="ch3_term19" id="idm46219941665464"/><a data-type="indexterm" data-startref="ch3_term21" id="idm46219941631432"/><a data-type="indexterm" data-startref="ch3_term22" id="idm46219941630760"/><a data-type="indexterm" data-startref="ch3_term23" id="idm46219941630088"/><a data-type="indexterm" data-startref="ch3_term24" id="idm46219941629416"/><a data-type="indexterm" data-startref="ch3_term25" id="idm46219941628744"/><a data-type="indexterm" data-startref="ch3_term26" id="idm46219941628072"/><a data-type="indexterm" data-startref="ch3_term27" id="idm46219941627400"/><a data-type="indexterm" data-startref="ch3_term28" id="idm46219941626728"/>later sections, we will discuss CNI.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Container Network Basics"><div class="sect1" id="idm46219942534648">
<h1>Container Network Basics</h1>

<p>The previous section showed us all the commands needed to create namespaces for our networking. Let’s
investigate how Docker does this for us. We also only used the bridge mode; there several other modes for container
networking.  This section will deploy several Docker containers and examine their networking and explain how containers
communicate externally to the host and with each other.</p>

<p>Let’s start by <a data-type="indexterm" data-primary="containers" data-secondary="network modes with" id="ch3_term32"/>discussing the several network “modes” used when working with 
<span class="keep-together">containers:</span></p>
<dl>
<dt>None</dt>
<dd>
<p>No <a data-type="indexterm" data-primary="none (null) container network mode" id="idm46219941620248"/>networking disables networking for the container.  Use this mode when the container does not need network
access.</p>
</dd>
<dt>Bridge</dt>
<dd>
<p>In <a data-type="indexterm" data-primary="bridge container network mode" id="idm46219941618008"/><a data-type="indexterm" data-primary="NAT (network address translation)" data-secondary="with bridge network mode" data-secondary-sortas="bridge network mode" id="idm46219941617304"/>bridge networking, the container runs in a private network internal to the host. Communication with other
containers in the network is open. Communication with services outside the host goes through Network Address Translation
(NAT) before exiting the host. Bridge mode is the default mode of networking when the <code>--net</code> option is not specified.</p>
</dd>
<dt>Host</dt>
<dd>
<p>In host <a data-type="indexterm" data-primary="host container network mode, Linux" id="idm46219941613800"/>networking, the container shares the same IP address and the network namespace as that of the host.
Processes running inside this container have the same network capabilities as services running directly on the host.
This mode is useful if the container needs access to network resources on the hosts. The container loses the benefit
of network segmentation with this mode of networking. Whoever is deploying the containers will have to manage and
contend with the ports of services running this node.</p>
</dd>
</dl>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>The host networking driver works only on Linux hosts. Docker Desktop for Mac and Windows, or Docker EE for Windows
Server, does not support host networking mode.</p>
</div>
<dl>
<dt>Macvlan</dt>
<dd>
<p>Macvlan <a data-type="indexterm" data-primary="Docker container technology" data-secondary="Macvlan networks and" id="idm46219941609448"/><a data-type="indexterm" data-primary="Macvlan container network mode" id="idm46219941608472"/>uses a parent interface. That interface can be a host interface such as
eth0, a subinterface, or even a bonded host adapter that bundles Ethernet interfaces into a single logical
interface. Like all Docker networks, Macvlan networks are segmented from each other, providing access within a
network, but not between networks. <a data-type="indexterm" data-primary="MAC (Media Access Control) sublayer" data-secondary="addresses of" id="idm46219941607320"/><a data-type="indexterm" data-primary="MAC (Media Access Control) sublayer" data-secondary="with Macvlan and IPvlan" id="idm46219941606360"/>Macvlan allows a physical interface to have multiple MAC and IP addresses
using Macvlan subinterfaces. Macvlan has four types: Private, VEPA, Bridge (which Docker default uses), and
Passthrough. With a bridge,  use NAT for external connectivity. With Macvlan, since hosts are directly mapped to
the physical network, external connectivity can be done using the same DHCP server and switch that the host uses.</p>
</dd>
</dl>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Most cloud providers block Macvlan networking. Administrative access to networking equipment is needed.</p>
</div>
<dl>
<dt>IPvlan</dt>
<dd>
<p>IPvlan is <a data-type="indexterm" data-primary="Ipvlan container network mode" id="idm46219941601640"/>similar to Macvlan, with a significant difference: IPvlan does not assign MAC addresses to
created subinterfaces. All subinterfaces share the parent’s interface MAC address but use different IP addresses.
IPvlan <a data-type="indexterm" data-primary="Network (Internet) layer, TCP/IP (L3)" data-secondary="IPvlan mode at" id="idm46219941600584"/>has two modes, L2 or L3. In IPvlan, L2, or layer 2, mode is analog to the Macvlan bridge mode.  IPvlan L3, or
layer 3, mode masquerades as a layer 3 device between the subinterfaces and parent interface.</p>
</dd>
<dt>Overlay</dt>
<dd>
<p>Overlay allows <a data-type="indexterm" data-primary="overlay container network mode" id="idm46219941580072"/>for the extension of the same network across hosts in a container cluster. The overlay
network virtually sits on top of the underlay/physical networks. Several <a data-type="indexterm" data-primary="open source projects" id="idm46219941579272"/>open source projects create these
overlay networks, which we will discuss later in the chapter.</p>
</dd>
<dt>Custom</dt>
<dd>
<p>Custom <a data-type="indexterm" data-primary="custom container network mode" id="idm46219941577112"/>bridge networking is the same as bridge networking but uses a bridge explicitly created for that
container. An example of using this would be a container that runs on a database bridge network. A separate container
can have an interface on the default and database bridge, enabling it to communicate with both networks as needed.</p>
</dd>
</dl>

<p>Container-defined <a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="in container-defined networking" data-secondary-sortas="container-defined networking" id="idm46219941575416"/>networking allows a container to share the address and network configuration of another container.
This sharing enables process isolation between containers, where each container runs one service but where services
can still communicate with one another on <code>127.0.0.1</code>.</p>

<p>To test all these <a data-type="indexterm" data-startref="ch3_term32" id="idm46219941572632"/>modes, we need to <a data-type="indexterm" data-primary="Vagrant/Vagrantfile" id="idm46219941571800"/><a data-type="indexterm" data-primary="Ubuntu" id="idm46219941571064"/>continue to use a Vagrant Ubuntu host but now with Docker
installed. Docker for Mac and Windows does not support host networking mode, so we must use Linux for this example.
You can do this with the provisioned machine in <a data-type="xref" href="ch01.xhtml#minmal_web_server_in_go">Example 1-1</a>  or use the Docker Vagrant version in the book’s code repo.
The <a data-type="indexterm" data-primary="Docker container technology" data-secondary="installing" id="ch3_term33"/>Ubuntu Docker install directions are as follows if you want to do it 
<span class="keep-together">manually:</span></p>

<pre data-type="programlisting">$ vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==&gt; default: Importing base box 'ubuntu/xenial64'...
==&gt; default: Matching MAC address for NAT networking...
==&gt; default: Checking if box
'ubuntu/xenial64' version '20200904.0.0' is up to date...
==&gt; default: Setting the name of the VM:
advanced_networking_code_examples_default_1600085275588_55198
==&gt; default: Clearing any previously set network interfaces...
==&gt; default: Available bridged network interfaces:
1) en12: USB 10/100 /1000LAN
2) en5: USB Ethernet(?)
3) en0: Wi-Fi (Wireless)
4) llw0
5) en11: USB 10/100/1000 LAN 2
6) en4: Thunderbolt 4
7) en1: Thunderbolt 1
8) en2: Thunderbolt 2
9) en3: Thunderbolt 3
==&gt; default: When choosing an interface, it is usually the one that is
==&gt; default: being used to connect to the internet.
==&gt; default:
    default: Which interface should the network bridge to? 1
==&gt; default: Preparing network interfaces based on configuration...
    default: Adapter 1: nat
    default: Adapter 2: bridged
==&gt; default: Forwarding ports...
    default: 22 (guest) =&gt; 2222 (host) (adapter 1)
==&gt; default: Running 'pre-boot' VM customizations...
==&gt; default: Booting VM...
==&gt; default: Waiting for machine to boot. This may take a few minutes...
    default: SSH address: 127.0.0.1:2222
    default: SSH username: vagrant
    default: SSH auth method: private key
    default: Warning: Connection reset. Retrying...
    default:
    default: Vagrant insecure key detected. Vagrant will automatically replace
    default: this with a newly generated keypair for better security.
    default:
    default: Inserting generated public key within guest...
    default: Removing insecure key from the guest if it's present...
    default: Key inserted! Disconnecting and reconnecting using new SSH key...
==&gt; default: Machine booted and ready!
==&gt; default: Checking for guest additions in VM...
==&gt; default: Configuring and enabling network interfaces...
==&gt; default: Mounting shared folders...
    default: /vagrant =&gt;
    /Users/strongjz/Documents/code/advanced_networking_code_examples
    default: + sudo docker run hello-world
    default: Unable to find image 'hello-world:latest' locally
    default: latest: Pulling from library/hello-world
    default: 0e03bdcc26d7:
    default: Pulling fs layer
    default: 0e03bdcc26d7:
    default: Verifying Checksum
    default: 0e03bdcc26d7:
    default: Download complete
    default: 0e03bdcc26d7:
    default: Pull complete
    default: Digest:
    sha256:4cf9c47f86df71d48364001ede3a4fcd85ae80ce02ebad74156906caff5378bc
    default: Status: Downloaded newer image for hello-world:latest
    default:
    default: Hello from Docker!
    default: This message shows that your
    default: installation appears to be working correctly.
    default:
    default: To generate this message, Docker took the following steps:
    default:  1. The Docker client contacted the Docker daemon.
    default:  2. The Docker daemon pulled the "hello-world" image
    default: from the Docker Hub.
    default:     (amd64)
    default:  3. The Docker daemon created a new container from that image
    default: which runs the executable that produces the output you are
    default: currently reading.
    default:  4. The Docker daemon streamed that output to the Docker
    default: client, which sent it to your terminal.
    default:
    default: To try something more ambitious, you can run an Ubuntu
    default: container with:
    default:  $ docker run -it ubuntu bash
    default:
    default: Share images, automate workflows, and more with a free Docker ID:
    default:  https://hub.docker.com
    default:
    default: For more examples and ideas, visit:
    default:  https://docs.docker.com/get-started</pre>

<p>Now that we have the host <a data-type="indexterm" data-startref="ch3_term33" id="idm46219941562536"/>up, let’s begin <a data-type="indexterm" data-primary="Ubuntu" id="ch3_term36"/><a data-type="indexterm" data-primary="Vagrant/Vagrantfile" id="ch3_term37"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="network types in" id="ch3_term38"/>investigating the different networking setups we have to work with in
Docker. <a data-type="xref" href="#docker_networks">Example 3-4</a> shows that Docker creates three network types during the install: bridge, host, and none.</p>
<div id="docker_networks" data-type="example">
<h5><span class="label">Example 3-4. </span>Docker networks</h5>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker network ls
NETWORK ID          NAME                DRIVER              SCOPE
1fd1db59c592        bridge              bridge              <code class="nb">local</code>
eb34a2105b0f        host                host                <code class="nb">local</code>
941ce103b382        none                null                <code class="nb">local</code>
vagrant@ubuntu-xenial:~<code class="err">$</code></pre></div>

<p>The default is a <a data-type="indexterm" data-primary="docker0 bridge interface" id="idm46219941552456"/><a data-type="indexterm" data-primary="bridge networking, Docker" id="ch3_term34"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="bridge networking in" id="ch3_term35"/>Docker bridge, and a container gets attached to it and provisioned with an IP address in the <code>172.17.0.0/16</code>
default subnet. <a data-type="xref" href="#docker_bridge_interfaces">Example 3-5</a> is a view of Ubuntu’s default interfaces and the Docker install that creates the <code>docker0</code>
bridge interface for the host.</p>
<div id="docker_bridge_interfaces" data-type="example">
<h5><span class="label">Example 3-5. </span>Docker bridge interface</h5>

<pre data-type="programlisting" data-code-language="bash"><code>vagrant@ubuntu-xenial:~</code><code class="nv">$ </code><code>ip</code><code> </code><code>a</code><code>
</code><code>1:</code><code> </code><code>lo:</code><code> </code><code>&lt;</code><code>LOOPBACK,UP,LOWER_UP&gt;</code><code> </code><code>mtu</code><code> </code><code class="m">65536</code><code> </code><code>qdisc</code><code>
</code><code>noqueue</code><code> </code><code>state</code><code> </code><code>UNKNOWN</code><code> </code><code>group</code><code> </code><code>default</code><code> </code><code>qlen</code><code> </code><code class="m">1</code><code> </code><a class="co" id="co_container_networking_basics_CO1-1" href="#callout_container_networking_basics_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
    </code><code>link/loopback</code><code> </code><code>00:00:00:00:00:00</code><code> </code><code>brd</code><code> </code><code>00:00:00:00:00:00</code><code>
    </code><code>inet</code><code> </code><code>127.0.0.1/8</code><code> </code><code>scope</code><code> </code><code>host</code><code> </code><code>lo</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
    </code><code>inet6</code><code> </code><code>::1/128</code><code> </code><code>scope</code><code> </code><code>host</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
</code><code>2:</code><code> </code><code>enp0s3:</code><code>
</code><code>&lt;</code><code>BROADCAST,MULTICAST,UP,LOWER_UP&gt;</code><code> </code><code>mtu</code><code> </code><code class="m">1500</code><code> </code><code>qdisc</code><code> </code><code>pfifo_fast</code><code> </code><code>state</code><code> </code><code>UP</code><code> </code><code>group</code><code>
</code><code>default</code><code> </code><code>qlen</code><code> </code><code class="m">1000</code><code> </code><a class="co" id="co_container_networking_basics_CO1-2" href="#callout_container_networking_basics_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
    </code><code>link/ether</code><code> </code><code>02:8f:67:5f:07:a5</code><code> </code><code>brd</code><code> </code><code>ff:ff:ff:ff:ff:ff</code><code>
    </code><code>inet</code><code> </code><code>10.0.2.15/24</code><code> </code><code>brd</code><code> </code><code>10.0.2.255</code><code> </code><code>scope</code><code> </code><code>global</code><code> </code><code>enp0s3</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
    </code><code>inet6</code><code> </code><code>fe80::8f:67ff:fe5f:7a5/64</code><code> </code><code>scope</code><code> </code><code>link</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
</code><code>3:</code><code> </code><code>enp0s8:</code><code>
</code><code>&lt;</code><code>BROADCAST,MULTICAST,UP,LOWER_UP&gt;</code><code> </code><code>mtu</code><code> </code><code class="m">1500</code><code> </code><code>qdisc</code><code> </code><code>pfifo_fast</code><code> </code><code>state</code><code> </code><code>UP</code><code> </code><code>group</code><code>
</code><code>default</code><code> </code><code>qlen</code><code> </code><code class="m">1000</code><code> </code><a class="co" id="co_container_networking_basics_CO1-3" href="#callout_container_networking_basics_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
    </code><code>link/ether</code><code> </code><code>08:00:27:22:0e:46</code><code> </code><code>brd</code><code> </code><code>ff:ff:ff:ff:ff:ff</code><code>
    </code><code>inet</code><code> </code><code>192.168.1.19/24</code><code> </code><code>brd</code><code> </code><code>192.168.1.255</code><code> </code><code>scope</code><code> </code><code>global</code><code> </code><code>enp0s8</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
    </code><code>inet</code><code> </code><code>192.168.1.20/24</code><code> </code><code>brd</code><code> </code><code>192.168.1.255</code><code> </code><code>scope</code><code> </code><code>global</code><code> </code><code>secondary</code><code> </code><code>enp0s8</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
    </code><code>inet6</code><code> </code><code>2605:a000:160d:517:a00:27ff:fe22:e46/64</code><code> </code><code>scope</code><code> </code><code>global</code><code> </code><code>mngtmpaddr</code><code> </code><code>dynamic</code><code>
    </code><code>valid_lft</code><code> </code><code>604600sec</code><code> </code><code>preferred_lft</code><code> </code><code>604600sec</code><code>
    </code><code>inet6</code><code> </code><code>fe80::a00:27ff:fe22:e46/64</code><code> </code><code>scope</code><code> </code><code>link</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
</code><code>4:</code><code> </code><code>docker0:</code><code>
</code><code>&lt;</code><code>NO-CARRIER,BROADCAST,MULTICAST,UP&gt;</code><code> </code><code>mtu</code><code> </code><code class="m">1500</code><code> </code><code>qdisc</code><code> </code><code>noqueue</code><code> </code><code>state</code><code> </code><code>DOWN</code><code> </code><code>group</code><code>
</code><code>default</code><code> </code><a class="co" id="co_container_networking_basics_CO1-4" href="#callout_container_networking_basics_CO1-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
    </code><code>link/ether</code><code> </code><code>02:42:7d:50:c7:01</code><code> </code><code>brd</code><code> </code><code>ff:ff:ff:ff:ff:ff</code><code>
    </code><code>inet</code><code> </code><code>172.17.0.1/16</code><code> </code><code>brd</code><code> </code><code>172.17.255.255</code><code> </code><code>scope</code><code> </code><code>global</code><code> </code><code>docker0</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code><code>
    </code><code>inet6</code><code> </code><code>fe80::42:7dff:fe50:c701/64</code><code> </code><code>scope</code><code> </code><code>link</code><code>
    </code><code>valid_lft</code><code> </code><code>forever</code><code> </code><code>preferred_lft</code><code> </code><code>forever</code></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_container_networking_basics_CO1-1" href="#co_container_networking_basics_CO1-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>This is the <a data-type="indexterm" data-primary="loopback interface (lo)" id="idm46219941444184"/>loopback interface.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO1-2" href="#co_container_networking_basics_CO1-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p>enp0s3 is our <a data-type="indexterm" data-primary="virtual box interface" id="idm46219941384472"/>NAT’ed virtual box interface.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO1-3" href="#co_container_networking_basics_CO1-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p>enp0s8 is the host interface; this is on the same network as our host and uses DHCP to get the <code>192.168.1.19</code>
address of default Docker bridge.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO1-4" href="#co_container_networking_basics_CO1-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p>The default Docker container interface uses bridge mode.</p></dd>
</dl></div>

<p><a data-type="xref" href="#docker_bridge">Example 3-6</a> started a <a data-type="indexterm" data-primary="busybox containers" id="ch3_term39"/>busybox container with the <code>docker run</code> command and requested that the Docker returns the
container’s IP address. Docker default NATed address is <code>172.17.0.0/16</code>, with our busybox container getting <code>172.17.0.2</code>.</p>
<div id="docker_bridge" data-type="example">
<h5><span class="label">Example 3-6. </span>Docker bridge</h5>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker run -it busybox ip a
Unable to find image <code class="s1">'busybox:latest'</code> locally
latest: Pulling from library/busybox
df8698476c65: Pull <code class="nb">complete</code>
Digest: sha256:d366a4665ab44f0648d7a00ae3fae139d55e32f9712c67accd604bb55df9d05a
Status: Downloaded newer image <code class="k">for</code> busybox:latest
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu <code class="m">1500</code> qdisc noqueue
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
    valid_lft forever preferred_lft forever</pre></div>

<p>The host <a data-type="indexterm" data-startref="ch3_term35" id="idm46219941391144"/><a data-type="indexterm" data-startref="ch3_term34" id="idm46219941390648"/><a data-type="indexterm" data-primary="host networking, Docker" id="ch3_term40"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="host networking in" id="ch3_term41"/>networking in <a data-type="xref" href="#docker_host_networking">Example 3-7</a> shows that the container shares the same network namespace as the host. We can
see that the interfaces are the same as that of the host; enp0s3, enp0s8, and docker0 are present in the container
<code>ip a</code> command 
<span class="keep-together">output.</span></p>
<div id="docker_host_networking" data-type="example">
<h5><span class="label">Example 3-7. </span>Docker host networking</h5>

<pre data-type="programlisting">vagrant@ubuntu-xenial:~$ sudo docker run -it --net=host busybox ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
    valid_lft forever preferred_lft forever`
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000
    link/ether 02:8f:67:5f:07:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global enp0s3
    valid_lft forever preferred_lft forever
    inet6 fe80::8f:67ff:fe5f:7a5/64 scope link
    valid_lft forever preferred_lft forever
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast qlen 1000
    link/ether 08:00:27:22:0e:46 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.19/24 brd 192.168.1.255 scope global enp0s8
    valid_lft forever preferred_lft forever
    inet 192.168.1.20/24 brd 192.168.1.255 scope global secondary enp0s8
    valid_lft forever preferred_lft forever
    inet6 2605:a000:160d:517:a00:27ff:fe22:e46/64 scope global dynamic
    valid_lft 604603sec preferred_lft 604603sec
    inet6 fe80::a00:27ff:fe22:e46/64 scope link
    valid_lft forever preferred_lft forever
4: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue
    link/ether 02:42:7d:50:c7:01 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    valid_lft forever preferred_lft forever
    inet6 fe80::42:7dff:fe50:c701/64 scope link
    valid_lft forever preferred_lft forever</pre></div>

<p>From the veth bridge example previously set up, let’s see how much simpler it is when Docker manages that
for us. To view this, we need a process to keep the container running. The following command
starts up a busybox container and drops into an <code>sh</code> command line:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker run -it --rm busybox /bin/sh
/#</pre>

<p>We have a <a data-type="indexterm" data-primary="Ethernet" data-secondary="in host networking, Docker" data-secondary-sortas="host networking, Docker" id="idm46219941242936"/><a data-type="indexterm" data-primary="loopback interface (lo)" id="idm46219941493112"/><a data-type="indexterm" data-primary="IP (Internet Protocol) addresses" data-secondary="of veth interface" data-secondary-sortas="veth interface" id="idm46219941492440"/>loopback interface, <code>lo</code>, and an Ethernet interface <code>eth0</code> connected to veth12, with a Docker default IP
address of <code>172.17.0.2</code>. Since our previous command only outputted an <code>ip a</code> result and the container exited
afterward, Docker reused the IP address <code>172.17.0.2</code> for the running busybox <a data-type="indexterm" data-startref="ch3_term39" id="idm46219941457912"/>container:</p>

<pre data-type="programlisting" data-code-language="bash">/# ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
11: eth0@if12: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu <code class="m">1500</code> qdisc noqueue
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
    valid_lft forever preferred_lft forever</pre>

<p>Running the <code>ip r</code> inside the container’s network namespace, we can see that the <a data-type="indexterm" data-primary="route tables" id="idm46219941321880"/>container’s route table is automatically
set up as well:</p>

<pre data-type="programlisting" data-code-language="bash">/ <code class="c"># ip r</code>
default via 172.17.0.1 dev eth0
172.17.0.0/16 dev eth0 scope link  src 172.17.0.2</pre>

<p>If we open a <a data-type="indexterm" data-primary="ssh command, Linux" id="idm46219941301320"/>new terminal and <code>vagrant ssh</code> into our Vagrant Ubuntu instance and run the <code>docker ps</code> command, it
shows all the information in the running busybox container:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker ps
CONTAINER ID        IMAGE       COMMAND
3b5a7c3a74d5        busybox     <code class="s2">"/bin/sh"</code>

CREATED         STATUS        PORTS     NAMES
<code class="m">47</code> seconds ago  Up <code class="m">46</code> seconds           competent_mendel</pre>

<p>We can see the <a data-type="indexterm" data-primary="veth (virtual Ethernet) pairs" data-secondary="bridge interface for" id="idm46219941113048"/><a data-type="indexterm" data-primary="docker0 bridge interface" id="idm46219941112200"/>veth interface Docker set up for the container <code>veth68b6f80@if11</code> on the same host’s
networking namespace. It is a member of the bridge for <code>docker0</code> and is turned on <code>master docker0
state UP</code>:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue state UNKNOWN group
default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
    valid_lft forever preferred_lft forever
2: enp0s3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc pfifo_fast state UP
group default qlen 1000
    link/ether 02:8f:67:5f:07:a5 brd ff:ff:ff:ff:ff:ff
    inet 10.0.2.15/24 brd 10.0.2.255 scope global enp0s3
    valid_lft forever preferred_lft forever
    inet6 fe80::8f:67ff:fe5f:7a5/64 scope link
    valid_lft forever preferred_lft forever
3: enp0s8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc pfifo_fast state UP
group default qlen 1000
    link/ether 08:00:27:22:0e:46 brd ff:ff:ff:ff:ff:ff
    inet 192.168.1.19/24 brd 192.168.1.255 scope global enp0s8
    valid_lft forever preferred_lft forever
    inet 192.168.1.20/24 brd 192.168.1.255 scope global secondary enp0s8
    valid_lft forever preferred_lft forever
    inet6 2605:a000:160d:517:a00:27ff:fe22:e46/64 scope global mngtmpaddr dynamic
    valid_lft 604745sec preferred_lft 604745sec
    inet6 fe80::a00:27ff:fe22:e46/64 scope link
    valid_lft forever preferred_lft forever
4: docker0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc noqueue state UP
group default
    link/ether 02:42:7d:50:c7:01 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
    valid_lft forever preferred_lft forever
    inet6 fe80::42:7dff:fe50:c701/64 scope link
    valid_lft forever preferred_lft forever
12: veth68b6f80@if11: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc noqueue
master docker0 state UP group default
    link/ether 3a:64:80:02:87:76 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet6 fe80::3864:80ff:fe02:8776/64 scope link
    valid_lft forever preferred_lft forever</pre>

<p>The Ubuntu <a data-type="indexterm" data-primary="route tables" id="idm46219941337480"/>host’s route table shows Docker’s routes for reaching containers running on the host:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>ip r
default via 192.168.1.1 dev enp0s8
10.0.2.0/24 dev enp0s3  proto kernel  scope link  src 10.0.2.15
172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1
192.168.1.0/24 dev enp0s8  proto kernel  scope link  src 192.168.1.19</pre>

<p>By default, Docker <a data-type="indexterm" data-primary="namespaces, network" data-secondary="in Docker" data-secondary-sortas="Docker" id="idm46219941177800"/><a data-type="indexterm" data-primary="netns list" id="idm46219941176648"/>does not add the network namespaces it creates to <code>/var/run</code> where <code>ip netns list</code> expects newly
created network namespaces. Let’s work through how we can see those namespaces now. Three <a data-type="indexterm" data-primary="PID namespace, Linux" id="idm46219941266520"/>steps are required to list the
Docker network namespaces from the <code>ip</code> command:</p>
<ol>
<li>
<p>Get the running container’s PID.</p>
</li>
<li>
<p>Soft link the network namespace from <code>/proc/PID/net/</code> to <code>/var/run/netns</code>.</p>
</li>
<li>
<p>List the network namespace.</p>
</li>

</ol>

<p class="pagebreak-before"><code>docker ps</code> outputs the <a data-type="indexterm" data-primary="container ID" id="idm46219941080488"/><a data-type="indexterm" data-primary="process id" id="idm46219941079752"/>container ID needed to inspect the running PID on the host PID namespace:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker ps
CONTAINER ID        IMAGE               COMMAND
1f3f62ad5e02        busybox             <code class="s2">"/bin/sh"</code>

CREATED             STATUS              PORTS NAMES
<code class="m">11</code> minutes ago      Up <code class="m">11</code> minutes       determined_shamir</pre>

<p><code>docker inspect</code> allows us to parse the output and get the host’s process’s PID. If we run <code>ps -p</code> on the host
PID namespace, we can see it is running <code>sh</code>, which tracks our <code>docker run</code> command:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker inspect -f <code class="s1">'{{.State.Pid}}'</code> 1f3f62ad5e02
25719
vagrant@ubuntu-xenial:~<code class="nv">$ </code>ps -p 25719
  PID TTY          TIME CMD
<code class="m">25719</code> pts/0    00:00:00 sh</pre>

<p><code>1f3f62ad5e02</code> is the container ID, and <code>25719</code> is the PID of the busybox container running <code>sh</code>, so now we can create a
symbolic link for the container’s network namespace created by Docker to where <code>ip</code> expects with the following command:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo ln -sfT /proc/25719/ns/net /var/run/netns/1f3f62ad5e02</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>When using the container ID and process ID from the examples, keep in mind they will be different on your systems.</p>
</div>

<p>Now <a data-type="indexterm" data-primary="netns command" id="idm46219941050456"/>the <code>ip netns exec</code> commands return the same IP address, <code>172.17.0.2</code>, that the <code>docker exec</code> command
does:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo ip netns <code class="nb">exec </code>1f3f62ad5e02 ip a
1: lo:
&lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
13: eth0@if14:
&lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <code class="m">1500</code> qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever</pre>

<p class="pagebreak-before">We can verify with <code>docker exec</code> and run <code>ip an</code> inside the busybox container. The IP address, MAC address, and
network interfaces all match the output:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker <code class="nb">exec </code>1f3f62ad5e02 ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
13: eth0@if14: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu <code class="m">1500</code> qdisc noqueue
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
       valid_lft forever preferred_lft forever</pre>

<p>Docker <a data-type="indexterm" data-startref="ch3_term36" id="idm46219941049080"/><a data-type="indexterm" data-startref="ch3_term37" id="idm46219941048472"/><a data-type="indexterm" data-primary="containers" data-secondary="benefits of" id="idm46219941047832"/>starts our container; creates the network namespace, the veth pair, and the docker0 bridge (if it does not already
exist); and then attaches them all for every container creation and deletion, in a single command! That is powerful from an
application developer’s perspective. There’s no need to remember all those Linux commands and possibly break the networking on
a host. This discussion has mostly been about a single host. How Docker coordinates container communication between hosts in
a cluster is <a data-type="indexterm" data-startref="ch3_term40" id="idm46219941046232"/><a data-type="indexterm" data-startref="ch3_term41" id="idm46219941058392"/>discussed in the next section.</p>








<section data-type="sect2" data-pdf-bookmark="Docker Networking Model"><div class="sect2" id="idm46219941057464">
<h2>Docker Networking Model</h2>

<p>Libnetwork is <a data-type="indexterm" data-primary="CNM (container networking model)" id="ch3_term43"/><a data-type="indexterm" data-primary="container networking model (CNM)" id="ch3_term44"/><a data-type="indexterm" data-primary="sandbox, Docker" id="idm46219940964008"/><a data-type="indexterm" data-primary="Libnetwork, Docker" id="idm46219940963336"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="networking model in" id="ch3_term42"/>Docker’s take on container networking, and its design philosophy is in the container networking
model (CNM). Libnetwork implements the CNM and works in three components: the sandbox,
endpoint, and network. The sandbox implements the management of the Linux <a data-type="indexterm" data-primary="namespaces, network" data-secondary="in Docker" data-secondary-sortas="Docker" id="idm46219940957896"/>network namespaces for all containers
running on the host. The <a data-type="indexterm" data-primary="network, Libnetwork CNM" id="idm46219940956552"/>network component is a collection of endpoints on the same network. <a data-type="indexterm" data-primary="endpoints, Docker" id="idm46219940955752"/>Endpoints are hosts on
the network. The network controller manages all of this via APIs in the Docker engine.</p>

<p>On the endpoint, <a data-type="indexterm" data-primary="iptables, Linux" data-secondary="Docker and" id="idm46219940954472"/>Docker uses <code>iptables</code> for network isolation. The container publishes a port to be accessed externally. <a data-type="indexterm" data-primary="RFC 1918 addresses" id="idm46219940952920"/><a data-type="indexterm" data-primary="containers" data-secondary="addresses for" id="idm46219940930168"/>Containers do not receive a public IPv4 address; they receive a private RFC 1918 address. <a data-type="indexterm" data-primary="ports" data-secondary="container connections to" id="idm46219940928984"/>Services running on a
container must be exposed port by port, and container ports have to be mapped to the host port so conflicts are
avoided. When Docker starts, it <a data-type="indexterm" data-primary="docker0 bridge interface" id="idm46219940927768"/>creates a virtual bridge interface, <code>docker0</code>, on the host machine and assigns it a random
IP address from the private 1918 range. This bridge passes packets between two connected devices, just like a physical
bridge does. Each new container gets one interface automatically attached to the <code>docker0</code> bridge; <a data-type="xref" href="#docker-bridge">Figure 3-9</a>
represents this and is similar to the approach we demonstrated in the previous sections.</p>

<figure><div id="docker-bridge" class="figure">
<img src="Images/neku_0309.png" alt="neku 0309" width="668" height="646"/>
<h6><span class="label">Figure 3-9. </span>Docker bridge</h6>
</div></figure>

<p>The CNM maps the <a data-type="indexterm" data-primary="containers" data-secondary="network modes with" id="idm46219940922664"/><a data-type="indexterm" data-primary="custom container network mode" id="idm46219940921656"/><a data-type="indexterm" data-primary="none (null) container network mode" id="idm46219940920968"/><a data-type="indexterm" data-primary="remote container network mode" id="idm46219940920280"/><a data-type="indexterm" data-primary="bridge container network mode" id="idm46219940919592"/><a data-type="indexterm" data-primary="overlay container network mode" id="ch3_term45"/>network modes to drives we have already discussed. Here is a list of the networking
mode and the Docker engine equivalent:</p>
<dl>
<dt>Bridge</dt>
<dd>
<p>Default Docker bridge (see <a data-type="xref" href="#docker-bridge">Figure 3-9</a>, and our previous examples show this)</p>
</dd>
<dt>Custom or Remote</dt>
<dd>
<p>User-defined bridge, or allows users to create or use their plugin</p>
</dd>
<dt>Overlay</dt>
<dd>
<p>Overlay</p>
</dd>
<dt>Null</dt>
<dd>
<p>No networking options</p>
</dd>
</dl>

<p>Bridge networks are for containers running on the same host. Communicating with containers running on different hosts
can use an overlay network. <a data-type="indexterm" data-startref="ch3_term38" id="idm46219940897432"/>Docker uses the <a data-type="indexterm" data-primary="drivers, local and global" id="idm46219940896696"/><a data-type="indexterm" data-primary="local drivers" id="idm46219940896088"/><a data-type="indexterm" data-primary="containers" data-secondary="on local systems" data-secondary-sortas="local systems" id="idm46219940895480"/><a data-type="indexterm" data-primary="global drivers" id="idm46219940894392"/>concept of local and global drivers. Local drivers, a bridge, for example,
are host-centric and do not do cross-node coordination. That is the job of global drivers such as Overlay. <a data-type="indexterm" data-primary="key-value store (libkv)" id="idm46219940893464"/><a data-type="indexterm" data-primary="libkv (key-value store)" id="idm46219940892856"/>Global
drivers rely on libkv, a key-value store abstraction, to coordinate across machines.  The CNM does not provide the
key-value store, so external ones like Consul, etcd, and Zookeeper are needed.</p>

<p>The next section will discuss in depth the technologies enabling overlay networks.</p>
</div></section>













<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Overlay Networking"><div class="sect2" id="idm46219940891512">
<h2>Overlay Networking</h2>

<p>Thus far, our examples have been on a single host, but <a data-type="indexterm" data-primary="containers" data-secondary="on global systems" data-secondary-sortas="global systems" id="ch3_term46"/><a data-type="indexterm" data-primary="communication" data-secondary="host-to-host" id="ch3_term47"/><a data-type="indexterm" data-primary="data" data-secondary="host-to-host transfer of" id="ch3_term48"/><a data-type="indexterm" data-primary="host-to-host data transfers" id="ch3_term49"/><a data-type="indexterm" data-primary="VXLAN (virtual extensible LAN)" id="ch3_term50"/>production applications at scale do not run on a
single host. For applications running in containers on separate nodes to communicate, several issues need to be
solved, such as how to coordinate routing information between hosts, port conflicts, and IP address management, to name a few. One
technology that helps with routing between hosts for containers is a VXLAN. In <a data-type="xref" href="#vxlan-hosts">Figure 3-10</a>, we can see a layer 2 overlay network created with a VXLAN running over the physical L3 network.</p>

<p>We briefly discussed VXLANs in <a data-type="xref" href="ch01.xhtml#networking_introduction">Chapter 1</a>, but a more in-depth explanation of how the data transfer works to enable
the container-to-container communication is warranted here.</p>

<figure><div id="vxlan-hosts" class="figure">
<img src="Images/neku_0310.png" alt="neku 0310" width="677" height="423"/>
<h6><span class="label">Figure 3-10. </span>VXLAN tunnel</h6>
</div></figure>

<p>A VXLAN is an extension of the <a data-type="indexterm" data-primary="VLAN (virtual local area network)" id="idm46219940879560"/><a data-type="indexterm" data-primary="Ethernet" data-secondary="VLAN identifiers for" id="idm46219940878952"/><a data-type="indexterm" data-primary="local area network, virtual (VLAN)" id="idm46219940878104"/>VLAN protocol creating 16 million unique identifiers. <a data-type="indexterm" data-primary="IEEE 802 standards" id="idm46219940877368"/>Under IEEE 802.1Q, the maximum
number of VLANs on a given Ethernet network is 4,094. The transport protocol over a  physical data center network is
IP plus UDP. <a data-type="indexterm" data-primary="MAC-in-UDP, VXLAN and" id="idm46219940876456"/><a data-type="indexterm" data-primary="UDP (User Datagram textProtocol)" data-secondary="with VXLAN" data-secondary-sortas="VXLAN" id="idm46219940875848"/>VXLAN defines a MAC-in-UDP encapsulation scheme where the original layer 2 frame has a VXLAN header
added wrapped in a UDP IP packet. <a data-type="xref" href="#docker-overlay-advance">Figure 3-11</a> shows the IP packet encapsulated in the UDP packet and its headers.</p>

<p>A VXLAN packet is a MAC-in-UDP encapsulated packet. The layer 2 frame has a VXLAN header added to it and is placed in a
UDP-IP packet. The VXLAN identifier is 24 bits. That is how a VXLAN can support 16 million segments.</p>

<p><a data-type="xref" href="#docker-overlay-advance">Figure 3-11</a> is a more detailed version of <a data-type="xref" href="ch01.xhtml#networking_introduction">Chapter 1</a>. We have the <a data-type="indexterm" data-primary="VTEPs, VXLAN and" id="idm46219940871528"/><a data-type="indexterm" data-primary="endpoints, Docker" id="idm46219940870920"/>VXLAN tunnel endpoints, VTEPs, on both
hosts, and they are attached to the host’s bridge interfaces with the containers attached to the bridge. The VTEP performs data frame
encapsulation and decapsulation. The VTEP peer interaction ensures that the data gets
forwarded to the relevant destination container addresses. The data leaving the containers is encapsulated with
VXLAN information and transferred over the VXLAN tunnels to be de-encapsulated by the peer VTEP.</p>

<p>Overlay networking enables cross-host communication on the network for containers. The CNM still has other issues
that make it incompatible with Kubernetes.  The <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="overview of" id="idm46219940868920"/><a data-type="indexterm" data-primary="CoreOS CNI" id="idm46219940868072"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI for" id="ch3_term51"/>Kubernetes maintainers decided to use the CNI
project started at CoreOS. It is simpler than CNM, does not require daemons, and is designed to be <a data-type="indexterm" data-startref="ch3_term42" id="idm46219940865880"/><a data-type="indexterm" data-startref="ch3_term43" id="idm46219940865208"/><a data-type="indexterm" data-startref="ch3_term44" id="idm46219940864536"/><a data-type="indexterm" data-startref="ch3_term45" id="idm46219940863864"/><a data-type="indexterm" data-startref="ch3_term46" id="idm46219940863192"/><a data-type="indexterm" data-startref="ch3_term47" id="idm46219940862520"/><a data-type="indexterm" data-startref="ch3_term48" id="idm46219940861848"/><a data-type="indexterm" data-startref="ch3_term49" id="idm46219940861176"/><a data-type="indexterm" data-startref="ch3_term50" id="idm46219940860504"/>cross-platform.</p>

<figure><div id="docker-overlay-advance" class="figure">
<img src="Images/neku_0311.png" alt="neku 0311" width="1073" height="918"/>
<h6><span class="label">Figure 3-11. </span>VXLAN tunnel detailed</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Container Network Interface"><div class="sect2" id="idm46219940891048">
<h2>Container Network Interface</h2>

<p>CNI is the software interface between the container runtime and the
network implementation. There are many options to choose from when implementing a CNI; we will discuss a few notable
ones. CNI started at CoreOS as part of the rkt project; it is now a CNCF project. The
CNI project consists of a specification and libraries for developing plugins to configure network interfaces in Linux
containers. CNI is concerned with a container’s network connectivity by allocating resources when the container gets
created and removing them when deleted. A CNI plugin is responsible for associating a network interface to the
container network namespace and making any necessary changes to the host. It then assigns the IP to the interface and
sets up the routes for it. <a data-type="xref" href="#cni">Figure 3-12</a> outlines the CNI architecture. The container runtime uses a configuration file for
the host’s network information; in <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="Kubernetes plugins with" id="idm46219940854632"/><a data-type="indexterm" data-primary="Kubelet, Kubernetes" data-secondary="configuration file of" id="idm46219940853720"/><a data-type="indexterm" data-primary="Kubernetes networking" data-secondary="CNI plug-ins for" id="idm46219940852776"/>Kubernetes, the Kubelet also uses this configuration file. The CNI and container
runtime communicate with each other and apply commands to the configured CNI plugin.</p>

<figure><div id="cni" class="figure">
<img src="Images/neku_0312.png" alt="neku 0312" width="1052" height="788"/>
<h6><span class="label">Figure 3-12. </span>CNI architecture</h6>
</div></figure>

<p>There are several <a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="projects using" id="ch3_term52"/><a data-type="indexterm" data-primary="open source projects" id="idm46219940847848"/>open source projects that implement CNI plugins with various features and functionality. Here is an
outline of several:</p>
<dl>
<dt>Cilium</dt>
<dd>
<p>Cilium is <a data-type="indexterm" data-primary="Cilium" data-secondary="as CNI plugin" data-secondary-sortas="CNI plugin" id="idm46219940845224"/>open source software for securing network connectivity between application containers. Cilium
is an L7/HTTP-aware CNI and can enforce network policies on L3–L7 using an identity-based security model decoupled from
network addressing. A <a data-type="indexterm" data-primary="Cilium" data-secondary="eBPF for" id="idm46219940843560"/>Linux technology eBPF powers it.</p>
</dd>
<dt>Flannel</dt>
<dd>
<p>Flannel is a <a data-type="indexterm" data-primary="Flannel CNI plugin" id="idm46219940841208"/>simple way to configure a layer 3 network fabric designed for Kubernetes. Flannel focuses
on networking. Flannel
uses the Kubernetes cluster’s existing <code>etcd</code> datastore to store its state information to avoid providing a dedicated one.</p>
</dd>
<dt>Calico</dt>
<dd>
<p>According to <a data-type="indexterm" data-primary="Calico CNI plugin" id="idm46219940838472"/>Calico, it “combines flexible networking capabilities with run-anywhere security enforcement to provide
a solution with native Linux kernel performance and true cloud-native scalability.” It has full network policy
support and works well in conjunction with other CNIs. Calico does not use an overlay network. Instead, <a data-type="indexterm" data-primary="Border Gateway Protocol (BGP)" id="idm46219940837272"/>Calico
configures a layer 3 network that uses the BGP routing protocol to route packets between hosts. <a data-type="indexterm" data-primary="Istio service mesh" id="idm46219940836344"/>Calico can also
integrate with Istio, a service mesh, to interpret and enforce policy for workloads within the cluster, both at the
service mesh and the network infrastructure layers.</p>
</dd>
<dt>AWS</dt>
<dd>
<p>AWS has its <a data-type="indexterm" data-primary="AWS (Amazon Web Services)" data-secondary="AWS VPC CNI in" id="idm46219940833848"/><a data-type="indexterm" data-primary="cloud networking and Kubernetes" data-secondary="CNIs for" id="idm46219940832856"/><a data-type="indexterm" data-primary="cluster administrators" data-secondary="AWS VPC networks and" id="idm46219940831912"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="AWS VPC CNI as" id="idm46219940830968"/><a data-type="indexterm" data-primary="CNI (Container Network Interface)" data-secondary="cloud networking with" id="idm46219940830008"/><a data-type="indexterm" data-primary="network administrators" id="idm46219940829048"/><a data-type="indexterm" data-primary="VPCs (virtual private clouds)" data-secondary="with AWS" data-secondary-sortas="AWS" id="idm46219940828376"/>open source implementation of a CNI, the AWS VPC CNI. It provides high throughput and availability by
being directly on the AWS network. There is low latency using this CNI by providing little overhead because of no
additional overlay network and minimal network jitter running on the AWS network. Cluster and network administrators
can apply existing AWS VPC networking and security best practices for building Kubernetes networks on AWS. They can
accomplish those best practices because the AWS CNI includes the capability to use native AWS services like VPC flow
logs for analyzing network events and patterns, VPC routing policies for traffic management, and security groups and
network access control lists for network traffic isolation. We will discuss more about the AWS VPC CNI in <a data-type="xref" href="ch06.xhtml#kubernetes_and_cloud_networking">Chapter 6</a>.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The Kubernetes.io website offers a <a href="https://oreil.ly/imDMP">list of the CNI options available</a>.</p>
</div>

<p>There are many more options for a CNI, and it is up to the cluster administrator, network admins, and application
developers to best decide which CNI solves their business use cases. In later chapters, we will walk through use cases
and deploy several to help admins make a <a data-type="indexterm" data-startref="ch3_term51" id="idm46219940822872"/><a data-type="indexterm" data-startref="ch3_term52" id="idm46219940822168"/>decision.</p>

<p>In our next section, we will walk through container connectivity examples using the Golang web server and Docker.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Container Connectivity"><div class="sect1" id="idm46219941624504">
<h1>Container Connectivity</h1>

<p>Like we experimented with in the previous chapter, we will <a data-type="indexterm" data-primary="containers" data-secondary="networking scenarios with" id="ch3_term53"/><a data-type="indexterm" data-primary="Golang (Go) web server" data-secondary="with container connectivity example" data-secondary-sortas="container connectivity example" id="ch3_term54"/><a data-type="indexterm" data-primary="Docker container technology" data-secondary="container-to-container connectivity with" id="ch3_term58"/>use the Go minimal web server to walk through the concept of container
connectivity. We will explain what is happening at the container level when we deploy the web server as a container on
our <a data-type="indexterm" data-primary="Ubuntu" id="ch3_term59"/>Ubuntu host.</p>

<p>The following are the two networking scenarios we will walk through:</p>

<ul>
<li>
<p>Container to container on the Docker host</p>
</li>
<li>
<p>Container to container on separate hosts</p>
</li>
</ul>

<p>The Golang web server is hardcoded to run on port 8080, <code>http.ListenAndServe("0.0.0.0:8080", nil)</code>, as we can see in
<a data-type="xref" href="#minimal_web_server_in_go">Example 3-8</a>.</p>
<div id="minimal_web_server_in_go" data-type="example">
<h5><span class="label">Example 3-8. </span>Minimal web server in Go</h5>

<pre data-type="programlisting" data-code-language="go"><code class="kn">package</code> <code class="nx">main</code>

<code class="kn">import</code> <code class="p">(</code>
	<code class="s">"fmt"</code>
	<code class="s">"net/http"</code>
<code class="p">)</code>

<code class="kd">func</code> <code class="nx">hello</code><code class="p">(</code><code class="nx">w</code> <code class="nx">http</code><code class="p">.</code><code class="nx">ResponseWriter</code><code class="p">,</code> <code class="nx">_</code> <code class="o">*</code><code class="nx">http</code><code class="p">.</code><code class="nx">Request</code><code class="p">)</code> <code class="p">{</code>
	<code class="nx">fmt</code><code class="p">.</code><code class="nx">Fprintf</code><code class="p">(</code><code class="nx">w</code><code class="p">,</code> <code class="s">"Hello"</code><code class="p">)</code>
<code class="p">}</code>

<code class="kd">func</code> <code class="nx">main</code><code class="p">()</code> <code class="p">{</code>
	<code class="nx">http</code><code class="p">.</code><code class="nx">HandleFunc</code><code class="p">(</code><code class="s">"/"</code><code class="p">,</code> <code class="nx">hello</code><code class="p">)</code>
	<code class="nx">http</code><code class="p">.</code><code class="nx">ListenAndServe</code><code class="p">(</code><code class="s">"0.0.0.0:8080"</code><code class="p">,</code> <code class="kc">nil</code><code class="p">)</code>
<code class="p">}</code></pre></div>

<p>To provision our <a data-type="indexterm" data-primary="Docker container technology" data-secondary="with Dockerfile" data-secondary-sortas="Dockerfile" id="ch3_term55"/><a data-type="indexterm" data-primary="Dockerfile, building image in" id="ch3_term56"/><a data-type="indexterm" data-primary="images, container" data-secondary="building with Dockerfile" id="ch3_term57"/>minimal Golang web server, we need to create it from a Dockerfile. <a data-type="xref" href="#dockerfile_for_golang_minimal_webserver">Example 3-9</a> displays our Golang
web server’s Dockerfile. The Dockerfile contains instructions to specify what to do when building the image. It begins with
the <code>FROM</code> instruction and specifies what the base image should be. The <code>RUN</code> instruction specifies a command to execute.
Comments start with <code>#</code>. Remember, each line in a Dockerfile creates a new layer if it changes the image’s state.
Developers need to find the right balance between having lots of layers created for the image and the readability of
the Dockerfile.</p>
<div id="dockerfile_for_golang_minimal_webserver" data-type="example">
<h5><span class="label">Example 3-9. </span>Dockerfile for Golang minimal web server</h5>

<pre data-type="programlisting" data-code-language="docker"><code class="k">FROM</code><code class="s"> golang:1.15 AS builder </code><a class="co" id="co_container_networking_basics_CO2-1" href="#callout_container_networking_basics_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a><code>
</code><code class="k">WORKDIR</code><code class="s"> /opt </code><a class="co" id="co_container_networking_basics_CO2-2" href="#callout_container_networking_basics_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a><code>
</code><code>COPY</code><code> </code><code>web-server.go</code><code> </code><code>.</code><code> </code><a class="co" id="co_container_networking_basics_CO2-3" href="#callout_container_networking_basics_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a><code>
</code><code class="k">RUN</code><code> </code><code class="nv">CGO_ENABLED</code><code class="o">=</code><code class="m">0</code><code> </code><code class="nv">GOOS</code><code class="o">=</code><code>linux</code><code> </code><code>go</code><code> </code><code>build</code><code> </code><code>-o</code><code> </code><code>web-server</code><code> </code><code>.</code><code> </code><a class="co" id="co_container_networking_basics_CO2-4" href="#callout_container_networking_basics_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a><code>
</code><code>
</code><code class="k">FROM</code><code class="s"> golang:1.15 </code><a class="co" id="co_container_networking_basics_CO2-5" href="#callout_container_networking_basics_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a><code>
</code><code class="k">WORKDIR</code><code class="s"> /opt </code><a class="co" id="co_container_networking_basics_CO2-6" href="#callout_container_networking_basics_CO2-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a><code>
</code><code>COPY</code><code> </code><code>--from</code><code class="o">=</code><code class="m">0</code><code> </code><code>/opt/web-server</code><code> </code><code>.</code><code> </code><a class="co" id="co_container_networking_basics_CO2-7" href="#callout_container_networking_basics_CO2-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a><code>
</code><code class="k">CMD</code><code class="s"> ["/opt/web-server"] </code><a class="co" id="co_container_networking_basics_CO2-8" href="#callout_container_networking_basics_CO2-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_container_networking_basics_CO2-1" href="#co_container_networking_basics_CO2-1"><img src="Images/1.png" alt="1" width="12" height="12"/></a></dt>
<dd><p>Since our web server is written in Golang, we can compile our Go server in a container to reduce the image’s size
to only the compiled Go binary. We start by using the Golang base image with version 1.15 for our web server.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-2" href="#co_container_networking_basics_CO2-2"><img src="Images/2.png" alt="2" width="12" height="12"/></a></dt>
<dd><p><code>WORKDIR</code> sets the working directory for all the subsequent commands to run from.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-3" href="#co_container_networking_basics_CO2-3"><img src="Images/3.png" alt="3" width="12" height="12"/></a></dt>
<dd><p><code>COPY</code> copies the <code>web-server.go</code> file that defines our application as the working directory.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-4" href="#co_container_networking_basics_CO2-4"><img src="Images/4.png" alt="4" width="12" height="12"/></a></dt>
<dd><p><code>RUN</code> instructs Docker to compile our Golang application in the builder container.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-5" href="#co_container_networking_basics_CO2-5"><img src="Images/5.png" alt="5" width="12" height="12"/></a></dt>
<dd><p>Now to run our application, we define <code>FROM</code> for the application base image, again as <code>golang:1.15</code>; we can
further minimize the final size of the image by using other minimal images like alpine.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-6" href="#co_container_networking_basics_CO2-6"><img src="Images/6.png" alt="6" width="12" height="12"/></a></dt>
<dd><p>Being a new container, we again set the working directory to <code>/opt</code>.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-7" href="#co_container_networking_basics_CO2-7"><img src="Images/7.png" alt="7" width="12" height="12"/></a></dt>
<dd><p><code>COPY</code> here will copy the compiled Go binary from the builder container into the application container.</p></dd>
<dt><a class="co" id="callout_container_networking_basics_CO2-8" href="#co_container_networking_basics_CO2-8"><img src="Images/8.png" alt="8" width="12" height="12"/></a></dt>
<dd><p><code>CMD</code> instructs Docker that the command to run our application is to start our web server.</p></dd>
</dl></div>

<p>There are some Dockerfile best practices that developers and admins should adhere to when containerizing their
applications:</p>

<ul>
<li>
<p>Use one <code>ENTRYPOINT</code> per Dockerfile. The <code>ENTRYPOINT</code>
or <code>CMD</code> tells <a data-type="indexterm" data-primary="CMD instruction, Dockerfile" id="idm46219940600472"/>Docker what process starts inside the running container, so there should be only one running process;
containers are all about process isolation.</p>
</li>
<li>
<p>To cut down on the <a data-type="indexterm" data-primary="containers" data-secondary="layers in" id="idm46219940598584"/>container layers, developers should combine
similar commands into one using &amp; &amp; and \. Each new command in the Dockerfile adds a layer to the Docker
container image, thus increasing its storage.</p>
</li>
<li>
<p>Use the caching <a data-type="indexterm" data-primary="caching system, container" id="idm46219940596408"/>system to improve the containers’ build times. If there is no change to
a layer, it should be at the top of the Dockerfile. Caching is part of the reason that the order of statements is
essential. Add files that are least likely to change first and the ones most likely to change last.</p>
</li>
<li>
<p>Use multistage builds to reduce the size of the final image drastically.</p>
</li>
<li>
<p>Do not install unnecessary tools or packages. Doing this will reduce the containers’ attack surface and size,
reducing network transfer times from the registry to the hosts running the containers.</p>
</li>
</ul>

<p>Let’s build our Golang web server and review the Docker commands to do so.</p>

<p><code>docker build</code> instructs Docker to build our images from the Dockerfile instructions:</p>

<pre data-type="programlisting">$ sudo docker build .
Sending build context to Docker daemon   4.27MB
Step 1/8 : FROM golang:1.15 AS builder
1.15: Pulling from library/golang
57df1a1f1ad8: Pull complete
71e126169501: Pull complete
1af28a55c3f3: Pull complete
03f1c9932170: Pull complete
f4773b341423: Pull complete
fb320882041b: Pull complete
24b0ad6f9416: Pull complete
Digest:
sha256:da7ff43658854148b401f24075c0aa390e3b52187ab67cab0043f2b15e754a68
Status: Downloaded newer image for golang:1.15
 ---&gt; 05c8f6d2538a
Step 2/8 : WORKDIR /opt
 ---&gt; Running in 20c103431e6d
Removing intermediate container 20c103431e6d
 ---&gt; 74ba65cfdf74
Step 3/8 : COPY web-server.go .
 ---&gt; 7a36ec66be52
Step 4/8 : RUN CGO_ENABLED=0 GOOS=linux go build -o web-server .
 ---&gt; Running in 5ea1c0a85422
Removing intermediate container 5ea1c0a85422
 ---&gt; b508120db6ba
Step 5/8 : FROM golang:1.15
 ---&gt; 05c8f6d2538a
Step 6/8 : WORKDIR /opt
 ---&gt; Using cache
 ---&gt; 74ba65cfdf74
Step 7/8 : COPY --from=0 /opt/web-server .
 ---&gt; dde6002760cd
Step 8/8 : CMD ["/opt/web-server"]
 ---&gt; Running in 2bcb7c8f5681
Removing intermediate container 2bcb7c8f5681
 ---&gt; 72fd05de6f73
Successfully built 72fd05de6f73</pre>

<p>The Golang minimal web server for our <a data-type="indexterm" data-primary="container ID" id="idm46219940589896"/>testing has the container ID <code>72fd05de6f73</code>, which is not friendly to read, so
we can use the <code>docker tag</code> command to give it a friendly name:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker tag 72fd05de6f73 go-web:v0.0.1</pre>

<p><code>docker images</code> returns the list of locally available images to run. We have one from the test on the Docker installation and
the busybox we have been using to test our networking setup. If a container is not available locally, it is downloaded
from the registry; network load times impact this, so we need to have as small an image
as 
<span class="keep-together">possible:</span></p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker images
REPOSITORY      TAG         IMAGE ID          SIZE
&lt;none&gt;          &lt;none&gt;      b508120db6ba      857MB
go-web          v0.0.1      72fd05de6f73      845MB
golang          1.15        05c8f6d2538a      839MB
busybox         latest      6858809bf669      1.23MB
hello-world     latest      bf756fb1ae65      13.3kB</pre>

<p><code>docker ps</code> shows us the running containers on the host. From our network namespace example, we still have one running
busybox container:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker ps
CONTAINER ID IMAGE    COMMAND      STATUS          PORTS NAMES
1f3f62ad5e02 busybox  <code class="s2">"/bin/sh"</code>    Up <code class="m">11</code> minutes   determined_shamir</pre>

<p><code>docker logs</code> will print out any logs that the container is producing from standard out; currently, our busybox image
is not printing anything out for us to see:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker logs 1f3f62ad5e02
vagrant@ubuntu-xenial:~<code class="err">$</code></pre>

<p><code>docker exec</code> allows devs and admins to execute commands inside the Docker container. We did this previously while
investigating the Docker networking setups:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker <code class="nb">exec </code>1f3f62ad5e02 ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu <code class="m">65536</code> qdisc noqueue qlen 1
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
7: eth0@if8: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu <code class="m">1500</code> qdisc noqueue
link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0
valid_lft forever preferred_lft forever
vagrant@ubuntu-xenial:~<code class="err">$</code></pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>You can find more <a data-type="indexterm" data-primary="CLI (client/command-line interface)" data-secondary="in Docker" data-secondary-sortas="Docker" id="idm46219940517576"/>commands for the Docker CLI in the 
<span class="keep-together"><a href="https://oreil.ly/xWkad">documentation</a>.</span></p>
</div>

<p>In the previous section, we built the Golang web server as a container. <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="image" id="idm46219940514632"/><a data-type="indexterm" data-primary="testing for network connections" id="idm46219940513656"/>To test the connectivity, we will also
employ the <code>dnsutils</code> image used by end-to-end testing for Kubernetes. That image is available from the Kubernetes
project at <code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.3</code>.</p>

<p>The image name will copy the Docker images from the Google container registry to our local
Docker filesystem:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker pull gcr.io/kubernetes-e2e-test-images/dnsutils:1.3
1.3: Pulling from kubernetes-e2e-test-images/dnsutils
5a3ea8efae5d: Pull <code class="nb">complete</code>
7b7e943444f2: Pull <code class="nb">complete</code>
59c439aa0fa7: Pull <code class="nb">complete</code>
3702870470ee: Pull <code class="nb">complete</code>
Digest: sha256:b31bcf7ef4420ce7108e7fc10b6c00343b21257c945eec94c21598e72a8f2de0
Status: Downloaded newer image <code class="k">for</code> gcr.io/kubernetes-e2e-test-images/dnsutils:1.3
gcr.io/kubernetes-e2e-test-images/dnsutils:1.3</pre>

<p>Now that our Golang application can run as a <a data-type="indexterm" data-startref="ch3_term55" id="idm46219940471768"/><a data-type="indexterm" data-startref="ch3_term56" id="idm46219940471160"/><a data-type="indexterm" data-startref="ch3_term57" id="idm46219940470552"/>container, we can explore the container networking scenarios.</p>








<section data-type="sect2" data-pdf-bookmark="Container to Container"><div class="sect2" id="idm46219940469624">
<h2>Container to Container</h2>

<p>Our first walk-through is the <a data-type="indexterm" data-primary="containers" data-secondary="on local systems" data-secondary-sortas="local systems" id="ch3_term61"/>communication between two containers running on the same host. We begin by starting the
<code>dnsutils</code> image and getting in a shell:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker run -it gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 /bin/sh
/ <code class="c">#</code></pre>

<p>The default Docker network setup gives the <code>dnsutils</code> image connectivity to the 
<span class="keep-together">internet:</span></p>

<pre data-type="programlisting" data-code-language="bash">/ <code class="c"># ping google.com -c 4</code>
PING google.com <code class="o">(</code>172.217.9.78<code class="o">)</code>: <code class="m">56</code> data bytes
<code class="m">64</code> bytes from 172.217.9.78: <code class="nv">seq</code><code class="o">=</code><code class="m">0</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">114</code> <code class="nb">time</code><code class="o">=</code>39.677 ms
<code class="m">64</code> bytes from 172.217.9.78: <code class="nv">seq</code><code class="o">=</code><code class="m">1</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">114</code> <code class="nb">time</code><code class="o">=</code>38.180 ms
<code class="m">64</code> bytes from 172.217.9.78: <code class="nv">seq</code><code class="o">=</code><code class="m">2</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">114</code> <code class="nb">time</code><code class="o">=</code>43.150 ms
<code class="m">64</code> bytes from 172.217.9.78: <code class="nv">seq</code><code class="o">=</code><code class="m">3</code> <code class="nv">ttl</code><code class="o">=</code><code class="m">114</code> <code class="nb">time</code><code class="o">=</code>38.140 ms

--- google.com ping statistics ---
<code class="m">4</code> packets transmitted, <code class="m">4</code> packets received, 0% packet loss
round-trip min/avg/max <code class="o">=</code> 38.140/39.786/43.150 ms
/ <code class="c">#</code></pre>

<p>The Golang web server starts with the default Docker bridge; in a <a data-type="indexterm" data-primary="Vagrant/Vagrantfile" id="ch3_term63"/>separate SSH connection, then our Vagrant host, we
start the Golang web server with the following command:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker run -it -d -p 80:8080 go-web:v0.0.1
a568732bc191bb1f5a281e30e34ffdeabc624c59d3684b93167456a9a0902369</pre>

<p>The <code>-it</code> options are for interactive processes (such as a shell); we must use <code>-it</code> to allocate a
TTY for the container process. <code>-d</code> runs the container in detached mode; <a data-type="indexterm" data-primary="container ID" id="idm46219940344280"/>this allows us to continue to use the
terminal and outputs the full Docker container ID. <a data-type="indexterm" data-primary="ports" data-secondary="container connections to" id="ch3_term60"/><a data-type="indexterm" data-primary="port 80" id="ch3_term62"/>The <code>-p</code> is probably the essential option in terms of the network;
this one creates the port connections between the host and the containers. Our Golang web server runs on port 8080 and
exposes that port on port 80 on the host.</p>

<p><code>docker ps</code> verifies that we now have two containers running: the Go web server container with port 8080 exposed on
the host port 80 and the shell running inside our <code>dnsutils</code> container:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>sudo docker ps
CONTAINER ID  IMAGE           COMMAND          CREATED       STATUS
906fd860f84d  go-web:v0.0.1  <code class="s2">"/opt/web-server"</code> <code class="m">4</code> minutes ago Up <code class="m">4</code> minutes
25ded12445df  dnsutils:1.3   <code class="s2">"/bin/sh"</code>         <code class="m">6</code> minutes ago Up <code class="m">6</code> minutes

PORTS                   NAMES
0.0.0.0:8080-&gt;8080/tcp  frosty_brown
                        brave_zhukovsky</pre>

<p>Let’s use the <code>docker inspect</code> command to get the Docker IP address of the Golang web server container:</p>

<pre data-type="programlisting" data-code-language="bash"><code class="nv">$ </code>sudo docker inspect
-f <code class="s1">'{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'</code>
906fd860f84d
172.17.0.2</pre>

<p>On the <code>dnsutils</code> image, we <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="image" id="idm46219940330120"/>can use the Docker network address of the Golang web server <code>172.17.0.2</code> and the
container port <code>8080</code>:</p>

<pre data-type="programlisting" data-code-language="bash">/ <code class="c"># wget 172.17.0.2:8080</code>
Connecting to 172.17.0.2:8080 <code class="o">(</code>172.17.0.2:8080<code class="o">)</code>
index.html           100% <code class="p">|</code>*******************************************<code class="p">|</code>
                     <code class="m">5</code>   0:00:00 ETA
/ <code class="c"># cat index.html</code>
Hello/ <code class="c">#</code></pre>

<p>Each container can <a data-type="indexterm" data-primary="docker0 bridge interface" id="idm46219940282392"/>reach the other over the <code>docker0</code> bridge and the container ports because they are on the same
Docker host and the same network. The Docker host has routes to the container’s IP address to reach the container on
its IP address and port:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>curl 172.17.0.2:8080
Hello</pre>

<p>But it does not for the Docker IP address and host port from the <code>docker run</code> 
<span class="keep-together">command:</span></p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>curl 172.17.0.2:80
curl: <code class="o">(</code>7<code class="o">)</code> Failed to connect to 172.17.0.2 port 80: Connection refused
vagrant@ubuntu-xenial:~<code class="nv">$ </code>curl 172.17.0.2:8080
Hello</pre>

<p>Now for the reverse, <a data-type="indexterm" data-primary="loopback interface (lo)" id="idm46219940221896"/>using the loopback interface, we demonstrate that the host can reach the web server only on the host
port exposed, 80, not the Docker port, 8080:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>curl 127.0.0.1:8080
curl: <code class="o">(</code>7<code class="o">)</code> Failed to connect to 127.0.0.1 port 8080: Connection refused
vagrant@ubuntu-xenial:~<code class="nv">$ </code>curl 127.0.0.1:80
Hellovagrant@ubuntu-xenial:~<code class="err">$</code></pre>

<p>Now back on the <code>dnsutils</code>, the same is true: the <code>dnsutils</code> image on the Docker network, using the Docker IP address of
the Go web container, can use only the Docker port, 8080, not the exposed host port 80:</p>

<pre data-type="programlisting">/ # wget 172.17.0.2:8080 -qO-
Hello/ #
/ # wget 172.17.0.2:80 -qO-
wget: can't connect to remote host (172.17.0.2): Connection refused</pre>

<p>Now to show it is an entirely separate stack, let’s <a data-type="indexterm" data-primary="dnsutils, Kubernetes" data-secondary="loopback address" id="idm46219940192008"/>try the <code>dnsutils</code> loopback address and both the Docker port and
the exposed host port:</p>

<pre data-type="programlisting" data-code-language="bash">/ <code class="c"># wget localhost:80 -qO-</code>
wget: can<code class="s1">'t connect to remote host (127.0.0.1): Connection refused</code>
<code class="s1">/ # wget localhost:8080 -qO-</code>
<code class="s1">wget: can'</code>t connect to remote host <code class="o">(</code>127.0.0.1<code class="o">)</code>: Connection refused</pre>

<p>Neither works as expected; the <code>dnsutils</code> image has a separate network stack and does not share the Go web server’s
network namespace. <a data-type="indexterm" data-primary="namespaces, network" data-secondary="pods sharing of" id="idm46219940166632"/>Knowing why it does not work is vital in Kubernetes to understand since pods are a collection of
containers that share the same network namespace. <a data-type="indexterm" data-startref="ch3_term60" id="idm46219940165496"/><a data-type="indexterm" data-startref="ch3_term61" id="idm46219940164824"/><a data-type="indexterm" data-startref="ch3_term62" id="idm46219940146616"/>Now we will examine how two containers communicate on two separate hosts.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Container to Container Separate Hosts"><div class="sect2" id="idm46219940467464">
<h2>Container to Container Separate Hosts</h2>

<p>Our previous example showed us how a container network runs on a local system, but how can <a data-type="indexterm" data-primary="containers" data-secondary="on global systems" data-secondary-sortas="global systems" id="idm46219940144376"/>two containers across the
network on separate hosts communicate? In this example, we will deploy containers on separate hosts and
investigate that and how it differs from being on the same host.</p>

<p>Let’s start a second Vagrant Ubuntu host, <code>host-2</code>, and SSH into it as we did with our Docker host. We can see that
our IP address is different from the Docker host running our Golang web server:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@host-2:~<code class="nv">$ </code>ifconfig enp0s8
enp0s8    Link encap:Ethernet  HWaddr 08:00:27:f9:77:12
          inet addr:192.168.1.23  Bcast:192.168.1.255  Mask:255.255.255.0
          inet6 addr: fe80::a00:27ff:fef9:7712/64 Scope:Link
          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1
          RX packets:65630 errors:0 dropped:0 overruns:0 frame:0
          TX packets:2967 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:96493210 <code class="o">(</code>96.4 MB<code class="o">)</code>  TX bytes:228989 <code class="o">(</code>228.9 KB<code class="o">)</code></pre>

<p>We can access our web server from the Docker host’s IP address, <code>192.168.1.20</code>, on port 80 exposed in the
<code>docker run</code> command options. Port 80 is exposed on the Docker host but not reachable on container port 8080 with
the host IP address:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@ubuntu-xenial:~<code class="nv">$ </code>curl 192.168.1.20:80
Hellovagrant@ubuntu-xenial:~<code class="err">$</code>
vagrant@host-2:~<code class="nv">$ </code>curl 192.168.1.20:8080
curl: <code class="o">(</code>7<code class="o">)</code> Failed to connect to 192.168.1.20 port 8080: Connection refused
vagrant@ubuntu-xenial:~<code class="err">$</code></pre>

<p>The same is true if <code>host-2</code> tries to reach the container on the containers’ IP address, using either the Docker port or
the host port. Remember, Docker uses the private address range, <code>172.17.0.0/16</code>:</p>

<pre data-type="programlisting" data-code-language="bash">vagrant@host-2:~<code class="nv">$ </code>curl 172.17.0.2:8080 -t 5
curl: <code class="o">(</code>7<code class="o">)</code> Failed to connect to 172.17.0.2 port 8080: No route to host
vagrant@host-2:~<code class="nv">$ </code>curl 172.17.0.2:80 -t 5
curl: <code class="o">(</code>7<code class="o">)</code> Failed to connect to 172.17.0.2 port 80: No route to host
vagrant@host-2:~<code class="err">$</code></pre>

<p>For the host to route to the Docker IP address, it uses an overlay network or some <a data-type="indexterm" data-primary="routing" data-secondary="for external traffic" data-secondary-sortas="external traffic" id="idm46219940106504"/>external routing outside Docker.
Routing is also external to Kubernetes; many CNIs help with this issue, and this is explored when looking at deploy
clusters in <a data-type="xref" href="ch06.xhtml#kubernetes_and_cloud_networking">Chapter 6</a>.</p>

<p>The previous examples used the Docker default network bridge with exposed ports to the hosts. That is how <code>host-2</code>
was able to communicate to the Docker container running on the Docker <a data-type="indexterm" data-startref="ch3_term51" id="idm46219940079208"/><a data-type="indexterm" data-startref="ch3_term52" id="idm46219940046264"/><a data-type="indexterm" data-startref="ch3_term53" id="idm46219940045592"/><a data-type="indexterm" data-startref="ch3_term54" id="idm46219940044920"/><a data-type="indexterm" data-startref="ch3_term58" id="idm46219940044248"/><a data-type="indexterm" data-startref="ch3_term59" id="idm46219940043576"/><a data-type="indexterm" data-startref="ch3_term63" id="idm46219940042904"/>host. This chapter only scratches the surface of
container networks. There are many more abstractions to explore, like ingress and egress traffic to the entire
cluster, service discovery, and routing internal and external to the cluster. Later chapters will continue to
build on these container networking basics.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm46219940820696">
<h1>Conclusion</h1>

<p>In this introduction to container networking, we worked through how containers have evolved to help with application
deployment and advance host efficiency by allowing and segmenting multiple applications on a host. We have walked
through the myriad history of containers with the various projects that have come and gone. Containers are powered
and managed with namespaces and cgroups, features inside the Linux kernel. We walked through the abstractions that
container runtimes maintain for application developers and learned how to deploy them ourselves. Understanding those Linux kernel abstractions is essential to deciding which CNI to deploy and its trade-offs and benefits. Administrators now have a base understanding of how container runtimes manage the Linux networking abstractions.</p>

<p>We have completed the basics of container networking! Our knowledge has expanded from using a simple network stack to
running different unrelated stacks inside our containers. Knowing about namespaces, how ports are exposed, and
communication flow empowers administrators to troubleshoot networking issues quickly and prevent downtime of their
applications running in a Kubernetes cluster. <a data-type="indexterm" data-primary="network engineers" id="idm46219940020280"/><a data-type="indexterm" data-primary="developers" id="idm46219940019688"/>Troubleshooting port issues or testing if a port is open on the host,
on the container, or across the network is a must-have skill for any network engineer and indispensable for developers to
troubleshoot their container issues. Kubernetes is built on these basics and abstracts them for developers.
The next chapter will review how Kubernetes creates those abstractions and integrates them into the Kubernetes networking
model.</p>
</div></section>







</div></section></div></body></html>