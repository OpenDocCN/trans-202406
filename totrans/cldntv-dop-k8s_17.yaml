- en: Chapter 15\. Observability and Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nothing is ever completely right aboard a ship.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: William Langewiesche, *The Outlaw Sea*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we’ll consider the question of observability and monitoring
    for cloud native applications. What is observability? How does it relate to monitoring?
    How do you do monitoring, logging, metrics, and tracing in Kubernetes?
  prefs: []
  type: TYPE_NORMAL
- en: What Is Observability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Observability* may not be a familiar term to you, though it’s becoming increasingly
    popular as a way to express the larger world beyond traditional monitoring. Let’s
    tackle *monitoring* first before we see how observability extends it.'
  prefs: []
  type: TYPE_NORMAL
- en: What Is Monitoring?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Is your website working right now? Go check; we’ll wait. The most basic way
    to know whether all your applications and services are working as they should
    is to look at them yourself. But when we talk about monitoring in a DevOps context,
    we mostly mean *automated monitoring*.
  prefs: []
  type: TYPE_NORMAL
- en: Automated monitoring is checking the availability or behavior of a website or
    service, in some programmatic way, usually on a regular schedule, and usually
    with some automated way of alerting human engineers if there’s a problem. But
    what defines a problem?
  prefs: []
  type: TYPE_NORMAL
- en: Closed-Box Monitoring
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take the simple case of a static website. If it’s not working at all,
    it just won’t respond, or you’ll see an error message in the browser. So the simplest
    possible monitoring check for this site is to fetch the home page and check the
    HTTP status code (200 indicates a successful request). You could do this with
    a command-line HTTP client such as `httpie` or `curl`. If the exit status from
    the client is nonzero, there was a problem fetching the website.
  prefs: []
  type: TYPE_NORMAL
- en: But suppose something went wrong with the web server configuration, and although
    the server is working and responding with HTTP `200 OK` status, it is actually
    serving a blank page (or some sort of default or welcome page, or maybe the wrong
    site altogether). Our simplistic monitoring check won’t detect any issue because
    the HTTP request succeeds; however, the site is not working as expected for users.
  prefs: []
  type: TYPE_NORMAL
- en: A more sophisticated monitoring check might look for some specific text on the
    page that you know should be there, like the name of the organization. This would
    catch the problem of a misconfigured, but working, web server.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond static pages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can imagine that more complex websites might need more complex monitoring.
    For example, if the site had a facility for users to log in, the monitoring check
    might also try to log in with a known test-user account and alert if the login
    fails. Or if the site had a search function, the check might fill in a text field
    with some search text, simulate clicking the search button, and verify that the
    results contain some expected text.
  prefs: []
  type: TYPE_NORMAL
- en: 'For simple websites, a yes/no answer to the question “Is it working?” may be
    sufficient. For cloud native applications, which tend to be more complex distributed
    systems, the question may turn into multiple questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Is my application available everywhere in the world? Or only in some regions?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How long does it take to load for most of my users?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What about users who may have slow download speeds?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are all of the features of my website working as intended?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Are certain features working slowly or not at all, and how many users are affected?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If it relies on a third-party service, what happens to my application when that
    external service is faulty or unavailable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens when my cloud provider has an outage?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It starts to become clear that, in the world of monitoring cloud native distributed
    systems, not very much is clear at all.
  prefs: []
  type: TYPE_NORMAL
- en: The limits of closed-box monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'However, no matter how complicated these checks get, they all fall into the
    same category of monitoring: *closed-box monitoring*. Closed-box checks, as the
    name suggests, observe only the external behavior of a system, without any attempt
    to observe what’s going on inside it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Until a few years ago, closed-box monitoring, as performed by popular tools
    such as Nagios, Icinga, Zabbix, Sensu, and Check_MK, was pretty much state of
    the art. To be sure, having *any* kind of automated monitoring of your systems
    is a huge improvement on having none. But there are a few limitations of closed-box
    checks:'
  prefs: []
  type: TYPE_NORMAL
- en: They can only detect predictable failures (for example, a website not responding).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They only check the behavior of the parts of the system that are exposed to
    the outside.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are passive and reactive; they only tell you about a problem *after* it’s
    happened.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can answer the question “What’s broken?” but not the more important question
    “Why?”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To answer the “why?” question, we need to move beyond traditional monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: There’s a further issue with this kind of *up*/*down* test; what does *up* even
    mean?
  prefs: []
  type: TYPE_NORMAL
- en: What Does “Up” Mean?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In operations we’re used to measuring the resilience and availability of our
    applications in *uptime*, usually measured as a percentage. For example, an application
    with 99% uptime was unavailable for no more than 1% of the relevant time period.
    99.9% uptime, referred to as *three nines*, translates to about nine hours downtime
    a year, which would be a good figure for the average web application. Four nines
    (99.99%) is less than an hour’s downtime per year, and five nines (99.999%) is
    about five minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the more nines the better, you might think. But looking at things this
    way misses an important point:'
  prefs: []
  type: TYPE_NORMAL
- en: Nines don’t matter if users aren’t happy.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Charity Majors](https://red.ht/2FMZcMZ)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Nines don’t matter if users aren’t happy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As the saying goes, what gets measured gets maximized. So you’d better be very
    careful what you measure. If your service isn’t working for users, it doesn’t
    matter what your internal metrics say: *the service is down*. There are lots of
    ways a service can be making users unhappy, even if it’s nominally *up*.'
  prefs: []
  type: TYPE_NORMAL
- en: To take an obvious example, what if your website takes 10 seconds to load? It
    might work fine after that, but if it’s too slow to respond, it might as well
    be down completely. Users will just go elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional closed-box monitoring might attempt to deal with this problem by
    defining a load time of, say, five seconds as *up*, and anything over that is
    considered *down* and an alert generated. But what if users are experiencing all
    sorts of different load times, from 2 seconds to 10 seconds? With a hard threshold
    like this, you could consider the service *down* for some users, but *up* for
    others. What if load times are fine for users in North America, but unusable in
    Europe or Asia?
  prefs: []
  type: TYPE_NORMAL
- en: Cloud native applications are never “up”
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While you could go on refining more complex rules and thresholds to enable us
    to give an *up*/*down* answer about the status of the service, the truth is that
    the question is irredeemably flawed. Distributed systems like cloud native applications
    are [never *up*](http://red.ht/2hMHwSL); they exist in a constant state of partially
    degraded service.
  prefs: []
  type: TYPE_NORMAL
- en: This is an example of a class of problems called [*gray failures*](https://oreil.ly/4r86k).^([1](ch15.html#idm45979374465760))
    Gray failures are, by definition, hard to detect, especially from a single point
    of view or with a single observation.
  prefs: []
  type: TYPE_NORMAL
- en: So while closed-box monitoring may be a good place to start your observability
    journey, it’s important to recognize that you shouldn’t stop there. Let’s see
    if we can do better.
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most applications produce *logs* of some kind. Logs are a series of records,
    usually with some kind of timestamps to indicate when records were written, and
    in what order. For example, a web server records each request in its logs, including
    information such as:'
  prefs: []
  type: TYPE_NORMAL
- en: The URI requested
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The IP address of the client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HTTP status of the response
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the application encounters an error, it usually logs this fact, along with
    some information that may or may not be helpful for operators to figure out what
    caused the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Often, logs from a wide range of applications and services will be *aggregated*
    into a central database (Elasticsearch, for example), where they can be queried
    and graphed to help with troubleshooting. Tools like Logstash and Kibana, or hosted
    services such as Splunk and Loggly, are designed to help you gather and analyze
    large volumes of log data.
  prefs: []
  type: TYPE_NORMAL
- en: The limits of logging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logs can be useful, but they have their limitations too. The decision about
    what to log or not to log is taken by the programmer at the time the application
    is written. Therefore, like closed-box checks, logs can only answer questions
    or detect problems that can be predicted in advance.
  prefs: []
  type: TYPE_NORMAL
- en: It can also be hard to extract information from logs, because every application
    writes logs in a different format, and operators often need to write customized
    parsers for each type of log record to turn it into usable numerical or event
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Because logs have to record enough information to diagnose any conceivable kind
    of problem, they usually have a poor signal-to-noise ratio. If you log everything,
    it’s difficult and time-consuming to wade through hundreds of pages of logs to
    find the one error message you need. If you log only occasional errors, it’s hard
    to know what *normal* looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Using a standard format for your logs can greatly improve their usefulness.
    Logging events in something like JSON with a known structure will make it much
    easier to sift through the noise when trying to make sense of what the logs are
    saying.
  prefs: []
  type: TYPE_NORMAL
- en: Logs are hard to scale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Logs also don’t scale very well with traffic. If every user request generates
    a log line that has to be sent to the aggregator, you can end up using a lot of
    network bandwidth (which is thus unavailable to serve users), and your log aggregator
    can become a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many hosted logging providers also charge by the volume of logs you generate,
    which is understandable but unfortunate: it incentivizes you financially to log
    less information, and to have fewer users and serve less traffic!'
  prefs: []
  type: TYPE_NORMAL
- en: 'The same applies to self-hosted logging solutions: the more data you store,
    the more hardware, storage, and network resources you have to pay for, and the
    more engineering time goes into merely keeping log aggregation working.'
  prefs: []
  type: TYPE_NORMAL
- en: Is logging useful in Kubernetes?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We talked a little about how containers generate logs and how you can inspect
    them directly in Kubernetes, in [“Viewing a Container’s Logs”](ch07.html#containerlogs).
    This is a useful debugging technique for individual containers.
  prefs: []
  type: TYPE_NORMAL
- en: If you do use logging, you should use some form of structured data, like JSON,
    which can be automatically parsed (see [“The Observability Pipeline”](#o11ypipeline))
    rather than plain-text records.
  prefs: []
  type: TYPE_NORMAL
- en: Centralized log aggregation with services like [Loki](https://oreil.ly/1r4BG)
    can be useful with Kubernetes applications, but it’s not the whole story. While
    there are some business use-cases for centralized logging (audit and security
    requirements, for example, or customer analytics), logs can’t give us all the
    information we need for true observability.
  prefs: []
  type: TYPE_NORMAL
- en: For that, we need to look beyond logs, to something much more powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A more sophisticated way of gathering information about your services is to
    use *metrics*. As the name suggests, a metric is a numerical measure of something.
    Depending on the application, relevant metrics might include:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of requests currently being processed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of requests handled per minute (or per second, or per hour)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of errors encountered when handling requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The average time it took to serve requests (or the peak time, or the 99th percentile)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s also useful to gather metrics about your infrastructure as well as your
    applications:'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU usage of individual processes or containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The disk I/O activity of nodes and servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inbound and outbound network traffic of machines, clusters, or load balancers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics help answer the “why?” question
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Metrics open up a new dimension of monitoring beyond simply finding out if
    something is *working* or *not working*. Like the speedometer in your car, or
    the temperature scale on your thermometer, they give you numerical information
    about what’s happening. Unlike logs, metrics can easily be processed in all sorts
    of useful ways: drawing graphs, taking statistics, or alerting on predefined thresholds.
    For example, your monitoring system might alert you if the error rate for an application
    exceeds 10% for a given time period.'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics can also help answer the “why?” question about problems. For example,
    suppose users are experiencing long response times (high *latency*) from your
    app. You check your metrics, and you see that the spike in the *latency* metric
    coincides with a similar spike in the *CPU usage* metric for a particular machine
    or component. That immediately gives you a clue about where to start looking for
    the problem. The component may be wedged, or repeatedly retrying some failed operation,
    or its host node may have a hardware problem.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics help predict problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Also, metrics can be *predictive*: when things go wrong, it usually doesn’t
    happen all at once. Before a problem is noticeable to you or your users, an increase
    in some metric may indicate that trouble is on the way.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, the disk usage metric for a server may creep up and up over time,
    and eventually reach the point where the disk actually runs out of space and things
    start failing. If you alerted on that metric before it got into failure territory,
    you could prevent the failure from happening at all.
  prefs: []
  type: TYPE_NORMAL
- en: Some systems even use machine learning techniques to analyze metrics, detect
    anomalies, and reason about the cause. This can be helpful, especially in complex
    distributed systems, but for most purposes, simply having a way to gather, graph,
    and alert on metrics is plenty good enough.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics monitor applications from the inside
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With closed-box checks, operators have to make guesses about the internal implementation
    of the app or service, and predict what kind of failures might happen and what
    effect this would have on external behavior. By contrast, metrics allow application
    developers to export key information about the hidden aspects of the system, based
    on their knowledge of how it actually works (and how it fails):'
  prefs: []
  type: TYPE_NORMAL
- en: Stop reverse engineering applications and start monitoring from the inside.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Kelsey Hightower, Monitorama 2016](https://vimeo.com/173610242)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tools like Prometheus, StatsD, and Graphite, or hosted services such as Datadog,
    New Relic, and Dynatrace, are widely used to gather and manage metrics data.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll talk much more in [Chapter 16](ch16.html#metrics) about metrics in the
    context of Kubernetes, including what kinds you should focus on, and what you
    should do with them. For now, let’s complete our survey of observability with
    a look at tracing.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another useful technique in the monitoring toolbox is *tracing*. It’s especially
    important in distributed systems. While metrics and logs tell you what’s going
    on with each individual component of your system, tracing follows a single user
    request through its whole life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you’re trying to figure out why some users are experiencing very high
    latency for requests. You check the metrics for each of your system components:
    load balancer, ingress, web server, application server, database, message bus,
    and so on; and everything appears normal. So what’s going on?'
  prefs: []
  type: TYPE_NORMAL
- en: When you trace an individual (hopefully representative) request from the moment
    the user’s connection is opened to the moment it’s closed, you’ll get a picture
    of how that overall latency breaks down for each stage of the request’s journey
    through the system.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you may find that the time spent handling the request in each stage
    of the pipeline is normal, except for the database hop, which is one hundred times
    longer than normal. Although the database is working fine and its metrics show
    no problems, for some reason the application server is having to wait a very long
    time for requests to the database to complete.
  prefs: []
  type: TYPE_NORMAL
- en: Eventually you track down the problem to excessive packet loss over one particular
    network link between the application servers and the database server. Without
    the *request’s eye view* provided by distributed tracing, it’s hard to find problems
    like this.
  prefs: []
  type: TYPE_NORMAL
- en: Some popular distributed tracing tools include [Zipkin](https://zipkin.io),
    [Jaeger](https://www.jaegertracing.io), and [Lightstep](https://lightstep.com/product).
    Engineer Masroor Hasan has written a useful blog post, [“Distributed Tracing Infrastructure
    with Jaeger on Kubernetes”](https://oreil.ly/esVav) that describes how to use
    Jaeger for distributed tracing in Kubernetes.^([2](ch15.html#idm45979374404496))
  prefs: []
  type: TYPE_NORMAL
- en: The [OpenTracing framework](https://opentracing.io) (part of the CNCF) aims
    to provide a standard set of APIs and libraries for distributed tracing.
  prefs: []
  type: TYPE_NORMAL
- en: '[Pixie](https://px.dev) is another interesting project that adds tracing and
    application profiling for Kubernetes applications, along with a rich query language
    that allows you to explore what is happening inside of the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because the term *monitoring* means different things to different people—from
    plain old closed-box checks to a combination of metrics, logging, and tracing—it’s
    becoming common to use *observability* as a catch-all term that covers all these
    techniques. The observability of your system is a measure of how well instrumented
    it is, and how easily you can find out what’s going on inside it. Some people
    say that observability is a superset of monitoring, others that observability
    reflects a completely different mindset from traditional monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the most useful way to distinguish these terms is to say that monitoring
    tells you *whether the system is working*, while observability prompts you to
    ask *why it’s not working*, along with considering *how well it is performing*.
  prefs: []
  type: TYPE_NORMAL
- en: Observability is about understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'More generally, observability is about *understanding*: understanding what
    your system does and how it does it. For example, if you roll out a code change
    that is designed to improve the performance of a particular feature by 10%, then
    observability can tell you whether or not it worked. If performance only went
    up a tiny bit, or worse, went down slightly, you need to revisit the code.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if performance went up 20%, the change exceeded your expectations,
    and maybe you need to think about why your predictions fell short. Observability
    helps you build and refine your mental model of how the different parts of your
    system interact.
  prefs: []
  type: TYPE_NORMAL
- en: Observability is also about *data*. We need to know what data to generate, what
    to collect, how to aggregate it (if appropriate), what results to focus on, and
    how to query and display them.
  prefs: []
  type: TYPE_NORMAL
- en: Software is opaque
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In traditional monitoring we have lots of data about the *machinery*: CPU loads,
    disk activity, network packets, and so on. But it’s hard to reason backward from
    that about what our *software* is doing. To do that, we need to instrument the
    software itself:'
  prefs: []
  type: TYPE_NORMAL
- en: Software is opaque by default; it must generate data in order to clue humans
    in on what it is doing. Observable systems allow humans to answer the question,
    “Is it working properly?”, and if the answer is no, to diagnose the scope of impact
    and identify what is going wrong.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Christine Spang](https://oreil.ly/iJeFI)'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Building an observability culture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even more generally, observability is about *culture*. It’s a key tenet of the
    DevOps philosophy to close the loop between developing code and running it at
    scale in production. Observability is the primary tool for closing that loop.
    Developers and operations staff need to work closely together to instrument services
    for observability, and then figure out the best way to consume and act on the
    information it provides.
  prefs: []
  type: TYPE_NORMAL
- en: The Observability Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does observability work, from a practical point of view? It’s common to
    have multiple data sources (logs, metrics, and so on) connected to various different
    data stores in a fairly ad hoc way.
  prefs: []
  type: TYPE_NORMAL
- en: For example, your logs might go to an ELK server, while metrics go to three
    or four different managed services, and traditional monitoring checks report to
    yet another service. This isn’t ideal.
  prefs: []
  type: TYPE_NORMAL
- en: For one thing, it’s hard to scale. The more data sources and stores you have,
    the more interconnections there are, and the more traffic over those connections.
    It doesn’t make sense to put engineering time into making all of those different
    kinds of connections stable and reliable.
  prefs: []
  type: TYPE_NORMAL
- en: Also, the more tightly integrated your systems become with specific solutions
    or providers, the harder it is to change them or to try out alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'An increasingly popular way to address this problem is the [*observability
    pipeline*](https://oreil.ly/Vyn5M):'
  prefs: []
  type: TYPE_NORMAL
- en: With an observability pipeline, we decouple the data sources from the destinations
    and provide a buffer. This makes the observability data easily consumable. We
    no longer have to figure out what data to send from containers, VMs, and infrastructure,
    where to send it, and how to send it. Rather, all the data is sent to the pipeline,
    which handles filtering it and getting it to the right places. This also gives
    us greater flexibility in terms of adding or removing data sinks, and it provides
    a buffer between data producers and consumers.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Tyler Treat
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: An observability pipeline brings great advantages. Now, adding a new data source
    is just a matter of connecting it to your pipeline. Similarly, a new visualization
    or alerting service just becomes another consumer of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Because the pipeline buffers data, nothing gets lost. If there’s a sudden surge
    in traffic and an overload of metrics data, the pipeline will buffer it rather
    than drop samples.
  prefs: []
  type: TYPE_NORMAL
- en: Using an observability pipeline requires a standard metrics format (see [“Prometheus”](ch16.html#prometheus))
    and, ideally, structured logging from applications using JSON or some other sensible
    serialized data format. Instead of emitting raw text logs, and parsing them later
    with fragile regular expressions, start with structured data from the very beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So now that we understand a little more about what closed-box monitoring is
    and how it relates to observability in general, let’s see how it applies to Kubernetes
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: External Closed-Box Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we’ve seen, closed-box monitoring can only tell you that your application
    is down. But that’s still very useful information. All kinds of things could be
    wrong with a cloud native application, and it might still be able to serve some
    requests acceptably. Engineers can work on fixing internal problems like slow
    queries and elevated error rates, without users really being aware of an issue.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a more serious class of problems results in a full-scale *outage*:
    the application is unavailable or not working for the majority of users. This
    is bad for the users, and depending on the application, it may be bad for your
    business as well. In order to detect an outage, your monitoring needs to consume
    the service in the same way that a user would.'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring mimics user behavior
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For example, if it’s an HTTP service, the monitoring system needs to make HTTP
    requests to it, not just TCP connections. If the service just returns static text,
    monitoring can check the text matches some expected string. Usually, it’s a little
    bit more complicated than that, but your checks can also be more robust.
  prefs: []
  type: TYPE_NORMAL
- en: 'In an outage situation, though, it’s quite likely that a simple text match
    will be sufficient to tell you the application is down. But making these closed-box
    checks from inside your infrastructure (for example, in Kubernetes) isn’t enough.
    An outage can result from all sorts of problems and failures between the user
    and the outside edge of your infrastructure, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Bad DNS records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network partitions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Packet loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misconfigured routers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing or bad firewall rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud provider outage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In all these situations, your internal metrics and monitoring might show no
    problems at all. Therefore, your top-priority observability task should be to
    monitor the availability of your services from some point external to your own
    infrastructure. There are many third-party services that can do this kind of monitoring
    for you, including Uptime Robot, Pingdom, and Wormly.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t build your own monitoring infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of these services have either a free tier, or fairly inexpensive subscriptions—and
    whatever you pay for them you should regard as an essential operating expense.
    Don’t bother trying to build your own external monitoring infrastructure; it’s
    not worth it. The cost of a year’s Pro subscription to Uptime Robot likely would
    not pay for a single hour of your engineers’ time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look for the following critical features in an external monitoring provider:'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP/HTTPS checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect if your TLS certificate is invalid or expired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keyword matching (alert when the keyword is missing *or* when it’s present)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically create or update checks via an API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alerts by email, SMS, webhook, or some other straightforward mechanism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this book we champion the idea of infrastructure as code, so it should
    be possible to automate your external monitoring checks with code as well. For
    example, Uptime Robot has a simple REST API for creating new checks, and you can
    automate it using a client library or command-line tool like [`uptimerobot`](https://oreil.ly/WP5eG).
  prefs: []
  type: TYPE_NORMAL
- en: It doesn’t matter which external monitoring service you use, so long as you
    use one. But don’t stop there. In the next section, we’ll see what we can do to
    monitor the health of applications inside the Kubernetes cluster itself.
  prefs: []
  type: TYPE_NORMAL
- en: Internal Health Checks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cloud native applications fail in complex, unpredictable, and hard-to-detect
    ways. Applications have to be designed to be resilient and degrade gracefully
    in the face of unexpected failures, but ironically, the more resilient they are,
    the harder it is to detect these failures by closed-box monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, applications can, and should, do their own health checking.
    The developer of a particular feature or service is best placed to know what it
    needs to be *healthy*, and they can write code to check this that exposes the
    results in a way that can be monitored from outside the container (like an HTTP
    endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Are users happy?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes gives us a simple mechanism for applications to advertise their liveness
    or readiness, as we saw in [“Liveness Probes”](ch05.html#liveness), so this is
    a good place to start. Usually, Kubernetes liveness or readiness probes are pretty
    simple; the application always responds “OK” to any requests. If it doesn’t respond,
    Kubernetes considers it to be down or unready.
  prefs: []
  type: TYPE_NORMAL
- en: However, as many programmers know from bitter experience, just because a program
    runs, doesn’t necessarily mean it works correctly. A more sophisticated readiness
    probe should ask, “What does this application need in order to do its job?”
  prefs: []
  type: TYPE_NORMAL
- en: For example, if it needs to talk to a database, it can check that it has a valid
    and responsive database connection. If it depends on other services, it can check
    the services’ availability. (If health checks are run frequently, they shouldn’t
    do anything too expensive that might affect serving requests from real users.)
  prefs: []
  type: TYPE_NORMAL
- en: Note that we’re still giving a binary yes/no response to the readiness probe.
    It’s just a more informed answer. What we’re trying to do is answer the question
    “Are users happy?” as accurately as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Services and circuit breakers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you know, if a container’s *liveness* check fails, Kubernetes will restart
    it automatically, in an exponential backoff loop. This isn’t really that helpful
    in the situation where there’s nothing wrong with the container, but one of its
    dependencies is failing. The semantics of a failed *readiness* check, on the other
    hand, is “I’m fine, but I can’t serve user requests at the moment.”
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, the container will be removed from any Services that it’s
    a backend for, and Kubernetes will stop sending it requests until it becomes ready
    again. This is a better way to deal with a failed dependency.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you have a chain of 10 microservices, each of which depends on the next
    for some critical part of its work. The last service in the chain fails. The next-to-last
    service will detect this and start failing its readiness probe. Kubernetes will
    disconnect it, and the next service in line detects this, and so on up the chain.
    Eventually the frontend service will fail, and (hopefully) a closed-box monitoring
    alert will be tripped.
  prefs: []
  type: TYPE_NORMAL
- en: Once the problem with the base service is fixed, or maybe cured by an automatic
    restart, all the other services in the chain will automatically become ready again
    in turn, without being restarted or losing any state. This is an example of what’s
    called a [*circuit breaker pattern*](https://oreil.ly/F3lKZ). When an application
    detects a downstream failure, it takes itself out of service (via the readiness
    check) to prevent any more requests being sent to it until the problem is fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Graceful degradation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While a circuit breaker is useful for surfacing problems as soon as possible,
    you should design your services to avoid having the whole system fail when one
    or more component services are unavailable. Instead, try to make your services
    *degrade gracefully*: even if they can’t do everything they’re supposed to, maybe
    they can still do some things.'
  prefs: []
  type: TYPE_NORMAL
- en: In distributed systems, we have to assume that services, components, and connections
    will fail mysteriously and intermittently more or less all the time. A resilient
    system can handle this without failing completely.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There’s a lot to say about the topic of monitoring and observability. We hope
    this chapter has given you some useful information about traditional monitoring
    techniques, what they can do and what they can’t do, and how things need to adapt
    in a cloud native environment.
  prefs: []
  type: TYPE_NORMAL
- en: The notion of *observability* introduces us to a bigger picture than traditional
    log files and closed-box checks. Metrics form an important part of this picture,
    and in the next and final chapter, we’ll take you on a deep dive into the world
    of metrics in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before turning the page, though, you might like to recall these key points:'
  prefs: []
  type: TYPE_NORMAL
- en: Closed-box monitoring checks observe the external behavior of a system to detect
    predictable failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributed systems expose the limitations of traditional monitoring because
    they’re not in either *up* or *down* states: they exist in a constant state of
    partially degraded service. In other words, nothing is ever completely right aboard
    a ship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logs can be useful for post-incident troubleshooting, but they’re expensive
    to scale. Loki and ELK are popular centralized logging tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics open up a new dimension beyond simply *working*/*not working*, and give
    you continuous numerical time-series data on hundreds or thousands of aspects
    of your system. Prometheus is a popular option for aggregating application metrics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics can help you answer the “why?” question, as well as identify problematic
    trends before they lead to outages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracing records events with precise timing through the life cycle of an individual
    request to help you debug performance problems. Jaeger, Zipkin, and Pixie are
    some examples of tools that offer application tracing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability is the union of traditional monitoring, logging, metrics, and
    tracing, and all the other ways you can understand your system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability also represents a shift toward a team culture of engineering based
    on facts and feedback.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s still important to check that your user-facing services are up, with external
    closed-box checks, but don’t try to build your own: use a third-party monitoring
    service like Uptime Robot.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nines don’t matter if users aren’t happy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](ch15.html#idm45979374465760-marker)) [“Gray failure: the Achilles’ heel
    of cloud-scale systems”](https://blog.acolyer.org), by Adrian Colyer, June 2017.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch15.html#idm45979374404496-marker)) [“Distributed Tracing Infrastructure
    with Jaeger on Kubernetes”](https://medium.com/@masroor.hasan), by Masroor Hasan,
    September 2018.
  prefs: []
  type: TYPE_NORMAL
