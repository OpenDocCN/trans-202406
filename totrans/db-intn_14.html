<html><head></head><body><section data-pdf-bookmark="Chapter 11. Replication and Consistency" data-type="chapter" epub:type="chapter"><div class="chapter" id="keeping_multiple_copies_of_data">&#13;
<h1><span class="label">Chapter 11. </span>Replication and Consistency</h1>&#13;
&#13;
&#13;
<p>Before<a data-primary="replication and consistency" data-secondary="overview of" data-type="indexterm" id="idm46466886385208"/><a data-primary="consistency" data-secondary="overview of" data-type="indexterm" id="idm46466886384360"/> we move on to discuss consensus and atomic commitment algorithms, let’s put together the last piece required for their in-depth understanding: <em>consistency models</em>. Consistency models<a data-primary="consistency models" data-secondary="role of" data-type="indexterm" id="idm46466886381912"/> are important, since they explain visibility semantics and behavior of the system in the presence of multiple copies of data.</p>&#13;
&#13;
<p><em>Fault tolerance</em> is<a data-primary="fault tolerance" data-type="indexterm" id="idm46466886379768"/> a property of a system that can continue operating correctly in the presence of failures of its components. Making a system fault-tolerant is not an easy task, and it may be difficult to add fault tolerance to the existing system. The primary goal is to remove a single point of failure from the system and make sure that we have redundancy in mission-critical components. Usually, redundancy is entirely transparent for the user.</p>&#13;
&#13;
<p>A system can continue operating correctly by storing multiple copies of data so that, when one of the machines fails, the other one can serve as a failover. In systems with a single source of truth (for example, primary/replica databases), failover can be done explicitly, by promoting a replica to become a new master. Other systems do not require explicit reconfiguration and ensure consistency by collecting responses from multiple participants during read and write queries.</p>&#13;
&#13;
<p>Data <em>replication</em> is<a data-primary="data replication" data-type="indexterm" id="idm46466886371112"/> a way of introducing redundancy by maintaining multiple copies of data in the system. However, since updating multiple copies of data atomically is a problem equivalent to consensus <a data-type="xref" href="app01.html#MILOSEVIC11">[MILOSEVIC11]</a>, it might be quite costly to perform this operation for <em>every</em> operation in the database. We can explore some more cost-effective and flexible ways to make data <em>look</em> consistent from the user’s perspective, while allowing some degree of divergence between participants.</p>&#13;
&#13;
<p>Replication is particularly important in multidatacenter deployments. Geo-replication, in this case, serves multiple purposes: it increases availability and the <span class="keep-together">ability</span> to withstand a failure of one or more datacenters by providing redundancy. It can also help to reduce the latency by placing a copy of data physically closer to the client.</p>&#13;
&#13;
<p>When data records are modified, their copies have to be updated accordingly. When talking about replication, we care most about three events: <em>write</em>, <em>replica update</em>, and <em>read</em>. These operations trigger a sequence of events initiated by the client. In some cases, updating replicas can happen after the write has finished from the client perspective, but this still does not change the fact that the client has to be able to observe operations in a particular order.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Achieving Availability" data-type="sect1"><div class="sect1" id="idm46466886374728">&#13;
<h1>Achieving Availability</h1>&#13;
&#13;
<p>We’ve<a data-primary="replication and consistency" data-secondary="system availability" data-type="indexterm" id="idm46466886369432"/><a data-primary="availability" data-type="indexterm" id="idm46466886368584"/><a data-primary="system availability" data-type="indexterm" id="idm46466886367976"/> talked about the fallacies of distributed systems and have identified many things that can go wrong. In the real world, nodes aren’t always alive or able to communicate with one another. However, intermittent failures should not impact <em>availability</em>: from the user’s perspective, the system as a whole has to continue operating as if nothing has happened.</p>&#13;
&#13;
<p>System availability is an incredibly important property: in software engineering, we always strive for high availability, and try to minimize downtime. Engineering teams brag about their uptime metrics. We care so much about availability for several reasons: software has become an integral part of our society, and many important things cannot happen without it: bank transactions, communication, travel, and so on.</p>&#13;
&#13;
<p>For companies, lack of availability can mean losing customers or money: you can’t shop in the online store if it’s down, or transfer the money if your bank’s website isn’t responding.</p>&#13;
&#13;
<p>To make the system highly available, we need to design it in a way that allows handling failures or unavailability of one or more participants gracefully. For that, we need to introduce redundancy and replication. However, as soon as we add redundancy, we face the problem of keeping several copies of data in sync and have to implement recovery mechanisms.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Infamous CAP" data-type="sect1"><div class="sect1" id="infamous_cap">&#13;
<h1>Infamous CAP</h1>&#13;
&#13;
<p><em>Availability</em> is<a data-primary="replication and consistency" data-secondary="CAP conjecture" data-type="indexterm" id="RACcap11"/><a data-primary="CAP (consistency, availability, and partition tolerance)" data-type="indexterm" id="CAP11"/> a property that measures the ability of the system to serve a response for every request successfully. The theoretical definition of availability mentions eventual response, but of course, in a real-world system, we’d like to avoid services that take indefinitely long to respond.</p>&#13;
&#13;
<p>Ideally, we’d like every operation to be <em>consistent</em>. Consistency is defined here as atomic<a data-primary="linearizability" data-type="indexterm" id="idm46466886354440"/><a data-primary="consistency" data-secondary="linearizable consistency" data-type="indexterm" id="idm46466886356232"/> or <em>linearizable</em> consistency (see <a data-type="xref" href="#linearizability">“Linearizability”</a>). Linearizable history can be expressed as a sequence of instantaneous operations that preserves the original operation order. Linearizability simplifies reasoning about the possible <span class="keep-together">system</span> states and makes a distributed system appear as if it was running on a single machine.</p>&#13;
&#13;
<p>We would like to achieve both consistency and availability while tolerating network partitions. The network can get split into several parts where processes are not able to communicate with each other: some of the messages sent between partitioned nodes won’t reach their destinations.</p>&#13;
&#13;
<p>Availability requires any nonfailing node to deliver results, while consistency requires results to be linearizable. CAP conjecture, formulated by Eric Brewer, discusses trade-offs between Consistency, Availability, and Partition tolerance <a data-type="xref" href="app01.html#BREWER00">[BREWER00]</a>.</p>&#13;
&#13;
<p>Availability<a data-primary="best effort availability and consistency" data-type="indexterm" id="idm46466886352376"/> requirement is impossible to satisfy in an asynchronous system, and we cannot implement a system that simultaneously guarantees both <em>availability</em> and <em>consistency</em> in the presence<a data-primary="network partitions" data-type="indexterm" id="idm46466886351224"/> of <em>network partitions</em> <a data-type="xref" href="app01.html#GILBERT02">[GILBERT02]</a>. We can build systems that guarantee strong consistency while providing <em>best effort</em> availability, or guarantee availability while providing <em>best effort</em> consistency <a data-type="xref" href="app01.html#GILBERT12">[GILBERT12]</a>. Best effort here implies that if everything works, the system will not <em>purposefully</em> violate any guarantees, but guarantees are allowed to be weakened and violated in the case of network partitions.</p>&#13;
&#13;
<p>In other words, CAP describes a continuum of potential choices, where on different sides of the spectrum we have systems that are:</p>&#13;
<dl>&#13;
<dt>Consistent and partition tolerant</dt>&#13;
<dd>&#13;
<p>CP systems<a data-primary="CP systems" data-type="indexterm" id="idm46466886343080"/> prefer failing requests to serving potentially inconsistent data.</p>&#13;
</dd>&#13;
<dt>Available and partition tolerant</dt>&#13;
<dd>&#13;
<p>AP systems loosen the consistency requirement and allow serving potentially inconsistent values during the request.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>An example of a CP system is an implementation of a consensus algorithm, requiring a majority of nodes for progress: always consistent, but might be unavailable in the case of a network partition. A database always accepting writes and serving reads as long as even a single replica is up is an example of an AP system, which may end up losing data or serving inconsistent results.</p>&#13;
&#13;
<p>PACELEC conjecture<a data-primary="PACELEC conjecture" data-type="indexterm" id="idm46466886339496"/> <a data-type="xref" href="app01.html#ABADI12">[ABADI12]</a>, an extension of CAP, states that in presence of network partitions there’s a choice between consistency and availability (PAC). Else (E), even if the system is running normally, we <em>still</em> have to make a choice between latency and consistency.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Use CAP Carefully" data-type="sect2"><div class="sect2" id="idm46466886337480">&#13;
<h2>Use CAP Carefully</h2>&#13;
&#13;
<p>It’s important<a data-primary="node crashes" data-type="indexterm" id="idm46466886335144"/> to note that CAP discusses <em>network partitions</em> rather than <em>node crashes</em> or any other type of failure (such as crash-recovery). A node, partitioned from the rest of the cluster, can serve inconsistent requests, but a crashed node will not respond at all. On the one hand, this implies that it’s not necessary to have any nodes down to face consistency problems. On the other hand, this isn’t the case in the real world: there are many different failure scenarios (some of which can be simulated with network partitions).</p>&#13;
&#13;
<p>CAP implies that we can face consistency problems even if all the nodes are up, but there are connectivity issues between them since we expect every nonfailed node to respond correctly, with no regard to how many nodes may be down.</p>&#13;
&#13;
<p>CAP conjecture is sometimes illustrated as a triangle, as if we could turn a knob and have more or less of all of the three parameters. However, while we can turn a knob and trade consistency for availability, partition tolerance is a property we cannot realistically tune or trade for anything <a data-type="xref" href="app01.html#HALE10">[HALE10]</a>.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Consistency in CAP is defined quite differently from what ACID (see <a data-type="xref" href="ch05.html#transaction_processing">Chapter 5</a>) defines as consistency. ACID<a data-primary="transaction processing and recovery" data-secondary="ACID properties" data-type="indexterm" id="idm46466886327960"/><a data-primary="ACID properties" data-type="indexterm" id="idm46466886326680"/> consistency describes transaction consistency: transaction brings the database from one valid state to another, maintaining all the database invariants (such as uniqueness constraints and referential integrity). In CAP, it<a data-primary="atomicity" data-type="indexterm" id="idm46466886324216"/> means that operations are <em>atomic</em> (operations succeed or fail in their entirety) and<a data-primary="consistency" data-secondary="defined" data-type="indexterm" id="idm46466886332168"/> <em>consistent</em> (operations never leave the data in an inconsistent state).</p>&#13;
</div>&#13;
&#13;
<p>Availability<a data-primary="high availability" data-type="indexterm" id="idm46466886330552"/> in CAP is also different from the aforementioned <em>high availability</em> <a data-type="xref" href="app01.html#KLEPPMANN15">[KLEPPMANN15]</a>. The CAP definition puts no bounds on execution latency. Additionally, availability in databases, contrary to CAP, doesn’t require <em>every</em> nonfailed node to respond to <em>every</em> request.</p>&#13;
&#13;
<p>CAP conjecture is used to explain distributed systems, reason about failure scenarios, and evaluate possible situations, but it’s important to remember that there’s a fine line between <em>giving up</em> consistency and serving unpredictable results.</p>&#13;
&#13;
<p>Databases that claim to be on the availability side, when used correctly, are still able to serve consistent results from replicas, given there are enough replicas alive. Of course, there are more complicated failure scenarios and CAP conjecture is just a rule of thumb, and it doesn’t necessarily tell the whole truth.<sup><a data-type="noteref" href="ch11.html#idm46466886319240" id="idm46466886319240-marker">1</a></sup></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Harvest and Yield" data-type="sect2"><div class="sect2" id="idm46466886316184">&#13;
<h2>Harvest and Yield</h2>&#13;
&#13;
<p>CAP conjecture discusses consistency and availability only in their<a data-primary="linearizability" data-type="indexterm" id="idm46466886313896"/> strongest forms: <em>linearizability</em> and the ability of the system to eventually respond to every request. This forces us to make a hard trade-off between the two properties. However, some applications can benefit from slightly relaxed assumptions and we can think about these properties in their weaker forms.</p>&#13;
&#13;
<p>Instead of being <em>either</em> consistent <em>or</em> available, systems can provide relaxed guarantees. We can define two tunable<a data-primary="harvest and yield" data-type="indexterm" id="idm46466886310376"/><a data-primary="yield" data-type="indexterm" id="idm46466886309880"/> metrics: <em>harvest</em> and <em>yield</em>, choosing between which still constitutes correct behavior <a data-type="xref" href="app01.html#FOX99">[FOX99]</a>:</p>&#13;
<dl>&#13;
<dt>Harvest</dt>&#13;
<dd>&#13;
<p>Defines how complete the query is: if the query has to return 100 rows, but can fetch only 99 due to unavailability of some nodes, it still can be better than failing the query completely and returning nothing.</p>&#13;
</dd>&#13;
<dt>Yield</dt>&#13;
<dd>&#13;
<p>Specifies the number of requests that were completed successfully, compared to the total number of attempted requests. Yield is different from the uptime, since, for example, a busy node is not down, but still can fail to respond to some of the requests.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>This shifts the focus of the trade-off from the absolute to the relative terms. We can trade harvest for yield and allow some requests to return incomplete data. One of the ways to increase yield is to return query results only from the available partitions (see <a data-type="xref" href="ch13.html#database_partitioning">“Database Partitioning”</a>). For example, if a subset of nodes storing records of some users is down, we can still continue serving requests for other users. Alternatively, we can require the critical application data to be returned only in its entirety, but allow some deviations for other requests.</p>&#13;
&#13;
<p>Defining, measuring, and making a conscious choice between harvest and yield helps us to build systems that are more resilient to failures.<a data-primary="" data-startref="CAP11" data-type="indexterm" id="idm46466886302792"/><a data-primary="" data-startref="RACcap11" data-type="indexterm" id="idm46466886298536"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Shared Memory" data-type="sect1"><div class="sect1" id="idm46466886357608">&#13;
<h1>Shared Memory</h1>&#13;
&#13;
<p>For<a data-primary="replication and consistency" data-secondary="shared memory" data-type="indexterm" id="idm46466886301768"/><a data-primary="shared memory" data-type="indexterm" id="idm46466886300920"/> a client, the distributed system storing the data acts as if it has shared storage, similar to a single-node system. Internode communication and message passing are abstracted away and happen behind the scenes. This creates an illusion of a shared memory.</p>&#13;
&#13;
<p>A single unit of storage, accessible by read or write operations, is usually called<a data-primary="registers" data-type="indexterm" id="idm46466886295736"/> a <em>register</em>. We can view <em>shared memory</em> in a distributed database as an array of such <span class="keep-together">registers.</span></p>&#13;
&#13;
<p>We<a data-primary="invocation events" data-type="indexterm" id="idm46466886294136"/><a data-primary="completion events" data-type="indexterm" id="idm46466886293304"/><a data-primary="failed processes" data-type="indexterm" id="idm46466886292216"/> identify every operation by its <em>invocation</em> and <em>completion</em> events. We define an operation as <em>failed</em> if the process that invoked it crashes before it completes. If both invocation and completion events for one operation happen before the other operation is invoked, we say that this operation <em>precedes</em> the other one, and these two operations<a data-primary="sequential operations" data-type="indexterm" id="idm46466886289144"/><a data-primary="concurrent operations" data-type="indexterm" id="idm46466886288536"/> are <em>sequential</em>. Otherwise, we say that they are <em>concurrent</em>.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#shared_memory_1">Figure 11-1</a>, you can see processes <code>P<sub>1</sub></code> and <code>P<sub>2</sub></code> executing different operations:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>a) The operation performed by process <code>P<sub>2</sub></code> starts <em>after</em> the operation executed by <code>P<sub>1</sub></code> has already finished, and the two operations are <em>sequential</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>b) There’s an overlap between the two operations, so these operations are <em>concurrent</em>.</p>&#13;
</li>&#13;
<li>&#13;
<p>c) The operation executed by <code>P<sub>2</sub></code> starts <em>after</em> and completes <em>before</em> the operation executed by <code>P<sub>1</sub></code>. These operations are <em>concurrent</em>, too.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="shared_memory_1">&#13;
<img alt="dbin 1101" src="assets/dbin_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Sequential and concurrent operations</h6>&#13;
</div></figure>&#13;
&#13;
<p>Multiple readers or writers can access the register simultaneously. Read and write operations on registers are <em>not immediate</em> and take some time. Concurrent read/write operations performed by different processes<a data-primary="serial operations" data-type="indexterm" id="idm46466886271560"/> are not <em>serial</em>: depending on how registers behave when operations overlap, they might be ordered differently and may produce different results. Depending on how the register behaves in the presence of concurrent operations, we distinguish among three types of registers:</p>&#13;
<dl>&#13;
<dt>Safe</dt>&#13;
<dd>&#13;
<p>Reads to the safe registers<a data-primary="safe registers" data-type="indexterm" id="idm46466886269368"/> may return <em>arbitrary</em> values within the range of the register during a concurrent write operation (which does not sound very practical, but might describe the semantics of an asynchronous system that does not impose the order). Safe registers with binary values might appear to be <em>flickering</em> (i.e., returning results alternating between the two values) during reads concurrent to writes.</p>&#13;
</dd>&#13;
<dt>Regular</dt>&#13;
<dd>&#13;
<p>For regular registers, <a data-primary="regular registers" data-type="indexterm" id="idm46466886267352"/>we have slightly stronger guarantees: a read operation can return only the value written by the most recent <em>completed</em> write or the value written by the write operation that overlaps with the current read. In this case, the system has some notion of order, but write results are not visible to all the readers simultaneously (for example, this may happen in a replicated database, where the master accepts writes and replicates them to workers serving reads).</p>&#13;
</dd>&#13;
<dt>Atomic</dt>&#13;
<dd>&#13;
<p>Atomic registers<a data-primary="atomic registers" data-type="indexterm" id="idm46466886263864"/> guarantee linearizability: every write operation has a single moment before which every read operation returns an old value and after which every read operation returns a new one. Atomicity is a fundamental property that simplifies reasoning about the system state.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ordering" data-type="sect1"><div class="sect1" id="idm46466886257688">&#13;
<h1>Ordering</h1>&#13;
&#13;
<p>When<a data-primary="replication and consistency" data-secondary="ordering" data-type="indexterm" id="idm46466886261240"/><a data-primary="ordering" data-type="indexterm" id="idm46466886260504"/> we see a sequence of events, we have some intuition about their execution order. However, in a distributed system it’s not always that easy, because it’s hard to know when <em>exactly</em> something has happened and have this information available instantly across the cluster. Each participant may have its view of the state, so we have to look at every operation and define it in terms of its <em>invocation</em> and <em>completion</em> events and describe the operation bounds.</p>&#13;
&#13;
<p>Let’s define a system in which processes can execute <code>read(register)</code> and <code>write(register, value)</code> operations on shared registers. Each process executes its own set of operations sequentially (i.e., every invoked operation has to complete before it can start the next one). The combination of sequential process executions forms a global history, in which operations can be executed concurrently.</p>&#13;
&#13;
<p>The simplest way to think about consistency models is in terms of read and write operations and ways they can overlap: read operations have no side effects, while writes change the register state. This helps to reason about when exactly data becomes readable after the write. For example, consider a history in which two processes execute the following events concurrently:</p>&#13;
&#13;
<pre data-type="programlisting">Process 1:      Process 2:&#13;
write(x, 1)     read(x)&#13;
                read(x)</pre>&#13;
&#13;
<p>When looking at these events, it’s unclear what is an outcome of the <code>read(x)</code> operations in both cases. We have several possible histories:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Write completes before both reads.</p>&#13;
</li>&#13;
<li>&#13;
<p>Write and two reads can get interleaved, and can be executed between the reads.</p>&#13;
</li>&#13;
<li>&#13;
<p>Both reads complete before the write.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>There’s no simple answer to what should happen if we have just one copy of data. In a replicated system, we have  more combinations of possible states, and it can get even more complicated when we have multiple processes reading and writing the data.</p>&#13;
&#13;
<p>If all of these operations were executed by the single process, we could enforce a strict order of events, but it’s harder to do so with multiple processes. We can group the potential difficulties into two groups:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Operations may overlap.</p>&#13;
</li>&#13;
<li>&#13;
<p>Effects of the nonoverlapping calls might not be visible immediately.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>To reason about the operation order and have nonambiguous descriptions of possible outcomes, we have to define consistency models. We discuss concurrency in <span class="keep-together">distributed</span> systems in terms of shared memory and concurrent systems, since most of the definitions and rules defining consistency still apply. Even though a lot of terminology between concurrent and distributed systems overlap, we can’t directly apply most of the concurrent algorithms, because of differences in communication patterns, performance, and reliability.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Consistency Models" data-type="sect1"><div class="sect1" id="consistency_models">&#13;
<h1>Consistency Models</h1>&#13;
&#13;
<p>Since<a data-primary="replication and consistency" data-secondary="consistency models" data-type="indexterm" id="RACmodel11"/> operations on shared memory registers are allowed to overlap, we should define clear semantics: what happens if multiple clients read or modify different copies of data simultaneously or within a short period. There’s no single right answer to that question, since these semantics are different depending on the application, but they are well studied in the context of consistency models.</p>&#13;
&#13;
<p><em>Consistency models</em> provide<a data-primary="consistency models" data-secondary="overview of" data-type="indexterm" id="idm46466886233304"/> different semantics and guarantees. You can think of a consistency model as a contract between the participants: what each replica has to do to satisfy the required semantics, and what users can expect when issuing read and write operations.</p>&#13;
&#13;
<p>Consistency models describe what expectations clients might have in terms of possible returned values despite the existence of multiple copies of data and concurrent accesses to it. In<a data-primary="single-operation consistency  models" data-type="indexterm" id="idm46466886236488"/> this section, we will discuss <em>single-operation</em> consistency models.</p>&#13;
&#13;
<p>Each model describes how far the behavior of the system is from the behavior we might expect or find natural. It helps us to distinguish between “all possible histories” of interleaving operations and “histories permissible under model X,” which significantly simplifies reasoning about the visibility of state changes.</p>&#13;
&#13;
<p>We can think about consistency from the perspective<a data-primary="state" data-type="indexterm" id="idm46466886235752"/> of <em>state</em>, describe which state invariants are acceptable, and establish allowable relationships between copies of the data placed onto different replicas. Alternatively, we can consider<a data-primary="operation consistency" data-type="indexterm" id="idm46466886241016"/><a data-primary="consistency" data-secondary="operation consistency" data-type="indexterm" id="idm46466886239624"/> <em>operation</em> consistency, which provides an outside view on the data store, describes operations, and puts constraints on the order in which they occur <a data-type="xref" href="app01.html#TANENBAUM06">[TANENBAUM06]</a> <a data-type="xref" href="app01.html#AGUILERA16">[AGUILERA16]</a>.</p>&#13;
&#13;
<p>Without a global clock, it is difficult to give distributed operations a precise and deterministic order. It’s like a Special Relativity Theory for data: every participant has its own perspective on state and time.</p>&#13;
&#13;
<p>Theoretically, we could grab a system-wide lock every time we want to change the system state, but it’d be highly impractical. Instead, we use a set of rules, definitions, and restrictions that limit the number of possible histories and outcomes.</p>&#13;
&#13;
<p>Consistency models add another dimension to what we discussed in <a data-type="xref" href="#infamous_cap">“Infamous CAP”</a>. Now we have to juggle not only consistency and availability, but also consider consistency in terms of synchronization costs <a data-type="xref" href="app01.html#ATTIYA94">[ATTIYA94]</a>. Synchronization costs may include latency, additional CPU cycles spent executing additional <span class="keep-together">operations,</span> disk I/O used to persist recovery information, wait time, network I/O, and everything else that can be prevented by avoiding synchronization.</p>&#13;
&#13;
<p>First, we’ll focus on visibility and propagation of operation results. Coming back to the example with concurrent reads and writes, we’ll be able to limit the number of possible histories by either positioning dependent writes after one another or defining a point at which the new value is propagated.</p>&#13;
&#13;
<p>We<a data-primary="processes" data-secondary="consistency models and" data-type="indexterm" id="idm46466886223208"/> discuss consistency models in terms of <em>processes</em> (clients) issuing <code>read</code> and <code>write</code> operations against the database state. Since we discuss consistency in the context of replicated data, we assume that the database can have multiple replicas.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Strict Consistency" data-type="sect2"><div class="sect2" id="idm46466886222088">&#13;
<h2>Strict Consistency</h2>&#13;
&#13;
<p><em>Strict consistency</em> is<a data-primary="consistency  models" data-secondary="strict consistency" data-type="indexterm" id="idm46466886220776"/><a data-primary="strict consistency" data-type="indexterm" id="idm46466886219224"/> the equivalent of complete replication transparency: any write by any process is instantly available for the subsequent reads by any process. It involves the concept of a global clock and, if there was a <code>write(x, 1)</code> at instant <code>t<sub>1</sub></code>, any <code>read(x)</code> will return a newly written value <code>1</code> at <em>any</em> instant <code>t<sub>2</sub> &gt; t<sub>1</sub></code>.</p>&#13;
&#13;
<p>Unfortunately, this is just a theoretical model, and it’s impossible to implement, as the laws of physics and the way distributed systems work set limits on how fast things may happen <a data-type="xref" href="app01.html#SINHA97">[SINHA97]</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linearizability" data-type="sect2"><div class="sect2" id="linearizability">&#13;
<h2>Linearizability</h2>&#13;
&#13;
<p><em>Linearizability</em> is<a data-primary="consistency models" data-secondary="linearizability" data-type="indexterm" id="CMlinear11"/><a data-primary="linearizability" data-type="indexterm" id="linezb11"/> the strongest single-object, single-operation consistency model.&#13;
Under this model, effects of the write become visible to all readers exactly once at some point in time between its start and end, and no client can observe state transitions or side effects of partial (i.e., unfinished, still in-flight) or incomplete (i.e., interrupted before completion) write operations <a data-type="xref" href="app01.html#LEE15">[LEE15]</a>.</p>&#13;
&#13;
<p>Concurrent operations are represented as one of the possible sequential histories for which visibility properties hold. There is some indeterminism in linearizability, as there may exist more than one way in which the events can be ordered <a data-type="xref" href="app01.html#HERLIHY90">[HERLIHY90]</a>.</p>&#13;
&#13;
<p>If two operations overlap, they may take effect in any order. All read operations that occur after write operation completion can observe the effects of this operation. As soon as a single read operation returns a particular value, all reads that come after it return the value <em>at least</em> as recent as the one it returns <a data-type="xref" href="app01.html#BAILIS14a">[BAILIS14a]</a>.</p>&#13;
&#13;
<p>There is some flexibility in terms of the order in which concurrent events occur in a global history, but they cannot be reordered arbitrarily. Operation results should not become effective before the operation starts as that would require an oracle able to predict future operations. At the same time, results have to take effect before completion, since otherwise, we cannot define a linearization point.</p>&#13;
&#13;
<p>Linearizability respects both sequential process-local operation order and the order of operations running in parallel relative to other processes, and defines a <em>total order</em> of the events.</p>&#13;
&#13;
<p>This order should be <em>consistent</em>, which means that every read of the shared value should return the latest value written to this shared variable preceding this read, or the value of a write that overlaps with this read. Linearizable write access to a shared variable also implies mutual exclusion: between the two concurrent writes, only one can go first.</p>&#13;
&#13;
<p>Even though operations are concurrent and have some overlap, their effects become visible in a way that makes them appear sequential. No operation happens instantaneously, but still <em>appears</em> to be atomic.</p>&#13;
&#13;
<p>Let’s consider the following history:</p>&#13;
&#13;
<pre data-type="programlisting">Process 1:      Process 2:     Process 3:&#13;
write(x, 1)     write(x, 2)    read(x)&#13;
                               read(x)&#13;
                               read(x)</pre>&#13;
&#13;
<p>In <a data-type="xref" href="#linearizability_1">Figure 11-2</a>, we have three processes, two of which perform write operations on the register <code>x</code>, which has an initial value of <code>∅</code>. Read operations can observe these writes in one of the following ways:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>a) The first read operation can return <code>1</code>, <code>2</code>, or <code>∅</code> (the initial value, a state before both writes), since both writes are still in-flight. The first read can get ordered <em>before</em> both writes, <em>between</em> the first and second writes, and <em>after</em> both writes.</p>&#13;
</li>&#13;
<li>&#13;
<p>b) The second read operation can return only <code>1</code> and <code>2</code>, since the first write has completed, but the second write didn’t return yet.</p>&#13;
</li>&#13;
<li>&#13;
<p>c) The third read can only return <code>2</code>, since the second write is ordered after the first.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="linearizability_1">&#13;
<img alt="dbin 1102" src="assets/dbin_1102.png"/>&#13;
<h6><span class="label">Figure 11-2. </span>Example of linearizability</h6>&#13;
</div></figure>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linearization point" data-type="sect3"><div class="sect3" id="idm46466886184744">&#13;
<h3>Linearization point</h3>&#13;
&#13;
<p>One<a data-primary="linearization point" data-type="indexterm" id="idm46466886183048"/> of the most important traits of linearizability is visibility: once the operation is complete, everyone must see it, and the system can’t “travel back in time,” reverting it or making it invisible for some participants. In other words, linearization prohibits stale reads and requires reads to be monotonic.</p>&#13;
&#13;
<p>This consistency model is best explained in terms of atomic (i.e., uninterruptible, indivisible) operations. Operations do not have to <em>be</em> instantaneous (also because there’s no such thing), but their <em>effects</em> have to become visible at some point in time, making an illusion that they were instantaneous. This moment is called a <em>linearization point</em>.</p>&#13;
&#13;
<p>Past the linearization point of the write operation (in other words, when the value becomes visible for other processes) every process has to see either the value this operation wrote or some later value, if some additional write operations are ordered after it. A visible value should remain stable until the next one becomes visible after it, and the register should not alternate between the two recent states.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Most<a data-primary="atomicity" data-type="indexterm" id="idm46466886180472"/><a data-primary="compare-and-swap instructions" data-type="indexterm" id="idm46466886178824"/> of the programming languages these days offer atomic primitives that allow atomic <code>write</code> and <code>compare-and-swap</code> (CAS) operations. Atomic <code>write</code> operations do not consider current register values, unlike CAS, that move from one value to the next only when the previous value is unchanged <a data-type="xref" href="app01.html#HERLIHY94">[HERLIHY94]</a>. Reading the value, modifying it, and then writing it with CAS is more complex than simply checking and setting the value, because of the<a data-primary="ABA problem" data-type="indexterm" id="idm46466886171160"/> possible <em>ABA problem</em> <a data-type="xref" href="app01.html#DECHEV10">[DECHEV10]</a>: if CAS expects the value <code>A</code> to be present in the register, it will be installed even if the value <code>B</code> was set and then switched back to <code>A</code> by the other two concurrent write operations. In other words, the presence of the value <code>A</code> alone does not guarantee that the value hasn’t been changed since the last read.</p>&#13;
</div>&#13;
&#13;
<p>The linearization point serves as a cutoff, after which operation effects become visible. We can implement it by using locks to guard a critical section, atomic read/write, or read-modify-write primitives.</p>&#13;
&#13;
<p><a data-type="xref" href="#linearizability_2">Figure 11-3</a> shows that linearizability assumes hard time bounds and the clock is <em>real time</em>, so the operation effects have to become visible <em>between</em> <code>t<sub>1</sub></code>, when the operation request was issued, and <code>t<sub>2</sub></code>, when the process received a response.</p>&#13;
&#13;
<figure><div class="figure" id="linearizability_2">&#13;
<img alt="dbin 1103" src="assets/dbin_1103.png"/>&#13;
<h6><span class="label">Figure 11-3. </span>Time bounds of a linearizable operation</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#linearizability_3">Figure 11-4</a> illustrates that the linearization point <em>cuts</em> the history into <em>before</em> and <em>after</em>. Before the linearization point, the old value is visible, after it, the new value is visible.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="linearizability_3">&#13;
<img alt="dbin 1104" src="assets/dbin_1104.png"/>&#13;
<h6><span class="label">Figure 11-4. </span>Linearization point</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cost of linearizability" data-type="sect3"><div class="sect3" id="idm46466886184168">&#13;
<h3>Cost of linearizability</h3>&#13;
&#13;
<p>Many systems avoid implementing linearizability today. Even CPUs do not offer linearizability when accessing main memory by default. This has happened because synchronization instructions are expensive, slow, and involve cross-node CPU traffic and cache invalidations. However, it is possible to implement linearizability using low-level primitives <a data-type="xref" href="app01.html#MCKENNEY05a">[MCKENNEY05a]</a>, <a data-type="xref" href="app01.html#MCKENNEY05b">[MCKENNEY05b]</a>.</p>&#13;
&#13;
<p>In concurrent programming, you can use compare-and-swap operations to introduce linearizability. Many algorithms work by <em>preparing</em> results and then using CAS for swapping pointers and <em>publishing</em> them. For example, we can implement a concurrent queue by creating a linked list node and then atomically appending it to the tail of the list <a data-type="xref" href="app01.html#KHANCHANDANI18">[KHANCHANDANI18]</a>.</p>&#13;
&#13;
<p>In distributed systems, linearizability requires coordination and ordering. It can be implemented<a data-primary="consensus" data-secondary="linearizability using" data-type="indexterm" id="idm46466886149736"/> using <em>consensus</em>: clients interact with a replicated store using messages, and the consensus module is responsible for ensuring that applied operations are consistent and identical across the cluster. Each write operation will appear instantaneously, exactly once at some point between its invocation and completion events <a data-type="xref" href="app01.html#HOWARD14">[HOWARD14]</a>.</p>&#13;
&#13;
<p>Interestingly, linearizability<a data-primary="local execution" data-type="indexterm" id="idm46466886148648"/> in its traditional understanding is regarded as a <em>local</em> property and implies composition of independently implemented and verified elements. Combining linearizable histories produces a history that is also linearizable <a data-type="xref" href="app01.html#HERLIHY90">[HERLIHY90]</a>. In other words, a system in which all objects are linearizable, is also linearizable. This is a very useful property, but we should remember that its scope is limited to a single object and, even though operations on two independent objects are linearizable, operations that involve both objects have to rely on additional synchronization means.</p>&#13;
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46466886151592">&#13;
<h5>Reusable Infrastructure for Linearizability</h5>&#13;
<p>Reusable Infrastructure for Linearizability (RIFL), is<a data-primary="Reusable Infrastructure for Linearizability (RIFL)" data-type="indexterm" id="idm46466886143416"/> a mechanism for implementing linearizable remote procedure calls (RPCs) <a data-type="xref" href="app01.html#LEE15">[LEE15]</a>. In RIFL, messages are uniquely identified with the client ID and a client-local monotonically increasing sequence number.</p>&#13;
&#13;
<p>To assign client IDs, RIFL uses<a data-primary="leases" data-type="indexterm" id="idm46466886142456"/> <em>leases</em>, issued by the system-wide service: unique identifiers used to establish uniqueness and break sequence number ties. If the failed client tries to execute an operation using an expired lease, its operation will not be committed: the client has to receive a new lease and retry.</p>&#13;
&#13;
<p>If the server crashes before it can acknowledge the write, the client may attempt to retry this operation without knowing that it has already been applied. We can even end up in a situation in which client <code>C1</code> writes value <code>V1</code>, but doesn’t receive an acknowledgment. Meanwhile, client <code>C2</code> writes value <code>V2</code>. If <code>C1</code> retries its operation and successfully writes <code>V1</code>, the write of <code>C2</code> would be lost. To avoid this, the system needs to prevent repeated execution of retried operations. When the client retries the operation, instead of reapplying it, RIFL returns a completion object, indicating that the operation it’s associated with has already been executed, and returns its result.</p>&#13;
&#13;
<p>Completion objects are stored in a durable storage, along with the actual data records. However, their lifetimes are different: the completion object should exist until either the issuing client promises it won’t retry the operation associated with it, or until the server detects a client crash, in which case all completion objects associated with it can be safely removed. Creating a completion object should be atomic with the mutation of the data record it is associated with.</p>&#13;
&#13;
<p>Clients have to periodically renew their leases to signal their liveness. If the client fails to renew its lease, it is marked as crashed and all the data associated with its lease is garbage collected. Leases have a limited lifetime to make sure that operations that belong to the failed process won’t be retained in the log forever. If the failed client tries to continue operation using an expired lease, its results will not be committed and the client will have to start from scratch.</p>&#13;
&#13;
<p>The advantage of RIFL is that, by guaranteeing that the RPC cannot be executed more than once, an operation can be made linearizable by ensuring that its results are made visible atomically, and most of its implementation details are independent from the underlying storage system.<a data-primary="" data-startref="linezb11" data-type="indexterm" id="idm46466886132728"/><a data-primary="" data-startref="CMlinear11" data-type="indexterm" id="idm46466886135352"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sequential Consistency" data-type="sect2"><div class="sect2" id="idm46466886211784">&#13;
<h2>Sequential Consistency</h2>&#13;
&#13;
<p>Achieving<a data-primary="consistency models" data-secondary="sequential consistency" data-type="indexterm" id="idm46466886133672"/><a data-primary="sequential consistency" data-type="indexterm" id="idm46466886125320"/> linearizability might be too expensive, but it is possible to relax the model, while still providing rather strong consistency guarantees. <em>Sequential consistency</em> allows ordering operations as if they were executed in <em>some</em> sequential order, while requiring operations of each individual process to be executed in the same order they were performed by the process.</p>&#13;
&#13;
<p>Processes can observe operations executed by other participants in the order consistent with their own history, but this view can be arbitrarily stale from the global perspective <a data-type="xref" href="app01.html#KINGSBURY18a">[KINGSBURY18a]</a>. Order of execution <em>between</em> processes is undefined, as there’s no shared notion of time.</p>&#13;
&#13;
<p>Sequential consistency was initially introduced in the context of concurrency, describing it as a way to execute multiprocessor programs correctly. The original description required memory requests to the same cell to be ordered in the queue (FIFO, arrival order), did not impose global ordering on the overlapping writes to independent memory cells, and allowed reads to fetch the value from the memory cell, or the latest value from the queue if the queue was nonempty <a data-type="xref" href="app01.html#LAMPORT79">[LAMPORT79]</a>. This example helps to understand the semantics of sequential consistency. Operations can be ordered in different ways (depending on the arrival order, or even arbitrarily in case two writes arrive simultaneously), but all processes <em>observe</em> the operations in the same order.</p>&#13;
&#13;
<p>Each process can issue read and write requests in an order specified by its own program, which is very intuitive. Any nonconcurrent, single-threaded program executes its steps this way: one after another. All write operations propagating from the same process appear in the order they were submitted by this process. Operations propagating from different sources may be ordered <em>arbitrarily</em>, but this order will be consistent from the readers’ perspective.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Sequential consistency is often confused with linearizability since both have similar semantics. Sequential consistency, just as linearizability, requires operations to be globally ordered, but linearizability requires the local order of each process and global order to be consistent. In other words, linearizability respects a real-time operation order. Under sequential consistency, ordering holds only for the same-origin writes <a data-type="xref" href="app01.html#VIOTTI16">[VIOTTI16]</a>. Another important distinction is composition: we can combine linearizable histories and still expect results to be linearizable, while sequentially consistent schedules are not composable <a data-type="xref" href="app01.html#ATTIYA94">[ATTIYA94]</a>.</p>&#13;
</div>&#13;
&#13;
<p><a data-type="xref" href="#sequential_consistency_3">Figure 11-5</a> shows how <code>write(x,1)</code> and <code>write(x,2)</code> can become visible to <code>P<sub>3</sub></code> and <code>P<sub>4</sub></code>. Even though in wall-clock terms, <code>1</code> was written <em>before</em> <code>2</code>, it can get ordered after <code>2</code>. At the same time, while <code>P<sub>3</sub></code> already reads the value <code>1</code>, <code>P<sub>4</sub></code> can still read <code>2</code>. However, <em>both</em> orders, <code>1 → 2</code> and <code>2 → 1</code>, are valid, as long as they’re consistent for different readers. What’s important here is that both <code>P<sub>3</sub></code> and <code>P<sub>4</sub></code> have observed values <em>in the same order</em>: first <code>2</code>, and then <code>1</code> <a data-type="xref" href="app01.html#TANENBAUM14">[TANENBAUM14]</a>.</p>&#13;
&#13;
<figure><div class="figure" id="sequential_consistency_3">&#13;
<img alt="dbin 1105" src="assets/dbin_1105.png"/>&#13;
<h6><span class="label">Figure 11-5. </span>Ordering in sequential consistency</h6>&#13;
</div></figure>&#13;
&#13;
<p>Stale reads can be explained, for example, by replica divergence: even though writes propagate to different replicas in the same order, they can arrive there at different times.</p>&#13;
&#13;
<p>The main difference with linearizability is the absence of globally enforced time bounds. Under linearizability, an operation has to become effective within its wall-clock time bounds. By the time the write <code>W₁</code> operation completes, its results have to be applied, and every reader should be able to see the value <em>at least</em> as recent as one written by <code>W₁</code>. Similarly, after a read operation <code>R₁</code> returns, any read operation that happens after it should return the value that <code>R₁</code> has seen or a later value (which, of course, has to follow the same rule).</p>&#13;
&#13;
<p>Sequential consistency relaxes this requirement: an operation’s results can become visible <em>after</em> its completion, as long as the order is consistent from the individual processors’ perspective. Same-origin writes can’t “jump” over each other: their program order, relative to their own executing process, has to be preserved. The other restriction is that the order in which operations have appeared must be consistent for <em>all</em> readers.</p>&#13;
&#13;
<p>Similar to linearizability, modern CPUs do not guarantee sequential consistency by default and, since the processor can reorder instructions, we should use memory barriers (also called fences) to make sure that writes become visible to concurrently running threads in order <a data-type="xref" href="app01.html#DREPPER07">[DREPPER07]</a> <a data-type="xref" href="app01.html#GEORGOPOULOS16">[GEORGOPOULOS16]</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Causal Consistency" data-type="sect2"><div class="sect2" id="idm46466886130760">&#13;
<h2>Causal Consistency</h2>&#13;
  <blockquote>&#13;
    <p>You see, there is only one constant, one universal, it is the only real truth: causality. Action. Reaction. Cause and effect.</p>&#13;
    <p data-type="attribution">Merovingian from <em>The Matrix Reloaded</em></p>&#13;
  </blockquote>&#13;
&#13;
<p>Even<a data-primary="consistency models" data-secondary="causal consistency" data-type="indexterm" id="CMcasual11"/><a data-primary="causal consistency" data-type="indexterm" id="casualc11"/> though having a global operation order is often unnecessary, it might be necessary to establish order between <em>some</em> operations. Under the <em>causal consistency</em> model, all processes have to see <em>causally related</em> operations in the same order. <em>Concurrent writes</em> with no causal relationship can be observed in a different order by different processors.</p>&#13;
&#13;
<p>First, let’s take a look at <em>why</em> we need causality and how writes that have no causal relationship can propagate. In <a data-type="xref" href="#causal_consistency_no_order">Figure 11-6</a>, processes <code>P<sub>1</sub></code> and <code>P<sub>2</sub></code> make writes that <em>aren’t</em> causally ordered. The results of these operations can propagate to readers at different times and out of order. Process <code>P<sub>3</sub></code> will see the value <code>1</code> before it sees <code>2</code>, while <code>P<sub>4</sub></code> will first see <code>2</code>, and then <code>1</code>.</p>&#13;
&#13;
<figure><div class="figure" id="causal_consistency_no_order">&#13;
<img alt="dbin 1106" src="assets/dbin_1106.png"/>&#13;
<h6><span class="label">Figure 11-6. </span>Write operations with no causal relationship</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-type="xref" href="#causal_consistency_establishing_order">Figure 11-7</a> shows an example of causally related writes. In addition to a written value, we now have to specify a logical clock value that would establish a causal order between operations. <code>P<sub>1</sub></code> starts with a write operation <code>write(x,∅,1)→t<sub>1</sub></code>, which starts from the initial value <code>∅</code>. <code>P<sub>2</sub></code> performs another write operation, <code>write(x, t<sub>1</sub>, 2)</code>, and specifies that it is logically ordered <em>after</em> <code>t<sub>1</sub></code>, requiring operations to propagate <em>only</em> in the order established by the logical clock.</p>&#13;
&#13;
<figure class="width-75"><div class="figure" id="causal_consistency_establishing_order">&#13;
<img alt="dbin 1107" src="assets/dbin_1107.png"/>&#13;
<h6><span class="label">Figure 11-7. </span>Causally related write operations</h6>&#13;
</div></figure>&#13;
&#13;
<p>This establishes a <em>causal order</em> between these operations. Even if the latter write propagates faster than the former one, it isn’t made visible until all of its dependencies arrive, and the event order is reconstructed from their logical timestamps. In other words, a happened-before relationship is established logically, without using physical clocks, and all processes agree on this order.</p>&#13;
&#13;
<p><a data-type="xref" href="#causal_consistency_with_order">Figure 11-8</a> shows processes <code>P<sub>1</sub></code> and <code>P<sub>2</sub></code> making causally related writes, which propagate to <code>P<sub>3</sub></code> and <code>P<sub>4</sub></code> in their logical order. This prevents us from the situation shown in <a data-type="xref" href="#causal_consistency_no_order">Figure 11-6</a>; you can compare histories of <code>P<sub>3</sub></code> and <code>P<sub>4</sub></code> in both figures.</p>&#13;
&#13;
<figure><div class="figure" id="causal_consistency_with_order">&#13;
<img alt="dbin 1108" src="assets/dbin_1108.png"/>&#13;
<h6><span class="label">Figure 11-8. </span>Write operations with causal relationship</h6>&#13;
</div></figure>&#13;
&#13;
<p>You can think of this in terms of communication on some online forum: you post something online, someone sees your post and responds to it, and a third person sees this response and continues the conversation thread. It is possible for conversation threads to diverge: you can choose to respond to one of the conversations in the thread and continue the chain of events, but some threads will have only a few messages in common, so there might be no single history for all the messages.</p>&#13;
&#13;
<p>In a causally consistent system, we get session guarantees for the application, ensuring the view of the database is consistent with its own actions, even if it executes read and write requests against different, potentially inconsistent, servers <a data-type="xref" href="app01.html#TERRY94">[TERRY94]</a>. These guarantees are: monotonic reads, monotonic writes, read-your-writes, writes-follow-reads. You can find more information on these session models in <a data-type="xref" href="#client_centric_consistency">“Session Models”</a>.</p>&#13;
&#13;
<p>Causal consistency can be implemented using logical clocks <a data-type="xref" href="app01.html#LAMPORT78">[LAMPORT78]</a> and sending context metadata with every message, summarizing which operations logically precede the current one. When the update is received from the server, it contains the latest version of the context. Any operation can be processed only if all operations preceding it have already been applied. Messages for which contexts do not match are buffered on the server as it is too early to deliver them.</p>&#13;
&#13;
<p>The two prominent and frequently cited projects implementing causal consistency are Clusters of Order-Preserving Servers (COPS) <a data-type="xref" href="app01.html#LLOYD11">[LLOYD11]</a> and Eiger <a data-type="xref" href="app01.html#LLOYD13">[LLOYD13]</a>. Both projects implement causality through a library (implemented as a frontend server that users connect to) and track dependencies to ensure consistency. COPS tracks dependencies through key versions, while Eiger establishes operation order instead (operations in Eiger can depend on operations executed on the other nodes; for example, in the case of multipartition transactions). Both projects do not expose out-of-order operations like eventually consistent stores might do. Instead, they detect and handle conflicts: in COPS, this is done by checking the key order and using application-specific functions, while Eiger implements the last-write-wins rule.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vector clocks" data-type="sect3"><div class="sect3" id="vector_clocks">&#13;
<h3>Vector clocks</h3>&#13;
&#13;
<p>Establishing<a data-primary="clocks" data-type="indexterm" id="idm46466886056168"/> causal order allows the system to reconstruct the sequence of events even if messages are delivered out of order, fill the gaps between the messages, and avoid publishing operation results in case some messages are still missing. For example, if messages <code>{M1(∅, t1), M2(M1, t2), M3(M2, t3)}</code>, each specifying their dependencies, are causally related and were propagated out of order, the process buffers them until it can collect all operation dependencies and restore their causal order <a data-type="xref" href="app01.html#KINGSBURY18b">[KINGSBURY18b]</a>. Many databases, for example, Dynamo <a data-type="xref" href="app01.html#DECANDIA07">[DECANDIA07]</a> and Riak <a data-type="xref" href="app01.html#SHEEHY10a">[SHEEHY10a]</a>, use <em>vector clocks</em> <a data-type="xref" href="app01.html#LAMPORT78">[LAMPORT78]</a> <a data-type="xref" href="app01.html#MATTERN88">[MATTERN88]</a> for establishing causal order.</p>&#13;
&#13;
<p>A <em>vector clock</em> <a data-primary="vector clocks" data-type="indexterm" id="idm46466886041736"/><a data-primary="logical clocks" data-type="indexterm" id="idm46466886041128"/>is a structure for establishing a <em>partial order</em> between the events, detecting and resolving divergence between the event chains. With vector clocks, we can simulate common time, global state, and represent asynchronous events as synchronous ones. Processes maintain vectors of <em>logical clocks</em>, with one clock per process. Every clock starts at the initial value and is incremented every time a new event arrives (for example, a write occurs). When receiving clock vectors from other processes, a process updates its local vector to the highest clock values per process from the received vectors (i.e., highest clock values the transmitting node has ever seen).</p>&#13;
&#13;
<p>To<a data-primary="conflict resolution" data-type="indexterm" id="idm46466886039336"/> use vector clocks for conflict resolution, whenever we make a write to the database, we first check if the value for the written key already exists locally. If the previous value already exists, we append a new version to the version vector and establish the causal relationship between the two writes. Otherwise, we start a new chain of events and initialize the value with a single version.</p>&#13;
&#13;
<p>We were talking about consistency in terms of access to shared memory registers and wall-clock operation ordering, and first mentioned potential replica divergence when talking about sequential consistency. Since only write operations to the same memory location have to be ordered, we cannot end up in a situation where we have a write conflict if values are independent <a data-type="xref" href="app01.html#LAMPORT79">[LAMPORT79]</a>.</p>&#13;
&#13;
<p>Since we’re looking for a consistency model that would improve availability and performance, we have to allow replicas to diverge not only by serving stale reads but also by accepting potentially conflicting writes, so the system is allowed to create two independent chains of events. <a data-type="xref" href="#causal_consistency_3">Figure 11-9</a> shows such a divergence: from the perspective of one replica, we see history as <code>1, 5, 7, 8</code> and the other one reports <code>1, 5, 3</code>. Riak allows users to see and resolve divergent histories <a data-type="xref" href="app01.html#DAILY13">[DAILY13]</a>.</p>&#13;
&#13;
<figure><div class="figure" id="causal_consistency_3">&#13;
<img alt="dbin 1109" src="assets/dbin_1109.png"/>&#13;
<h6><span class="label">Figure 11-9. </span>Divergent histories under causal consistency</h6>&#13;
</div></figure>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>To implement causal consistency, we have to store causal history, add garbage collection, and ask the user to reconcile divergent histories in case of a conflict. Vector clocks can tell you that the conflict has occurred, but do not propose exactly how to resolve it, since resolution semantics are often application-specific. Because of that, some eventually consistent databases, for<a data-primary="Apache Cassandra" data-secondary="conflict resolution in" data-type="indexterm" id="idm46466886028456"/> example, Apache Cassandra, do not order operations causally and use the last-write-wins rule for conflict resolution instead <a data-type="xref" href="app01.html#ELLIS13">[ELLIS13]</a>.<a data-primary="" data-startref="RACmodel11" data-type="indexterm" id="idm46466886027224"/><a data-primary="" data-startref="CMcasual11" data-type="indexterm" id="idm46466886026376"/><a data-primary="" data-startref="casualc11" data-type="indexterm" id="idm46466886024440"/></p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Session Models" data-type="sect1"><div class="sect1" id="client_centric_consistency">&#13;
<h1>Session Models</h1>&#13;
&#13;
<p>Thinking<a data-primary="replication and consistency" data-secondary="session models" data-type="indexterm" id="idm46466886021016"/><a data-primary="session models" data-type="indexterm" id="idm46466886020168"/> about consistency in terms of value propagation is useful for database developers, since it helps to understand and impose required data invariants, but some things are easier understood and explained from the client point of view. We can look at our distributed system from the perspective of a single client instead of multiple clients.</p>&#13;
&#13;
<p><em>Session models</em> <a data-type="xref" href="app01.html#VIOTTI16">[VIOTTI16]</a> (also<a data-primary="client centric consistency model" data-type="indexterm" id="idm46466886018120"/><a data-primary="consistency models" data-secondary="client centric consistency model" data-type="indexterm" id="idm46466886017512"/> called client-centric consistency models <a data-type="xref" href="app01.html#TANENBAUM06">[TANENBAUM06]</a>) help to reason about the state of the distributed system from the client perspective: how each client observes the state of the system while issuing read and write operations.</p>&#13;
&#13;
<p>If other consistency models we discussed so far focus on explaining operation ordering in the presence of concurrent clients, client-centric consistency focuses on how a single client interacts with the system. We still assume that each client’s operations are sequential: it has to finish one operation before it can start executing the next one. If the client crashes or loses connection to the server before its operation completes, we do not make any assumptions about the state of incomplete operations.</p>&#13;
&#13;
<p>In a distributed system, clients often can connect to any available replica and, if the results of the recent write against one replica did not propagate to the other one, the client might not be able to observe the state change it has made.</p>&#13;
&#13;
<p>One of the reasonable expectations is that every write issued by the client is visible to it. This<a data-primary="read-own-writes consistency model" data-type="indexterm" id="idm46466886012248"/><a data-primary="consistency models" data-secondary="read-own-writes consistency model" data-type="indexterm" id="idm46466886006264"/> assumption holds under the <em>read-own-writes</em> consistency model, which states that every read operation following the write on the same or the other replica has to observe the updated value. For example, <code>read(x)</code> that was executed immediately after <code>write(x,V)</code> will return the value <code>V</code>.</p>&#13;
&#13;
<p>The <em>monotonic reads</em> model restricts<a data-primary="monotonic reads/writes models" data-type="indexterm" id="idm46466886010184"/><a data-primary="consistency models" data-secondary="monotonic reads/writes models" data-type="indexterm" id="idm46466886009320"/> the value visibility and states that if the <code>read(x)</code> has observed the value <code>V</code>, the following reads have to observe a value at least as recent as <code>V</code> or some later value.</p>&#13;
&#13;
<p>The <em>monotonic writes</em> model assumes that values originating from the same client appear in the order this client has executed them. If, according to the client session order, <code>write(x,V2)</code> was made <em>after</em> <code>write(x,V1)</code>, their effects have to become visible in the same order (i.e., <code>V1</code> first, and then <code>V2</code>) to <em>all</em> other processes. Without this assumption, old data can be “resurrected,” resulting in data loss.</p>&#13;
&#13;
<p><em>Writes-follow-reads</em> (sometimes referred as session causality)<a data-primary="writes-follow-reads model" data-type="indexterm" id="idm46466886000872"/><a data-primary="session causality" data-type="indexterm" id="idm46466885998824"/><a data-primary="consistency models" data-secondary="writes-follow-reads model" data-type="indexterm" id="idm46466885998216"/> ensures that writes are ordered after writes that were observed by previous read operations. For example, if <code>write(x,V2)</code> is ordered after <code>read(x)</code> that has returned <code>V1</code>, <code>write(x,V2)</code> will be ordered <em>after</em> <code>write(x,V1)</code>.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Session models make <em>no</em> assumptions about operations made by <em>different</em> processes (clients) or from the different logical session <a data-type="xref" href="app01.html#TANENBAUM14">[TANENBAUM14]</a>. These models describe operation ordering from the point of view of a single process. However, the same guarantees have to hold for <em>every</em> process in the system. In other words, if <code>P<sub>1</sub></code> can read its own writes, <code>P<sub>2</sub></code> should be able to read <em>its</em> own writes, too.</p>&#13;
</div>&#13;
&#13;
<p>Combining monotonic reads, monotonic writes, and read-own-writes gives Pipelined RAM (PRAM) consistency <a data-type="xref" href="app01.html#LIPTON88">[LIPTON88]</a> <a data-type="xref" href="app01.html#BRZEZINSKI03">[BRZEZINSKI03]</a>, also known as FIFO consistency. PRAM guarantees that write operations originating from one process will propagate in the order they were executed by this process. Unlike under sequential consistency, writes from different processes can be observed in different order.</p>&#13;
&#13;
<p>The properties described by client-centric consistency models are desirable and, in the majority of cases, are used by distributed systems developers to validate their systems and simplify their usage.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Eventual Consistency" data-type="sect1"><div class="sect1" id="eventual_consistency">&#13;
<h1>Eventual Consistency</h1>&#13;
&#13;
<p>Synchronization<a data-primary="replication and consistency" data-secondary="eventual consistency" data-type="indexterm" id="idm46466885985896"/><a data-primary="eventual consistency" data-type="indexterm" id="idm46466885985160"/><a data-primary="consistency" data-secondary="eventual consistency" data-type="indexterm" id="idm46466885984552"/> is expensive, both in multiprocessor programming and in distributed systems. As we discussed in <a data-type="xref" href="#consistency_models">“Consistency Models”</a>, we can relax consistency guarantees and use models that allow some divergence between the nodes. For example, sequential consistency allows reads to be propagated at different speeds.</p>&#13;
&#13;
<p>Under <em>eventual consistency</em>, updates propagate through the system asynchronously. Formally, it states that if there are no <em>additional</em> updates performed against the data item, <em>eventually</em> all accesses return the latest written value <a data-type="xref" href="app01.html#VOGELS09">[VOGELS09]</a>. In case of a conflict, the notion of <em>latest</em> value might change, as the values from diverged replicas are reconciled using a conflict resolution strategy, such as last-write-wins or using vector clocks (see <a data-type="xref" href="#vector_clocks">“Vector clocks”</a>).</p>&#13;
&#13;
<p><em>Eventually</em> is an interesting term to describe value propagation, since it specifies no hard time bound in which it has to happen. If the delivery service provides nothing more than an “eventually” guarantee, it doesn’t sound like it can be relied upon. <span class="keep-together">However,</span> in practice, this works well, and many databases these days are described as <em>eventually consistent</em>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tunable Consistency" data-type="sect1"><div class="sect1" id="tunable_consistency">&#13;
<h1>Tunable Consistency</h1>&#13;
&#13;
<p>Eventually<a data-primary="replication and consistency" data-secondary="tunable consistency" data-type="indexterm" id="idm46466885974408"/><a data-primary="tunable consistency" data-type="indexterm" id="idm46466885973560"/><a data-primary="consistency" data-secondary="tunable consistency" data-type="indexterm" id="idm46466885971128"/> consistent systems are sometimes described in CAP terms: you can trade availability for consistency or vice versa (see <a data-type="xref" href="#infamous_cap">“Infamous CAP”</a>). From the server-side perspective, eventually consistent systems usually implement tunable consistency, where data is replicated, read, and written using three variables:</p>&#13;
<dl>&#13;
<dt>Replication Factor <code>N</code></dt>&#13;
<dd>&#13;
<p>Number of nodes that will store a copy of data.</p>&#13;
</dd>&#13;
<dt>Write Consistency <code>W</code></dt>&#13;
<dd>&#13;
<p>Number of nodes that have to acknowledge a write for it to succeed.</p>&#13;
</dd>&#13;
<dt>Read Consistency <code>R</code></dt>&#13;
<dd>&#13;
<p>Number of nodes that have to respond to a read operation for it to succeed.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Choosing consistency levels where (<code>R + W &gt; N</code>), the system can guarantee returning the most recent written value, because there’s always an overlap between read and write sets. For example, if <code>N = 3</code>, <code>W = 2</code>, and <code>R = 2</code>, the system can tolerate a failure of just one node. Two nodes out of three must acknowledge the write. In the ideal scenario, the system also asynchronously replicates the write to the third node. If the third node is down, anti-entropy mechanisms (see <a data-type="xref" href="ch12.html#anti_entropy">Chapter 12</a>) eventually propagate it.</p>&#13;
&#13;
<p>During the read, two replicas out of three have to be available to serve the request for us to respond with consistent results. Any combination of nodes will give us at least one node that will have the most up-to-date record for a given key.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>When performing a write, the coordinator should submit it to <code>N</code> nodes, but can wait for only <code>W</code> nodes before it proceeds (or <code>W - 1</code> in case the coordinator is also a replica). The rest of the write operations can complete asynchronously or fail. Similarly, when performing a read, the coordinator has to collect <em>at least</em> <code>R</code> responses. Some databases use speculative execution and submit extra read requests to reduce coordinator response latency. This means if one of the originally submitted read requests fails or arrives slowly, speculative requests can be counted toward <code>R</code> instead.</p>&#13;
</div>&#13;
&#13;
<p>Write-heavy systems may sometimes pick <code>W = 1</code> and <code>R = N</code>, which allows writes to be acknowledged by just one node before they succeed, but would require <em>all</em> the replicas (even potentially failed ones) to be available for reads. The same is true for the <code>W = N</code>, <code>R = 1</code> combination: the latest value can be read from any node, as long as writes succeed only after being applied on <em>all</em> replicas.</p>&#13;
&#13;
<p>Increasing read or write consistency levels increases latencies and raises requirements for node availability during requests. Decreasing them improves system availability while sacrificing consistency.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="quorums">&#13;
<h5>Quorums</h5>&#13;
<p>A consistency level that consists of <code>⌊N/2⌋ + 1</code> nodes is called<a data-primary="quorums" data-type="indexterm" id="idm46466885948344"/><a data-primary="replication and consistency" data-secondary="quorums" data-type="indexterm" id="idm46466885947736"/> a <em>quorum</em>, a majority of nodes. In the case of a network partition or node failures, in a system with <code>2f + 1</code> nodes, live nodes can continue accepting writes or reads, if up to <code>f</code> nodes are unavailable, until the rest of the cluster is available again. In other words, such systems can tolerate at most <code>f</code> node failures.</p>&#13;
&#13;
<p>When executing read and write operations using quorums, a system cannot tolerate failures of the majority of nodes. For example, if there are three replicas in total, and two of them are down, read and write operations won’t be able to achieve the number of nodes necessary for read and write consistency, since only one node out of three will be able to respond to the request.</p>&#13;
&#13;
<p>Reading and writing using quorums does not guarantee monotonicity in cases of incomplete writes. If some write operation has failed after writing a value to one replica out of three, depending on the contacted replicas, a quorum read can return either the result of the incomplete operation, or the old value. Since subsequent same-value reads are not required to contact the same replicas, values they return can alternate. To achieve read monotonicity (at the cost of availability), we have to use blocking read-repair (see <a data-type="xref" href="ch12.html#read_repair">“Read Repair”</a>).</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Witness Replicas" data-type="sect1"><div class="sect1" id="idm46466885972440">&#13;
<h1>Witness Replicas</h1>&#13;
&#13;
<p>Using quorums<a data-primary="replication and consistency" data-secondary="witness replicas" data-type="indexterm" id="idm46466885934760"/><a data-primary="witness replicas" data-type="indexterm" id="idm46466885933912"/> for read consistency helps to improve availability: even if some of the nodes are down, a database system can still accept reads and serve writes. The majority requirement guarantees that, since there’s an overlap of at least one node in any majority, any quorum read will observe the most recent completed quorum write. However, using replication and majorities increases storage costs: we have to store a copy of the data on each replica. If our replication factor is five, we have to store five copies.</p>&#13;
&#13;
<p>We can improve storage costs by using a concept called <em>witness replicas</em>. Instead of storing a copy of the record on each replica, we can split replicas into <em>copy</em> and <em>witness</em> subsets. Copy replicas still hold data records as previously. Under normal operation, witness replicas merely store the record indicating the fact that the write operation occurred. However, a situation might occur when the number of copy <span class="keep-together">replicas</span> is too low. For example, if we have three copy replicas and two witness ones, and two copy replicas go down, we end up with a quorum of one copy and two witness replicas.</p>&#13;
&#13;
<p>In cases of write timeouts or copy replica failures, witness replicas can be <em>upgraded</em> to temporarily store the record in place of failed or timed-out copy replicas. As soon as the original copy replicas recover, upgraded replicas can revert to their previous state, or recovered replicas can become witnesses.</p>&#13;
&#13;
<p>Let’s consider a replicated system with three nodes, two of which are holding copies of data and the third serves as a witness: <code>[1c, 2c, 3w]</code>. We attempt to make a write, but <code>2c</code> is temporarily unavailable and cannot complete the operation. In this case, we temporarily store the record on the witness replica <code>3w</code>. Whenever <code>2c</code> comes back up, repair mechanisms can bring it back up-to-date and remove redundant copies from witnesses.</p>&#13;
&#13;
<p>In a different scenario, we can attempt to perform a read, and the record is present on <code>1c</code> and <code>3w</code>, but not on <code>2c</code>. Since any two replicas are enough to constitute a quorum, if any subset of nodes of size two is available, whether it’s two copy replicas <code>[1c, 2c]</code>, or one copy replica and one witness <code>[1c, 3w]</code> or <code>[2c, 3w]</code>, we can guarantee to serve consistent results. If we read from <code>[1c, 2c]</code>, we fetch the latest record from <code>1c</code> and can replicate it to <code>2c</code>, since the value is missing there. In case only <code>[2c, 3w]</code> are available, the latest record can be fetched from <code>3w</code>. To restore the original configuration and bring <code>2c</code> up-to-date, the record can be replicated to it, and removed from the <span class="keep-together">witness.</span></p>&#13;
&#13;
<p>More generally, having <code>n</code> copy and <code>m</code> witness replicas has same availability guarantees as <code>n + m</code> copies, given that we follow two rules:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Read and write operations are performed using majorities (i.e., with <code>N/2 + 1</code> <span class="keep-together">participants</span>)</p>&#13;
</li>&#13;
<li>&#13;
<p>At least one of the replicas in this quorum is <em>necessarily</em> a copy one</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This works because data is guaranteed to be either on the copy or witness replicas. Copy replicas are brought up-to-date by the repair mechanism in case of a failure, and witness replicas store the data in the interim.</p>&#13;
&#13;
<p>Using witness replicas helps to reduce storage costs while preserving consistency invariants. There are several implementations of this approach; for example, Spanner <a data-type="xref" href="app01.html#CORBETT12">[CORBETT12]</a> and <a href="https://databass.dev/links/105">Apache Cassandra</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Strong Eventual Consistency and CRDTs" data-type="sect1"><div class="sect1" id="idm46466885935784">&#13;
<h1>Strong Eventual Consistency and CRDTs</h1>&#13;
&#13;
<p>We’ve<a data-primary="replication and consistency" data-secondary="strong eventual consistency" data-type="indexterm" id="idm46466885913688"/><a data-primary="strong eventual consistency" data-type="indexterm" id="idm46466885912840"/> discussed several strong consistency models, such as linearizability and serializability, and a form of weak consistency: eventual consistency. A possible middle ground between the two, offering some benefits of both models, is <em>strong eventual consistency</em>. Under this model, updates are allowed to propagate to servers late or out of order, but when all updates finally propagate to target nodes, conflicts between them can be resolved and they can be merged to produce the same valid state <a data-type="xref" href="app01.html#GOMES17">[GOMES17]</a>.</p>&#13;
&#13;
<p>Under<a data-primary="replication and consistency" data-secondary="Conflict-Free Replicated Data Types (CRDTs)" data-type="indexterm" id="idm46466885912104"/><a data-primary="Conflict-Free Replicated Data Types (CRDTs)" data-type="indexterm" id="idm46466885911128"/> some conditions, we can relax our consistency requirements by allowing operations to preserve additional state that allows the diverged states to be reconciled (in other words, merged) after execution. One of the most prominent examples of such an approach is <em>Conflict-Free Replicated Data Types</em> (CRDTs, <a data-type="xref" href="app01.html#SHAPIRO11a">[SHAPIRO11a]</a>) implemented, for example, in Redis <a data-type="xref" href="app01.html#BIYIKOGLU13">[BIYIKOGLU13]</a>.</p>&#13;
&#13;
<p>CRDTs are specialized data structures that preclude the existence of conflict and allow operations on these data types to be applied in any order without changing the result. This property can be extremely useful in a distributed system. For example, in a multinode system that uses conflict-free replicated counters, we can increment counter values on each node independently, even if they cannot communicate with one another due to a network partition. As soon as communication is restored, results from all nodes can be reconciled, and none of the operations applied during the partition will be lost.</p>&#13;
&#13;
<p>This makes CRDTs useful in eventually consistent systems, since replica states in such systems are allowed to temporarily diverge. Replicas can execute operations locally, without prior synchronization with other nodes, and operations eventually propagate to all other replicas, potentially out of order. CRDTs allow us to reconstruct the complete system state from local individual states or operation sequences.</p>&#13;
&#13;
<p>The simplest example of CRDTs is operation-based Commutative Replicated Data Types (CmRDTs). For CmRDTs to work, we need the allowed operations to be:</p>&#13;
<dl>&#13;
<dt>Side-effect free</dt>&#13;
<dd>&#13;
<p>Their application does not change the system state.</p>&#13;
</dd>&#13;
<dt>Commutative</dt>&#13;
<dd>&#13;
<p>Argument order does not matter: <code>x • y = y • x</code>. In other words, it doesn’t matter whether <code>x</code> is merged with <code>y</code>, or <code>y</code> is merged with <code>x</code>.</p>&#13;
</dd>&#13;
<dt>Causally ordered</dt>&#13;
<dd>&#13;
<p>Their successful delivery depends on the precondition, which ensures that the system has reached the state the operation can be applied to.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>For example, we could implement a <em>grow-only counter</em>. Each server can hold a state vector consisting of last known counter updates from all other participants, initialized with zeros. Each server is only allowed to modify its own value in the vector. When updates are propagated, the function <code>merge(state1, state2)</code> merges the states from the two servers.</p>&#13;
&#13;
<p>For example, we have three servers, with initial state vectors initialized:</p>&#13;
&#13;
<pre data-type="programlisting">Node 1:          Node 2:          Node 3:&#13;
[0, 0, 0]        [0, 0, 0]        [0, 0, 0]</pre>&#13;
&#13;
<p>If we update counters on the first and third nodes, their states change as follows:</p>&#13;
&#13;
<pre data-type="programlisting">Node 1:          Node 2:          Node 3:&#13;
[1, 0, 0]        [0, 0, 0]        [0, 0, 1]</pre>&#13;
&#13;
<p>When updates propagate, we use a merge function to combine the results by picking the maximum value for each slot:</p>&#13;
&#13;
<pre data-type="programlisting">Node 1 (Node 3 state vector propagated):&#13;
merge([1, 0, 0], [0, 0, 1]) = [1, 0, 1]&#13;
&#13;
Node 2 (Node 1 state vector propagated):&#13;
merge([0, 0, 0], [1, 0, 0]) = [1, 0, 0]&#13;
&#13;
Node 2 (Node 3 state vector propagated):&#13;
merge([1, 0, 0], [0, 0, 1]) = [1, 0, 1]&#13;
&#13;
Node 3 (Node 1 state vector propagated):&#13;
merge([0, 0, 1], [1, 0, 0]) = [1, 0, 1]</pre>&#13;
&#13;
<p>To determine the current vector state, the sum of values in all slots is computed: <code>sum([1, 0, 1]) = 2</code>. The merge function is commutative. Since servers are only allowed to update their own values and these values are independent, no additional coordination is required.</p>&#13;
&#13;
<p>It<a data-primary="Positive-Negative-Counter (PN-Counter)" data-type="indexterm" id="idm46466885890456"/> is possible to produce a <em>Positive-Negative-Counter</em> (PN-Counter) that supports both increments and decrements by using payloads consisting of two vectors: <code>P</code>, which nodes use for increments, and <code>N</code>, where they store decrements. In a larger system, to avoid propagating huge vectors, we<a data-primary="super-peers" data-type="indexterm" id="idm46466885884568"/> can use <em>super-peers</em>. Super-peers replicate counter states and help to avoid constant peer-to-peer chatter <a data-type="xref" href="app01.html#SHAPIRO11b">[SHAPIRO11b]</a>.</p>&#13;
&#13;
<p>To save and replicate values, we can use<a data-primary="registers" data-type="indexterm" id="idm46466885882808"/><a data-primary="last-write-wins register (LWW register)" data-type="indexterm" id="idm46466885882200"/> <em>registers</em>. The simplest version of the register is the <em>last-write-wins</em> register (LWW register), which stores a unique, globally ordered timestamp attached to each value to resolve conflicts. In case of a conflicting write, we preserve only the one with the larger timestamp. The merge operation (picking the value with the largest timestamp) here is also commutative, since it relies on the timestamp. If we cannot allow values to be discarded, we can supply application-specific merge logic and use a <em>multivalue</em> register, which stores all values that were written and allows the application to pick the right one.</p>&#13;
&#13;
<p>Another<a data-primary="grow-only sets (G-Sets)" data-type="indexterm" id="idm46466885880408"/> example of CRDTs is an unordered <em>grow-only</em> set (G-Set). Each node maintains its local state and can append elements to it. Adding elements produces a valid set. Merging two sets is also a commutative operation. Similar to counters, we can use two sets to support both additions and removals. In this case, we have to preserve an invariant: only the values contained in the addition set can be added into the removal set. To reconstruct the current state of the set, all elements contained in the removal set are subtracted from the addition set <a data-type="xref" href="app01.html#SHAPIRO11b">[SHAPIRO11b]</a>.</p>&#13;
&#13;
<p>An example of a conflict-free type that combines more complex structures is a conflict-free replicated JSON data type, allowing modifications such as insertions, deletions, and assignments on deeply nested JSON documents with list and map types. This algorithm performs merge operations on the client side and does not require operations to be propagated in any specific order <a data-type="xref" href="app01.html#KLEPPMANN14">[KLEPPMANN14]</a>.</p>&#13;
&#13;
<p>There are quite a few possibilities CRDTs provide us with, and we can see more data stores using this concept to provide Strong Eventual Consistency (SEC). This is a powerful concept that we can add to our arsenal of tools for building fault-tolerant distributed systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm46466885915560">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Fault-tolerant systems<a data-primary="replication and consistency" data-secondary="overview of" data-type="indexterm" id="idm46466885872952"/> use replication to improve availability: even if some processes fail or are unresponsive, the system as a whole can continue functioning correctly. However, keeping multiple copies in sync requires additional coordination.</p>&#13;
&#13;
<p>We’ve discussed several single-operation consistency models, ordered from the one with the most guarantees to the one with the least:<sup><a data-type="noteref" href="ch11.html#idm46466885870904" id="idm46466885870904-marker">2</a></sup></p>&#13;
<dl>&#13;
<dt>Linearizability</dt>&#13;
<dd>&#13;
<p>Operations appear to be applied instantaneously, and the real-time operation order is maintained.</p>&#13;
</dd>&#13;
<dt>Sequential consistency</dt>&#13;
<dd>&#13;
<p>Operation effects are propagated in <em>some</em> total order, and this order is consistent with the order they were executed by the individual processes.</p>&#13;
</dd>&#13;
<dt>Causal consistency</dt>&#13;
<dd>&#13;
<p>Effects of the causally related operations are visible in the same order to all <span class="keep-together">processes.</span></p>&#13;
</dd>&#13;
<dt>PRAM/FIFO consistency</dt>&#13;
<dd>&#13;
<p>Operation effects become visible in the same order they were executed by individual processes. Writes from different processes can be observed in different orders.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>After that, we discussed multiple session models:</p>&#13;
<dl>&#13;
<dt>Read-own-writes</dt>&#13;
<dd>&#13;
<p>Read operations reflect the previous writes. Writes propagate through the system and become available for later reads that come from the same client.</p>&#13;
</dd>&#13;
<dt>Monotonic reads</dt>&#13;
<dd>&#13;
<p>Any read that has observed a value cannot observe a value that is older that the observed one.</p>&#13;
</dd>&#13;
<dt>Monotonic writes</dt>&#13;
<dd>&#13;
<p>Writes coming from the same client propagate to other clients in the order they were made by this client.</p>&#13;
</dd>&#13;
<dt>Writes-follow-reads</dt>&#13;
<dd>&#13;
<p>Write operations are ordered after the writes whose effects were observed by the previous reads executed by the same client.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Knowing and understanding these concepts can help you to understand the guarantees of  the underlying systems and use them for application development. Consistency models describe rules that operations on data have to follow, but their scope is limited to a specific system. Stacking systems with weaker guarantees on top of ones with stronger guarantees or ignoring consistency implications of underlying systems may lead to unrecoverable inconsistencies and data loss.</p>&#13;
&#13;
<p>We also discussed the concept of <em>eventual</em> and <em>tunable</em> consistency. Quorum-based systems use majorities to serve consistent data. <em>Witness replicas</em> can be used to reduce storage costs.</p>&#13;
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46466885854168">&#13;
<h5>Further Reading</h5>&#13;
<p>If you’d like to learn more about the concepts mentioned in this chapter, you can refer to the following sources:</p>&#13;
<dl>&#13;
<dt>Consistency models</dt>&#13;
<dd>&#13;
<p>Perrin, Matthieu. 2017. <em>Distributed Systems: Concurrency and Consistency</em> (1st Ed.). Elsevier, UK: ISTE Press.</p>&#13;
&#13;
<p>Viotti, Paolo and Marko Vukolić. 2016. “Consistency in Non-Transactional Distributed Storage Systems.” <em>ACM Computing Surveys</em> 49, no. 1 (July): Article 19. <em><a href="https://doi.org/0.1145/2926965"><em class="hyperlink">https://doi.org/0.1145/2926965</em></a></em>.</p>&#13;
&#13;
<p>Bailis, Peter, Aaron Davidson, Alan Fekete, Ali Ghodsi, Joseph M. Hellerstein, and Ion Stoica. 2013. “Highly available transactions: virtues and limitations.” <em>Proceedings of the VLDB Endowment</em> 7, no. 3 (November): 181-192. <em><a href="https://doi.org/10.14778/2732232.2732237"><em class="hyperlink">https://doi.org/10.14778/2732232.2732237</em></a></em>.</p>&#13;
&#13;
<p>Aguilera, M.K., and D.B. Terry. 2016. “The Many Faces of Consistency.” <em>Bulletin of the Technical Committee on Data Engineering</em> 39, no. 1 (March): 3-13.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm46466886319240"><sup><a href="ch11.html#idm46466886319240-marker">1</a></sup> Quorum reads and writes in the context of eventually consistent stores, which are discussed in more detail in <a data-type="xref" href="#eventual_consistency">“Eventual Consistency”</a>.</p><p data-type="footnote" id="idm46466885870904"><sup><a href="ch11.html#idm46466885870904-marker">2</a></sup> These short definitions are given for recap only, the reader is advised to refer to the complete definitions for context.</p></div></div></section></body></html>