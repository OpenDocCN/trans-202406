- en: Chapter 9\. Managing Pods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are no big problems, there are just a lot of little problems.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Henry Ford
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the previous chapter, we covered containers in some detail and explained
    how in Kubernetes, containers are composed together to form Pods. There are a
    few other interesting aspects of Pods, which we’ll turn to in this chapter, including
    labels, guiding Pod scheduling using node affinities, barring Pods from running
    on certain nodes with taints and tolerations, keeping Pods together or apart using
    Pod affinities, and orchestrating applications using Pod controllers such as DaemonSets
    and StatefulSets. We’ll also cover some advanced networking features including
    Ingress controllers and service mesh tools.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Labels
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You know that Pods (and other Kubernetes resources) can have labels attached
    to them, and that these play an important role in connecting related resources
    (for example, sending requests from a Service to the appropriate backends). Let’s
    take a closer look at labels and selectors in this section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: What Are Labels?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Labels are key/value pairs that are attached to objects, such as pods. Labels
    are intended to be used to specify identifying attributes of objects that are
    meaningful and relevant to users, but do not directly imply semantics to the core
    system.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Kubernetes [documentation](https://oreil.ly/C4j1y)
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In other words, labels exist to tag resources with information that’s meaningful
    to us, but they don’t mean anything to Kubernetes. For example, it’s common to
    label Pods with the application they belong to:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, by itself, this label has no effect. It’s still useful as documentation:
    someone can look at this Pod and see what application it’s running. But the real
    power of a label comes when we use it with a *selector*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Selectors
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A selector is an expression that matches a label (or set of labels). It’s a
    way of specifying a group of resources by their labels. For example, a Service
    resource has a selector that identifies the Pods it will send requests to. Remember
    our demo Service from [“Service Resources”](ch04.html#services)?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is a very simple selector that matches any resource that has the `app`
    label with a value of `demo`. If a resource doesn’t have the `app` label at all,
    it won’t match this selector. If it has the `app` label, but its value is not
    `demo`, it won’t match the selector either. Only suitable resources (in this case,
    Pods) with the label `app: demo` will match, and all such resources will be selected
    by this Service.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'Labels aren’t just used for connecting Services and Pods; you can use them
    directly when querying the cluster with `kubectl get`, using the `--selector`
    flag:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You may recall from [“Using Short Flags”](ch07.html#shortflags) that `--selector`
    can be abbreviated to just `-l` (for ***l**abels*).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to see what labels are defined on your Pods, use the `--show-labels`
    flag to `kubectl get`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: More Advanced Selectors
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the time, a simple selector like `app: demo` (known as an *equality
    selector*) will be all you need. You can combine different labels to make more
    specific selectors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will return only Pods that have *both* `app: demo` and `environment: production`
    labels. The YAML equivalent of this (in a Service, for example) would be:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Equality selectors like this are the only kind available with a Service, but
    for interactive queries with `kubectl`, or more sophisticated resources such as
    Deployments, there are other options.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'One is selecting for label *inequality*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will return all Pods that have an `app` label with a different value to
    `demo`, or that don’t have an `app` label at all.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also ask for label values that are in a *set*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The YAML equivalent would be:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also ask for label values not in a given set:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The YAML equivalent of this would be:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see another example of using `matchExpressions` in [“Using node affinities
    to control scheduling”](ch05.html#nodeaffinities-intro).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Other Uses for Labels
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve seen how to link Pods to Services using an `app` label (actually, you
    can use any label, but `app` is common). But what other uses are there for labels?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'In our Helm chart for the demo application (see [“What’s Inside a Helm Chart?”](ch12.html#helmcharts)),
    we set an `environment` label, which can be, for example, `staging` or `production`.
    If you’re running staging and production Pods in the same cluster (see [“Do I
    need multiple clusters?”](ch06.html#multiclusters)), you might want to use a label
    like this to distinguish between the two environments. For example, your Service
    selector for production might be:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Without the extra `environment` selector, the Service would match any and all
    Pods with `app: demo`, including the staging ones, which you probably don’t want.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on your applications, you might want to use labels to slice and dice
    your resources in a number of different ways. Here are some examples:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This allows you to query the cluster along these various different dimensions
    to see what’s going on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'You could also use labels as a way of doing canary deployments (see [“Canary
    Deployments”](ch13.html#canary)). If you want to roll out a new version of the
    application to just a small percentage of Pods, you could use labels like `track:
    stable` and `track: canary` for two separate Deployments.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: If your Service’s selector matches only the `app` label, it will send traffic
    to all Pods matching that selector, including both `stable` and `canary`. You
    can alter the number of replicas for both Deployments to gradually increase the
    proportion of `canary` Pods. Once all running Pods are on the canary track, you
    can relabel them as `stable` and begin the process again with the next version.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Labels and Annotations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might be wondering what the difference is between labels and annotations.
    They’re both sets of key-value pairs that provide metadata about resources.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The difference is that *labels identify resources*. They’re used to select groups
    of related resources, like in a Service’s selector. Annotations, on the other
    hand, are for non-identifying information, to be used by tools or services outside
    Kubernetes. For example, in [“Helm Hooks”](ch13.html#helmhooks) there’s an example
    of using annotations to control Helm workflows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Because labels are often used in internal queries that are performance-critical
    to Kubernetes, there are some fairly tight restrictions on valid labels. For example,
    label names are limited to 63 characters, though they may have an optional 253-character
    prefix in the form of a DNS subdomain, separated from the label by a slash character.
    Labels can only begin with an alphanumeric character (a letter or a digit), and
    can only contain alphanumeric characters plus dashes, underscores, and dots. Label
    values are [similarly restricted](https://oreil.ly/pR82Y).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we doubt you’ll run out of characters for your labels, since most
    labels in common use are just a single word (for example, `app`).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Node Affinities
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We mentioned node affinities briefly in [“Using node affinities to control scheduling”](ch05.html#nodeaffinities-intro),
    in relation to preemptible nodes. In that section, you learned how to use node
    affinities to preferentially schedule Pods on certain nodes (or not). Let’s take
    a more detailed look at node affinities now.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: In most cases, you don’t need node affinities. Kubernetes is pretty smart about
    scheduling Pods onto the right nodes. If all your nodes are equally suitable to
    run a given Pod, then don’t worry about it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: There are exceptions, however (like preemptible nodes in the previous example).
    If a Pod is expensive to restart, you probably want to avoid scheduling it on
    a preemptible node wherever possible; preemptible nodes can disappear from the
    cluster without warning. You can express this kind of preference using node affinities.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of affinity: hard and soft, and in Kubernetes these are
    called:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '`requiredDuringSchedulingIgnoredDuringExecution` (hard)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preferredDuringSchedulingIgnoredDuringExecution` (soft)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may help you to remember that `required` means a hard affinity (the rule
    *must* be satisfied to schedule this Pod) and `preferred` means a soft affinity
    (it would be *nice* if the rule were satisfied, but it’s not critical).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The long names of the hard and soft affinity types make the point that these
    rules apply *during scheduling*, but not *during execution*. That is, once the
    Pod has been scheduled to a particular node satisfying the affinity, it will stay
    there. If things change while the Pod is running so that the rule is no longer
    satisfied, Kubernetes won’t move the Pod.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Hard Affinities
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An affinity is expressed by describing the kind of nodes that you want the
    Pod to run on. There might be several rules about how you want Kubernetes to select
    nodes for the Pod. Each one is expressed using the `nodeSelectorTerms` field.
    Here’s an example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Only nodes that are in the `us-central1-a` zone will match this rule, so the
    overall effect is to ensure that this Pod is only scheduled in that zone.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Soft Affinities
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Soft affinities are expressed in much the same way, except that each rule is
    assigned a numerical *weight* from 1 to 100 that determines the effect it has
    on the result. Here’s an example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Because this is a `preferred...` rule, it’s a soft affinity: Kubernetes can
    schedule the Pod on any node, but it will give priority to nodes that match these
    rules.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the two rules have different `weight` values. The first rule
    has weight 10, but the second has weight 100\. If there are nodes that match both
    rules, Kubernetes will give 10 times the priority to nodes that match the second
    rule (being in availability zone `us-central1-b`).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Weights are a useful way of expressing the relative importance of your preferences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Pod Affinities and Anti-Affinities
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve seen how you can use node affinities to nudge the scheduler toward or
    away from running a Pod on certain kinds of nodes. But is it possible to influence
    scheduling decisions based on what other Pods are already running on a node?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes there are pairs of Pods that work better when they’re together on
    the same node; for example, a web server and a content cache, such as Redis. It
    would be useful if you could add information to the Pod spec that tells the scheduler
    it would prefer to be colocated with a Pod matching a particular set of labels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, sometimes you want Pods to avoid each other. In [“Keeping Your Workloads
    Balanced”](ch05.html#balanced), we saw the kind of problems that can arise if
    Pod replicas end up together on the same node, instead of distributed across the
    cluster. Can you tell the scheduler to avoid scheduling a Pod where another replica
    of that Pod is already running?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s exactly what you can do with Pod affinities. Like node affinities, Pod
    affinities are expressed as a set of rules: either hard requirements, or soft
    preferences with a set of weights.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Keeping Pods Together
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take the first case first: scheduling Pods together. Suppose you have
    one Pod, labeled `app: server`, which is your web server, and another, labeled
    `app: cache`, which is your content cache. They can still work together even if
    they’re on separate nodes, but it’s better if they’re on the same node because
    they can communicate without having to go over the network. How do you ask the
    scheduler to colocate them?'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of the required Pod affinity, expressed as part of the `server`
    Pod spec. The effect would be just the same if you added it to the `cache` spec,
    or to both Pods:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The overall effect of this affinity is to ensure that the `server` Pod is scheduled,
    if possible, on a node that is also running a Pod labeled `cache`. If there is
    no such node, or if there is no matching node that has sufficient spare resources
    to run the Pod, it will not be able to run.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: This probably isn’t the behavior you want in a real-life situation. If the two
    Pods absolutely must be colocated, put their containers in the same Pod. If it’s
    just preferable for them to be colocated, use a soft Pod affinity (`preferredDuringSchedulingIgnoredDuringExecution`).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Keeping Pods Apart
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s take the anti-affinity case: keeping certain Pods apart. Instead
    of `podAffinity`, we use `podAntiAffinity`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It’s very similar to the previous example, except that it’s a `podAntiAffinity`,
    so it expresses the opposite sense, and the match expression is different. This
    time, the expression is: “The `app` label must have the value `server`.”'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'The effect of this affinity is to ensure that the Pod will *not* be scheduled
    on any node matching this rule. In other words, no Pod labeled `app: server` can
    be scheduled on a node that already has an `app: server` Pod running. This will
    enforce an even distribution of `server` Pods across the cluster, at the possible
    expense of the desired number of replicas.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Soft Anti-Affinities
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'However, we usually care more about having enough replicas available than distributing
    them as fairly as possible. A hard rule is not really what we want here. Let’s
    modify it slightly to make it a soft anti-affinity:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that now the rule is `preferred...`, not `required...`, making it a soft
    anti-affinity. If the rule can be satisfied, it will be, but if not, Kubernetes
    will schedule the Pod anyway.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Because it’s a preference, we specify a `weight` value, just as we did for soft
    node affinities. If there were multiple affinity rules to consider, Kubernetes
    would prioritize them according to the weight you assign each rule.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: When to Use Pod Affinities
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just as with node affinities, you should treat Pod affinities as a fine-tuning
    enhancement for special cases. The scheduler is already good at placing Pods to
    get the best performance and availability from the cluster. Pod affinities restrict
    the scheduler’s freedom, trading one application for another.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Taints and Tolerations
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [“Node Affinities”](#nodeaffinities), you learned about a property of Pods
    that can steer them toward (or away from) a set of nodes. Conversely, *taints*
    allow a node to repel a set of Pods, based on certain properties of the node.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you could use taints to set aside particular nodes: nodes that
    are reserved only for specific kinds of Pods. Kubernetes also creates taints for
    you if certain problems exist on the node, such as low memory, or a lack of network
    connectivity.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a taint to a particular node, use the `kubectl taint` command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This adds a taint called `dedicated=true` to the `docker-for-desktop` node,
    with the effect `NoSchedule`: no Pod can now be scheduled there unless it has
    a matching *toleration*.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: To see the taints configured on a particular node, use `kubectl describe node...`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'To remove a taint from a node, repeat the `kubectl taint` command but with
    a trailing minus sign after the name of the taint:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Tolerations are properties of Pods that describe the taints that they’re compatible
    with. For example, to make a Pod tolerate the `dedicated=true` taint, add this
    to the Pod’s spec:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This is effectively saying, “This Pod is allowed to run on nodes that have the
    `dedicated=true` taint with the effect `NoSchedule`.” Because the toleration *matches*
    the taint, the Pod can be scheduled. Any Pod without this toleration will not
    be allowed to run on the tainted node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'When a Pod can’t run at all because of tainted nodes, it will stay in `Pending`
    status, and you’ll see a message like this in the Pod description:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Other uses for taints and tolerations include marking nodes with specialized
    hardware (such as GPUs), and allowing certain Pods to tolerate certain kinds of
    node problems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: For example, if a node falls off the network, Kubernetes automatically adds
    the taint `node.kubernetes.io/unreachable`. Normally, this would result in its
    `kubelet` evicting all Pods from the node. However, you might want to keep certain
    Pods running, in the hope that the network will come back in a reasonable time.
    To do this, you could add a toleration to those Pods that matches the `unreachable`
    taint.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about taints and tolerations in the Kubernetes [documentation](https://oreil.ly/qbqpz).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Pod Controllers
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve talked a lot about Pods in this chapter, and that makes sense: all Kubernetes
    applications run in a Pod. You might wonder, though, why we need other kinds of
    objects at all. Isn’t it enough just to create a Pod for an application and run
    it?'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s effectively what you get by running a container directly with `docker
    container run`, as we did in [“Running a Container Image”](ch02.html#runningcontainer).
    It works, but it’s very limited:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: If the container exits for some reason, you have to manually restart it.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s only one replica of your container and no way to load-balance traffic
    across multiple replicas if you run them manually.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want highly available replicas, you have to decide which nodes to run
    them on, and take care of keeping the cluster balanced.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you update the container, you have to take care of stopping each running
    image in turn, pulling the new image and restarting it.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s the kind of work that Kubernetes is designed to take off your hands using
    *controllers*. In [“ReplicaSets”](ch04.html#replicaset-intro), we introduced the
    ReplicaSet controller, which manages a group of replicas of a particular Pod.
    It works continuously to make sure there are always the specified number of replicas,
    starting new ones if there aren’t enough, and killing off replicas if there are
    too many.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: You’re also now familiar with Deployments, which as we saw in [“Deployments”](ch04.html#deployments-intro),
    manage ReplicaSets to control the rollout of application updates. When you update
    a Deployment—for example, with a new container spec—it creates a new ReplicaSet
    to start up the new Pods, and eventually closes down the ReplicaSet that was managing
    the old Pods.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: For most simple applications, a Deployment is all you need. But there are a
    few other useful kinds of Pod controllers, and we’ll look briefly at a few of
    them in this section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you want to send logs from all your applications to a centralized log
    server, like an Elasticsearch-Logstash-Kibana (ELK) stack, or a SaaS monitoring
    product such as Datadog (see [“Datadog”](ch16.html#datadog)). There are a few
    ways to do that.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: You could have each application include code to connect to the logging service,
    authenticate, write logs, and so on, but this results in a lot of duplicated code,
    which is inefficient.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you could run an extra container in every Pod that acts as a
    logging agent (this is called a *sidecar* pattern). This means that the application
    doesn’t need built-in knowledge of how to talk to the logging service, but it
    does mean you would potentially have several copies of the same logging agent
    running on a node.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Since all it does is manage a connection to the logging service and pass on
    log messages to it, you really only need one copy of the logging agent on each
    node. This is such a common requirement that Kubernetes provides a special controller
    object for it: the *DaemonSet*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term *daemon* traditionally refers to long-running background processes
    on a server that handle things like logging, so by analogy, Kubernetes DaemonSets
    run a *daemon* container on each node in the cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'The manifest for a DaemonSet, as you might expect, looks very much like that
    for a Deployment:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Use a DaemonSet when you need to run one copy of a Pod on each of the nodes
    in your cluster. If you’re running an application where maintaining a given number
    of replicas is more important than exactly which node the Pods run on, use a Deployment
    instead.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like a Deployment or DaemonSet, a StatefulSet is a kind of Pod controller. What
    a StatefulSet adds is the ability to start and stop Pods in a specific sequence.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: With a Deployment, for example, all your Pods are started and stopped in a random
    order. This is fine for stateless services, where every replica is identical and
    does the same job.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, though, you need to start Pods in a specific numbered sequence, and
    be able to identify them by their number. For example, distributed applications
    such as Redis, MongoDB, or Cassandra create their own clusters, and need to be
    able to identify the cluster leader by a predictable name.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: A StatefulSet is ideal for this. For example, if you create a StatefulSet named
    `redis`, the first Pod started will be named `redis-0`, and Kubernetes will wait
    until that Pod is ready before starting the next one, `redis-1`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the application, you can use this property to cluster the Pods
    in a reliable way. For example, each Pod can run a startup script that checks
    if it is running on `redis-0`. If it is, it will be the redis cluster leader.
    If not, it will attempt to join the cluster as a replica by contacting `redis-0`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Each replica in a StatefulSet must be running and ready before Kubernetes starts
    the next one, and similarly when the StatefulSet is terminated, the replicas will
    be shut down in reverse order, waiting for each Pod to finish before moving on
    to the next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these special properties, a StatefulSet looks very similar to a
    normal Deployment:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To be able to address each of the Pods by a predictable DNS name, such as `redis-1`,
    you also need to create a Service with a `clusterIP` type of `None` (known as
    a *headless service*).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: With a nonheadless Service, you get a single DNS entry (such as `redis`) that
    load-balances across all the backend Pods. With a headless service, you still
    get that single service DNS name, but you also get individual DNS entries for
    each numbered Pod, like `redis-0`, `redis-1`, `redis-2`, and so on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Pods that need to join the Redis cluster can contact `redis-0` specifically,
    but applications that simply need a load-balanced Redis service can use the `redis`
    DNS name to talk to a randomly selected Redis Pod.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets can also manage disk storage for their Pods, using a VolumeClaimTemplate
    object that automatically creates a PersistentVolumeClaim (see [“Persistent Volumes”](ch08.html#persistent)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another useful type of Pod controller in Kubernetes is the Job. Whereas a Deployment
    runs a specified number of Pods and restarts them continually, a Job only runs
    a Pod for a specified number of times. After that, it is considered completed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: For example, a batch-processing task or queue-worker Pod usually starts up,
    does its work, and then exits. This is an ideal candidate to be managed by a Job.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two fields that control Job execution: `completions` and `parallelism`.
    The first, `completions`, determines the number of times the specified Pod needs
    to run successfully before the Job is considered complete. The default value is
    1, meaning the Pod will run once.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: The `parallelism` field specifies how many Pods should run at once. Again, the
    default value is 1, meaning that only one Pod will run at a time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose you want to run a queue-worker Job whose purpose is to
    consume work items from a queue. You could set `parallelism` to 10, and leave
    `completions` unset. This will start 10 Pods, each of which will keep consuming
    work from the queue until there is no more work to do, and then exit, at which
    point the Job will be completed:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Alternatively, if you want to run a single one-off task, you could leave both
    `completions` and `parallelism` at 1\. This will start one copy of the Pod, and
    wait for it to complete successfully. If it crashes, fails, or exits in any nonsuccessful
    way, the Job will restart it, just like a Deployment does. Only successful exits
    count toward the required number of `completions`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: How do you start a Job? You could do it manually, by applying a Job manifest
    using `kubectl` or Helm. Alternatively, a Job might be triggered by automation;
    your continuous deployment pipeline, for example (see [Chapter 14](ch14.html#continuous)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: When your Job is finished, if you want Kubernetes to automatically clean up
    after itself you can use the `ttlSecondsAfterFinished` setting. Once the specified
    number of seconds passes after the Job exits, it will automatically be deleted.
    You can also set `ttlSecondsAfterFinished` to `0`, which means your Job will be
    deleted as soon as it completes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: When you need to run a Job periodically, at a given time of day, or at a given
    interval, Kubernetes also has a `CronJob` object.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: CronJobs
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Unix environments, scheduled jobs are run by the `cron` daemon (whose name
    comes from the Greek word χρόνος, meaning “time”). Accordingly, they’re known
    as *CronJobs*, and the Kubernetes `CronJob` object does exactly the same thing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'A `CronJob` looks like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The two important fields to look at in the `CronJob` manifest are `spec.schedule`
    and `spec.jobTemplate`. The `schedule` field specifies when the job will run,
    using the same [format](https://oreil.ly/TaRek) as the Unix `cron` utility.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The `jobTemplate` specifies the template for the Job that is to be run, and
    is exactly the same as a normal Job manifest (see [“Jobs”](#jobs)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal Pod Autoscalers
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that a Deployment controller maintains a specified number of Pod replicas.
    If one replica fails, another will be started to replace it in order to achieve
    the target number of replicas.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: The desired replica count is set in the Deployment manifest, and we’ve seen
    that you can adjust this to increase the number of Pods if there is heavy traffic,
    or reduce it to scale down the Deployment if there are idle Pods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: But what if Kubernetes could adjust the number of replicas for you automatically,
    responding to increased demand? This is exactly what the [Horizontal Pod Autoscaler](https://oreil.ly/d4cEE)
    does. (*Horizontal* scaling refers to adjusting the number of replicas of a service,
    in contrast to *vertical* scaling, which makes individual replicas bigger or smaller
    in terms of CPU or memory.)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: A Horizontal Pod Autoscaler (HPA) watches a specified Deployment, constantly
    monitoring a given metric to see if it needs to scale the number of replicas up
    or down.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common autoscaling metrics is CPU utilization. Remember from
    [“Resource Requests”](ch05.html#resourcerequests) that Pods can request a certain
    amount of CPU resources; for example, 500 millicpus. As the Pod runs, its CPU
    usage will fluctuate, meaning that, at any given moment, the Pod is actually using
    some percentage of its original CPU request.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'You can autoscale the Deployment based on this value: for example, you could
    create an HPA that targets 80% CPU utilization for the Pods. If the mean CPU usage
    over all the Pods in the Deployment is only 70% of their requested amount, the
    HPA will scale down by decreasing the target number of replicas. If the Pods aren’t
    working very hard, we don’t need so many of them and the HPA can scale them down.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the average CPU utilization is 90%, this exceeds the target
    of 80%, so we need to add more replicas until the average CPU usage comes down.
    The HPA will modify the Deployment to increase the target number of replicas.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Each time the HPA determines that it needs to do a scaling operation, it adjusts
    the replicas by a different amount, based on the ratio of the actual metric value
    to the target. If the Deployment is very close to the target CPU utilization,
    the HPA will only add or remove a small number of replicas; but if it’s way out
    of scale, the HPA will adjust it by a larger number.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: The HPA uses another popular Kubernetes project called the Metrics Server for
    getting the data it needs for making autoscaling decisions. You can install it
    following the instructions in the [metrics-server repo](https://oreil.ly/6nZ20).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of an HPA based on CPU utilization:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The interesting fields here are:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '`spec.scaleTargetRef` specifies the Deployment to scale'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec.minReplicas` and `spec.maxReplicas` specify the limits of scaling'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec.metrics` determines the metrics that will be used for scaling'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although CPU utilization is the most common scaling metric, you can use any
    metrics available to Kubernetes, including both the built-in *system metrics*
    like CPU and memory usage, and app-specific *service metrics*, which you define
    and export from your application (see [Chapter 16](ch16.html#metrics)). For example,
    you could scale based on the application error rate or number of incoming requests
    per second.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about autoscalers and custom metrics in the Kubernetes [documentation](https://oreil.ly/17zTB).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling on a known schedule
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `HorizontalPodAutoscaler` when combined with a `CronJob` can be useful
    in cases where your traffic patterns for an application are predictable based
    on the time of day. If you know, for example, that you definitely need 20 Pods
    to be already up and running by 8 a.m. for a big flood of requests that always
    come in at the start of your business day, then you could create a `CronJob` that
    runs the `kubectl` command with an internal service account (see [“Pod Service
    Accounts”](ch08.html#serviceaccounts)) for scaling up just before that time:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Then you could create a similar `CronJob` at the end of your business day to
    scale the `minReplicas` back down. When combined with cluster autoscaling as discussed
    in [“Autoscaling”](ch06.html#autoscaling), you could use this trick to save on
    your total compute costs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Using the plain HPA without a cron may work fine for your use-cases, but remember
    that scaling up new nodes and Pods does not happen instantly. In the cases when
    you already know that you will need a certain capacity to handle an upcoming load
    spike, adding a `CronJob` can help ensure that you have everything up and running
    at the beginning of the spike.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Operators and Custom Resource Definitions (CRDs)
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw in [“StatefulSets”](#statefulsets) that, while the standard Kubernetes
    objects such as Deployment and Service are fine for simple, stateless applications,
    they have their limitations. Some applications require multiple, collaborating
    Pods that have to be initialized in a particular order (for example, replicated
    databases or clustered services).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: For applications that need more complicated management or complex types of resources,
    Kubernetes allows you to create your own new types of object. These are called
    *Custom Resource Definitions* (CRDs). For example, the Velero backup tool creates
    and uses new custom Kubernetes objects it calls `Configs` and `Backups` (see [“Velero”](ch11.html#velero)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes is designed to be extensible, and you’re free to define and create
    any type of object you want to, using the CRD mechanism. Some CRDs just exist
    to store data, like the Velero `BackupStorageLocation` object. But you can go
    further and create objects that act as Pod controllers, just like a Deployment
    or StatefulSet.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you wanted to create a controller object that sets up replicated,
    high-availability MySQL database clusters in Kubernetes, how would you go about
    it?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: The first step would be to create a CRD for your custom controller object. In
    order to make it do anything, you then need to write a program that communicates
    with the Kubernetes API. This is easy to do, as we saw in [“Building Your Own
    Kubernetes Tools”](ch07.html#client-go). Such a program is called an *Operator*
    (perhaps because it automates the kinds of actions that a human operator might
    perform).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: You can see lots of examples of Operators built and maintained by the community
    in the [OperatorHub.io site](https://operatorhub.io). This is a repository of
    hundreds of Operators that you can install on your clusters, or just browse their
    code to get ideas for your building your own Operators.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Services (see [“Service Resources”](ch04.html#services)) are for routing
    *internal* traffic in your cluster (for example, from one microservice to another),
    Ingress is used for routing *external* traffic into your cluster and to the appropriate
    microservice (see [Figure 9-1](#img-ingress)). You can think of the concept of
    Ingress as a load balancer that works in coordination with a Service to get requests
    from external clients to the correct Pods based on their label selectors. All
    of this happens using an Ingress controller, which we will cover shortly. For
    now, let’s see what a typical Ingress resource looks like for exposing your applications
    outside of the cluster.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes Ingress diagram](assets/cndk_0901.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. The Ingress resource
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is a manifest for a generic Ingress resource:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This Ingress in this example looks at a Service named `demo-service`, and that
    Service then uses the selector labels and readiness status (see [“Readiness Probes”](ch05.html#readiness-probes))
    to determine a suitable Pod for receiving the request. Pods that do not match
    the selector labels defined in the `demo-service`, and any Pods that have a failing
    “Ready” status, will not receive any requests. In addition to this basic routing
    of requests, Ingress can also handle more advanced tasks, such as managing SSL
    certificates, rate-limiting, and other features commonly associated with load
    balancers. The specifics of how these work are handled by an Ingress controller.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Controllers
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An Ingress controller is responsible for managing Ingress resources in a cluster.
    Depending on where you are running your clusters, and the functionality you need,
    the controller you use may vary.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, selecting which Ingress controller to use, and configuring the behavior
    of the controller, is done using annotations in the Ingress manifest. For example,
    to have an Ingress in an EKS cluster use a public-facing AWS Application Load
    Balancer, you would add annotations like this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Each Ingress controller will have its own sets of annotations that configure
    the various features available to that controller.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Hosted GKE clusters in GCP can use [Google Cloud’s Load Balancer Controller
    (GLBC)](https://oreil.ly/CsWhb) for Ingress resources. AWS has a similar product
    we mentioned above called the [AWS Load Balancer Controller](https://oreil.ly/ttxp4)
    and Azure also has its own [Application Gateway Ingress Controller (AGIC)](https://oreil.ly/7u1qA).
    If you are using one of these major public cloud providers, and you have applications
    that you need to expose outside of your clusters, then we recommend exploring
    using the particular Ingress controller maintained by your cloud provider first.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'You also have the option to install and use a different Ingress controller
    inside your clusters, or even run multiple controllers if you like. There are
    [lots of different Ingress controller options](https://oreil.ly/Eu9k0) out there,
    and some of the more popular ones include:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[nginx-ingress](https://oreil.ly/ZSpg6)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: NGINX has long been a popular load balancer tool, even before Kubernetes came
    on the scene. The `nginx-ingress` project is maintained by the Kubernetes community.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[NGINX Ingress Controller](https://oreil.ly/8k7JH)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: This controller is backed by the NGINX company itself. There are some differences
    between this project and the Kubernetes community one mentioned in the previous
    paragraph.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[Contour](https://oreil.ly/wbTye)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Contour actually uses another tool under the hood called [Envoy](https://www.envoyproxy.io)
    to proxy requests between clients and Pods.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[Traefik](https://oreil.ly/PBTPR)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: This is a lightweight proxy tool that can also automatically manage TLS certificates
    for your Ingress.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[Kong](https://oreil.ly/DQ2KE)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Kong hosts the [Kong Plugin Hub](https://docs.konghq.com/hub) with plugins that
    integrate with their Ingress controller to configure things like OAuth authentication,
    LetsEncrypt certificates, IP restriction, metrics, and other useful features for
    load balancers.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[HAProxy](https://oreil.ly/IbvBL)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy has been another popular tool for load balancers for several years,
    and they also have their own Ingress controller for Kubernetes along with a Helm
    chart for installing it in your clusters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Ingress Rules
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ingress can also be used to forward traffic to different backend services,
    depending on certain rules that you specify. One common use for this is to route
    requests to different places, depending on the request URL (known as a *fanout*):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Terminating TLS with Ingress
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most Ingress controllers can also handle securing connections using Transport
    Layer Security (TLS) (the protocol formerly known as Secure Sockets Layer [SSL]).
    This is typically done using a Kubernetes Secret (we will cover these in [“Kubernetes
    Secrets”](ch10.html#secrets)) containing the contents of the certificate and key,
    and the `tls` section of the Ingress manifest:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Automating LetsEncrypt certificates with Cert-Manager
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to automatically request and renew TLS certificates using the popular
    [LetsEncrypt](https://letsencrypt.org) authority (or another ACME certificate
    provider), you can use [`cert-manager`](http://docs.cert-manager.io/en/latest).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: If you run `cert-manager` in your cluster, it will automatically detect TLS
    Ingresses that have no certificate, and request one from the specified provider.
    It can also handle automatically renewing these certificates when they are close
    to expiring.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Service Mesh
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes Ingress and Services may be all that you need for routing requests
    from clients to your applications, depending on the complexity of your organization.
    But there is also a growing interest in a newer concept commonly referred to as
    a *service mesh*. A service mesh is responsible for managing more complex network
    operations such as rate-limiting and encrypting network traffic between microservices.
    Service mesh tools can also add metrics and logging for requests flowing through
    the network, keeping track of how long requests are taking, or tracing where a
    request started and what path it took through the various microservices along
    the way. Some service mesh tools can handle automatic retries of failed requests,
    and have the ability to deny or block inbound or outbound requests as needed.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: There are several options for implementing a service mesh that we will list
    here. We expect that the service mesh tooling landscape will continue to rapidly
    evolve in the coming years and will become a central part of any cloud native
    infrastructure stack. If you are just starting out with a few applications to
    deploy, then you can likely begin with just using the standard Service and Ingress
    resources provided by Kubernetes. But if you find yourself needing to delve into
    these more advanced capabilities of a service mesh, these are some good options
    to explore.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Istio
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Istio was one of the first tools associated with providing a service mesh.
    It is available as an optional add-on component to many hosted Kubernetes clusters,
    including [GKE](https://oreil.ly/BnQMV). If you want to install Istio yourself,
    see the [Istio installation docs](https://oreil.ly/SWldX) for more info. [*Istio:
    Up and Running*](https://oreil.ly/KuTMV) (O’Reilly) is a great book for learning
    more about Istio, as well as the broader concepts commonly associated with service
    meshes.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Linkerd
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Linkerd](https://linkerd.io) offers many of the key service mesh features,
    but with a much lighter footprint and less complexity involved compared to Istio.
    It can be used for setting up mutual TLS between services, gathering metrics for
    request rates and latency, blue-green deployments, and request retries.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Consul Connect
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before Kubernetes was widely known and used, HashiCorp was offering a popular
    tool called Consul, which was focused on service discovery. It handled application
    health checks and the automatic routing of requests to the right place in a distributed
    compute environment. That piece of functionality is now handled natively in Kubernetes,
    but Consul has now expanded to include a newer tool called [Consul Connect](https://oreil.ly/vxlsB)
    with service mesh capabilities. If you run a mixed environment with applications
    running outside of Kubernetes, or are already familiar with using Consul, then
    Consul Connect may be worth exploring for your service mesh.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: NGINX Service Mesh
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the Ingress controllers, NGINX also offers a full [Service Mesh](https://oreil.ly/HOmvE)
    product for Kubernetes. It also uses the sidecar pattern where an NGINX container
    runs alongside your applications and handles routing the network traffic. This
    service mesh container provides mTLS encryption, traffic-splitting capabilities,
    and tracks metrics on network performance for observability.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ultimately, everything in Kubernetes is about running Pods. Those Pods can be
    configured and managed using Kubernetes controllers and objects, either for long-running
    processes or short-lived jobs and CronJobs. More complex setups may use Custom
    Resource Definitions and Operators. Routing network requests to Pods involves
    using Services and Ingress controllers.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic ideas to remember:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Labels are key-value pairs that identify resources, and can be used with selectors
    to match a specified group of resources.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node affinities attract or repel Pods to or from nodes with specified attributes.
    For example, you can specify that a Pod can only run on a node in a specified
    availability zone.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While hard node affinities can block a Pod from running, soft node affinities
    are more like suggestions to the scheduler. You can combine multiple soft affinities
    with different weights.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod affinities express a preference for Pods to be scheduled on the same node
    as other Pods. For example, Pods that benefit from running on the same node can
    express that using a Pod affinity for each other.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod anti-affinities repel other Pods instead of attracting. For example, an
    anti-affinity to replicas of the same Pod can help spread your replicas evenly
    across the cluster.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taints are a way of tagging nodes with specific information, usually about node
    problems or failures. By default, Pods won’t be scheduled on tainted nodes.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tolerations allow a Pod to be scheduled on nodes with a specific taint. You
    can use this mechanism to run certain Pods only on dedicated nodes.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSets allow you to schedule one copy of a Pod on every node (for example,
    a logging agent).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSets start and stop Pod replicas in a specific numbered sequence, allowing
    you to address each by a predictable DNS name. This is ideal for clustered applications,
    such as databases.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs run a Pod once (or a specified number of times) before completing. Similarly,
    CronJobs run a Pod periodically at specified times.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业（Jobs）在完成之前运行一个 Pod（或指定次数）。类似地，CronJobs 定期在指定的时间运行一个 Pod。
- en: Horizontal Pod Autoscalers (HPAs) watch a set of Pods, trying to optimize a
    given metric (such as CPU utilization). They increase or decrease the desired
    number of replicas to achieve the specified goal.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平 Pod 自动缩放器（HPA）监视一组 Pod，试图优化给定的指标（如 CPU 利用率）。它们增加或减少所需的副本数量以达到指定的目标。
- en: Custom Resource Definitions (CRDs) allow you to create your own custom Kubernetes
    objects, to store any data you wish. Operators are Kubernetes client programs
    that can implement orchestration behavior for your specific application. OperatorHub.io
    is a great resource for searching community-built Operators.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源定义（CRD）允许您创建自己的自定义 Kubernetes 对象，以存储您希望的任何数据。运算符是 Kubernetes 客户端程序，可以为您的特定应用程序实现编排行为。OperatorHub.io
    是搜索社区构建的运算符的绝佳资源。
- en: Ingress resources route requests to different services, depending on a set of
    rules, for example, matching parts of the request URL. They can also terminate
    TLS connections for your application.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 资源根据一组规则将请求路由到不同的服务，例如匹配请求 URL 的部分。它们还可以为您的应用程序终止 TLS 连接。
- en: Istio, Linkerd, and Consul Connect are advanced service mesh tools that provide
    networking features for microservice environments such as encryption, QoS, metrics,
    logging, and more complex routing strategies.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio、Linkerd 和 Consul Connect 是高级服务网格工具，为微服务环境提供网络功能，如加密、QoS、指标、日志记录以及更复杂的路由策略。
