- en: Chapter 9\. Managing Pods
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 管理Pods
- en: There are no big problems, there are just a lot of little problems.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 没有大问题，只有很多小问题。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Henry Ford
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亨利·福特
- en: In the previous chapter, we covered containers in some detail and explained
    how in Kubernetes, containers are composed together to form Pods. There are a
    few other interesting aspects of Pods, which we’ll turn to in this chapter, including
    labels, guiding Pod scheduling using node affinities, barring Pods from running
    on certain nodes with taints and tolerations, keeping Pods together or apart using
    Pod affinities, and orchestrating applications using Pod controllers such as DaemonSets
    and StatefulSets. We’ll also cover some advanced networking features including
    Ingress controllers and service mesh tools.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们详细介绍了容器，并解释了在Kubernetes中，容器如何组合在一起形成Pods。Pods还有一些其他有趣的方面，我们将在本章中介绍，包括标签、使用节点亲和性指导Pod调度、使用污点和容忍来阻止Pods在特定节点上运行、使用Pod亲和性将Pods保持在一起或分开、以及使用诸如DaemonSets和StatefulSets等Pod控制器编排应用程序。我们还将介绍一些高级网络功能，包括Ingress控制器和服务网格工具。
- en: Labels
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签
- en: You know that Pods (and other Kubernetes resources) can have labels attached
    to them, and that these play an important role in connecting related resources
    (for example, sending requests from a Service to the appropriate backends). Let’s
    take a closer look at labels and selectors in this section.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道Pod（以及其他Kubernetes资源）可以附加标签，并且这些标签在连接相关资源（例如，将请求从Service发送到适当的后端）中起着重要作用。让我们在本节中更详细地了解标签和选择器。
- en: What Are Labels?
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是标签？
- en: Labels are key/value pairs that are attached to objects, such as pods. Labels
    are intended to be used to specify identifying attributes of objects that are
    meaningful and relevant to users, but do not directly imply semantics to the core
    system.
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 标签是附加到对象（例如Pods）的键/值对。标签旨在用于指定对象的标识属性，这些属性对用户来说是有意义且相关的，但并不直接向核心系统提供语义。
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: The Kubernetes [documentation](https://oreil.ly/C4j1y)
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Kubernetes [文档](https://oreil.ly/C4j1y)
- en: 'In other words, labels exist to tag resources with information that’s meaningful
    to us, but they don’t mean anything to Kubernetes. For example, it’s common to
    label Pods with the application they belong to:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，标签存在是为了为资源打上对我们有意义的信息，但它们对Kubernetes来说没有特别的意义。例如，通常会为Pod打上它们所属的应用程序的标签：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, by itself, this label has no effect. It’s still useful as documentation:
    someone can look at this Pod and see what application it’s running. But the real
    power of a label comes when we use it with a *selector*.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，单独看这个标签没有任何效果。它作为文档仍然很有用：某人可以查看这个Pod并查看它运行的应用程序。但是标签的真正威力是在我们与*选择器*一起使用它时才体现出来。
- en: Selectors
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择器
- en: A selector is an expression that matches a label (or set of labels). It’s a
    way of specifying a group of resources by their labels. For example, a Service
    resource has a selector that identifies the Pods it will send requests to. Remember
    our demo Service from [“Service Resources”](ch04.html#services)?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 选择器是匹配标签（或一组标签）的表达式。这是一种通过它们的标签指定一组资源的方法。例如，Service资源有一个选择器，用于标识它将发送请求到的Pods。还记得我们在[“Service资源”](ch04.html#services)中的演示Service吗？
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This is a very simple selector that matches any resource that has the `app`
    label with a value of `demo`. If a resource doesn’t have the `app` label at all,
    it won’t match this selector. If it has the `app` label, but its value is not
    `demo`, it won’t match the selector either. Only suitable resources (in this case,
    Pods) with the label `app: demo` will match, and all such resources will be selected
    by this Service.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的选择器，匹配所有具有`app`标签且其值为`demo`的资源。如果一个资源根本没有`app`标签，它将不会匹配这个选择器。如果它有`app`标签，但其值不是`demo`，它也不会匹配选择器。只有符合条件的资源（在本例中为Pod）才会匹配，所有这些资源都将被此Service选中。
- en: 'Labels aren’t just used for connecting Services and Pods; you can use them
    directly when querying the cluster with `kubectl get`, using the `--selector`
    flag:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 标签不仅仅用于连接Service和Pod；当使用`kubectl get`命令查询集群时，你可以直接使用它们，使用`--selector`标志：
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You may recall from [“Using Short Flags”](ch07.html#shortflags) that `--selector`
    can be abbreviated to just `-l` (for ***l**abels*).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得从[“使用简短标志”](ch07.html#shortflags)中学到的，`--selector`可以缩写为`-l`（用于***标签***）。
- en: 'If you want to see what labels are defined on your Pods, use the `--show-labels`
    flag to `kubectl get`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看Pod上定义的标签，可以使用`kubectl get`命令的`--show-labels`标志：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: More Advanced Selectors
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高级的选择器
- en: 'Most of the time, a simple selector like `app: demo` (known as an *equality
    selector*) will be all you need. You can combine different labels to make more
    specific selectors:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数情况下，像`app: demo`这样的简单选择器（称为*等值选择器*）就足够了。你可以结合不同的标签来创建更具体的选择器：'
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will return only Pods that have *both* `app: demo` and `environment: production`
    labels. The YAML equivalent of this (in a Service, for example) would be:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '这将仅返回具有`app: demo`和`environment: production`标签的Pods。这在YAML中等效于（例如在服务中）：'
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Equality selectors like this are the only kind available with a Service, but
    for interactive queries with `kubectl`, or more sophisticated resources such as
    Deployments, there are other options.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 像这样的等值选择器是Service中唯一可用的一种，但对于与`kubectl`进行交互式查询或更复杂的资源（如Deployments），还有其他选项。
- en: 'One is selecting for label *inequality*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个是选择*不相等*的标签：
- en: '[PRE6]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This will return all Pods that have an `app` label with a different value to
    `demo`, or that don’t have an `app` label at all.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回所有具有与`demo`不同值的`app`标签的Pods，或者根本没有`app`标签的Pods。
- en: 'You can also ask for label values that are in a *set*:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以请求处于*集合*中的标签值：
- en: '[PRE7]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The YAML equivalent would be:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在YAML中的等效方式如下：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also ask for label values not in a given set:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以请求不在给定集合中的标签值：
- en: '[PRE9]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The YAML equivalent of this would be:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这在YAML中的等效方式如下：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see another example of using `matchExpressions` in [“Using node affinities
    to control scheduling”](ch05.html#nodeaffinities-intro).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[“使用节点亲和性控制调度”](ch05.html#nodeaffinities-intro)中看到另一个使用`matchExpressions`的例子。
- en: Other Uses for Labels
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签的其他用途
- en: We’ve seen how to link Pods to Services using an `app` label (actually, you
    can use any label, but `app` is common). But what other uses are there for labels?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何使用`app`标签将Pods与Services链接起来（实际上，你可以使用任何标签，但`app`很常见）。但标签还有哪些其他用途呢？
- en: 'In our Helm chart for the demo application (see [“What’s Inside a Helm Chart?”](ch12.html#helmcharts)),
    we set an `environment` label, which can be, for example, `staging` or `production`.
    If you’re running staging and production Pods in the same cluster (see [“Do I
    need multiple clusters?”](ch06.html#multiclusters)), you might want to use a label
    like this to distinguish between the two environments. For example, your Service
    selector for production might be:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的演示应用程序的Helm图表中（参见[“Helm图表的内部结构是什么？”](ch12.html#helmcharts)），我们设置了一个`environment`标签，例如可以是`staging`或`production`。如果你在同一个集群中运行staging和production
    Pods（参见[“我需要多个集群吗？”](ch06.html#multiclusters)），你可能希望使用这样的标签来区分这两个环境。例如，你的服务选择器可能是：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Without the extra `environment` selector, the Service would match any and all
    Pods with `app: demo`, including the staging ones, which you probably don’t want.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '如果没有额外的`environment`选择器，该服务将匹配具有`app: demo`的所有Pods，包括可能不想要的staging Pods。'
- en: 'Depending on your applications, you might want to use labels to slice and dice
    your resources in a number of different ways. Here are some examples:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的应用程序，你可能希望使用标签以多种不同的方式对资源进行分组。以下是一些例子：
- en: '[PRE12]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This allows you to query the cluster along these various different dimensions
    to see what’s going on.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许你沿着这些不同的维度查询集群，了解正在发生的情况。
- en: 'You could also use labels as a way of doing canary deployments (see [“Canary
    Deployments”](ch13.html#canary)). If you want to roll out a new version of the
    application to just a small percentage of Pods, you could use labels like `track:
    stable` and `track: canary` for two separate Deployments.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '你也可以将标签用作金丝雀部署的一种方式（参见[“金丝雀部署”](ch13.html#canary)）。如果你想向仅有少量Pods中部署新版本的应用程序，你可以使用像`track:
    stable`和`track: canary`这样的标签来进行两个独立的部署。'
- en: If your Service’s selector matches only the `app` label, it will send traffic
    to all Pods matching that selector, including both `stable` and `canary`. You
    can alter the number of replicas for both Deployments to gradually increase the
    proportion of `canary` Pods. Once all running Pods are on the canary track, you
    can relabel them as `stable` and begin the process again with the next version.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的服务选择器只匹配`app`标签，它将发送流量到所有匹配该选择器的Pods，包括`stable`和`canary`。你可以逐步增加`canary`
    Pods的副本数量，以逐渐增加`canary` Pods的比例。一旦所有运行中的Pods都在canary轨道上，你可以将它们重新标记为`stable`，然后开始下一个版本的过程。
- en: Labels and Annotations
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标签和注释
- en: You might be wondering what the difference is between labels and annotations.
    They’re both sets of key-value pairs that provide metadata about resources.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道标签和注释之间的区别是什么。它们都是提供有关资源元数据的键值对集合。
- en: The difference is that *labels identify resources*. They’re used to select groups
    of related resources, like in a Service’s selector. Annotations, on the other
    hand, are for non-identifying information, to be used by tools or services outside
    Kubernetes. For example, in [“Helm Hooks”](ch13.html#helmhooks) there’s an example
    of using annotations to control Helm workflows.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不同之处在于*标签标识资源*。它们用于选择相关资源组，例如服务的选择器中的标签。另一方面，注释用于非标识信息，供 Kubernetes 外的工具或服务使用。例如，在[“Helm
    钩子”](ch13.html#helmhooks)中，有一个使用注释来控制 Helm 工作流程的示例。
- en: Because labels are often used in internal queries that are performance-critical
    to Kubernetes, there are some fairly tight restrictions on valid labels. For example,
    label names are limited to 63 characters, though they may have an optional 253-character
    prefix in the form of a DNS subdomain, separated from the label by a slash character.
    Labels can only begin with an alphanumeric character (a letter or a digit), and
    can only contain alphanumeric characters plus dashes, underscores, and dots. Label
    values are [similarly restricted](https://oreil.ly/pR82Y).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因为标签通常用于对 Kubernetes 的内部查询是性能关键的，所以对有效标签有一些相当严格的限制。例如，标签名称限制为 63 个字符，尽管可以通过
    DNS 子域的可选 253 字符前缀来扩展，用斜杠字符与标签分隔。标签只能以字母或数字（字母或数字）开头，并且只能包含字母数字字符以及破折号、下划线和点号。标签值也有[类似的限制](https://oreil.ly/pR82Y)。
- en: In practice, we doubt you’ll run out of characters for your labels, since most
    labels in common use are just a single word (for example, `app`).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们怀疑您会因为标签字符用尽而遇到问题，因为常见使用的大多数标签只是一个单词（例如，`app`）。
- en: Node Affinities
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点亲和力
- en: We mentioned node affinities briefly in [“Using node affinities to control scheduling”](ch05.html#nodeaffinities-intro),
    in relation to preemptible nodes. In that section, you learned how to use node
    affinities to preferentially schedule Pods on certain nodes (or not). Let’s take
    a more detailed look at node affinities now.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[“使用节点亲和力控制调度”](ch05.html#nodeaffinities-intro)中简要提到了节点亲和力，与可抢占节点有关。在那一节中，您学习了如何使用节点亲和力优先在特定节点上调度
    Pod（或者不调度）。现在让我们更详细地看一下节点亲和力。
- en: In most cases, you don’t need node affinities. Kubernetes is pretty smart about
    scheduling Pods onto the right nodes. If all your nodes are equally suitable to
    run a given Pod, then don’t worry about it.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您不需要节点亲和力。Kubernetes 在将 Pod 调度到正确节点方面非常聪明。如果所有节点都同样适合运行特定 Pod，则无需担心此问题。
- en: There are exceptions, however (like preemptible nodes in the previous example).
    If a Pod is expensive to restart, you probably want to avoid scheduling it on
    a preemptible node wherever possible; preemptible nodes can disappear from the
    cluster without warning. You can express this kind of preference using node affinities.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，也有例外情况（例如上一个示例中的可抢占节点）。如果重新启动 Pod 成本高昂，您可能希望尽可能避免将其调度到可抢占节点上；可抢占节点可能在没有警告的情况下从集群中消失。您可以使用节点亲和力表达这种偏好。
- en: 'There are two types of affinity: hard and soft, and in Kubernetes these are
    called:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有两种类型的亲和力：硬和软，分别称为：
- en: '`requiredDuringSchedulingIgnoredDuringExecution` (hard)'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`（硬）'
- en: '`preferredDuringSchedulingIgnoredDuringExecution` (soft)'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preferredDuringSchedulingIgnoredDuringExecution`（软）'
- en: It may help you to remember that `required` means a hard affinity (the rule
    *must* be satisfied to schedule this Pod) and `preferred` means a soft affinity
    (it would be *nice* if the rule were satisfied, but it’s not critical).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，`required`表示硬亲和力（规则*必须*满足才能调度此 Pod），而`preferred`表示软亲和力（*最好*满足规则，但不是必须的）可能会有所帮助。
- en: Tip
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The long names of the hard and soft affinity types make the point that these
    rules apply *during scheduling*, but not *during execution*. That is, once the
    Pod has been scheduled to a particular node satisfying the affinity, it will stay
    there. If things change while the Pod is running so that the rule is no longer
    satisfied, Kubernetes won’t move the Pod.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 硬亲和力和软亲和力类型的长名称表明这些规则适用于*调度*期间，但不适用于*执行*期间。也就是说，一旦将 Pod 调度到满足亲和性的特定节点上，它将保留在那里。如果在
    Pod 运行时发生变化导致规则不再满足，Kubernetes 不会移动 Pod。
- en: Hard Affinities
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬亲和力
- en: 'An affinity is expressed by describing the kind of nodes that you want the
    Pod to run on. There might be several rules about how you want Kubernetes to select
    nodes for the Pod. Each one is expressed using the `nodeSelectorTerms` field.
    Here’s an example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性通过描述您希望Pod在其上运行的节点类型来表达。可能有关于您希望Kubernetes为Pod选择节点的几条规则。每个规则都使用`nodeSelectorTerms`字段表示。这里是一个例子：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Only nodes that are in the `us-central1-a` zone will match this rule, so the
    overall effect is to ensure that this Pod is only scheduled in that zone.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在`us-central1-a`区域的节点才会匹配此规则，因此总体效果是确保此Pod仅在该区域中调度。
- en: Soft Affinities
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软亲和性
- en: 'Soft affinities are expressed in much the same way, except that each rule is
    assigned a numerical *weight* from 1 to 100 that determines the effect it has
    on the result. Here’s an example:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 软亲和性的表达方式基本相同，只是每个规则被分配一个从1到100的数值*权重*，以确定其对结果的影响。这里是一个例子：
- en: '[PRE14]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Because this is a `preferred...` rule, it’s a soft affinity: Kubernetes can
    schedule the Pod on any node, but it will give priority to nodes that match these
    rules.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个“preferred…”规则，它是软亲和性：Kubernetes可以将Pod调度到任何节点，但它会优先选择符合这些规则的节点。
- en: You can see that the two rules have different `weight` values. The first rule
    has weight 10, but the second has weight 100\. If there are nodes that match both
    rules, Kubernetes will give 10 times the priority to nodes that match the second
    rule (being in availability zone `us-central1-b`).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到这两条规则具有不同的`weight`值。第一条规则的权重是10，但第二条规则的权重是100。如果有符合两条规则的节点，Kubernetes会给予第二条规则匹配的节点10倍的优先级（位于可用性区`us-central1-b`）。
- en: Weights are a useful way of expressing the relative importance of your preferences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 权重是表达您偏好的相对重要性的一种有用方式。
- en: Pod Affinities and Anti-Affinities
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod亲和性和反亲和性
- en: We’ve seen how you can use node affinities to nudge the scheduler toward or
    away from running a Pod on certain kinds of nodes. But is it possible to influence
    scheduling decisions based on what other Pods are already running on a node?
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到您可以使用节点亲和性来引导调度程序向或远离在某些类型节点上运行Pod。但是是否可以根据节点上已经运行的其他Pod来影响调度决策呢？
- en: Sometimes there are pairs of Pods that work better when they’re together on
    the same node; for example, a web server and a content cache, such as Redis. It
    would be useful if you could add information to the Pod spec that tells the scheduler
    it would prefer to be colocated with a Pod matching a particular set of labels.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，一些Pod对在同一个节点上一起工作更好；例如，像Redis这样的Web服务器和内容缓存。如果您可以向Pod规范添加信息，告诉调度程序它希望与匹配特定标签集的Pod一起放置，那将非常有用。
- en: Conversely, sometimes you want Pods to avoid each other. In [“Keeping Your Workloads
    Balanced”](ch05.html#balanced), we saw the kind of problems that can arise if
    Pod replicas end up together on the same node, instead of distributed across the
    cluster. Can you tell the scheduler to avoid scheduling a Pod where another replica
    of that Pod is already running?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，有时您希望Pod避免彼此。在[“保持工作负载平衡”](ch05.html#balanced)中，我们看到如果Pod副本最终在同一节点上结束，而不是分布在整个集群中可能会出现的问题。您能告诉调度程序避免在已经运行同一Pod的另一个副本的节点上调度Pod吗？
- en: 'That’s exactly what you can do with Pod affinities. Like node affinities, Pod
    affinities are expressed as a set of rules: either hard requirements, or soft
    preferences with a set of weights.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是您可以通过Pod亲和性做的事情。像节点亲和性一样，Pod亲和性被表达为一组规则：要么是硬要求，要么是带有一组权重的软偏好。
- en: Keeping Pods Together
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持Pod在一起
- en: 'Let’s take the first case first: scheduling Pods together. Suppose you have
    one Pod, labeled `app: server`, which is your web server, and another, labeled
    `app: cache`, which is your content cache. They can still work together even if
    they’re on separate nodes, but it’s better if they’re on the same node because
    they can communicate without having to go over the network. How do you ask the
    scheduler to colocate them?'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '首先考虑第一个案例：将Pod一起调度。假设您有一个标记为`app: server`的Pod，这是您的Web服务器，另一个标记为`app: cache`的Pod，这是您的内容缓存。即使它们在不同的节点上，它们仍然可以一起工作，但如果它们在同一个节点上会更好，因为它们可以在不经过网络的情况下进行通信。您如何要求调度程序将它们放在一起？'
- en: 'Here’s an example of the required Pod affinity, expressed as part of the `server`
    Pod spec. The effect would be just the same if you added it to the `cache` spec,
    or to both Pods:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是所需的Pod亲和性的示例，作为`server` Pod规范的一部分表达。如果将其添加到`cache`规范或两个Pod都添加，效果将完全相同：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The overall effect of this affinity is to ensure that the `server` Pod is scheduled,
    if possible, on a node that is also running a Pod labeled `cache`. If there is
    no such node, or if there is no matching node that has sufficient spare resources
    to run the Pod, it will not be able to run.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种亲和性的整体效果是确保如果可能的话，`server` Pod 将安排在同时运行有带有 `cache` 标签的 Pod 的节点上。如果没有这样的节点，或者没有匹配的节点具有足够的空闲资源来运行
    Pod，则无法运行。
- en: This probably isn’t the behavior you want in a real-life situation. If the two
    Pods absolutely must be colocated, put their containers in the same Pod. If it’s
    just preferable for them to be colocated, use a soft Pod affinity (`preferredDuringSchedulingIgnoredDuringExecution`).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，这可能不是您想要的行为。如果两个 Pod 绝对必须共存，请将它们的容器放在同一个 Pod 中。如果它们只是最好共存，请使用软 Pod 亲和性（`preferredDuringSchedulingIgnoredDuringExecution`）。
- en: Keeping Pods Apart
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持 Pod 分开
- en: 'Now let’s take the anti-affinity case: keeping certain Pods apart. Instead
    of `podAffinity`, we use `podAntiAffinity`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看反亲和性的情况：保持某些 Pod 分开。与其使用 `podAffinity`，我们使用 `podAntiAffinity`：
- en: '[PRE16]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It’s very similar to the previous example, except that it’s a `podAntiAffinity`,
    so it expresses the opposite sense, and the match expression is different. This
    time, the expression is: “The `app` label must have the value `server`.”'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前的例子非常相似，只是它是 `podAntiAffinity`，因此表达了相反的意义，匹配表达式也不同。这次，表达式是：“`app` 标签必须具有
    `server` 的值。”
- en: 'The effect of this affinity is to ensure that the Pod will *not* be scheduled
    on any node matching this rule. In other words, no Pod labeled `app: server` can
    be scheduled on a node that already has an `app: server` Pod running. This will
    enforce an even distribution of `server` Pods across the cluster, at the possible
    expense of the desired number of replicas.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '这种亲和性的效果是确保 Pod *不会*被安排在符合此规则的任何节点上。换句话说，不能将带有 `app: server` 标签的 Pod 安排在已运行
    `app: server` Pod 的节点上。这将在集群中平均分布 `server` Pod，但可能会以所需副本数量为代价。'
- en: Soft Anti-Affinities
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 软反亲和性
- en: 'However, we usually care more about having enough replicas available than distributing
    them as fairly as possible. A hard rule is not really what we want here. Let’s
    modify it slightly to make it a soft anti-affinity:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通常更关心是否有足够的副本可用，而不是尽可能公平地分配它们。硬规则并不是我们真正想要的。让我们稍微修改一下，使其成为软反亲和性：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that now the rule is `preferred...`, not `required...`, making it a soft
    anti-affinity. If the rule can be satisfied, it will be, but if not, Kubernetes
    will schedule the Pod anyway.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，现在规则是 `preferred...`，而不是 `required...`，使其成为软反亲和性。如果可以满足规则，将会满足，但如果不能，Kubernetes
    仍将安排 Pod。
- en: Because it’s a preference, we specify a `weight` value, just as we did for soft
    node affinities. If there were multiple affinity rules to consider, Kubernetes
    would prioritize them according to the weight you assign each rule.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一种偏好，我们指定一个 `weight` 值，就像我们为软节点亲和性所做的一样。如果有多个亲和性规则需要考虑，Kubernetes 将根据您为每条规则分配的权重进行优先级排序。
- en: When to Use Pod Affinities
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时使用 Pod 亲和性
- en: Just as with node affinities, you should treat Pod affinities as a fine-tuning
    enhancement for special cases. The scheduler is already good at placing Pods to
    get the best performance and availability from the cluster. Pod affinities restrict
    the scheduler’s freedom, trading one application for another.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 就像节点亲和性一样，您应将 Pod 亲和性视为特殊情况的精细调整增强。调度程序已经很擅长放置 Pod，以获得集群的最佳性能和可用性。Pod 亲和性限制了调度程序的自由度，以换取一个应用程序来换取另一个应用程序。
- en: Taints and Tolerations
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 污点和容忍性
- en: In [“Node Affinities”](#nodeaffinities), you learned about a property of Pods
    that can steer them toward (or away from) a set of nodes. Conversely, *taints*
    allow a node to repel a set of Pods, based on certain properties of the node.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“节点亲和性”](#nodeaffinities)中，您了解到 Pod 的一个属性可以将其引导向（或远离）一组节点。相反，*污点* 允许节点根据节点的某些属性排斥一组
    Pod。
- en: 'For example, you could use taints to set aside particular nodes: nodes that
    are reserved only for specific kinds of Pods. Kubernetes also creates taints for
    you if certain problems exist on the node, such as low memory, or a lack of network
    connectivity.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以使用污点来保留特定节点：仅用于特定类型的 Pod 的节点。如果节点上存在某些问题，例如低内存或缺乏网络连接，Kubernetes 也会为您创建污点。
- en: 'To add a taint to a particular node, use the `kubectl taint` command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要向特定节点添加污点，请使用 `kubectl taint` 命令：
- en: '[PRE18]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This adds a taint called `dedicated=true` to the `docker-for-desktop` node,
    with the effect `NoSchedule`: no Pod can now be scheduled there unless it has
    a matching *toleration*.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这会在 `docker-for-desktop` 节点上添加一个名为 `dedicated=true` 的污点，并设置效果为 `NoSchedule`：除非具有匹配的
    *容忍*，否则现在无法在此处安排任何 Pod。
- en: To see the taints configured on a particular node, use `kubectl describe node...`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看特定节点上配置的污点，请使用 `kubectl describe node...`。
- en: 'To remove a taint from a node, repeat the `kubectl taint` command but with
    a trailing minus sign after the name of the taint:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要从节点上删除一个污点，请重复执行 `kubectl taint` 命令，但在污点名称后加上一个减号：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Tolerations are properties of Pods that describe the taints that they’re compatible
    with. For example, to make a Pod tolerate the `dedicated=true` taint, add this
    to the Pod’s spec:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 容忍是描述 Pod 的属性，用于描述它们与之兼容的污点。例如，要使 Pod 能够容忍 `dedicated=true` 污点，请将此添加到 Pod 的规范中：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This is effectively saying, “This Pod is allowed to run on nodes that have the
    `dedicated=true` taint with the effect `NoSchedule`.” Because the toleration *matches*
    the taint, the Pod can be scheduled. Any Pod without this toleration will not
    be allowed to run on the tainted node.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是在说：“此 Pod 允许在具有效果为 `NoSchedule` 的 `dedicated=true` 污点的节点上运行。”由于容忍 *匹配*
    了污点，因此可以安排该 Pod。任何没有这种容忍的 Pod 将不被允许在带有污点的节点上运行。
- en: 'When a Pod can’t run at all because of tainted nodes, it will stay in `Pending`
    status, and you’ll see a message like this in the Pod description:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当由于有污点的节点而无法运行 Pod 时，它将保持 `Pending` 状态，并且在 Pod 描述中会看到类似以下的消息：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Other uses for taints and tolerations include marking nodes with specialized
    hardware (such as GPUs), and allowing certain Pods to tolerate certain kinds of
    node problems.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 污点和容忍的其他用途包括标记具有专用硬件（如 GPU）的节点，并允许某些 Pod 容忍某些节点问题。
- en: For example, if a node falls off the network, Kubernetes automatically adds
    the taint `node.kubernetes.io/unreachable`. Normally, this would result in its
    `kubelet` evicting all Pods from the node. However, you might want to keep certain
    Pods running, in the hope that the network will come back in a reasonable time.
    To do this, you could add a toleration to those Pods that matches the `unreachable`
    taint.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个节点掉线，Kubernetes 会自动添加污点 `node.kubernetes.io/unreachable`。通常情况下，这会导致其
    `kubelet` 从节点上驱逐所有 Pod。然而，您可能希望保持某些 Pod 运行，希望网络能在合理时间内恢复。为此，您可以为这些 Pod 添加与 `unreachable`
    污点匹配的容忍。
- en: You can read more about taints and tolerations in the Kubernetes [documentation](https://oreil.ly/qbqpz).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Kubernetes [文档](https://oreil.ly/qbqpz) 中了解有关污点和容忍的更多信息。
- en: Pod Controllers
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 控制器
- en: 'We’ve talked a lot about Pods in this chapter, and that makes sense: all Kubernetes
    applications run in a Pod. You might wonder, though, why we need other kinds of
    objects at all. Isn’t it enough just to create a Pod for an application and run
    it?'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中我们已经讨论了许多关于 Pod 的内容，这是有道理的：所有 Kubernetes 应用程序都在一个 Pod 中运行。但您可能会想，为什么我们还需要其他类型的对象呢？仅仅为一个应用程序创建一个
    Pod 并运行它就足够了吗？
- en: 'That’s effectively what you get by running a container directly with `docker
    container run`, as we did in [“Running a Container Image”](ch02.html#runningcontainer).
    It works, but it’s very limited:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是直接使用 `docker container run` 运行容器时得到的结果，正如我们在 [“运行容器映像”](ch02.html#runningcontainer)
    中所做的那样。它有效，但非常有限：
- en: If the container exits for some reason, you have to manually restart it.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果容器由于某些原因退出，您必须手动重新启动它。
- en: There’s only one replica of your container and no way to load-balance traffic
    across multiple replicas if you run them manually.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果手动运行它们，那么您的容器只有一个副本，而且无法在多个副本之间进行负载均衡。
- en: If you want highly available replicas, you have to decide which nodes to run
    them on, and take care of keeping the cluster balanced.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果要高可用的副本，您必须决定在哪些节点上运行它们，并注意保持集群的平衡。
- en: When you update the container, you have to take care of stopping each running
    image in turn, pulling the new image and restarting it.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您更新容器时，必须小心依次停止每个运行中的映像，拉取新映像并重新启动它。
- en: That’s the kind of work that Kubernetes is designed to take off your hands using
    *controllers*. In [“ReplicaSets”](ch04.html#replicaset-intro), we introduced the
    ReplicaSet controller, which manages a group of replicas of a particular Pod.
    It works continuously to make sure there are always the specified number of replicas,
    starting new ones if there aren’t enough, and killing off replicas if there are
    too many.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是 Kubernetes 通过*控制器*来为您处理的工作。在[“ReplicaSets”](ch04.html#replicaset-intro)中，我们介绍了
    ReplicaSet 控制器，它管理特定 Pod 的一组副本。它持续工作以确保始终存在指定数量的副本，如果副本不足，则启动新副本；如果副本过多，则停止部分副本。
- en: You’re also now familiar with Deployments, which as we saw in [“Deployments”](ch04.html#deployments-intro),
    manage ReplicaSets to control the rollout of application updates. When you update
    a Deployment—for example, with a new container spec—it creates a new ReplicaSet
    to start up the new Pods, and eventually closes down the ReplicaSet that was managing
    the old Pods.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在还熟悉了 Deployment，正如我们在[“Deployments”](ch04.html#deployments-intro)中看到的，它管理
    ReplicaSets 来控制应用程序更新的推出。当您更新 Deployment 时，例如使用新的容器规范，它会创建一个新的 ReplicaSet 来启动新的
    Pod，并最终关闭管理旧 Pod 的 ReplicaSet。
- en: For most simple applications, a Deployment is all you need. But there are a
    few other useful kinds of Pod controllers, and we’ll look briefly at a few of
    them in this section.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数简单应用程序来说，Deployment 就足够了。但在本节中，我们将简要介绍一些其他有用的 Pod 控制器。
- en: DaemonSets
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DaemonSets
- en: Suppose you want to send logs from all your applications to a centralized log
    server, like an Elasticsearch-Logstash-Kibana (ELK) stack, or a SaaS monitoring
    product such as Datadog (see [“Datadog”](ch16.html#datadog)). There are a few
    ways to do that.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您希望将所有应用程序的日志发送到集中式日志服务器，如 Elasticsearch-Logstash-Kibana（ELK）堆栈或诸如 Datadog
    这样的 SaaS 监控产品（见[“Datadog”](ch16.html#datadog)）。有几种方法可以做到这一点。
- en: You could have each application include code to connect to the logging service,
    authenticate, write logs, and so on, but this results in a lot of duplicated code,
    which is inefficient.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用都可以包含连接到日志服务、认证、写日志等代码，但这样会导致大量重复的代码，效率低下。
- en: Alternatively, you could run an extra container in every Pod that acts as a
    logging agent (this is called a *sidecar* pattern). This means that the application
    doesn’t need built-in knowledge of how to talk to the logging service, but it
    does mean you would potentially have several copies of the same logging agent
    running on a node.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在每个 Pod 中运行一个额外的容器作为日志代理（这称为*sidecar*模式）。这意味着应用程序不需要内置如何与日志服务通信的知识，但这也意味着您可能会在节点上运行几个相同的日志代理副本。
- en: 'Since all it does is manage a connection to the logging service and pass on
    log messages to it, you really only need one copy of the logging agent on each
    node. This is such a common requirement that Kubernetes provides a special controller
    object for it: the *DaemonSet*.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它只是管理与日志服务的连接并将日志消息传递给它，因此您实际上只需要在每个节点上有一个日志代理副本。这是一个如此常见的需求，以至于 Kubernetes
    为此提供了一个特殊的控制器对象：*DaemonSet*。
- en: Tip
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The term *daemon* traditionally refers to long-running background processes
    on a server that handle things like logging, so by analogy, Kubernetes DaemonSets
    run a *daemon* container on each node in the cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*守护进程*传统上指运行在服务器上处理诸如日志记录之类的长时间后台进程，因此类比，Kubernetes 的 DaemonSets 在集群中的每个节点上运行一个*守护进程*容器。
- en: 'The manifest for a DaemonSet, as you might expect, looks very much like that
    for a Deployment:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 DaemonSet 的清单，正如您可能期望的那样，看起来非常像 Deployment 的清单：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Use a DaemonSet when you need to run one copy of a Pod on each of the nodes
    in your cluster. If you’re running an application where maintaining a given number
    of replicas is more important than exactly which node the Pods run on, use a Deployment
    instead.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要在集群的每个节点上运行一个 Pod 的副本时，请使用 DaemonSet。如果您运行的是一个应用程序，维护指定数量的副本比确切地在哪个节点上运行
    Pod 更重要，请改用 Deployment。
- en: StatefulSets
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StatefulSets
- en: Like a Deployment or DaemonSet, a StatefulSet is a kind of Pod controller. What
    a StatefulSet adds is the ability to start and stop Pods in a specific sequence.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 像 Deployment 或 DaemonSet 一样，StatefulSet 是一种 Pod 控制器。StatefulSet 添加的功能是能够按特定顺序启动和停止
    Pod。
- en: With a Deployment, for example, all your Pods are started and stopped in a random
    order. This is fine for stateless services, where every replica is identical and
    does the same job.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用 Deployment 时，所有 Pod 都以随机顺序启动和停止。对于无状态服务来说，这是可以接受的，因为每个副本都是相同的，执行相同的工作。
- en: Sometimes, though, you need to start Pods in a specific numbered sequence, and
    be able to identify them by their number. For example, distributed applications
    such as Redis, MongoDB, or Cassandra create their own clusters, and need to be
    able to identify the cluster leader by a predictable name.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时您需要按特定编号顺序启动 Pod，并能够通过其编号进行识别。例如，分布式应用程序（如 Redis、MongoDB 或 Cassandra）会创建自己的集群，并需要能够通过可预测的名称识别集群领导者。
- en: A StatefulSet is ideal for this. For example, if you create a StatefulSet named
    `redis`, the first Pod started will be named `redis-0`, and Kubernetes will wait
    until that Pod is ready before starting the next one, `redis-1`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种情况，StatefulSet 是理想选择。例如，如果您创建一个名为 `redis` 的 StatefulSet，第一个启动的 Pod 将被命名为
    `redis-0`，Kubernetes 将等待该 Pod 准备就绪后再启动下一个，即 `redis-1`。
- en: Depending on the application, you can use this property to cluster the Pods
    in a reliable way. For example, each Pod can run a startup script that checks
    if it is running on `redis-0`. If it is, it will be the redis cluster leader.
    If not, it will attempt to join the cluster as a replica by contacting `redis-0`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用程序的不同，您可以利用这一特性以可靠的方式对 Pod 进行集群化。例如，每个 Pod 可以运行一个启动脚本，检查它是否正在 `redis-0`
    上运行。如果是，则它将成为 Redis 集群的领导者。如果不是，则将尝试通过联系 `redis-0` 作为副本加入集群。
- en: Each replica in a StatefulSet must be running and ready before Kubernetes starts
    the next one, and similarly when the StatefulSet is terminated, the replicas will
    be shut down in reverse order, waiting for each Pod to finish before moving on
    to the next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，每个 StatefulSet 的副本必须在下一个开始之前运行并准备就绪，类似地，在终止 StatefulSet 时，将按相反顺序关闭副本，并等待每个
    Pod 完成后再继续。
- en: 'Apart from these special properties, a StatefulSet looks very similar to a
    normal Deployment:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些特殊属性外，StatefulSet 看起来与普通的部署（Deployment）非常相似：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To be able to address each of the Pods by a predictable DNS name, such as `redis-1`,
    you also need to create a Service with a `clusterIP` type of `None` (known as
    a *headless service*).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够通过可预测的 DNS 名称（如 `redis-1`）访问每个 Pod，还需要创建一个 `clusterIP` 类型为 `None` 的服务（称为
    *无头服务*）。
- en: With a nonheadless Service, you get a single DNS entry (such as `redis`) that
    load-balances across all the backend Pods. With a headless service, you still
    get that single service DNS name, but you also get individual DNS entries for
    each numbered Pod, like `redis-0`, `redis-1`, `redis-2`, and so on.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非无头服务（nonheadless Service），您会得到一个单一的 DNS 条目（比如 `redis`），它负载均衡到所有后端 Pod。使用无头服务时，您仍然会得到单一的服务
    DNS 名称，但同时还会为每个编号的 Pod（比如 `redis-0`、`redis-1`、`redis-2`等）得到单独的 DNS 条目。
- en: Pods that need to join the Redis cluster can contact `redis-0` specifically,
    but applications that simply need a load-balanced Redis service can use the `redis`
    DNS name to talk to a randomly selected Redis Pod.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 需要加入 Redis 集群的 Pod 可以特定地联系 `redis-0`，但只需要负载均衡 Redis 服务的应用程序可以使用 `redis` DNS
    名称与随机选择的 Redis Pod 进行通信。
- en: StatefulSets can also manage disk storage for their Pods, using a VolumeClaimTemplate
    object that automatically creates a PersistentVolumeClaim (see [“Persistent Volumes”](ch08.html#persistent)).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets 还可以管理其 Pod 的磁盘存储，使用 VolumeClaimTemplate 对象自动创建 PersistentVolumeClaim（参见[“持久卷”](ch08.html#persistent)）。
- en: Jobs
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 作业
- en: Another useful type of Pod controller in Kubernetes is the Job. Whereas a Deployment
    runs a specified number of Pods and restarts them continually, a Job only runs
    a Pod for a specified number of times. After that, it is considered completed.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中另一个有用的 Pod 控制器类型是作业（Job）。与部署（Deployment）运行指定数量的 Pod 并持续重启不同，作业仅运行指定次数的
    Pod。完成后，作业被认为已完成。
- en: For example, a batch-processing task or queue-worker Pod usually starts up,
    does its work, and then exits. This is an ideal candidate to be managed by a Job.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，批处理任务或队列工作 Pod 通常启动，完成工作，然后退出。这是由作业管理的理想候选者。
- en: 'There are two fields that control Job execution: `completions` and `parallelism`.
    The first, `completions`, determines the number of times the specified Pod needs
    to run successfully before the Job is considered complete. The default value is
    1, meaning the Pod will run once.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 控制作业执行的两个字段是 `completions` 和 `parallelism`。首先，`completions` 确定指定 Pod 需要成功运行的次数，然后作业才被认为已完成。默认值为
    1，意味着 Pod 将运行一次。
- en: The `parallelism` field specifies how many Pods should run at once. Again, the
    default value is 1, meaning that only one Pod will run at a time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`parallelism` 字段指定一次应该运行多少个 Pod。同样，默认值为 1，意味着一次只会运行一个 Pod。'
- en: 'For example, suppose you want to run a queue-worker Job whose purpose is to
    consume work items from a queue. You could set `parallelism` to 10, and leave
    `completions` unset. This will start 10 Pods, each of which will keep consuming
    work from the queue until there is no more work to do, and then exit, at which
    point the Job will be completed:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设您想运行一个队列工作 Job，其目的是从队列中消耗工作项。您可以将 `parallelism` 设置为 10，并且不设置 `completions`。这将启动
    10 个 Pod，每个 Pod 将继续从队列中消耗工作，直到没有更多的工作可做，然后退出，此时 Job 将完成：
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Alternatively, if you want to run a single one-off task, you could leave both
    `completions` and `parallelism` at 1\. This will start one copy of the Pod, and
    wait for it to complete successfully. If it crashes, fails, or exits in any nonsuccessful
    way, the Job will restart it, just like a Deployment does. Only successful exits
    count toward the required number of `completions`.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，如果您想运行一个单独的一次性任务，您可以将 `completions` 和 `parallelism` 都设置为 1。这将启动一个 Pod 的副本，并等待其成功完成。如果它崩溃、失败或以任何非成功的方式退出，Job
    将重新启动它，就像 Deployment 一样。只有成功的退出才算作所需的 `completions` 数量。
- en: How do you start a Job? You could do it manually, by applying a Job manifest
    using `kubectl` or Helm. Alternatively, a Job might be triggered by automation;
    your continuous deployment pipeline, for example (see [Chapter 14](ch14.html#continuous)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如何启动一个 Job？您可以通过使用 `kubectl` 或 Helm 应用 Job 清单来手动执行它。另外，Job 可能会被自动触发；例如您的持续部署流水线（参见[第14章](ch14.html#continuous)）。
- en: When your Job is finished, if you want Kubernetes to automatically clean up
    after itself you can use the `ttlSecondsAfterFinished` setting. Once the specified
    number of seconds passes after the Job exits, it will automatically be deleted.
    You can also set `ttlSecondsAfterFinished` to `0`, which means your Job will be
    deleted as soon as it completes.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当您的 Job 完成时，如果您希望 Kubernetes 在完成后自动清理，您可以使用 `ttlSecondsAfterFinished` 设置。一旦作业退出后指定的秒数过去，它将自动被删除。您还可以将
    `ttlSecondsAfterFinished` 设置为 `0`，这意味着您的 Job 将在完成后立即被删除。
- en: When you need to run a Job periodically, at a given time of day, or at a given
    interval, Kubernetes also has a `CronJob` object.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当您需要定期运行 Job，例如在一天的特定时间或给定间隔内，Kubernetes 还提供了 `CronJob` 对象。
- en: CronJobs
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CronJobs
- en: In Unix environments, scheduled jobs are run by the `cron` daemon (whose name
    comes from the Greek word χρόνος, meaning “time”). Accordingly, they’re known
    as *CronJobs*, and the Kubernetes `CronJob` object does exactly the same thing.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Unix 环境中，定时作业由 `cron` 守护进程运行（其名称来自希腊语单词 χρόνος，意思是“时间”）。因此，它们被称为 *CronJobs*，而
    Kubernetes 的 `CronJob` 对象正是做同样的事情。
- en: 'A `CronJob` looks like this:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`CronJob` 的样子是这样的：'
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The two important fields to look at in the `CronJob` manifest are `spec.schedule`
    and `spec.jobTemplate`. The `schedule` field specifies when the job will run,
    using the same [format](https://oreil.ly/TaRek) as the Unix `cron` utility.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `CronJob` 清单中要查看的两个重要字段是 `spec.schedule` 和 `spec.jobTemplate`。`schedule` 字段指定作业将使用与
    Unix `cron` 实用程序相同的[格式](https://oreil.ly/TaRek)运行的时间表。
- en: The `jobTemplate` specifies the template for the Job that is to be run, and
    is exactly the same as a normal Job manifest (see [“Jobs”](#jobs)).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`jobTemplate` 指定要运行的 Job 的模板，与普通的 Job 清单完全相同（参见[“Jobs”](#jobs)）。'
- en: Horizontal Pod Autoscalers
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 水平 Pod 自动伸缩器
- en: Remember that a Deployment controller maintains a specified number of Pod replicas.
    If one replica fails, another will be started to replace it in order to achieve
    the target number of replicas.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Deployment 控制器维护指定数量的 Pod 副本。如果一个副本失败，将启动另一个副本以取代它，以实现目标副本数。
- en: The desired replica count is set in the Deployment manifest, and we’ve seen
    that you can adjust this to increase the number of Pods if there is heavy traffic,
    or reduce it to scale down the Deployment if there are idle Pods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Deployment 清单中设置了所需的副本数，并且我们已经看到，您可以根据流量情况调整此值以增加 Pod 的数量，或者在 Pod 无人使用时减少以缩减
    Deployment。
- en: But what if Kubernetes could adjust the number of replicas for you automatically,
    responding to increased demand? This is exactly what the [Horizontal Pod Autoscaler](https://oreil.ly/d4cEE)
    does. (*Horizontal* scaling refers to adjusting the number of replicas of a service,
    in contrast to *vertical* scaling, which makes individual replicas bigger or smaller
    in terms of CPU or memory.)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，如果 Kubernetes 可以自动调整副本数量，以响应增加的需求呢？这正是 [水平 Pod 自动伸缩器](https://oreil.ly/d4cEE)
    的作用。(*水平* 扩展指调整服务的副本数量，与 *垂直* 扩展形成对比，后者根据 CPU 或内存大小调整单个副本的大小。)
- en: A Horizontal Pod Autoscaler (HPA) watches a specified Deployment, constantly
    monitoring a given metric to see if it needs to scale the number of replicas up
    or down.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 水平 Pod 自动缩放器（HPA）会监视指定的 Deployment，不断监视给定的度量标准，以确定是否需要增加或减少副本数量。
- en: One of the most common autoscaling metrics is CPU utilization. Remember from
    [“Resource Requests”](ch05.html#resourcerequests) that Pods can request a certain
    amount of CPU resources; for example, 500 millicpus. As the Pod runs, its CPU
    usage will fluctuate, meaning that, at any given moment, the Pod is actually using
    some percentage of its original CPU request.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的自动缩放指标之一是 CPU 利用率。记住来自[“资源请求”](ch05.html#resourcerequests)的内容，Pod 可以请求一定数量的
    CPU 资源；例如，500 毫核。随着 Pod 的运行，其 CPU 使用率会波动，这意味着在任何给定时刻，Pod 实际上使用了其原始 CPU 请求的一部分百分比。
- en: 'You can autoscale the Deployment based on this value: for example, you could
    create an HPA that targets 80% CPU utilization for the Pods. If the mean CPU usage
    over all the Pods in the Deployment is only 70% of their requested amount, the
    HPA will scale down by decreasing the target number of replicas. If the Pods aren’t
    working very hard, we don’t need so many of them and the HPA can scale them down.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据此值自动缩放 Deployment：例如，您可以创建一个 HPA，该 HPA 的目标是 Pod 的 80% CPU 利用率。如果 Deployment
    中所有 Pod 的平均 CPU 使用率仅为其请求量的 70%，HPA 将通过减少目标副本数来进行缩减。如果 Pod 的工作负载不太重，我们就不需要那么多 Pod，HPA
    可以将它们缩减。
- en: On the other hand, if the average CPU utilization is 90%, this exceeds the target
    of 80%, so we need to add more replicas until the average CPU usage comes down.
    The HPA will modify the Deployment to increase the target number of replicas.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果平均 CPU 利用率为 90%，这超过了 80% 的目标，因此我们需要增加更多的副本，直到平均 CPU 使用率降下来。HPA 将修改 Deployment
    以增加目标副本数。
- en: Each time the HPA determines that it needs to do a scaling operation, it adjusts
    the replicas by a different amount, based on the ratio of the actual metric value
    to the target. If the Deployment is very close to the target CPU utilization,
    the HPA will only add or remove a small number of replicas; but if it’s way out
    of scale, the HPA will adjust it by a larger number.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 每当 HPA 确定需要进行缩放操作时，它将根据实际度量值与目标的比率调整副本数。如果 Deployment 与目标 CPU 利用率非常接近，HPA 只会添加或删除少量副本；但如果它明显偏离，HPA
    将以更大的数量进行调整。
- en: The HPA uses another popular Kubernetes project called the Metrics Server for
    getting the data it needs for making autoscaling decisions. You can install it
    following the instructions in the [metrics-server repo](https://oreil.ly/6nZ20).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 使用另一个名为 Metrics Server 的流行 Kubernetes 项目来获取其自动缩放决策所需的数据。您可以按照 [metrics-server
    仓库](https://oreil.ly/6nZ20) 中的说明进行安装。
- en: 'Here’s an example of an HPA based on CPU utilization:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于 CPU 利用率的 HPA 示例：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The interesting fields here are:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的有趣字段包括：
- en: '`spec.scaleTargetRef` specifies the Deployment to scale'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.scaleTargetRef` 指定要缩放的 Deployment'
- en: '`spec.minReplicas` and `spec.maxReplicas` specify the limits of scaling'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.minReplicas` 和 `spec.maxReplicas` 指定缩放的限制'
- en: '`spec.metrics` determines the metrics that will be used for scaling'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spec.metrics` 确定用于缩放的度量标准'
- en: Although CPU utilization is the most common scaling metric, you can use any
    metrics available to Kubernetes, including both the built-in *system metrics*
    like CPU and memory usage, and app-specific *service metrics*, which you define
    and export from your application (see [Chapter 16](ch16.html#metrics)). For example,
    you could scale based on the application error rate or number of incoming requests
    per second.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 CPU 利用率是最常见的缩放指标，但您可以使用 Kubernetes 可用的任何度量标准，包括内置的*系统度量*，如 CPU 和内存使用情况，以及应用程序特定的*服务度量*，您可以从应用程序定义并导出（参见[第16章](ch16.html#metrics)）。例如，您可以根据应用程序的错误率或每秒传入请求的数量进行缩放。
- en: You can read more about autoscalers and custom metrics in the Kubernetes [documentation](https://oreil.ly/17zTB).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Kubernetes [文档](https://oreil.ly/17zTB) 中详细了解自动缩放器和自定义度量标准。
- en: Autoscaling on a known schedule
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据已知计划进行自动缩放
- en: 'The `HorizontalPodAutoscaler` when combined with a `CronJob` can be useful
    in cases where your traffic patterns for an application are predictable based
    on the time of day. If you know, for example, that you definitely need 20 Pods
    to be already up and running by 8 a.m. for a big flood of requests that always
    come in at the start of your business day, then you could create a `CronJob` that
    runs the `kubectl` command with an internal service account (see [“Pod Service
    Accounts”](ch08.html#serviceaccounts)) for scaling up just before that time:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`HorizontalPodAutoscaler`与`CronJob`结合使用时，在应用程序的流量模式基于一天中的时间可预测的情况下非常有用。例如，如果您确切地知道每天早上8点之前需要20个Pod处于运行状态以处理一大波请求，那么您可以创建一个`CronJob`，在这个时间之前使用内部服务帐户运行`kubectl`命令进行扩展（参见[“Pod
    Service Accounts”](ch08.html#serviceaccounts)）：'
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Then you could create a similar `CronJob` at the end of your business day to
    scale the `minReplicas` back down. When combined with cluster autoscaling as discussed
    in [“Autoscaling”](ch06.html#autoscaling), you could use this trick to save on
    your total compute costs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以在业务结束时创建类似的`CronJob`来缩减`minReplicas`。当与如[“Autoscaling”](ch06.html#autoscaling)中讨论的集群自动缩放结合使用时，您可以使用这个技巧来节省总计算成本。
- en: Using the plain HPA without a cron may work fine for your use-cases, but remember
    that scaling up new nodes and Pods does not happen instantly. In the cases when
    you already know that you will need a certain capacity to handle an upcoming load
    spike, adding a `CronJob` can help ensure that you have everything up and running
    at the beginning of the spike.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的用例来说，使用普通的HPA而不是cron可能运行良好，但请记住，扩展新节点和Pods并不会立即发生。在您已经知道将需要一定容量来处理即将到来的负载高峰的情况下，添加一个`CronJob`可以确保您在高峰开始时所有资源都已经运行。
- en: Operators and Custom Resource Definitions (CRDs)
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作员和自定义资源定义（CRDs）
- en: We saw in [“StatefulSets”](#statefulsets) that, while the standard Kubernetes
    objects such as Deployment and Service are fine for simple, stateless applications,
    they have their limitations. Some applications require multiple, collaborating
    Pods that have to be initialized in a particular order (for example, replicated
    databases or clustered services).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[“StatefulSets”](#statefulsets)中看到，虽然标准的Kubernetes对象如Deployment和Service对于简单的无状态应用程序很好，但它们也有它们的限制。一些应用程序需要多个协作的Pods，这些Pods必须按特定顺序初始化（例如，复制的数据库或集群服务）。
- en: For applications that need more complicated management or complex types of resources,
    Kubernetes allows you to create your own new types of object. These are called
    *Custom Resource Definitions* (CRDs). For example, the Velero backup tool creates
    and uses new custom Kubernetes objects it calls `Configs` and `Backups` (see [“Velero”](ch11.html#velero)).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要更复杂管理或复杂资源类型的应用程序，Kubernetes允许您创建自己的新类型的对象。这些被称为*自定义资源定义*（CRDs）。例如，Velero备份工具创建并使用它称为`Configs`和`Backups`的新自定义Kubernetes对象（参见[“Velero”](ch11.html#velero)）。
- en: Kubernetes is designed to be extensible, and you’re free to define and create
    any type of object you want to, using the CRD mechanism. Some CRDs just exist
    to store data, like the Velero `BackupStorageLocation` object. But you can go
    further and create objects that act as Pod controllers, just like a Deployment
    or StatefulSet.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes被设计为可扩展的，您可以自由地定义和创建任何类型的对象，使用CRD机制。一些CRD只是存在来存储数据，就像Velero的`BackupStorageLocation`对象一样。但是您可以更进一步，创建像Deployment或StatefulSet一样作为Pod控制器的对象。
- en: For example, if you wanted to create a controller object that sets up replicated,
    high-availability MySQL database clusters in Kubernetes, how would you go about
    it?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果您想要创建一个控制器对象，在Kubernetes中设置复制的高可用性MySQL数据库集群，您会如何去做呢？
- en: The first step would be to create a CRD for your custom controller object. In
    order to make it do anything, you then need to write a program that communicates
    with the Kubernetes API. This is easy to do, as we saw in [“Building Your Own
    Kubernetes Tools”](ch07.html#client-go). Such a program is called an *Operator*
    (perhaps because it automates the kinds of actions that a human operator might
    perform).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是为您的自定义控制器对象创建一个CRD。为了使其起作用，您需要编写一个与Kubernetes API通信的程序。正如我们在[“Building Your
    Own Kubernetes Tools”](ch07.html#client-go)中看到的那样，这很容易做到。这样的程序被称为*操作员*（可能是因为它自动执行人类操作员可能执行的动作）。
- en: You can see lots of examples of Operators built and maintained by the community
    in the [OperatorHub.io site](https://operatorhub.io). This is a repository of
    hundreds of Operators that you can install on your clusters, or just browse their
    code to get ideas for your building your own Operators.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[OperatorHub.io 站点](https://operatorhub.io)看到社区构建和维护的大量 Operator 示例。这是一个包含数百个
    Operator 的仓库，您可以在您的集群上安装它们，或者只需浏览它们的代码以获取构建您自己 Operator 的想法。
- en: Ingress
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Ingress
- en: While Services (see [“Service Resources”](ch04.html#services)) are for routing
    *internal* traffic in your cluster (for example, from one microservice to another),
    Ingress is used for routing *external* traffic into your cluster and to the appropriate
    microservice (see [Figure 9-1](#img-ingress)). You can think of the concept of
    Ingress as a load balancer that works in coordination with a Service to get requests
    from external clients to the correct Pods based on their label selectors. All
    of this happens using an Ingress controller, which we will cover shortly. For
    now, let’s see what a typical Ingress resource looks like for exposing your applications
    outside of the cluster.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然服务（见[“服务资源”](ch04.html#services)）用于在集群内部路由*内部*流量（例如，从一个微服务到另一个微服务），Ingress
    用于将*外部*流量路由到您的集群，并将其路由到适当的微服务（见[图 9-1](#img-ingress)）。您可以将 Ingress 的概念看作是一个负载均衡器，与
    Service 协调工作，根据其标签选择器将来自外部客户端的请求发送到正确的 Pod。所有这些都是通过一个 Ingress 控制器来实现的，我们稍后会详细介绍。现在，让我们看看典型的
    Ingress 资源是如何在集群外部暴露您的应用程序的。
- en: '![Kubernetes Ingress diagram](assets/cndk_0901.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes Ingress diagram](assets/cndk_0901.png)'
- en: Figure 9-1\. The Ingress resource
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 9-1\. Ingress 资源
- en: 'Here is a manifest for a generic Ingress resource:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个通用 Ingress 资源的清单：
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This Ingress in this example looks at a Service named `demo-service`, and that
    Service then uses the selector labels and readiness status (see [“Readiness Probes”](ch05.html#readiness-probes))
    to determine a suitable Pod for receiving the request. Pods that do not match
    the selector labels defined in the `demo-service`, and any Pods that have a failing
    “Ready” status, will not receive any requests. In addition to this basic routing
    of requests, Ingress can also handle more advanced tasks, such as managing SSL
    certificates, rate-limiting, and other features commonly associated with load
    balancers. The specifics of how these work are handled by an Ingress controller.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例中的这个 Ingress 查看名为`demo-service`的 Service，然后该 Service 使用选择器标签和就绪状态（见[“就绪探针”](ch05.html#readiness-probes)）来确定适合接收请求的
    Pod。不符合`demo-service`中定义的选择器标签的 Pod以及具有失败的“就绪”状态的任何 Pod 将不会接收任何请求。除了基本的请求路由外，Ingress
    还可以处理更高级的任务，例如管理 SSL 证书、速率限制以及与负载均衡器常见相关的其他功能。这些功能的具体实现由 Ingress 控制器处理。
- en: Ingress Controllers
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress 控制器
- en: An Ingress controller is responsible for managing Ingress resources in a cluster.
    Depending on where you are running your clusters, and the functionality you need,
    the controller you use may vary.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器负责管理集群中的 Ingress 资源。根据您运行集群的位置以及您需要的功能，您使用的控制器可能会有所不同。
- en: 'Usually, selecting which Ingress controller to use, and configuring the behavior
    of the controller, is done using annotations in the Ingress manifest. For example,
    to have an Ingress in an EKS cluster use a public-facing AWS Application Load
    Balancer, you would add annotations like this:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，选择要使用的 Ingress 控制器，并配置控制器的行为，是通过 Ingress 清单中的注释来完成的。例如，要在 EKS 集群中使用面向公众的
    AWS 应用负载均衡器，您可以添加类似以下的注释：
- en: '[PRE29]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Each Ingress controller will have its own sets of annotations that configure
    the various features available to that controller.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 Ingress 控制器都有其自己的一组注释，用于配置该控制器提供的各种功能。
- en: Hosted GKE clusters in GCP can use [Google Cloud’s Load Balancer Controller
    (GLBC)](https://oreil.ly/CsWhb) for Ingress resources. AWS has a similar product
    we mentioned above called the [AWS Load Balancer Controller](https://oreil.ly/ttxp4)
    and Azure also has its own [Application Gateway Ingress Controller (AGIC)](https://oreil.ly/7u1qA).
    If you are using one of these major public cloud providers, and you have applications
    that you need to expose outside of your clusters, then we recommend exploring
    using the particular Ingress controller maintained by your cloud provider first.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在GCP中托管的GKE集群可以使用[Google Cloud的负载均衡器控制器（GLBC）](https://oreil.ly/CsWhb)来处理Ingress资源。AWS有一个类似的产品，我们之前提到过叫做[AWS负载均衡器控制器](https://oreil.ly/ttxp4)，而Azure也有其自己的[应用程序网关Ingress控制器（AGIC）](https://oreil.ly/7u1qA)。如果您使用其中一个主要的公共云提供商，并且有需要将应用程序暴露在集群外部的情况，我们建议首先探索使用您的云提供商维护的特定Ingress控制器。
- en: 'You also have the option to install and use a different Ingress controller
    inside your clusters, or even run multiple controllers if you like. There are
    [lots of different Ingress controller options](https://oreil.ly/Eu9k0) out there,
    and some of the more popular ones include:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择在集群内安装和使用不同的Ingress控制器，甚至如果需要，可以运行多个控制器。有[许多不同的Ingress控制器选项](https://oreil.ly/Eu9k0)，其中一些较受欢迎的包括：
- en: '[nginx-ingress](https://oreil.ly/ZSpg6)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[nginx-ingress](https://oreil.ly/ZSpg6)'
- en: NGINX has long been a popular load balancer tool, even before Kubernetes came
    on the scene. The `nginx-ingress` project is maintained by the Kubernetes community.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: NGINX长久以来一直是一款流行的负载均衡工具，甚至在Kubernetes出现之前也是如此。`nginx-ingress`项目由Kubernetes社区维护。
- en: '[NGINX Ingress Controller](https://oreil.ly/8k7JH)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[NGINX Ingress控制器](https://oreil.ly/8k7JH)'
- en: This controller is backed by the NGINX company itself. There are some differences
    between this project and the Kubernetes community one mentioned in the previous
    paragraph.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个控制器由NGINX公司本身支持。这个项目与前面段落提到的Kubernetes社区项目有一些区别。
- en: '[Contour](https://oreil.ly/wbTye)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[Contour](https://oreil.ly/wbTye)'
- en: Contour actually uses another tool under the hood called [Envoy](https://www.envoyproxy.io)
    to proxy requests between clients and Pods.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Contour实际上在底层使用另一个工具叫做[Envoy](https://www.envoyproxy.io)来代理客户端和Pod之间的请求。
- en: '[Traefik](https://oreil.ly/PBTPR)'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '[Traefik](https://oreil.ly/PBTPR)'
- en: This is a lightweight proxy tool that can also automatically manage TLS certificates
    for your Ingress.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个轻量级的代理工具，还可以自动管理您的Ingress的TLS证书。
- en: '[Kong](https://oreil.ly/DQ2KE)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kong](https://oreil.ly/DQ2KE)'
- en: Kong hosts the [Kong Plugin Hub](https://docs.konghq.com/hub) with plugins that
    integrate with their Ingress controller to configure things like OAuth authentication,
    LetsEncrypt certificates, IP restriction, metrics, and other useful features for
    load balancers.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Kong托管了[Kong插件中心](https://docs.konghq.com/hub)，其中的插件与他们的Ingress控制器集成，用于配置OAuth认证、LetsEncrypt证书、IP限制、指标和其他负载均衡器的有用功能。
- en: '[HAProxy](https://oreil.ly/IbvBL)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[HAProxy](https://oreil.ly/IbvBL)'
- en: HAProxy has been another popular tool for load balancers for several years,
    and they also have their own Ingress controller for Kubernetes along with a Helm
    chart for installing it in your clusters.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，HAProxy一直是另一款流行的负载均衡工具，他们也为Kubernetes提供了自己的Ingress控制器以及一个Helm图表，用于在您的集群中安装它。
- en: Ingress Rules
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress规则
- en: 'Ingress can also be used to forward traffic to different backend services,
    depending on certain rules that you specify. One common use for this is to route
    requests to different places, depending on the request URL (known as a *fanout*):'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress还可以用于将流量转发到不同的后端服务，具体取决于您指定的某些规则。其中一个常见用法是根据请求URL将请求路由到不同的位置，称为*fanout*：
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Terminating TLS with Ingress
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Ingress终止TLS
- en: 'Most Ingress controllers can also handle securing connections using Transport
    Layer Security (TLS) (the protocol formerly known as Secure Sockets Layer [SSL]).
    This is typically done using a Kubernetes Secret (we will cover these in [“Kubernetes
    Secrets”](ch10.html#secrets)) containing the contents of the certificate and key,
    and the `tls` section of the Ingress manifest:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数Ingress控制器还可以使用传输层安全性（TLS）（前身为安全套接层[SSL]协议）来保护连接。这通常使用一个Kubernetes Secret（我们将在[“Kubernetes
    Secrets”](ch10.html#secrets)中介绍）来存储证书和密钥的内容，并在Ingress清单的`tls`部分进行配置：
- en: '[PRE31]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Automating LetsEncrypt certificates with Cert-Manager
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Cert-Manager自动化LetsEncrypt证书
- en: If you want to automatically request and renew TLS certificates using the popular
    [LetsEncrypt](https://letsencrypt.org) authority (or another ACME certificate
    provider), you can use [`cert-manager`](http://docs.cert-manager.io/en/latest).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要使用流行的[Let's Encrypt](https://letsencrypt.org)颁发机构（或其他ACME证书提供者）自动请求和更新TLS证书，您可以使用[`cert-manager`](http://docs.cert-manager.io/en/latest)。
- en: If you run `cert-manager` in your cluster, it will automatically detect TLS
    Ingresses that have no certificate, and request one from the specified provider.
    It can also handle automatically renewing these certificates when they are close
    to expiring.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在您的集群中运行`cert-manager`，它将自动检测没有证书的TLS入口，并从指定的提供者请求证书。它还可以在证书接近到期时自动续订这些证书。
- en: Service Mesh
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务网格
- en: Kubernetes Ingress and Services may be all that you need for routing requests
    from clients to your applications, depending on the complexity of your organization.
    But there is also a growing interest in a newer concept commonly referred to as
    a *service mesh*. A service mesh is responsible for managing more complex network
    operations such as rate-limiting and encrypting network traffic between microservices.
    Service mesh tools can also add metrics and logging for requests flowing through
    the network, keeping track of how long requests are taking, or tracing where a
    request started and what path it took through the various microservices along
    the way. Some service mesh tools can handle automatic retries of failed requests,
    and have the ability to deny or block inbound or outbound requests as needed.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的Ingress和Services可能是从客户端到您的应用程序路由请求所需的全部内容，这取决于您组织的复杂性。但是，有一个越来越大的兴趣集中在一个称为*服务网格*的新概念上。服务网格负责管理更复杂的网络操作，例如限流和加密微服务之间的网络流量。服务网格工具还可以为流经网络的请求添加度量和日志记录，跟踪请求花费的时间长短，或跟踪请求从何处开始以及沿途通过各种微服务的路径。某些服务网格工具可以处理失败请求的自动重试，并具有根据需要拒绝或阻止入站或出站请求的能力。
- en: There are several options for implementing a service mesh that we will list
    here. We expect that the service mesh tooling landscape will continue to rapidly
    evolve in the coming years and will become a central part of any cloud native
    infrastructure stack. If you are just starting out with a few applications to
    deploy, then you can likely begin with just using the standard Service and Ingress
    resources provided by Kubernetes. But if you find yourself needing to delve into
    these more advanced capabilities of a service mesh, these are some good options
    to explore.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种实现服务网格的选项，我们将在这里列出。我们预计服务网格工具的领域将继续在未来几年迅速发展，并将成为任何云原生基础架构堆栈的核心部分。如果您刚开始部署几个应用程序，那么您可能只需使用Kubernetes提供的标准服务和入口资源即可开始。但是，如果您发现自己需要深入研究服务网格的这些更高级功能，这些都是一些值得探索的好选择。
- en: Istio
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Istio
- en: 'Istio was one of the first tools associated with providing a service mesh.
    It is available as an optional add-on component to many hosted Kubernetes clusters,
    including [GKE](https://oreil.ly/BnQMV). If you want to install Istio yourself,
    see the [Istio installation docs](https://oreil.ly/SWldX) for more info. [*Istio:
    Up and Running*](https://oreil.ly/KuTMV) (O’Reilly) is a great book for learning
    more about Istio, as well as the broader concepts commonly associated with service
    meshes.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'Istio是与提供服务网格相关的最早的工具之一。它作为许多托管Kubernetes集群的可选附加组件提供，包括[GKE](https://oreil.ly/BnQMV)。如果您想自己安装Istio，请参阅[Istio安装文档](https://oreil.ly/SWldX)获取更多信息。[*Istio:
    Up and Running*](https://oreil.ly/KuTMV)（O''Reilly出版社）是学习有关Istio及与服务网格通常相关的更广泛概念的好书。'
- en: Linkerd
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Linkerd
- en: '[Linkerd](https://linkerd.io) offers many of the key service mesh features,
    but with a much lighter footprint and less complexity involved compared to Istio.
    It can be used for setting up mutual TLS between services, gathering metrics for
    request rates and latency, blue-green deployments, and request retries.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '[Linkerd](https://linkerd.io)提供了许多关键的服务网格功能，但与Istio相比，它的占用空间更小，复杂性更低。它可用于设置服务之间的互相TLS，收集请求速率和延迟的度量，蓝绿部署以及请求重试。'
- en: Consul Connect
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Consul Connect
- en: Before Kubernetes was widely known and used, HashiCorp was offering a popular
    tool called Consul, which was focused on service discovery. It handled application
    health checks and the automatic routing of requests to the right place in a distributed
    compute environment. That piece of functionality is now handled natively in Kubernetes,
    but Consul has now expanded to include a newer tool called [Consul Connect](https://oreil.ly/vxlsB)
    with service mesh capabilities. If you run a mixed environment with applications
    running outside of Kubernetes, or are already familiar with using Consul, then
    Consul Connect may be worth exploring for your service mesh.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 广为人知和使用之前，HashiCorp 提供了一个名为 Consul 的流行工具，专注于服务发现。它处理应用程序的健康检查和在分布式计算环境中自动路由请求到正确位置。这部分功能现在在
    Kubernetes 中已经本地化处理，但 Consul 现在扩展到包括一个名为 [Consul Connect](https://oreil.ly/vxlsB)
    的新工具，具有服务网格功能。如果您运行在 Kubernetes 之外的混合环境中的应用程序，或者已经熟悉使用 Consul，则可以考虑探索 Consul Connect
    作为您的服务网格的价值。
- en: NGINX Service Mesh
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NGINX 服务网格
- en: In addition to the Ingress controllers, NGINX also offers a full [Service Mesh](https://oreil.ly/HOmvE)
    product for Kubernetes. It also uses the sidecar pattern where an NGINX container
    runs alongside your applications and handles routing the network traffic. This
    service mesh container provides mTLS encryption, traffic-splitting capabilities,
    and tracks metrics on network performance for observability.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Ingress 控制器外，NGINX 还为 Kubernetes 提供了一个完整的 [服务网格](https://oreil.ly/HOmvE)
    产品。它还使用 sidecar 模式，其中一个 NGINX 容器与您的应用程序并行运行，并处理网络流量的路由。该服务网格容器提供 mTLS 加密、流量分割能力，并跟踪网络性能指标以进行可观察性分析。
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Ultimately, everything in Kubernetes is about running Pods. Those Pods can be
    configured and managed using Kubernetes controllers and objects, either for long-running
    processes or short-lived jobs and CronJobs. More complex setups may use Custom
    Resource Definitions and Operators. Routing network requests to Pods involves
    using Services and Ingress controllers.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，在 Kubernetes 中一切都围绕着运行 Pods 进行。这些 Pods 可以使用 Kubernetes 控制器和对象进行配置和管理，无论是长期运行的进程还是短期作业和
    CronJobs。更复杂的设置可能使用自定义资源定义和运算符。将网络请求路由到 Pods 包括使用 Services 和 Ingress 控制器。
- en: 'The basic ideas to remember:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的基本思想：
- en: Labels are key-value pairs that identify resources, and can be used with selectors
    to match a specified group of resources.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Labels 是标识资源的键值对，可以与选择器一起使用以匹配指定组的资源。
- en: Node affinities attract or repel Pods to or from nodes with specified attributes.
    For example, you can specify that a Pod can only run on a node in a specified
    availability zone.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Node affinities 吸引或排斥 Pods 到或远离具有指定属性的节点。例如，您可以指定 Pod 只能运行在指定可用区的节点上。
- en: While hard node affinities can block a Pod from running, soft node affinities
    are more like suggestions to the scheduler. You can combine multiple soft affinities
    with different weights.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然硬节点亲和性可以阻止 Pod 运行，但软节点亲和性更像是对调度器的建议。您可以将多个具有不同权重的软亲和性组合起来。
- en: Pod affinities express a preference for Pods to be scheduled on the same node
    as other Pods. For example, Pods that benefit from running on the same node can
    express that using a Pod affinity for each other.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 亲和性表达了希望将 Pods 调度到与其他 Pods 相同节点上的偏好。例如，受益于在同一节点上运行的 Pods 可以使用 Pod 亲和性来表达这一点。
- en: Pod anti-affinities repel other Pods instead of attracting. For example, an
    anti-affinity to replicas of the same Pod can help spread your replicas evenly
    across the cluster.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod anti-affinities 反对其他 Pods 而不是吸引它们。例如，对相同 Pod 副本的反亲和性可以帮助在集群中均匀地分布您的副本。
- en: Taints are a way of tagging nodes with specific information, usually about node
    problems or failures. By default, Pods won’t be scheduled on tainted nodes.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taints 是一种标记节点特定信息的方式，通常涉及节点问题或故障。默认情况下，Pods 不会被调度到有 taint 的节点上。
- en: Tolerations allow a Pod to be scheduled on nodes with a specific taint. You
    can use this mechanism to run certain Pods only on dedicated nodes.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tolerations 允许一个 Pod 被调度到具有特定 taint 的节点上。您可以使用这种机制仅在专用节点上运行某些 Pods。
- en: DaemonSets allow you to schedule one copy of a Pod on every node (for example,
    a logging agent).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSets 允许您在每个节点上调度一个 Pod 的副本（例如日志代理）。
- en: StatefulSets start and stop Pod replicas in a specific numbered sequence, allowing
    you to address each by a predictable DNS name. This is ideal for clustered applications,
    such as databases.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSets 按特定编号顺序启动和停止 Pod 副本，允许您通过可预测的 DNS 名称来访问每个副本。这对于集群应用程序（如数据库）非常理想。
- en: Jobs run a Pod once (or a specified number of times) before completing. Similarly,
    CronJobs run a Pod periodically at specified times.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业（Jobs）在完成之前运行一个 Pod（或指定次数）。类似地，CronJobs 定期在指定的时间运行一个 Pod。
- en: Horizontal Pod Autoscalers (HPAs) watch a set of Pods, trying to optimize a
    given metric (such as CPU utilization). They increase or decrease the desired
    number of replicas to achieve the specified goal.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平 Pod 自动缩放器（HPA）监视一组 Pod，试图优化给定的指标（如 CPU 利用率）。它们增加或减少所需的副本数量以达到指定的目标。
- en: Custom Resource Definitions (CRDs) allow you to create your own custom Kubernetes
    objects, to store any data you wish. Operators are Kubernetes client programs
    that can implement orchestration behavior for your specific application. OperatorHub.io
    is a great resource for searching community-built Operators.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义资源定义（CRD）允许您创建自己的自定义 Kubernetes 对象，以存储您希望的任何数据。运算符是 Kubernetes 客户端程序，可以为您的特定应用程序实现编排行为。OperatorHub.io
    是搜索社区构建的运算符的绝佳资源。
- en: Ingress resources route requests to different services, depending on a set of
    rules, for example, matching parts of the request URL. They can also terminate
    TLS connections for your application.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 资源根据一组规则将请求路由到不同的服务，例如匹配请求 URL 的部分。它们还可以为您的应用程序终止 TLS 连接。
- en: Istio, Linkerd, and Consul Connect are advanced service mesh tools that provide
    networking features for microservice environments such as encryption, QoS, metrics,
    logging, and more complex routing strategies.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Istio、Linkerd 和 Consul Connect 是高级服务网格工具，为微服务环境提供网络功能，如加密、QoS、指标、日志记录以及更复杂的路由策略。
