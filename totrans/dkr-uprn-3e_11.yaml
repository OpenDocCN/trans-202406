- en: Chapter 10\. Containers at Scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A major strength of containers is their ability to abstract away the underlying
    hardware and operating system so that your application is not constrained to any
    particular host or environment. It facilitates scaling a stateless application
    not just horizontally within your data center but also across cloud providers
    without many of the traditional barriers you would encounter. True to the shipping
    container metaphor, a container on one cloud looks like a container on another.
  prefs: []
  type: TYPE_NORMAL
- en: Many organizations find turnkey cloud deployments of Linux containers appealing
    because they can gain many of the immediate benefits of a scalable container-based
    platform without needing to completely build something in-house. Even though this
    is true, the barrier is actually pretty low for building your own platform in
    the cloud or in your own data center, and we’ll cover some options for doing that
    shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The major public cloud providers have all worked to support Linux containers
    natively in their offerings. Some of the largest efforts to support Linux containers
    in the public cloud include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Amazon Elastic Container Service](https://aws.amazon.com/ecs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Google Cloud Run](https://cloud.google.com/run)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Azure Container Apps](https://azure.microsoft.com/en-us/services/container-apps)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many of the same companies also have robust hosted Kubernetes offerings like
    these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Amazon Elastic Kubernetes Service](https://aws.amazon.com/eks)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Azure Kubernetes Service](https://azure.microsoft.com/en-us/services/kubernetes-service)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s trivial to install Docker on a Linux instance in one of the public clouds.
    But getting Docker onto the server is usually just one step in creating a full
    production environment. You could do this completely on your own, or you could
    use the many tools available from the major cloud providers, Docker, Inc., and
    the broader container community. Much of the tooling will work equally well in
    either a public cloud or your own data center.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of schedulers and more complex tooling systems, we have plenty
    of choices for systems that replicate much of the functionality you would get
    from a public cloud provider. Even if you run in a public cloud, there are some
    compelling reasons why you might choose to run your own Linux container environment
    rather than use one of the off-the-shelf offerings.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover some options for running Linux containers at scale,
    first going through the much simpler Docker Swarm mode and then diving into some
    more advanced tools like Kubernetes and some of the larger cloud offerings. All
    of these examples should give you a view of how you can leverage Docker to provide
    an incredibly flexible platform for your application workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Swarm Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After building the container runtime in the form of the Docker engine, the engineers
    at Docker turned to the problems of orchestrating a fleet of individual Docker
    hosts and effectively packing those hosts full of containers. The first tool that
    evolved from this work was called Docker Swarm. As we explained early on, and
    rather confusingly, there are now two things called “Swarm,” both of which come
    from Docker, Inc.
  prefs: []
  type: TYPE_NORMAL
- en: The original standalone Docker Swarm is now commonly referred to as [Docker
    Swarm (classic)](https://github.com/docker-archive/classicswarm), but there is
    a second “Swarm” implementation that is more specifically called [Swarm mode](https://docs.docker.com/engine/swarm).
    Instead of being a separate product, this is built into the Docker client. The
    built-in Swarm mode is a lot more capable than the original Docker Swarm and is
    intended to replace it entirely. Swarm mode has the major advantage of not requiring
    you to install anything separately. You already have this clustering capability
    on any of your systems that are running Docker! This is the Docker Swarm implementation
    that we’ll focus on here. Hopefully, now that you know that there have been two
    different Docker Swarm implementations, you won’t get confused by contradictory
    information on the internet.
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind Docker Swarm mode is to present a single interface to the `docker`
    client tool but have that interface be backed by a whole cluster rather than a
    single Docker daemon. Swarm is primarily aimed at managing clustered computing
    resources via the Docker tools. It has grown a lot since its first release and
    now contains several scheduler plug-ins with different strategies for assigning
    containers to hosts, and it comes with some basic service discovery built in.
    But it remains only one building block of a more complex solution.
  prefs: []
  type: TYPE_NORMAL
- en: Swarm clusters can contain one or more managers that act as the central management
    hub for your Docker cluster. It is best to set up an odd number of managers. Only
    one manager will act as the cluster leader at a time. As you add more nodes to
    Swarm, you are merging them into a single, cohesive cluster that can be easily
    controlled with the Docker tooling.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get a Swarm cluster up and running. To start, you will need three or more
    Linux servers that can talk to each other over the network. Each of these servers
    should be running recent releases of Docker Community Edition from the official
    Docker software repositories.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Refer to [Chapter 3](ch03.html#installing_docker) for details on installing
    the `docker-ce` packages on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will use three Ubuntu servers running `docker-ce`. The
    very first thing you’ll need to do is `ssh` to the server that you want to use
    as the Swarm manager, and then run the `swarm init` command using the IP address
    for your Swarm manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are steps that you must take to secure a Docker Swarm mode cluster, which
    we are not covering here. Before you run Docker Swarm mode on any long-lived systems,
    make sure that you understand the options and have taken proper steps to secure
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In many of this chapter’s examples, you must use the correct IP addresses for
    your manager and worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'This step will initialize the Swarm manager and give you the token that is
    required for nodes that want to join the cluster. Make note of this token somewhere
    safe, like a password manager. Don’t worry too much if you lose this token; you
    can always get it again by running the following command on the manager:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'You can inspect your progress so far by running your local `docker` client
    pointed at the new manager node’s IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also list all of the nodes that are currently in the cluster with the
    following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, you can add the two additional servers as workers to the Swarm
    cluster. This is what you’d do in production if you were going to scale up, and
    Swarm makes this pretty easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adding additional managers is important and can be done as easily as adding
    the workers. You just need to pass in the manager join token instead of the worker
    join token. You can get this token by running `docker swarm join-token manager`
    on any of the active nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you rerun `docker node ls`, you should now see that you have a total of
    three nodes in your cluster, and only one of them is marked as the `Leader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is all that’s required to get a Swarm cluster up and running in Swarm mode
    ([Figure 10-1](#figure10-1))!
  prefs: []
  type: TYPE_NORMAL
- en: '![Simple Docker Swarm Cluster](assets/dur3_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Simple Docker Swarm mode cluster
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The next thing you should do is create a network for your services to use.
    There is a default network called `ingress` in Swarm, but it is very easy to create
    additional ones for better isolation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Up to this point, we’ve just been getting the underlying pieces running, and
    so far we haven’t deployed any real business logic. So let’s launch your first
    service into the cluster. You can do that with a command like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The service we’re launching with starts containers that host the [*Quantum game*](https://github.com/stared/quantum-game).
    This is a browser-based puzzle game that uses real quantum mechanics. We hope
    that this is a more interesting example than another Hello World!
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although we’re using the `latest` tag in many of these examples, you shouldn’t
    ever use this tag in production. It is convenient for this book since we can easily
    push out updates to the code, but this tag floats and cannot be pinned to a specific
    release over a long period. That means if you use `latest`, then your deployments
    are not repeatable! It can also easily lead to a situation where you don’t have
    the same version of an application running on all the servers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see where those containers ended up by running `docker service ps` against
    the service name you created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Swarm mode uses a routing mesh between the nodes to automatically route traffic
    to a container that can serve the request. When you specify a published port in
    the `docker service create` command, the mesh makes it possible to hit this port
    on any of your three nodes and will route you to the web application. Notice that
    we said any of the *three* nodes even though you only have two instances running.
    Traditionally, you would have also had to set up a separate reverse proxy layer
    to accomplish this, but its batteries are included with Swarm mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'To prove it, you can test the service now by pointing a web browser to the
    IP address of any of your nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything is working as expected, you should see the first puzzle board
    for [the *Quantum Game*](https://quantumgame.io):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a summary view of the most commonly needed information, but sometimes
    that’s not enough. Docker maintains a lot of other metadata about services, just
    like it does for containers. We can get detailed information about a service with
    `service inspect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot of info here, so let’s point out some of the more important things.
    First, we can see that this is a replicated service with two replicas, just like
    we saw in the `service ls` command. We can also see that Docker is health-checking
    the service at 5-second intervals. Running an update to the service will use the
    `stop-first` method, which means it will take our service first to *N*−1 and then
    spin up a new instance to take us back to *N*. You might want to always run in
    *N*+1 mode so that you are never down a node during updates in production. You
    can change that with the `--update-order=start-first` option to the `service update`
    command. It will exhibit the same behavior in a rollback scenario, and we can
    likewise change that with `--rollback-order=start-first`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a real-world scenario, we not only need to be able to launch our service,
    but we also need to be able to scale it up and down. It would be a shame if we
    had to redeploy it to do that, not to mention it could introduce any number of
    additional issues. Luckily, Swarm mode makes it easy to scale our services with
    a single command. To double the number of instances you have running from two
    to four, you can simply run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We used `--detach=false` in the previous command so that it was easier to see
    what was happening.
  prefs: []
  type: TYPE_NORMAL
- en: We can now use `service ps` to show us that Swarm did what we asked. This is
    the same command we ran earlier, but now we should have more copies running! But
    wait, didn’t we ask for more copies than we have nodes?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You’ll notice that you have two services running on the same host. Did you expect
    that? This may not be ideal for host resiliency, but by default Swarm will prioritize
    ensuring that you have the number of instances that you requested over spreading
    individual containers across hosts when possible. If you don’t have enough nodes,
    you will get multiple copies on each node. In a real-world scenario, you need
    to think carefully about placement and scaling. You might not be able to get away
    with running multiple copies on the same host when you lose a whole node. Would
    your application still serve users at that reduced scale?
  prefs: []
  type: TYPE_NORMAL
- en: 'When you need to deploy a new release of your software, you will want to use
    the `docker service update` command. There are a lot of options for this command,
    but here’s one example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Running this command will cause Swarm to update your service one container at
    a time, pausing in between each update. Once this is done, you should be able
    to open up the service’s URL in a new private or incognito browsing session (to
    sidestep the browser’s local cache) and see that the game background is now green
    instead of blue.
  prefs: []
  type: TYPE_NORMAL
- en: 'Great, you have now successfully applied an update, but what if something were
    to go wrong? We might need to deploy a previous release to get back to working
    order. You could now roll back to the previous version, with the correct blue
    background, by using the `service rollback` command, which we discussed in passing
    a little bit earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: That’s about as nice a rollback mechanism as you could ask for a stateless service.
    You don’t have to keep track of the previous version; Docker does that for you.
    All you need to do is tell it to roll back and it pulls the previous metadata
    out of its internal storage and performs the rollback. Just like during deployment,
    Docker can health-check your containers to make sure the rollback is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This rollback mechanism will always go back to the last deployed version, so
    if you run it multiple times in a row, it will just flip between two versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Building on `docker service` is a command called `docker stack`, which enables
    you to deploy a specially designed *docker-compose.yaml* file to a Docker Swarm
    mode or Kubernetes cluster. If you go back and check out the Git repo that we
    used in [Chapter 8](ch08.html#docker_compose), we can deploy a modified version
    of that container stack into our current Swarm mode cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Inside that repository is a directory called *stack* that contains a modified
    version of the *docker-compose.yaml* file that we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If you wanted to spin up this setup in the Swarm mode cluster, you could run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can list what stacks are in the cluster and see what services were
    added by the stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This stack is for basic demonstration purposes and has not been well tested
    for this use case; however, it should give you an idea of how you could assemble
    something similar.
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that it takes a while for all the containers to come up and that
    Hubot will continue to restart. This is expected since Rocket.Chat has not been
    configured yet. The Rocket.Chat setup is covered in [Chapter 8](ch08.html#docker_compose).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you could point your web browser at port 3000 on one of the Swarm
    nodes (e.g., *http://172.17.4.1:3000/* in these examples), and you should see
    the initial setup page for Rocket.Chat.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see all the containers that are managed by the stack, with `docker
    stack ps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'When you are done, you can go ahead and tear down the stack like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you try to immediately spin everything back up, you might get some unexpected
    errors. Just waiting a few moments should fix things while the cluster finishes
    tearing down the old network for the stack, etc.
  prefs: []
  type: TYPE_NORMAL
- en: So, what happens if one of your servers is experiencing an issue and you need
    to take it offline? In this case, you can easily drain all the services off of
    a single node by using the `--availability` option to the `docker node update`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at the nodes that you have in the cluster again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also check where our containers are currently running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the previous command, we used a filter so that the output showed only the
    currently running processes. By default, Docker will also show you the previous
    containers that were running in a tree format so that you can see things like
    updates and rollbacks in the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have determined that the server at 172.17.4.3 needs downtime, you could
    drain the tasks of that node and move them to another host by modifying the `availability`
    state to `drain` in Swarm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'If we inspect the node, we can see that the availability is now set to `drain`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You might be wondering what effect that has on the service. We told one of
    the nodes to stop running copies of the service, and they either have to go away
    or migrate somewhere else. What did it do? We can look at the details of our service
    again and see that all the running containers on that host have been moved to
    a different node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it is safe to bring down the node and do whatever work is required
    to make it healthy again. When you are ready to add the node back into the Swarm
    cluster, you can do so by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We’ll spare you from reinspecting the node at the moment, but you can always
    rerun the `node inspect` command if you want to see what this looks like.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you add a node back to the cluster, containers will not automatically balance!
    However, a new deployment or update should result in the containers being evenly
    spread across the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you are done, you can remove your service and network with the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'And then verify that they are both indeed completely gone:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: That’s all for now! At this point, you can safely tear down all of the servers
    that were a part of your Swarm cluster if you no longer need them.
  prefs: []
  type: TYPE_NORMAL
- en: That was kind of a whirlwind tour, but it covers the basics of using Swarm mode
    in Docker Engine and should help get you started building your own Docker clusters
    wherever you might decide to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s take some time to look at Kubernetes. Since its release to the public
    during [DockerCon](https://events.docker.com/events/dockercon) 2014, Kubernetes
    has grown rapidly and is now probably the most widely adopted of the container
    platforms. It is not the oldest or most mature product today—that distinction
    goes to Mesos, which first launched in 2009 before containers were in widespread
    use—but Kubernetes was purpose-built for containerized workloads, has a great
    mix of functionality that is ever evolving, and also enjoys a very strong community
    that includes many early Docker and Linux container adopters. This mix has helped
    significantly increase its popularity over the years. At DockerCon EU 2017, Docker,
    Inc., announced that Kubernetes support will be coming to the Docker Engine tooling
    itself. Docker Desktop is capable of spinning up a single-node Kubernetes cluster,
    and the client can deploy container stacks for development purposes. This provides
    a nice bridge for developers who use Docker locally but deploy to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Like Linux itself, Kubernetes is available in several distributions, both free
    and commercial. There is a wide variety of distributions that are available and
    supported to varying degrees. Kubernetes widespread adoption means that it now
    has some pretty nice tooling for local development installations.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Kubernetes coverage in this book is intended to provide some basic guidance
    on how you can integrate your Linux container workflow with Kubernetes, but we
    do not go into a lot of detail about the Kubernetes ecosystem here. We highly
    recommend reading [*Kubernetes: Up & Running*, by Brendan Burns et al. (O’Reilly)](https://www.oreilly.com/library/view/kubernetes-up-and/9781098110192),
    or any of the other great materials out there to familiarize yourself with all
    the relevant concepts and terminology.'
  prefs: []
  type: TYPE_NORMAL
- en: Minikube
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Minikube was one of the original tools for managing a local Kubernetes installation
    and is the first one that we will be focusing on here. Most of the concepts that
    you’ll learn while working with Minikube can be applied to any Kubernetes implementation,
    including the options that we’ll discuss after Minikube, so it’s a great place
    to start.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many other options for running a local Kubernetes cluster. We are
    starting with `minikube` because the container or VM that it spins up is a pretty
    standard single-node Kubernetes install. In addition to the tools that we will
    be discussing in this section, we highly recommend exploring [k3s](https://k3s.io),
    [k3d](https://k3d.io), [k0s](https://k0sproject.io), and [microk8s](https://microk8s.io)
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Minikube?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Minikube is a whole distribution of Kubernetes for a single instance. It manages
    a container or VM on your computer that presents a working Kubernetes installation
    and allows you to use all the same tooling that you would use in a production
    system. In scope, it’s a little bit like Docker Compose: it will let you stand
    up a whole stack locally. It goes one step further than Compose, though, in that
    it has all the production APIs. As a result, if you run Kubernetes in production,
    you can have an environment on your desktop that is reasonably close in function,
    if not in scale, to what you are running in production.'
  prefs: []
  type: TYPE_NORMAL
- en: Minikube is fairly unique in that all of the distribution is controlled from
    a single binary you download and run locally. It will automatically detect which
    containerization or VM manager you have locally and will set up and run a container
    or VM with all of the necessary Kubernetes services in it. That means getting
    started with it is pretty simple.
  prefs: []
  type: TYPE_NORMAL
- en: So let’s install it!
  prefs: []
  type: TYPE_NORMAL
- en: Installing Minikube
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the installation is the same across all platforms because once you have
    the tools installed, they will be your gateway to the VM running your Kubernetes
    installation. To get started, just skip to the section that applies to your operating
    system. Once you have the tool up and running, you can follow the shared documentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need two tools to use Minikube effectively: `minikube` and `kubectl`. For
    our simple installation, we’re going to leverage the fact that both of these commands
    are static binaries with no outside dependencies, which makes them easy to install.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are several other ways to install Minikube. We’re going to show you what
    we think is the simplest path on each platform. If you have strong preferences
    about how to install these applications, feel free to use your preferred approach.
    On Windows, for example, you might prefer to use the [Chocolatey package manager](https://chocolatey.org),
    or the [Snap package system](https://snapcraft.io) on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: macOS
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just as in [Chapter 3](ch03.html#installing_docker), you will need to have
    Homebrew installed on your system. If you don’t, go back to [Chapter 3](ch03.html#installing_docker)
    and make sure you have it set up. Once you do, it’s trivial to install the `minikube`
    client:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This will cause Homebrew to download and install Minikube. It will look something
    like this depending, on your configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s it! Let’s test to make sure it’s in your path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Homebrew on *arm64* systems install into */opt/homebrew/bin* instead of */usr/local/bin*.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t get a response, you will need to make sure you have */usr/local/bin*
    and */opt/homebrew/bin* in your `PATH` environment variable. Assuming that passes,
    you now have the `minikube` tool installed.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` should have been automatically installed since it is a dependency
    of `minikube`, but you can also do it explicitly with `brew` as well. Generally,
    the version of `kubectl` in Homebrew will match the current release of `minikube`,
    so using `brew install` should help prevent mismatches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll test that the same way we tested `minikube`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We’re good to go!
  prefs: []
  type: TYPE_NORMAL
- en: Windows
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As with installing Docker Desktop on Windows, you may want to install Hyper-V
    or another supported virtualization platform to run a Kubernetes VM. To install
    `minikube`, you simply download the binary and put it in a place you have in your
    `PATH` so that you can execute it on the command line. You can download the [most
    recent release of `minikube` from GitHub](https://github.com/kubernetes/minikube/releases/latest).
    You’ll want to rename the Windows executable that you download to *minikube.exe*;
    otherwise, you’ll be doing a lot more typing than you probably want!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can find more details about the Windows install process and that binary
    executable from the [Minikube install documentaton](https://minikube.sigs.k8s.io/docs/start).
  prefs: []
  type: TYPE_NORMAL
- en: 'You then need to get the latest Kubernetes CLI tool, `kubectl`, to interact
    with your distribution. Unfortunately, there is not a */latest* path for downloading
    that. So, to make sure you have the latest version, you need to [get the latest
    version](https://storage.googleapis.com/kubernetes-release/release/stable.txt)
    from the website and then plug it into a URL, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*https://storage.googleapis.com/kubernetes-release/release/<VERSION>/bin/windows/amd64/kubectl.exe*.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve downloaded that, you again need to make sure it’s accessible from
    your `PATH` to make the rest of our exploration easier.
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'On Linux, you will want to have Docker installed and should consider installing
    either KVM (Linux’s Kernel-based Virtual Machine) or VirtualBox so that `minikube`
    can create and manage a Kubernetes VM for you. Because `minikube` is just a single
    binary, once you have it installed, there is no need to install any additional
    packages. And, because `minikube` is a statically linked binary, it should pretty
    much work on any distribution you want to run it on. Although we could do all
    the installation in a one-liner, we are going to break it up into a few steps
    to make it easier to understand and troubleshoot. Note that at the time of this
    writing, the binary is hosted on *googleapis*, which usually maintains very stable
    URLs. So, here we go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You’ll need to make sure that */usr/local/bin* is in your path. Now that we
    have `minikube`, we also need to fetch `kubectl`, which we can do like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the URLs in the example has been continued on the following line so that
    it fits in the margins. You may find that you need to reassemble the URL and remove
    the backslashes for the command to work properly in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: That’s it for installation—we’re ready to go.
  prefs: []
  type: TYPE_NORMAL
- en: Running Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have the `minikube` tool, we can use it to bootstrap our Kubernetes
    cluster. This is normally pretty straightforward. You usually don’t need to do
    any configuration beforehand. In this example, you will see that `minikube` decided
    to use the *docker driver*, although there are others that could be selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start `minikube`, go ahead and run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: So what did we just do? Minikube packs a lot into that one command. In this
    case, we launched a single Linux container that is providing us a functioning
    Kubernetes installation on our local system. If we had used one of the virtualization
    drivers with `minikube`, then we would have created a complete VM running Kubernetes
    instead on a single container.
  prefs: []
  type: TYPE_NORMAL
- en: 'It then runs all of the necessary components of Kubernetes inside Linux containers
    on the host. You can easily explore the `minikube` container or VM to see what
    you got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'On your Kubernetes cluster, you probably won’t be using SSH to get into the
    command line that often. But we want to see what’s installed and get a handle
    on the fact that when we run `minikube`, we’re controlling an environment that
    is running many processes. Let’s take a look at what is running on the Docker
    instance on our Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We won’t dive too much into what each component is, but by now you should hopefully
    see how the mechanism works. Also, it’s pretty easy to upgrade the components
    since they are just containers, are versioned, and can be pulled from an upstream
    container repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and exit the shell that you have on the Minikube system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: minikube commands
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the interest of space and time, we won’t go through all of the commands for
    `minikube`. We encourage you to run it without any options, take a look at the
    output, and play around with what’s available. That being said, let’s take a quick
    look at some of the most interesting commands. We’ll cover a few more later in
    the course of installing an application stack, but here’s a quick survey.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see what was going on inside the system, earlier we used `minikube ssh`,
    which is great for debugging or inspecting the system directly. Without directly
    accessing the Minikube system, we can always check on the cluster status using
    another `minikube` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows us that everything is looking good. Two other useful commands include:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Command | Action |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `minikube ip` | Retrieve the IP address of the Minikube VM. |'
  prefs: []
  type: TYPE_TB
- en: '| `minikube update-check` | Check your version of Minikube against the most
    recent release. |'
  prefs: []
  type: TYPE_TB
- en: To apply an upgrade, you can simply use the same mechanism you used to install
    it originally.
  prefs: []
  type: TYPE_NORMAL
- en: Critically, the `minikube status` command also shows us that the `kubeconfig`
    is properly configured. We will need this so that `kubectl` knows how to connect
    to our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We started the Kubernetes cluster with `minikube start`. As you might expect,
    following the style of Docker CLI arguments, `minikube stop` will stop all the
    Kubernetes components and the Linux container or VM. To completely clean up your
    environment, you can also delete the cluster by running `minikube delete`.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Dashboard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have Minikube up and running, we don’t just have the command-line
    tools to interact with; we have a whole Kubernetes Dashboard installed that we
    can explore. We can reach it via the `minikube dashboard` command. Go ahead and
    run that—it should launch your web browser pointed to the correct IP address and
    port of the Kubernetes Dashboard! There is a lot of stuff on the dashboard, and
    we’re not able to cover it all, but feel free to click around and explore. Depending
    on your previous exposure to Kubernetes, some of the terms in the dashboard’s
    sidebar will be familiar to you, but many of them may be completely foreign. If
    you don’t have a computer in front of you, [Figure 10-2](#figure10-2) shows a
    screenshot of what an empty Minikube installation looks like from the Service
    link in the dashboard sideboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![Kubernetes Dashboard](assets/dur3_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Kubernetes Dashboard (example)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you explore the Nodes link under Cluster in the left sidebar, you should
    see a single node in the cluster, named `minikube`. This is the container or VM
    that we started, and the dashboard, like the other components, is hosted in one
    of the containers we saw when we connected to the Minikube system earlier. We’ll
    take another look at the dashboard when we’ve deployed something into our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes exposes almost everything that you see on the dashboard with the
    `kubectl` command as well, which makes it very scriptable with shell scripts.
  prefs: []
  type: TYPE_NORMAL
- en: For example, running `kubectl get services` or `kubectl get nodes` should show
    you the same information that you can see on the dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: While clicking around, you may notice that Kubernetes itself shows up as a component
    inside the system, just like your applications will.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You will need to type Ctrl-C to exit the `minikube dashboard` process and return
    to your terminal prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes containers and pods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have a Kubernetes cluster up and running, and you’ve seen how easy
    that is to do locally, we need to pause to talk about a concept that Kubernetes
    adds on top of the container abstraction. Kubernetes came out of the experiences
    that Google had running its massive platform. Google encountered most of the situations
    you might see in a production platform and had to work out concepts to make it
    easier to understand and solve the kinds of problems you run into when managing
    a large installation. In doing so, Google created a complex set of new abstractions.
    Kubernetes embraces many of these and thus has a whole vocabulary unto itself.
    We won’t try to get into all of these, but it’s important to understand the most
    central of these new abstractions—​a concept that sits a layer above the container
    and is known as a *pod*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term *pod* came about because the Docker mascot is Moby, the whale, and
    a group of whales is called a *pod*.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes parlance, a pod is one or more containers sharing the same cgroups
    and namespaces. You can also isolate the containers themselves from one another
    inside the same pod using cgroups and namespaces. A pod is intended to encapsulate
    all of the processes or applications that need to be deployed together to create
    a functioning unit, which the scheduler can then manage. All of the containers
    in the pod can talk to one another on `localhost`, which eliminates any need to
    discover one another. So why not just deploy a big container with all the applications
    inside it? The advantage of a pod over a massive container is that you can still
    resource-limit the individual application separately and leverage the large library
    of public Linux containers to construct your application.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Kubernetes administrators often leverage the pod abstraction to
    have a container run on pod startup to make sure things are configured properly
    for the others, to maintain a shared resource, or to announce the application
    to others, for example. This allows you to make finer-grained containers than
    you might if you have to group things into the same container. Another nice part
    of the pod abstraction is the ability to share mounted volumes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pods have a life span much like a Linux container. They are essentially ephemeral
    and can be redeployed to new hosts according to the lifecycle of the application
    or the host it runs on. Containers in a pod even share the same IP address when
    facing the outside world, which means they look like a single entity from the
    network level. Just as you would run only one instance of an application per container,
    you generally run one instance of a given container inside a pod. The easiest
    way to think about pods is that they are a group of Linux containers that work
    together as if they were one container, for most purposes. If you need only one
    container, then you still get a pod deployed by Kubernetes, but that pod contains
    only one container. The nice thing about this is that there is only one abstraction
    as far as the Kubernetes scheduler is concerned: the pod. Containers are managed
    by some of the runtime pieces that construct the pod and also by the configuration
    that you use to define them.'
  prefs: []
  type: TYPE_NORMAL
- en: One critical difference between a pod and a container is that you don’t construct
    pods in a build step. They are a runtime abstraction that is defined in a JSON
    or YAML manifest and lives only inside Kubernetes. So you build your Linux containers
    and send them to a registry, then define and deploy your pods using Kubernetes.
    In reality, you don’t usually directly describe a pod either; the tools generate
    it for you through the concept of a deployment. But the pod is the unit of execution
    and scheduling in a Kubernetes cluster. There is a lot more to it, but that’s
    the basic concept, and it’s probably easiest to understand with a simple example.
    The pod abstraction is more complicated than thinking of your system in terms
    of individual containers, but it can be pretty powerful.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s deploy something
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with pods in Kubernetes, we usually manage them through the abstraction
    of a *deployment*. A deployment is just a pod definition with some additional
    information, including health monitoring and replication configuration. It contains
    the definition of the pod and a little metadata about it. So let’s look at a basic
    deployment and get it running.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest thing we can deploy on Kubernetes is a pod that contains just one
    container. We are going to use the [`httpbin`](https://httpbin.org) application
    to explore the basics of deployment on Kubernetes, and we’ll call our deployment
    `hello-minikube`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve used the `minikube` command, but to get things done on Kubernetes itself,
    we now need to leverage the `kubectl` command we installed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'To see what that did for us, we can use the `kubectl get all` command to list
    the most important objects that are now in our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With that one command, Kubernetes created a deployment, a ReplicaSet to manage
    scaling, and a pod. We want to ensure that our pod shows a `STATUS` of `Running`.
    If yours isn’t, just wait and run the command a couple more times until you see
    the status change. The `service/kubernetes` entry is a running service that represents
    Kubernetes itself. But where is our service? We can’t get to it yet. It’s essentially
    in the same state a Linux container would be if you didn’t tell it to expose any
    ports. So we need to tell Kubernetes to do that for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This has now created a service we can reach and interact with. A *service*
    is a wrapper for one or more deployments of an application and can tell us how
    to contact the application. In this case, we get a `NodePort`, which exposes a
    port on every node in the cluster that will be routed to the underlying pods.
    Let’s get Kubernetes to tell us how to get to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You might think you could now connect to *http://10.105.184.177:8080* to get
    to our service. But those addresses are not reachable from your host system because
    of the container or VM in which Minikube is running. So we need to get `minikube`
    to tell us where to find the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In some configurations, you may see a message like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that transparently wiring the networking from your host to the
    Kubernetes services is not possible at the moment, and you will need to leave
    the command running while you explore your application. You can use a local web
    browser or open up another terminal to run commands like `curl`.
  prefs: []
  type: TYPE_NORMAL
- en: When you are done, you can type Ctrl-C in the original terminal session to kill
    the `minikube service` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The nice thing about this command, like many of the other Kubernetes commands,
    is that it is scriptable and command-line friendly under normal circumstances.
    If we want to open it with `curl` on the command line, we can often just include
    the `minikube` command call in our request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`httpbin` is a simple HTTP request and response API that can be used to test
    and confirm HTTP services. Not the world’s most exciting application, but you
    can see that we are able to contact our service and get a response back from it
    via `curl`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the simplest use case. We didn’t configure much and relied on Kubernetes
    to do the right thing using its defaults. In the next step, we’ll take a look
    at something more complicated. But first, let’s shut down our new service and
    deployment. It takes two commands to do that: one to remove the service and the
    other to delete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Deploying a realistic stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s now deploy something that looks more like a production stack. We’ll deploy
    an application that can fetch PDF documents from an S3 bucket, cache them on disk
    locally, and rasterize individual pages to PNG images on request, using the cached
    document. To run this application, we’ll want to write our cache files somewhere
    other than inside the container. We want to have them go somewhere a little more
    permanent and stable. And this time we want to make things repeatable so that
    we’re not deploying our application through a series of CLI commands that we need
    to remember and hopefully get right each time. Kubernetes, much like Docker Compose,
    lets us define our stack in one or more YAML files that contain all of the definitions
    we care about in one place. This is what you want in a production environment
    and is similar to what you’ve seen for the other production tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service we’ll now create will be called `lazyraster` (as in “rasterize
    on demand”), and each time you see that in the YAML definition, you’ll know we’re
    referring to our application. Our persistent volume will be called `cache-data`.
    Again, Kubernetes has a huge vocabulary that we can’t entirely address here, but
    to make it clear what we’re looking at, we need to introduce two more concepts:
    `PersistentVolume` and `PersistentVolumeClaim`. A `PersistentVolume` is a physical
    resource that we provision inside the cluster. Kubernetes has support for many
    kinds of volumes, from local storage on a node to [Amazon Elastic Block Store
    (Amazon EBS) volumes](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AmazonEBS.html)
    on AWS and similar on other cloud providers. It also supports [Network File System
    (NFS)](https://en.wikipedia.org/wiki/Network_File_System) and other more modern
    network filesystems. A `PersistentVolume` stores data with a lifecycle that is
    independent of our application or deployments. This lets us store data that persists
    between application deployments. For our cache, that’s what we’ll use. A `PersistentVolumeClaim`
    is a link between the physical resource of the `PersistentVolume` and the application
    that needs to consume it. We can set a policy on the claim that allows either
    a single read/write claim or many read claims. For our application we just want
    a single read/write claim to our `cache-data` `PersistentVolume`.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you want more detail about some of the concepts we’ve talked about here,
    the Kubernetes project maintains a [glossary](https://kubernetes.io/docs/reference/glossary/?fundamental=true)
    of all the terms involved in operating Kubernetes. This can be very helpful. Each
    entry in the glossary is also linked to much more in-depth detail on other pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check out the file we will be using in this section by running the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The URL in the example has been continued on the following line so that it fits
    in the margins. You may find that you need to reassemble the URL and remove the
    backslashes for the command to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by looking at the manifest YAML file, called *lazyraster-service.yaml*.
    The full manifest contains multiple YAML documents separated by `---`. We will
    discuss each section individually here.
  prefs: []
  type: TYPE_NORMAL
- en: Service definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The first section defines our `Service`. The second and third sections, which
    we’ll see in a moment, respectively define our `PersistentVolumeClaim` and then
    our actual `Deployment`. We’ve told Kubernetes that our service will be called
    `lazyraster` and that it will be exposed on port 8000, which maps to the actual
    8000 in our container. We’ve exposed that with the `NodePort` mechanism, which
    simply makes sure that our application is exposed on the same port on each host,
    much like the `--publish` flag to `docker container run`. This is helpful with
    `minikube` since we’ll run only one instance, and the `NodePort` type makes it
    easy for us to access it from our computer just like we did earlier. As with many
    parts of Kubernetes, there are several options other than `NodePort`, and you
    can probably find a mechanism that’s ideal for your production environment. `NodePort`
    is good for `minikube`, but it might work well for more statically configured
    load balancers as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, back to our `Service` definition. The `Service` is going to be connected
    to the `Deployment` via the `selector`, which we apply in the `spec` section.
    Kubernetes widely uses labels as a way to reason about similar components and
    to help tie them all together. Labels are key/value pairs that are arbitrarily
    defined and that can then be queried to identify pieces of your system. Here the
    `selector` tells Kubernetes to look for `Deployments` with the label `app: lazyraster`.
    Notice that we also apply the same label to the `Service` itself. That’s helpful
    if we want to identify all the components together later, but it’s the `selector`
    section that ties the `Deployment` to our `Service`. So we now have a `Service`,
    but it doesn’t do anything yet. We need more definitions to make Kubernetes do
    what we want.'
  prefs: []
  type: TYPE_NORMAL
- en: PersistentVolumeClaim definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The next section defines our `PersistentVolumeClaim` and likewise the `PersistentVolume`
    that backs it. A `PersistentVolumeClaim` is a way to name a volume and claim that
    you have a token to access that particular volume in a particular way. Notice,
    though, that we didn’t define the `PersistentVolume` here. That’s because Kubernetes
    is doing that work for us using what it calls *Dynamic Volume Provisioning*. In
    our case, the use is pretty simple: we want a read/write claim to a volume, and
    we’ll let Kubernetes put that in a volume container for us. But you can imagine
    a scenario where an application is going to be deployed into a cloud provider
    and where dynamic provisioning would truly come into its own. In that scenario,
    we don’t want to have to make separate calls to have our volume created in the
    cloud for us. We want Kubernetes to handle that. That’s what Dynamic Volume Provisioning
    is all about. Here, it will just create a container for us to hold our persistent
    data, and mount it into our pod when we stake our claim. We don’t do a lot in
    this section except name it, ask for 100 MB of data, and tell Kubernetes it’s
    a read/write mount-once-only volume.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There’s a large number of possible volume providers in Kubernetes. Which ones
    are available to you is in part determined by which provider or cloud service
    you are running on. You should take a look and see which ones make the most sense
    for you when you are preparing to head into production.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The `Deployment` creates the pods for us and uses the Linux container for our
    application. We define some metadata about the application, including its name
    and one label, just like we did for the other definitions. We also apply another
    `selector` here to find the other resources we’re tied to. In the `strategy` section,
    we say we want to have a `RollingUpdate`, which is a strategy that causes our
    pods to be cycled through one by one during deployment. We could also pick `Recreate`,
    which would simply destroy all existing pods and then create new ones afterward.
  prefs: []
  type: TYPE_NORMAL
- en: In the `template` section, we define how to stamp out copies of this deployment.
    The container definition includes the Docker image name, the ports to map, volumes
    to mount, and some environment variables that the `lazyraster` application needs.
    The very last part of the `spec` asks to have our `PersistentVolumeClaim` named
    `cache-data-claim`.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it for the application definition. Now let’s stand it up!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many more options and a rich set of directives you can specify here
    to tell Kubernetes how to handle your application. We’ve walked through a couple
    of simple options, but we encourage you to explore the Kubernetes documentation
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we continue, let’s see what’s in our Kubernetes cluster by using the
    `kubectl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We have only one thing defined at the moment, a service called `service/kubernetes`.
    A naming convention used widely in Kubernetes is to preface the type of object
    with the object `Kind`, which is sometimes shortened to a two- or three-letter
    abbreviation. Sometimes you will see `service` represented as `svc`. If you are
    curious, you can see all of the resources and their short names by running the
    command `kubectl api-resources`. So let’s go ahead and get our service, deployment,
    and volume into the cluster!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'That output looks like what we expected: we have a service, a persistent volume
    claim, and a deployment. So let’s see what’s in the cluster now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that a bunch more happened behind the scenes. And also, where is
    our volume or persistent volume claim? We have to ask for that separately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`kubectl get all` does nothing of the sort. It would be more aptly named `get
    all-of-the-most-common-resources`, but there are several other resources you can
    fetch. The Kubernetes project hosts a handy [cheat sheet](https://kubernetes.io/docs/reference/kubectl/cheatsheet)
    to make this more discoverable.'
  prefs: []
  type: TYPE_NORMAL
- en: So what about that `replicaset.apps` that appeared in the `get all` output?
    That is a ReplicaSet. A ReplicaSet is a piece of Kubernetes that is responsible
    for making sure that our application is running the right number of instances
    all the time and that they are healthy. We don’t normally have to worry about
    what happens inside the ReplicaSet because the deployment we created manages it
    for us. You can manage the ReplicaSet yourself if need be, but most of the time
    you won’t need to or want to.
  prefs: []
  type: TYPE_NORMAL
- en: 'We didn’t tell `kubectl` any specific number of instances, so we got one. And
    we can see that both the desired and current states match. We’ll take a look at
    that in a moment. But first, let’s connect to our application and see what we’ve
    got:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: You will probably get a different IP address and port back. That’s fine! This
    is very dynamic stuff. And that’s why we use the `minikube` command to manage
    it for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, remember that `minikube` will warn you if you need to keep the `service`
    command running while you explore the `lazyraster` service. So grab the address
    that came back, open your web browser, and paste it into the URL bar like this:
    *http://<192.168.99.100:32185>/documents/docker-up-and-running-public/sample.pdf?page=1*.
    You’ll need to substitute the IP and port into the URL to make it work for you.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll need to be connected to the internet because the `lazyraster` application
    is going to go out to the internet, fetch a PDF from a public S3 bucket, and then
    render the first page from the document as a PNG in a process called *rasterization*.
    If everything worked, you should see a copy of the front cover of an earlier edition
    of this book! This particular PDF has two pages, so feel free to try changing
    the argument to `?page=2`. If you do that, you may notice it renders *much* faster
    than the first page. That’s because the application is using our persistent volume
    to cache the data. You can also specify `width=2048` or ask for a JPEG instead
    of a PNG with `imageType=image/jpeg`. You could rasterize the front page as a
    very large JPEG, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*http://<192.168.99.100:32185>/documents/docker-up-and-running-public/sample.pdf?page=1&imageType=image/jpeg&width=2048*'
  prefs: []
  type: TYPE_NORMAL
- en: If you have a public S3 bucket with other PDFs in it, you can simply substitute
    the bucket name for `docker-up-and-running-public` in the URL to hit your bucket
    instead. If you want to play with the application some more, check out [the *Nitro*/*lazyraster*
    repo on GitHub](https://github.com/Nitro/lazyraster).
  prefs: []
  type: TYPE_NORMAL
- en: Scaling up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In real life you don’t just deploy applications; you operate them as well. One
    of the huge advantages of scheduled workloads is the ability to scale them up
    and down at will, within the resource constraints available to the system. In
    our case, we only have one Minikube node, but we can still scale up our service
    to better handle load and provide more reliability during deployments. Kubernetes,
    as you might imagine, allows scaling up and down quite easily. For our service,
    we will need only one command to do it. Then we’ll take another look at the `kubectl`
    output and also at the Kubernetes Dashboard we introduced earlier so we can prove
    that the service scaled.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes, the thing we will scale is not the service; it’s the deployment.
    Here’s what that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Great, that did something! But what did we get?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have two instances of our application running. Let’s see what we got
    in the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: We asked for logs for the deployment, but Kubernetes tells us two pods are running,
    so it simply picked one of them to show us the logs from. We can see the replica
    starting up. If we want to specify a particular instance to look at, we can ask
    for the logs for that pod with something like `kubectl logs pod/lazyraster-644cb5c66c-zsjxd`,
    using the output from `kubectl get pods` to find the pod in question.
  prefs: []
  type: TYPE_NORMAL
- en: We now have a couple of copies of our application running. What does that look
    like on the Kubernetes Dashboard? Let’s navigate there with `minikube dashboard`.
    Once we’re there, we’ll select “Workloads - Deployments” from the left sidebar
    and then click on the `lazyraster` deployment, which should display a screen that
    looks like [Figure 10-3](#figure10-3).
  prefs: []
  type: TYPE_NORMAL
- en: '![Lazyraster Service Dashboard](assets/dur3_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. `lazyraster` service dashboard (example)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We encourage you to click around some more in the Kubernetes Dashboard to see
    what else is presented. With the concepts you’ve picked up here, a lot should
    be clearer now, and you can probably figure out some more on your own. Likewise,
    `kubectl` has a lot of other options available as well, many of which you’ll need
    in a real production system. The [cheat sheet we discussed earlier](https://kubernetes.io/docs/reference/kubectl/cheatsheet)
    is a real lifesaver here!
  prefs: []
  type: TYPE_NORMAL
- en: As always, you can type Ctrl-C at any time to exit the running `minikube dashboard`
    command.
  prefs: []
  type: TYPE_NORMAL
- en: kubectl API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We haven’t shown you an API yet, and, as we’ve discussed with Docker, it can
    be really useful to have a simple API to interact with for scripting, programming,
    and other general operational needs. You can write programs to talk directly to
    the Kubernetes API, but for local development and other simple use cases, you
    can use `kubectl` as a nice proxy to Kubernetes, and it presents a clean API that
    is accessible with `curl` and JSON command-line tools. Here’s an example of what
    you can do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ve now got `kubectl` itself serving up a web API on the local system! You’ll
    need to read more about what’s possible, but let’s get it to show us the individual
    instances of the `lazyraster` application. We can do that by opening the following
    URL in a browser or by using `curl` in another terminal window: *http://localhost:8001/api/v1/namespaces/default/endpoints/lazyraster*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a lot of output here, but the part we care about is the `subsets`
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: What’s interesting here is that we can see that both instances are running on
    the Minikube host and that they have different IP addresses. If we were building
    a cloud-native application that needed to know where the other instances of the
    application were running, this would be a good way to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can type Ctrl-C at any time to exit the running `kubectl proxy` processes,
    and then you can remove the deployment and all of its components by running the
    following command. It may take Kubernetes a minute or so to delete everything
    and return you to the terminal prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'And then finally, you can go ahead and remove your Minikube cluster if you
    are done with everything in it for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Kubernetes is a really big system, with great community involvement. We’ve just
    shown you the tip of the iceberg with Minikube, but if you are interested, there
    are many other Kubernetes distributions and tools to explore.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Desktop-Integrated Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Docker Desktop comes with support for an integrated single-node Kubernetes cluster
    that can be run by simply enabling an option in the application preferences.
  prefs: []
  type: TYPE_NORMAL
- en: The integrated Kubernetes cluster is not easily configurable, but it does provide
    a very accessible option for those who simply need to verify some basic functionality
    against a current Kubernetes installation.
  prefs: []
  type: TYPE_NORMAL
- en: To enable Docker Desktop’s built-in Kubernetes functionality, launch Docker
    Desktop and then open up Preferences from the Docker whale icon in your task/menu
    bar. Then select the Kubernetes tab, click Enable Kubernetes, and finally click
    the “Apply & Restart” button to make the required changes to the VM. The first
    time you do this, Docker will utilize the [`kubeadm`](https://kubernetes.io/docs/reference/setup-tools/kubeadm)
    command to set up the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are interested in a bit more information about how the Docker Desktop-integrated
    Kubernetes is set up, Docker has a good [blog post](https://www.docker.com/blog/how-kubernetes-works-under-the-hood-with-docker-desktop)
    that covers some of these details.
  prefs: []
  type: TYPE_NORMAL
- en: This will create a new `kubectl` context called `docker-desktop` and should
    automatically switch you to this context.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can confirm which context you are currently set to by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'If you need to change the current context, you can do so like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'And finally, if you want to completely unset the current context, you can use
    this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Once this cluster is running, you can interact with it just like any other Kubernetes
    cluster via the `kubectl` command. Whenever you shut down Docker Desktop, this
    will also shut down the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to completely disable this Kubernetes cluster, go back into the
    Preferences panel, select the Kubernetes tab, and un-check Enable Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kind
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The final option that we are going to discuss here is `kind`, a very simple
    but useful tool that allows you to manage a Kubernetes cluster made up of one
    or more Linux containers running in Docker. The tool name, `kind`, is an acronym
    that means “Kubernetes in Docker” but also refers to the fact that object types
    in Kubernetes are identified in the API by a field called `Kind`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You will find that searching for this tool on the web can be a bit difficult,
    but you can always find the tool and documentation on its primary [website](https://kind.sigs.k8s.io).
  prefs: []
  type: TYPE_NORMAL
- en: '`kind` provides a nice middle ground between the simplistic Kubernetes cluster
    that is embedded into the Docker VM and the `minikube` VM, which can be overly
    complex at times. `kind` is distributed as a single binary and can be installed
    with your favorite package manager or by simply navigating to the [`kind` project
    releases page](https://github.com/kubernetes-sigs/kind/releases) and downloading
    the most recent release for your system. If you manually download the binary,
    make sure that you rename the binary to `kind`, copy it to a directory in your
    path, and then ensure that it has the correct permissions so that users can run
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once `kind` is installed, you can try to create your first cluster with it
    by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: By default, this command will spin up a single Docker container that represents
    a one-node Kubernetes cluster, using the most current stable Kubernetes release
    that `kind` currently supports.
  prefs: []
  type: TYPE_NORMAL
- en: '`kind` has already set the Kubernetes current context to point at the cluster,
    so we can start running `kubectl` commands immediately:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see a redacted version of the information used by `kubectl` to connect
    to the Kubernetes server by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '`kind` has some [advanced features](https://kind.sigs.k8s.io/docs/user/quick-start/#advanced)
    that can generally be controlled by passing in a configuration file with the `--config`
    argument when spinning up the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may find some of the follwing features useful:'
  prefs: []
  type: TYPE_NORMAL
- en: Changing the version of Kubernetes that is used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinning up multiple worker nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spinning up multiple control plane nodes for HA testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mapping ports between Docker and the local host system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling and disabling [Kubernetes feature gates](https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exporting control plane component logs with `kind export logs`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One thing to remember when using `kind` is that Kubernetes is running inside
    one or more containers, which are potentially running inside a Linux VM when you
    are using something like Docker Desktop. This may mean that you need to set up
    some additional port forwarding when you spin up the cluster. This can be done
    using the `extraPortMappings` setting in the `kind` config.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you can go ahead and delete the cluster by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Amazon ECS and Fargate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most popular cloud providers is Amazon via their AWS offerings.
    Support for running containers natively has existed in [AWS Elastic Beanstalk](https://amzn.to/2wNa1rL)
    since mid-2014\. But that service assigns only a single container to an Amazon
    instance, which means that it’s not ideal for short-lived or lightweight containers.
    Amazon Elastic Compute Cloud (Amazon EC2) itself is a great platform for hosting
    your own Docker environment, though, and because Docker is powerful, you don’t
    necessarily need much on top of your instances to make this a productive environment
    to work in. But Amazon has spent a lot of engineering time building a service
    that treats containers as first-class citizens: the Amazon Elastic Container Service
    (Amazon ECS). In the last few years, Amazon has built upon this support with products
    like the Elastic Kubernetes Services (EKS) and AWS Fargate.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fargate is simply a marketing label Amazon uses for the feature of ECS that
    makes it possible for AWS to automatically manage all the nodes in your container
    cluster so that you can focus on deploying your service.
  prefs: []
  type: TYPE_NORMAL
- en: The ECS is a set of tools that coordinates several AWS components. With ECS,
    you have a choice of whether or not you will run the Fargate tooling on top. If
    you do, then you don’t need to handle as much of the work. If you don’t, then
    in addition to the cluster nodes to handle your workload, you will also need to
    add one or more EC2 instances to the cluster running Docker and Amazon’s special
    ECS agent. If you run Fargate, then the cluster is automatically managed for you.
    In either case, you spin up the cluster and then push your containers into it.
  prefs: []
  type: TYPE_NORMAL
- en: The [Amazon ECS agent](https://github.com/aws/amazon-ecs-agent) we just mentioned
    works with the ECS service to coordinate your cluster and schedule containers
    to your hosts. You will only be directly exposed to this when you manage a traditional
    non-Fargate ECS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Core AWS Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rest of this section assumes that you have access to an AWS account and
    some familiarity with the service. You can learn about pricing and create a new
    account at [*https://aws.amazon.com/free*](https://aws.amazon.com/free). Amazon
    offers a free service tier, which may be enough for you to experiment with if
    you don’t already have a paid account. After you have your AWS account set up,
    you will need at least one administrative user, a key pair, an Amazon virtual
    private cloud (AWS VPC), and a default security group in your environment. If
    you do not already have these set up, follow the directions in the [Amazon documentation](https://amzn.to/2FcPDSL).
  prefs: []
  type: TYPE_NORMAL
- en: IAM Role Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Amazon’s Identity and Access Management (Amazon IAM) roles are used to control
    what actions a user can take within your cloud environment. We need to make sure
    we can grant access to the right actions before moving on with the ECS. To work
    with the ECS, you must create a role called `ecsInstanceRole` that has the `AmazonEC2ContainerServiceRole`
    managed role attached to it. The easiest way to do this is by logging in to the
    [AWS console](https://console.aws.amazon.com) and navigating to [Identity and
    Access Management](https://console.aws.amazon.com/iam/home):'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Check to ensure that you don’t already have the proper role. If it already exists,
    then you should double-check that it is set up properly, as these directions have
    changed a bit over the years.
  prefs: []
  type: TYPE_NORMAL
- en: In the left sidebar, click Roles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, click the “Create role” button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under AWS Service, select Elastic Container Service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Under “Select your use case,” select Elastic Container Service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click Next: Permissions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click Next: Review.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Role Name, type **`ecsInstanceRole`**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click “Create role.”
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are interested in storing container configuration in an S3 object storage
    bucket, take a look at the Amazon ECS container agent configuration [documentation](https://amzn.to/2PNapOL).
  prefs: []
  type: TYPE_NORMAL
- en: AWS CLI Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Amazon supplies command-line tools that make it easy to work with their API-driven
    infrastructure. You will need to install a very recent version of the AWS CLI
    tools. Amazon has [detailed documentation](https://amzn.to/1PCpPNA) that covers
    the installation of their tools, but the basic steps are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here we’ll cover the native installation on a few different OSes, but be aware
    that you can also run the [AWS CLI via a Docker container](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-docker.html#cliv2-docker-install)!
    You can feel free to skip to the one you care about. If you’re curious or just
    like installation instructions, by all means, read them all!
  prefs: []
  type: TYPE_NORMAL
- en: macOS
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#installing_docker), we discussed installing Homebrew.
    If you previously did this, you can install the AWS CLI using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: Windows
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon provides a standard MSI installer for Windows, which can be downloaded
    from Amazon S3 for your architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[32-bit Windows](https://s3.amazonaws.com/aws-cli/AWSCLI32.msi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64-bit Windows](https://s3.amazonaws.com/aws-cli/AWSCLI64.msi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other
  prefs: []
  type: TYPE_NORMAL
- en: 'The Amazon CLI tools are written in Python. So on most platforms, you can install
    the tools with the Python `pip` package manager by running the following from
    a shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Some platforms won’t have `pip` installed by default. In that case, you can
    use the `easy_install` package manager, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Quickly verify that your AWS CLI version is at least 1.7.0 with the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'To configure the AWS CLI tool, ensure that you have access to your AWS access
    key ID and AWS secret access key, and then run the `configure` command. You will
    be prompted for your authentication information and some preferred defaults:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, it’s a really good idea to test that the CLI tools are working
    correctly before proceeding. You can easily do that by running the following command
    to list the IAM users in your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Assuming everything went according to plan and you chose JSON as your default
    output format, you should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Container Instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing you need to do after installing the required tools is to create
    at least a single cluster that your Docker hosts will register with when they
    are brought online.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The default cluster is imaginatively named *default*. If you keep this name,
    you do not need to specify `--cluster-name` in many of the commands that follow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you need to do is create a cluster in the container service.
    You will then launch your tasks in the cluster once it’s up and running. For these
    examples, you should start by creating a cluster called `fargate-testing`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Before AWS Fargate was released, you were required to create AWS EC2 instances
    running `docker` and the `ecs-agent`, and add them to your cluster. You can still
    use this approach if you want (`EC2 launch type`), but Fargate makes it much easier
    to run a dynamic cluster that can scale fluidly with your workload.
  prefs: []
  type: TYPE_NORMAL
- en: Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that our container cluster is set up, we need to start putting it to work.
    To do this, we need to create at least one task definition. The Amazon ECS defines
    the term *task definition* as a list of containers grouped together.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create your first task definition, open up your favorite editor, copy in
    the following JSON, and then save it as *webgame-task.json* in your current directory,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can also check out these files and a few others by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The URL has been continued on the following line so that it fits in the margins.
    You may find that you need to reassemble the URL and remove the backslashes for
    the command to work properly.
  prefs: []
  type: TYPE_NORMAL
- en: In this task definition, we are saying that we want to create a task family
    called `fargate-game` running a single container called `web-game` that is based
    on the [*Quantum* web game](https://github.com/stared/quantum-game). As you may
    have seen in an earlier chapter, this Docker image launches a browser-based puzzle
    game that uses real quantum mechanics.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fargate limits some of the options that you can set in this configuration, including
    `networkMode` and the `cpu` and `memory` settings. You can find out more about
    the options in the task definition from the official [AWS documentation](https://amzn.to/2PkliGR).
  prefs: []
  type: TYPE_NORMAL
- en: In this task definition, we define some constraints on memory and CPU usage
    for the container, in addition to telling Amazon whether this container is essential
    to the task. The `essential` flag is useful when you have multiple containers
    defined in a task, and not all of them are required for the task to be successful.
    If `essential` is true and the container fails to start, then all the containers
    defined in the task will be killed and the task will be marked as failed. We can
    also use the task definition to define almost all of the typical variables and
    settings that would be included in a *Dockerfile* or on the `docker container
    run` command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'To upload this task definition to Amazon, you will need to run a command similar
    to what is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then list all of our task definitions by running the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Now you are ready to create your first task in your cluster. You do so by running
    a command like the one shown next. The `count` argument in the command allows
    you to define how many copies of this task you want to be deployed into your cluster.
    For this job, one is enough.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need to modify the following command to reference a valid subnet ID
    and security-group ID from your AWS VPC. You should be able to find these in the
    [AWS console](https://console.aws.amazon.com/vpc/home) or by using the AWS CLI
    commands `aws ec2 describe-subnets` and `aws ec2 describe-security-groups`. You
    can also tell AWS to assign your tasks a public IP address by using a network
    configuration similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Assigning a public IP address may be required if you are using public subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Fargate and the `awsvpc` network require that you have a service-linked role
    for ECS. In the preceding output, you should see a line that ends like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Most of the time this will be autogenerated for you, but you can create it
    manually using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now list all of the services in your cluster with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve all the details about your service, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: This output will tell you a lot about all the tasks in your service. In this
    case, we have a single task running at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `task-definition` value is a name followed by a number (`fargate-game:1`).
    The number is the revision. If you edit your task and re-register it with the
    `aws ecs register-task-definition` command, you will get a new revision, which
    means that you will want to reference that new revision in various commands, like
    `aws ecs update-service`. If you don’t change that number, you will continue to
    launch containers using the older JSON. This versioning makes it very easy to
    roll back changes and test new revisions without impacting all future instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to see what individual tasks are running in your cluster, you can
    run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: Since you only have a single task in your cluster at the moment, this list is
    very small.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get more details about the individual task, you can run the following command
    after substituting the task ID with the correct one from your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: If you notice that the `lastStatus` key is displaying a value of `PENDING`,
    this most likely means that your service is still starting up. You can describe
    the task again to ensure that it has completed transitioning into a `RUNNING`
    state. After verifying that the `lastStatus` key is set to `RUNNING`, you should
    be able to test your container.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Depending on the network setup, your task may not be able to download the image.
    If you see an error like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '`"stoppedReason": "CannotPullContainerError: inspect image has been retried
    5 time(s): failed to resolve ref \"docker.io/spkane/quantum-game:latest\": failed
    to do request: Head [*https://registry-1.docker.io/v2/spkane/quantum-game/manifests/latest*](https://registry-1.docker.io/v2/spkane/quantum-game/manifests/latest):
    dial tcp 54.83.42.45:443: i/o timeout"`'
  prefs: []
  type: TYPE_NORMAL
- en: then you should read through this [troubleshooting guide](https://oreil.ly/FYo9Z).^([1](ch10.html#idm46803129919408))
  prefs: []
  type: TYPE_NORMAL
- en: Testing the Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will need a modern web browser installed on your system to connect to the
    container and test the web game.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous output, you’ll notice that the `privateIPv4Address` for the
    example task was listed as `172.31.42.184`. Yours will be different.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you need more information about the network setup for your task and the EC2
    instance that it is running on, you can grab the `networkInterfaceId` from the
    `aws ecs describe-tasks` output and then append that to the `aws ec2 describe-network-interfaces
    --network-interface-ids` command to get everything you should need, including
    the `PublicIp` value if you configured your service for that.
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you are connected to a network that can reach either the public
    or private IP address of your host, then launch your web browser and navigate
    to port 8080 on that IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the example, this private URL would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: If everything is working as expected, you should be greeted by the *Quantum
    Game* puzzle board.
  prefs: []
  type: TYPE_NORMAL
- en: The official version of the game can be found at [*https://quantumgame.io*](https://quantumgame.io).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We completely understand if you get distracted at this point and stop reading
    for a few hours to try to solve some puzzles and learn a little bit of quantum
    mechanics at the same time. The book won’t notice! Put it down, play the puzzles,
    and pick it back up later.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping the Task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Right, so we have a running task. Now let’s take a look at stopping it. To
    do that, you need to know the task ID. One way to obtain this is by relisting
    all the tasks running in your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also obtain it from the service information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can stop the task by running the following command with the correct
    task ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'If you describe the task again using the same task ID, you should now see that
    the `lastStatus` key is set to `STOPPED`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Listing all the tasks in our cluster should return an empty set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: At this point, you could start creating more complicated tasks that tie multiple
    containers together and rely on the ECS and Fargate tooling to spin up hosts and
    deploy the tasks into your cluster as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to tear down the rest of the ECS environment, you can run the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Wrap-Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve certainly presented you with a lot of options! It’s unlikely
    that you’ll ever need to use all of these, since many of them overlap. However,
    each one has a unique perspective on exactly what a production system should look
    like and what problems are the most important to solve. After exploring all of
    these tools, you should have a pretty good idea of the wide range of options you
    can choose from to build your production Linux container environment.
  prefs: []
  type: TYPE_NORMAL
- en: Underlying all of these tools is Docker’s highly portable image format for Linux
    containers and its ability to abstract away so much of the underlying Linux system,
    which makes it easy to move your applications fluidly between your data center
    and as many cloud providers as you want. Now you just have to choose which approach
    will work best for you and your organization and then implement it.
  prefs: []
  type: TYPE_NORMAL
- en: In the meantime, let’s jump into the next chapter and explore some of the most
    technical topics in the Docker ecosystem, including security, networking, and
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch10.html#idm46803129919408-marker)) Full URL: [*https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html*](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_cannot_pull_image.html)'
  prefs: []
  type: TYPE_NORMAL
