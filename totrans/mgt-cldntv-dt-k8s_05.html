<html><head></head><body><section data-pdf-bookmark="Chapter 4. Automating Database Deployment on Kubernetes with Helm" data-type="chapter" epub:type="chapter"><div class="chapter" id="automating_database_deployment_on_kuber">&#13;
<h1><span class="label">Chapter 4. </span>Automating Database Deployment <span class="keep-together">on Kubernetes</span> with Helm</h1>&#13;
<p><a contenteditable="false" data-primary="automation" data-secondary="of database deployment with Helm" data-type="indexterm" id="auto_helm"/><a contenteditable="false" data-primary="databases" data-secondary="automating deployment with Helm" data-type="indexterm" id="db_helm"/><a contenteditable="false" data-primary="Helm" data-secondary="automating database deployment with" data-type="indexterm" id="helm_ch"/>In the previous chapter, you learned how to deploy both single-node and multinode databases on Kubernetes by hand, creating one element at a time. We did things the “hard way” on purpose to help maximize your understanding of using Kubernetes primitives to set up the compute, network, and storage resources that a database requires. Of course, this doesn’t represent the experience of running databases in production on Kubernetes, for a couple of reasons.</p>&#13;
<p>First, teams typically don’t deploy databases by hand, one YAML file at a time. That can get pretty tedious. And even combining the configurations into a single file could start to get pretty complicated, especially for more sophisticated deployments. Consider the increase in the amount of configuration required in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a> for Cassandra as a multinode database compared with the single-node MySQL deployment. This won’t scale for large enterprises.</p>&#13;
<p>Second, while deploying a database is great, what about keeping it running over time? You need your data infrastructure to remain reliable and performant over the long haul, and data infrastructure is known for requiring a lot of care and feeding. Put another way, the task of running a system is often divided into “day one” (the joyous day when you deploy an application to production) and “day two” (every day after the first, when you need to operate and evolve your application while maintaining high availability).</p>&#13;
<p><a contenteditable="false" data-primary="CI/CD" data-secondary="tools for" data-type="indexterm" id="idm46183197824240"/><a contenteditable="false" data-primary="DevOps" data-type="indexterm" id="idm46183197805088"/>These considerations around database deployment and operations mirror the larger industry trends toward DevOps, an approach in which development teams take a more active role in supporting applications in production. DevOps practices include the use of automation tools for CI/CD of applications, shortening the amount of time it takes for code to get from a developer’s desktop into production.</p>&#13;
<p>In this chapter, we’ll look at tools that help standardize the deployment of databases and other applications. These tools take an infrastructure as code (IaC) approach, allowing you to represent software installation and configuration options in a format that can be executed automatically, reducing the overall amount of configuration code you have to write. We’ll also emphasize data infrastructure operations in these next two chapters and carry that theme throughout the remainder of the book.</p>&#13;
<section data-pdf-bookmark="Deploying Applications with Helm Charts" data-type="sect1"><div class="sect1" id="deploying_applications_with_helm_charts">&#13;
<h1>Deploying Applications with Helm Charts</h1>&#13;
<p><a contenteditable="false" data-primary="applications" data-secondary="deploying with Helm charts" data-type="indexterm" id="idm46183198702992"/><a contenteditable="false" data-primary="charts (Helm)" data-type="indexterm" id="idm46183198703344"/><a contenteditable="false" data-primary="Helm" data-secondary="about" data-type="indexterm" id="idm46183198065648"/><a contenteditable="false" data-primary="Helm" data-secondary="deploying applications using" data-type="indexterm" id="idm46183198015856"/><a contenteditable="false" data-primary="package manager" data-type="indexterm" id="idm46183197779792"/>Let’s start by taking a look at a tool that helps you manage the complexity of managing configurations: <a href="https://helm.sh">Helm</a>. This package manager for Kubernetes is open source and a CNCF <a href="https://oreil.ly/cDjD3">graduated project</a>. The concept of a package manager is a common one across multiple programming languages, such as <code>pip</code> for Python, the Node Package Manager (NPM) for JavaScript, and Ruby’s Gems feature. Package managers for specific operating systems also exist, such as Apt for Linux, or Homebrew for macOS. As shown in <a data-type="xref" href="#helmcomma_a_package_manager_for_kuberne">Figure 4-1</a>, the essential elements of a package manager system are the packages, the registries where the packages are stored, and the package manager application (or <em>client</em>), which helps the chart developers register charts and allows chart users to locate, install, and update packages on their local systems.</p>&#13;
<figure><div class="figure" id="helmcomma_a_package_manager_for_kuberne">&#13;
<img alt="Helm, a package manager for Kubernetes" src="assets/mcdk_0401.png"/>&#13;
<h6><span class="label">Figure 4-1. </span>Helm, a package manager for Kubernetes</h6>&#13;
</div></figure>&#13;
<p>Helm extends the package management concept to Kubernetes, with some interesting differences. If you’ve worked with one of the package managers listed previously, you’ll be familiar with the idea that a package consists of a binary (executable code) as well as metadata describing the binary, such as its functionality, API, and installation instructions. In Helm, the packages are called <em>charts</em>. Charts describe how to build a Kubernetes application piece by piece by using the Kubernetes resources for compute, networking, and storage introduced in previous chapters, such as Pods, Services, and PersistentVolumeClaims. For compute workloads, the descriptions point to container images that reside in public or private container registries.</p>&#13;
<p>Helm allows charts to reference other charts as dependencies, which provides a great way to compose applications by creating assemblies of charts. For example, you could define an application such as the WordPress/MySQL example from the previous chapter by defining a chart for your WordPress deployment that referenced a chart defining a MySQL deployment that you wish to reuse. Or, you might even find a Helm chart that defines an entire WordPress application including the database.</p>&#13;
<div data-type="warning" epub:type="warning">&#13;
<h1>Kubernetes Environment Prerequisites</h1>&#13;
<p>The examples in this chapter assume you have access to a Kubernetes cluster with a couple of characteristics:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183198307312"/><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183198308736"/>The cluster should have at least three Worker Nodes, in order to demonstrate mechanisms Kubernetes provides to allow you to request Pods to be spread across a cluster. You can create a simple cluster on your desktop by using an open source distribution called kind. See the kind quick start guide for instructions on installing kind and <a href="https://oreil.ly/8nOHi">creating a multinode cluster</a>. The code for this example also contains a configuration file you may find useful to create a simple three-node kind cluster.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="SC (StorageClasses)" data-type="indexterm" id="idm46183197831008"/>You will also need a StorageClass that supports dynamic provisioning. You may wish to follow the instructions in <a data-type="xref" href="ch02.html#storageclasses">“StorageClasses”</a> for installing a simple StorageClass and provisioner that expose local storage.</p></li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Using Helm to Deploy MySQL" data-type="sect1"><div class="sect1" id="using_helm_to_deploy_mysql">&#13;
<h1>Using Helm to Deploy MySQL</h1>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="deploying MySQL using" data-type="indexterm" id="helm_mysql"/><a contenteditable="false" data-primary="MySQL" data-secondary="deploying with Helm" data-type="indexterm" id="mysql_helm"/>To make things a bit more concrete, let’s use Helm to deploy the databases you worked with in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>. First, if it’s not already on your system, you’ll need to install Helm by using the <a href="https://oreil.ly/tUPWL">documentation</a> on the Helm website. Next, add the Bitnami Helm repository:</p>&#13;
<pre data-type="programlisting"><strong>helm repo add bitnami https://charts.bitnami.com/bitnami</strong></pre>&#13;
<p><a contenteditable="false" data-primary="Bitnami Helm" data-type="indexterm" id="idm46183197750256"/>The Bitnami Helm repository contains a variety of Helm charts to help you deploy infrastructure such as databases, analytics engines, and log management systems, as well as applications including ecommerce, customer relationship management (CRM), and you guessed it: WordPress. You can find the source code for the charts in the Bitnami Charts repository on <a href="https://oreil.ly/lmcml">GitHub</a>. The <em>README</em> for this repo provides helpful instructions for using the charts in various Kubernetes distributions.</p>&#13;
<p class="pagebreak-before">Now, let’s use the Helm chart provided in the <code>bitnami</code> repository to deploy MySQL. In Helm’s terminology, each deployment is known as a <em>release</em>. The simplest possible release that you could create using this chart would look something like this:</p>&#13;
<pre data-type="programlisting"># don’t execute me yet!&#13;
helm install mysql bitnami/mysql</pre>&#13;
<p><a contenteditable="false" data-primary="release (Helm)" data-type="indexterm" id="idm46183197744128"/>If you execute this command, it will create a release called <code>mysql</code> using the Bitnami MySQL Helm chart with its default settings. As a result, you’d have a single MySQL node. Since you’ve already deployed a single node of MySQL manually in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>, let’s do something a bit more interesting this time and create a MySQL cluster. To do this, you’ll create a <em>values.yaml</em> file with contents like the following, or you can reuse the <a href="https://oreil.ly/tsnuT">sample</a> provided in the source code:</p>&#13;
<pre data-type="programlisting">architecture: replication&#13;
secondary:&#13;
  replicaCount: 2</pre>&#13;
<p>The settings in this <em>values.yaml</em> file let Helm know that you want to use options in the Bitnami MySQL Helm chart to deploy MySQL in a replicated architecture in which there is a primary node and two secondary nodes.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>MySQL Helm Chart Configuration Options</h1>&#13;
<p><a contenteditable="false" data-primary="configuration options, for MySQL Helm charts" data-type="indexterm" id="idm46183197859616"/>If you examine the default <a href="https://oreil.ly/SGsN5"><em>values.yaml</em> file</a> provided with the Bitnami MySQL Helm chart, you’ll see quite a few options available beyond the simple selections shown here. The configurable values include the following:</p>&#13;
<ul>&#13;
<li><p>Images to pull and their locations</p></li>&#13;
<li><p>The Kubernetes StorageClass that will be used to generate PersistentVolumes</p></li>&#13;
<li><p>Security credentials for user and administrator accounts</p></li>&#13;
<li><p>MySQL configuration settings for primary and secondary <span class="keep-together">replicas</span></p></li>&#13;
<li><p>Number of secondary replicas to create</p></li>&#13;
<li><p>Details of liveness, readiness probes</p></li>&#13;
<li><p>Affinity and anti-affinity settings</p></li>&#13;
<li><p>Managing high availability of the database using Pod disruption budgets</p></li>&#13;
</ul>&#13;
<p>Many of these concepts you’ll be familiar with already, and others like affinity and Pod disruption budgets are covered later in the book.</p>&#13;
</div>&#13;
<p>Once you’ve created the <em>values.yaml</em> file, you can start the cluster using this <span class="keep-together">command:</span></p>&#13;
<pre data-type="programlisting"><strong>helm install mysql bitnami/mysql -f values.yaml</strong></pre>&#13;
<p>After running the command, you’ll see the status of the install from Helm, plus instructions that are provided with the chart under <code>NOTES</code>:</p>&#13;
<pre data-type="programlisting">NAME: mysql&#13;
LAST DEPLOYED: Thu Oct 21 20:39:19 2021&#13;
NAMESPACE: default&#13;
STATUS: deployed&#13;
REVISION: 1&#13;
TEST SUITE: None&#13;
NOTES:&#13;
…</pre>&#13;
<p>We’ve omitted the notes here since they are a bit lengthy. They describe suggested commands for monitoring the status as MySQL initializes, how clients and administrators can connect to the database, how to upgrade the database, and more.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Use Namespaces to Help Isolate Resources</h1>&#13;
<p><a contenteditable="false" data-primary="Namespaces" data-secondary="isolating resources using" data-type="indexterm" id="idm46183197729408"/><a contenteditable="false" data-primary="resources" data-secondary="isolating using Namespaces" data-type="indexterm" id="idm46183199060224"/>Since we did not specify a Namespace, the Helm release has been installed in the default Kubernetes Namespace unless you’ve separately configured a Namespace in your <a href="https://oreil.ly/C2vOM">kubeconfig</a>. If you want to install a Helm release in its own Namespace in order to work with its resources more effectively, you could run something like the following:</p>&#13;
<pre data-type="programlisting"><strong>helm install mysql bitnami/mysql \&#13;
  --namespace mysql --create-namespace</strong></pre>&#13;
<p>This creates a Namespace called <code>mysql</code> and installs the <code>mysql</code> release inside it.</p>&#13;
</div>&#13;
<p>To obtain information about the Helm releases you’ve created, use the <code>helm list</code> command, which produces output such as this (formatted for readability):</p>&#13;
<pre data-type="programlisting"><strong>helm list</strong>&#13;
NAME   NAMESPACE  REVISION  UPDATED   &#13;
mysql  default    1         2021-10-21 20:39:19&#13;
&#13;
STATUS    CHART        APP VERSION&#13;
deployed  mysql-8.8.8  8.0.26</pre>&#13;
<p>If you haven’t installed the release in its own Namespace, it’s still simple to see the compute resources that Helm has created on your behalf by running <code>kubectl get all</code>, because they have all been labeled with the name of your release. It may take several minutes for all the resources to initialize, but when complete, it will look something like this:</p>&#13;
<pre data-type="programlisting"><strong>kubectl get all</strong>&#13;
NAME                    READY   STATUS    RESTARTS   AGE&#13;
pod/mysql-primary-0     1/1     Running   0          3h40m&#13;
pod/mysql-secondary-0   1/1     Running   0          3h40m&#13;
pod/mysql-secondary-1   1/1     Running   0          3h38m&#13;
&#13;
NAME                              TYPE       CLUSTER-IP     EXTERNAL-IP  PORT     &#13;
service/mysql-primary             ClusterIP  10.96.107.156  &lt;none&gt;       ...  &#13;
service/mysql-primary-headless    ClusterIP  None           &lt;none&gt;       ...  &#13;
service/mysql-secondary           ClusterIP  10.96.250.52   &lt;none&gt;       ... &#13;
service/mysql-secondary-headless  ClusterIP  None           &lt;none&gt;       ...  &#13;
&#13;
NAME                               READY   AGE&#13;
statefulset.apps/mysql-primary     1/1     3h40m&#13;
statefulset.apps/mysql-secondary   2/2     3h40m</pre>&#13;
<p>As you can see, Helm has created two StatefulSets, one for primary replicas and one for secondary replicas. The <code>mysql-primary</code> StatefulSet is managing a single MySQL Pod containing a primary replica, while the <code>mysql-secondary</code> StatefulSet is managing two MySQL Pods containing secondary replicas. See if you can determine which Kubernetes Worker Node each MySQL replica is running on by using the <code>kubectl describe pod</code> command.</p>&#13;
<p><a contenteditable="false" data-primary="Services" data-secondary="in Helm" data-type="indexterm" id="idm46183197726912"/><a contenteditable="false" data-primary="StatefulSets" data-secondary="in Helm" data-type="indexterm" id="idm46183197727264"/>From the preceding output, you’ll also notice two Services created for each StatefulSet, one a headless service and another that has a dedicated IP address. Since <code>kubectl get all</code> tells you about only compute resources and services, you might also be wondering about the storage resources. To check on these, run the <code>kubectl get pv</code> command. Assuming you have a StorageClass installed that supports dynamic provisioning, you should see PersistentVolumes that are bound to PersistentVolumeClaims named <code>data-mysql-primary-0</code>, <code>data-mysql-secondary-0</code>, and <code>data-mysql-secondary-1</code>.</p>&#13;
<p>In addition to the resources we’ve discussed, installing the chart has also resulted in the creation of a few additional resources that we’ll explore next.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Namespaces and Kubernetes Resource Scope</h1>&#13;
<p><a contenteditable="false" data-primary="Namespaces" data-secondary="Kubernetes resource scope and" data-type="indexterm" id="idm46183197712512"/>If you have chosen to install your Helm release in a Namespace, you’ll need to specify the Namespace on most of your <code>kubectl get</code> commands in order to see the created resources. The exception is <code>kubectl get pv</code>, because PersistentVolumes are one of the Kubernetes resources that are not Namespaced; that is, they can be used by Pods in any Namespace. To learn more about which Kubernetes resources in your cluster are Namespaced and which are not, run the command <code>kubectl api-resources</code>.</p>&#13;
</div>&#13;
<section data-pdf-bookmark="How Helm Works" data-type="sect2"><div class="sect2" id="how_helm_works">&#13;
<h2>How Helm Works</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="how it works" data-type="indexterm" id="idm46183197930272"/><a contenteditable="false" data-primary="helm install command" data-type="indexterm" id="idm46183197702656"/>Did you wonder what happened when you executed the <code>helm install</code> command with a provided values file? To understand what’s going on, let’s take a look at the contents of a Helm chart, as shown in <a data-type="xref" href="#customizing_a_helm_release_using_a_valu">Figure 4-2</a>. As we discuss these contents, it will also be helpful to look at the <a href="https://oreil.ly/xQbvb">source code</a> of the MySQL Helm chart you just installed.</p>&#13;
<figure><div class="figure" id="customizing_a_helm_release_using_a_valu">&#13;
<img alt="Customizing a Helm release using a values.yaml file" src="assets/mcdk_0402.png"/>&#13;
<h6><span class="label">Figure 4-2. </span>Customizing a Helm release using a values.yaml file</h6>&#13;
</div></figure>&#13;
<p>Looking at the contents of a Helm chart, you’ll notice the following:</p>&#13;
<dl>&#13;
<dt><a contenteditable="false" data-primary="README file (Helm)" data-type="indexterm" id="idm46183197695040"/><a href="https://oreil.ly/i7XBa">README file</a></dt> &#13;
<dd>This explains how to use the chart. These instructions are provided along with the chart in registries.</dd>&#13;
<dt><a contenteditable="false" data-primary="Chart.yaml file (Helm)" data-type="indexterm" id="idm46183197691360"/><a href="https://oreil.ly/zZb2Y">Chart.yaml file</a></dt> &#13;
<dd>This contains metadata about the chart such as its name, publisher, version, keywords, and any dependencies on other charts. These properties are useful when searching Helm registries to find charts.</dd>&#13;
<dt><a contenteditable="false" data-primary="values.yaml file (Helm)" data-type="indexterm" id="idm46183197692960"/><a href="https://oreil.ly/mhfhZ">values.yaml file</a></dt> &#13;
<dd>This lists out the configurable values supported by the chart and their default values. These files typically contain a good number of comments that explain the available options. For the Bitnami MySQL Helm chart, a lot of options are available, as we’ve noted.</dd>&#13;
<dt><a contenteditable="false" data-primary="templates directory (Helm)" data-type="indexterm" id="idm46183198299808"/><a href="https://oreil.ly/F21Lg">templates directory</a></dt> &#13;
<dd>This contains <a href="https://oreil.ly/diTnu">Go templates</a> that define the chart. The templates include a <a href="https://oreil.ly/v0aky"><em>Notes.txt</em></a> file used to generate the output you saw previously after executing the <code>helm install</code> command, and one or more YAML files that describe a pattern for a Kubernetes resource. These YAML files may be organized in subdirectories (for example, the <a href="https://oreil.ly/iKedl">template</a> that defines a StatefulSet for MySQL primary replicas). Finally, a <em>_helpers.tpl</em> file describes how to use the templates. Some of the templates may be used multiple times or not at all, depending on the selected configuration values.</dd>&#13;
</dl>&#13;
<p>When you execute the <code>helm install</code> command, the Helm client makes sure it has an up-to-date copy of the chart you’ve named by checking with the source repository. Then it uses the template to generate YAML configuration code, overriding default values from the chart’s <em>values.yaml</em> file with any values you’ve provided. It then uses the <code>kubectl</code> command to apply this configuration to your currently configured Kubernetes cluster.</p>&#13;
<p>If you’d like to see the configuration that a Helm chart will produce before applying it, you can use the handy <code>template</code> command. It supports the same syntax as the <code>install</code> command:</p>&#13;
<pre data-type="programlisting"><strong>helm template mysql bitnami/mysql -f values.yaml</strong></pre>&#13;
<p>Running this command will produce quite a bit of output, so you may want to redirect it to a file (append <code>&gt; values-template.yaml</code> to the command) so you can take a longer look. Alternatively, you can look at the <a href="https://oreil.ly/DhEtc">copy</a> we have saved in the source code repository.</p>&#13;
<p>You’ll notice that several types of resources are created, as summarized in <a data-type="xref" href="#deploying_mysql_using_the_bitnami_helm">Figure 4-3</a>. Many of the resources shown have been discussed, including the StatefulSets for managing the primary and secondary replicas, each with its own service (the chart also creates headless services that are not shown in the figure). Each Pod has its own PersistentVolumeClaim that is mapped to a unique PersistentVolume.</p>&#13;
<p><a data-type="xref" href="#deploying_mysql_using_the_bitnami_helm">Figure 4-3</a> also includes resource types we haven’t discussed previously. Notice first that each StatefulSet has an associated ConfigMap that is used to provide a common set of configuration settings to its Pods. Next, notice the Secret named <code>mysql</code>, which stores passwords needed for accessing various interfaces exposed by the database nodes. Finally, a ServiceAccount resource is applied to every Pod created by this Helm release.</p>&#13;
&#13;
<p>Let’s focus on some interesting aspects of this deployment, including the usage of labels, ServiceAccounts, Secrets, and ConfigMaps.</p>&#13;
&#13;
<figure><div class="figure" id="deploying_mysql_using_the_bitnami_helm">&#13;
<img alt="Deploying MySQL using the Bitnami Helm chart" src="assets/mcdk_0403.png"/>&#13;
<h6><span class="label">Figure 4-3. </span>Deploying MySQL using the Bitnami Helm chart</h6>&#13;
</div></figure>&#13;
&#13;
</div></section>&#13;
<section data-pdf-bookmark="Labels" data-type="sect2"><div class="sect2" id="labels">&#13;
<h2>Labels</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="labels and" data-type="indexterm" id="idm46183197673952"/><a contenteditable="false" data-primary="labels, Helm and" data-type="indexterm" id="idm46183197672736"/>If you look through the output from the <code>helm template</code>, you’ll notice that the resources have a common set of labels:</p>&#13;
<pre data-type="programlisting">  labels:&#13;
    app.kubernetes.io/name: mysql&#13;
    helm.sh/chart: mysql-8.8.8&#13;
    app.kubernetes.io/instance: mysql&#13;
    app.kubernetes.io/managed-by: Helm</pre>&#13;
<p>These labels help identify the resources as being part of the <code>mysql</code> application and indicate that they are managed by Helm using a specific chart version. The labels are useful for selecting resources, which is often useful in defining configurations for other resources.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="ServiceAccounts" data-type="sect2"><div class="sect2" id="serviceaccounts">&#13;
<h2>ServiceAccounts</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="ServiceAccounts and" data-type="indexterm" id="idm46183198245136"/><a contenteditable="false" data-primary="ServiceAccounts, Helm and" data-type="indexterm" id="idm46183197665584"/>Kubernetes clusters make a distinction between human users and applications for access control purposes. A ServiceAccount is a Kubernetes resource that represents an application and what it is allowed to access. For example, a ServiceAccount may be given access to some portions of the Kubernetes API, or access to one or more secrets containing privileged information such as login credentials. This latter capability is used in your Helm installation of MySQL to share credentials between Pods.</p>&#13;
<p>Every Pod created in Kubernetes has a ServiceAccount assigned to it. If you do not specify one, the default ServiceAccount is used. Installing the MySQL Helm chart creates a ServiceAccount called <code>mysql</code>. You can see the specification for this resource in the generated template:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: ServiceAccount&#13;
metadata:&#13;
  name: mysql&#13;
  namespace: default&#13;
  labels: ...&#13;
  annotations:&#13;
secrets:&#13;
  - name: mysql</pre>&#13;
<p>As you can see, this ServiceAccount has access to a Secret called <code>mysql</code>, which we’ll discuss shortly. A ServiceAccount can also have an additional type of Secret known as an <code>imagePullSecret</code>. These Secrets are used when an application needs to use images from a private registry.</p>&#13;
<p>By default, a ServiceAccount does not have any access to the Kubernetes API. To give this ServiceAccount the access it needs, the MySQL Helm chart creates a Role specifying the Kubernetes resources and operations, and a RoleBinding to associate the ServiceAccount to the Role. We’ll discuss ServiceAccounts and role-based access in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Secrets" data-type="sect2"><div class="sect2" id="secrets">&#13;
<h2>Secrets</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="Secrets and" data-type="indexterm" id="idm46183197796912"/><a contenteditable="false" data-primary="Secrets, Helm and" data-type="indexterm" id="idm46183197762608"/>As you learned in <a data-type="xref" href="ch02.html#managing_data_storage_on_kubernetes">Chapter 2</a>, a Secret provides secure access to information you need to keep private. Your <code>mysql</code> Helm release contains a Secret called <code>mysql</code> containing login credentials for the MySQL instances themselves:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: Secret&#13;
metadata:&#13;
  name: mysql&#13;
  namespace: default&#13;
  labels: ...&#13;
type: Opaque&#13;
data:&#13;
  mysql-root-password: "VzhyNEhIcmdTTQ=="&#13;
  mysql-password: "R2ZtNkFHNDhpOQ=="&#13;
  mysql-replication-password: "bDBiTWVzVmVORA=="</pre>&#13;
<p>The three passwords represent different types of access: the <code>mysql-root-password</code> provides administrative access to the MySQL node, while the <code>mysql-replication-password</code> is used for nodes to communicate for the purposes of data replication between nodes. The <code>mysql-password</code> is used by client applications to access the database to write and read data.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="ConfigMaps" data-type="sect2"><div class="sect2" id="configmaps">&#13;
<h2>ConfigMaps</h2>&#13;
<p><a contenteditable="false" data-primary="ConfigMaps" data-type="indexterm" id="idm46183197655584"/><a contenteditable="false" data-primary="Helm" data-secondary="ConfigMaps and" data-type="indexterm" id="idm46183197648912"/>The Bitnami MySQL Helm chart creates Kubernetes ConfigMap resources to represent the configuration settings used for Pods that run the MySQL primary and secondary replica nodes. ConfigMaps store configuration data as key-value pairs. For example, the ConfigMap created by the Helm chart for the primary replicas looks like this:</p>&#13;
<pre data-type="programlisting">apiVersion: v1&#13;
kind: ConfigMap&#13;
metadata:&#13;
  name: mysql-primary&#13;
  namespace: default&#13;
  labels: ...&#13;
data:&#13;
  my.cnf: |-&#13;
   &#13;
    [mysqld]&#13;
    default_authentication_plugin=mysql_native_password&#13;
    ...</pre>&#13;
<p>In this case, the key is the name <code>my.cnf</code>, which represents a filename, and the value is a multiline set of configuration settings that represent the contents of a configuration file (which we’ve abbreviated here). Next, look at the definition of the StatefulSet for the primary replicas. Notice that the contents of the ConfigMap are mounted as a read-only file inside each template, according to the Pod specification for the StatefulSet (again, we’ve omitted some detail to focus on key areas):</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: StatefulSet&#13;
metadata:&#13;
  name: mysql-primary&#13;
  namespace: default&#13;
  labels: ...&#13;
spec:&#13;
  replicas: 1&#13;
  selector:&#13;
    matchLabels: ...&#13;
  serviceName: mysql-primary&#13;
  template:&#13;
    metadata:&#13;
      annotations: ...&#13;
      labels: ...&#13;
    spec:&#13;
      ...     &#13;
      serviceAccountName: mysql&#13;
      containers:&#13;
        - name: mysql&#13;
          image: docker.io/bitnami/mysql:8.0.26-debian-10-r60&#13;
          volumeMounts:&#13;
            - name: data&#13;
              mountPath: /bitnami/mysql&#13;
            - name: config&#13;
              mountPath: /opt/bitnami/mysql/conf/my.cnf&#13;
              subPath: my.cnf&#13;
      volumes:&#13;
        - name: config&#13;
          configMap:&#13;
            name: mysql-primary</pre>&#13;
<p>Mounting the ConfigMap as a volume in a container results in the creation of a read-only file in the mount directory that is named according to the key and has the value as its content. For our example, mounting the ConfigMap in the Pod’s <code>mysql</code> container results in the creation of the file <em>/opt/bitnami/mysql/conf/my.cnf</em>.</p>&#13;
<p>This is one of several ways that ConfigMaps can be used in Kubernetes applications:</p>&#13;
<ul>&#13;
<li><p>As described in the <a href="https://oreil.ly/yoEYv">Kubernetes documentation</a>, you could choose to store configuration data in more granular key-value pairs, which also makes it easier to access individual values in your application.</p></li>&#13;
<li><p>You can also reference individual key-value pairs as environment variables you pass to a container.</p></li>&#13;
<li><p>Finally, applications can access ConfigMap contents via the Kubernetes API.</p></li>&#13;
</ul>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>More Configuration Options</h1>&#13;
<p>Now that you have a Helm release with a working MySQL cluster, you can point an application to it, such as WordPress. Why not try seeing if you can adapt the WordPress deployment from <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a> to point to the MySQL cluster you’ve created here?</p>&#13;
<p>For further learning, you could also compare your resulting configuration with that produced by the Bitnami WordPress Helm chart, which uses MariaDB instead of MySQL but is otherwise quite similar.</p>&#13;
</div>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Updating Helm Charts" data-type="sect2"><div class="sect2" id="updating_helm_charts">&#13;
<h2>Updating Helm Charts</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="updating charts" data-type="indexterm" id="idm46183197635072"/><a contenteditable="false" data-primary="updating Helm charts" data-type="indexterm" id="idm46183197632816"/>If you’re running a Helm release in a production environment, chances are you’re going to need to maintain it over time. You might want to update a Helm release for various reasons:</p>&#13;
<ul>&#13;
<li><p>A new version of a chart is available.</p></li>&#13;
<li><p>A new version of an image used by your application is available.</p></li>&#13;
<li><p>You want to change the selected options.</p></li>&#13;
</ul>&#13;
<p><a contenteditable="false" data-primary="helm repo update command" data-type="indexterm" id="idm46183198259200"/>To check for a new version of a chart, execute the <code>helm repo update</code> command. Running this command with no options looks for updates in all of the chart repositories you have configured for your Helm client:</p>&#13;
<pre data-type="programlisting"><strong>helm repo update</strong>&#13;
Hang tight while we grab the latest from your chart repositories...&#13;
...Successfully got an update from the "bitnami" chart repository&#13;
Update Complete. ⎈Happy Helming!⎈</pre>&#13;
<p>Next, you’ll want to make any desired updates to your configured values. If you’re upgrading to a new version of a chart, make sure to check the release notes and documentation of the configurable values. It’s a good idea to test out an upgrade before applying it. The <code>--dry-run</code> option allows you to do this, producing similar values to the <code>helm template</code> command:</p>&#13;
<pre data-type="programlisting"><strong>helm upgrade mysql bitnami/mysql -f values.yaml --dry-run</strong></pre>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Using an Overlay Configuration File</h1>&#13;
<p><a contenteditable="false" data-primary="overlay configuration files" data-type="indexterm" id="idm46183197626304"/>One useful option you could use for the upgrade is to specify values you wish to override in a new configuration file, and apply both the new and old, something like this:</p>&#13;
<pre data-type="programlisting"><strong>helm upgrade mysql bitnami/mysql \&#13;
  -f values.yaml -f new-values.yaml</strong></pre>&#13;
<p>Configuration files are applied in the order they appear on the command line, so if you use this approach, make sure your overridden values file appears after your original values file.</p>&#13;
</div>&#13;
<p>Once you’ve applied the upgrade, Helm sets about its work, updating only those resources in the release that are affected by your configuration changes. If you’ve specified changes to the Pod template for a StatefulSet, the Pods will be restarted according to the update policy specified for the StatefulSet, as we discussed in <a data-type="xref" href="ch03.html#statefulset_life_cycle_management">“StatefulSet lifecycle management”</a>.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Uninstalling Helm Charts" data-type="sect2"><div class="sect2" id="uninstalling_helm_charts">&#13;
<h2>Uninstalling Helm Charts</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="uninstalling charts" data-type="indexterm" id="idm46183197645408"/><a contenteditable="false" data-primary="uninstalling Helm charts" data-type="indexterm" id="idm46183197618752"/>When you are finished using your Helm release, you can uninstall it by name:</p>&#13;
<pre data-type="programlisting"><strong>helm uninstall mysql</strong></pre>&#13;
<p>Note that Helm does not remove any of the PersistentVolumeClaims or PersistentVolumes that were created for this Helm chart, following the behavior of StatefulSets discussed in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="additional_deployment_tools_kustomize_a">&#13;
<h5>Additional Deployment Tools: Kustomize and Skaffold</h5>&#13;
<p><a contenteditable="false" data-primary="Kustomize tool" data-type="indexterm" id="idm46183197612848"/><a contenteditable="false" data-primary="Skaffold tool" data-type="indexterm" id="idm46183197611776"/>In addition to Helm, other tools in the Kubernetes ecosystem are available to help you manage the configuration and deployment of applications, such as Kustomize and Skaffold.</p>&#13;
<p><em>Kustomize</em> is a configuration management tool for Kubernetes. Unlike a package manager, Kustomize does not provide a registry; instead, its focus is helping you manage Kubernetes configuration YAML files for different environments. Kustomize uses a template-based approach in which you create snippets of configuration code called <em>overlays</em> that are intended to override sections of a base YAML file. These overlays are typically intended for different environments such as development, test, and production, or for isolating configurations specific to different Kubernetes providers, with a similar effect to a Helm <em>values.yaml</em> file. The sections to be overridden are identified by selectors such as Kubernetes labels or annotations. You provide a <em>kustomization.yaml</em> file to describe the mapping of templates to their selectors. Kustomize works best when the YAML file you want to customize is well structured and uses labels or annotations.</p>&#13;
<p><em>Skaffold</em> is a tool that automates application deployment in your development environment. You can execute Skaffold imperatively from the command line, or as a daemon that watches for code changes to build artifacts such as container images. When it detects a relevant change, the daemon automatically performs actions according to the workflow you define in a <em>skaffold.yaml</em> file. The workflow can include actions such as building and tagging images, updating Helm charts or regular Kubernetes configuration files, and deploying your app using <code>kubectl</code>, Helm, or Kustomize<a contenteditable="false" data-primary="" data-startref="helm_mysql" data-type="indexterm" id="idm46183197609584"/><a contenteditable="false" data-primary="" data-startref="mysql_helm" data-type="indexterm" id="idm46183197746960"/>.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Using Helm to Deploy Apache Cassandra" data-type="sect1"><div class="sect1" id="using_helm_to_deploy_apache_cassandra">&#13;
<h1>Using Helm to Deploy Apache Cassandra</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Cassandra" data-secondary="deploying using Helm" data-type="indexterm" id="ac_helm"/><a contenteditable="false" data-primary="Helm" data-secondary="deploying using Apache Cassandra" data-type="indexterm" id="helm_ac"/>Now let’s switch gears and look at deploying Apache Cassandra by using Helm. In this section, you’ll use another chart provided by Bitnami, so there’s no need to add another repository. You can find the implementation of this chart on <a href="https://oreil.ly/WzvXp">GitHub</a>. Helm provides a quick way to see the metadata about this chart:</p>&#13;
<pre data-type="programlisting"><strong>helm show chart bitnami/cassandra</strong></pre>&#13;
<p>After reviewing the metadata, you’ll also want to learn about the configurable values. You can examine the <a href="https://oreil.ly/z69Z7"><em>values.yaml</em> file</a> in the GitHub repo, or use another option on the <code>show</code> command:</p>&#13;
<pre data-type="programlisting"><strong>helm show values bitnami/cassandra</strong></pre>&#13;
<p>The list of options for this chart is shorter than the list for the MySQL chart, because Cassandra doesn’t have the concept of primary and secondary replicas. However, you’ll certainly see similar options for images, StorageClasses, security, liveness and readiness probes, and so on. Some configuration options are unique to Cassandra, such as those having to do with JVM settings and seed nodes (as discussed in <a data-type="xref" href="ch03.html#databases_on_kubernetes_the_hard_way">Chapter 3</a>).</p>&#13;
<p><a contenteditable="false" data-primary="metrics, exporting from Apache Cassandra" data-type="indexterm" id="idm46183197922608"/>One interesting feature of this chart is the ability to export metrics from Cassandra nodes. If you set <code>metrics.enabled=true</code>, the chart will inject a sidecar container into each Cassandra Pod that exposes a port that can be scraped by Prometheus. Other values under <code>metrics</code> configure what metrics are exported, the collection frequency, and more. While we won’t use this feature here, metrics reporting is a key part of managing data infrastructure we’ll cover in <a data-type="xref" href="ch06.html#integrating_data_infrastructure_in_a_ku">Chapter 6</a>.</p>&#13;
<p>For a simple three-node Cassandra configuration, you could set the replica count to 3 and set other configuration values to their defaults. However, since you’re overriding only a single configuration value, this is a good time to take advantage of Helm’s support for setting values on the command line, instead of providing a <em>values.yaml</em> file:</p>&#13;
<pre data-type="programlisting"><strong>helm install cassandra bitnami/cassandra --set replicaCount=3</strong></pre>&#13;
<p><a contenteditable="false" data-primary="helm template command" data-type="indexterm" id="idm46183197621824"/>As discussed previously, you can use the <code>helm template</code> command to check the configuration before installing it, or look at the file we’ve saved on GitHub. However, since you’ve already created the release, you can also use this command:</p>&#13;
<pre data-type="programlisting"><strong>helm get manifest cassandra</strong></pre>&#13;
<p>Looking through the resources in the YAML, you’ll see that a similar set of infrastructure has been established, as shown in <a data-type="xref" href="#deploying_apache_cassandra_using_the_bi">Figure 4-4</a>.</p>&#13;
&#13;
<p>The configuration includes the following:</p>&#13;
<ul>&#13;
<li><p>A ServiceAccount referencing a Secret, which contains the password for the <code>cassandra</code> administrator account.</p></li>&#13;
<li><p>A single StatefulSet, with a headless Service used to reference its Pods. The Pods are spread evenly across the available Kubernetes Worker Nodes, which we’ll discuss in the next section. The Service exposes Cassandra ports used for intra-node communication (<code>7000</code>, with <code>7001</code> used for secure communication via TLS), administration via JMX (<code>7199</code>), and client access via CQL (<code>9042</code>).</p></li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="deploying_apache_cassandra_using_the_bi">&#13;
<img alt="Deploying Apache Cassandra using the Bitnami Helm chart" src="assets/mcdk_0404.png"/>&#13;
<h6><span class="label">Figure 4-4. </span>Deploying Apache Cassandra using the Bitnami Helm chart</h6>&#13;
</div></figure>&#13;
&#13;
<p>This configuration represents a simple Cassandra topology, with all three nodes in a single Datacenter and rack. This simple topology reflects one of the limitations of this chart—it does not provide the ability to create a Cassandra cluster consisting of multiple Datacenters and racks. To create a more complex deployment, you’d have to install multiple Helm releases, using the same <code>clusterName</code> (in this case, you’re using the default name <code>cassandra</code>), but a different Datacenter and rack per deployment. You’d also need to obtain the IP address of a couple of nodes in the first Datacenter to use as <code>additionalSeeds</code> when configuring the releases for the other racks.</p>&#13;
<section data-pdf-bookmark="Affinity and Anti-Affinity" data-type="sect2"><div class="sect2" id="affinity_and_anti_affinity">&#13;
<h2>Affinity and Anti-Affinity</h2>&#13;
<p><a contenteditable="false" data-primary="affinity" data-type="indexterm" id="idm46183197987440"/><a contenteditable="false" data-primary="anti-affinity" data-type="indexterm" id="idm46183197572144"/>As shown in <a data-type="xref" href="#deploying_apache_cassandra_using_the_bi">Figure 4-4</a>, the Cassandra nodes are spread evenly across the Worker Nodes in your cluster. To verify this in your own Cassandra release, you could run something like the following:</p>&#13;
<pre class="pagebreak-before" data-type="programlisting"><strong>kubectl describe pods | grep "^Name:" -A 3</strong>&#13;
Name:         cassandra-0&#13;
Namespace:    default&#13;
Priority:     0&#13;
Node:         kind-worker/172.20.0.7&#13;
--&#13;
Name:         cassandra-1&#13;
Namespace:    default&#13;
Priority:     0&#13;
Node:         kind-worker2/172.20.0.6&#13;
--&#13;
Name:           cassandra-2&#13;
Namespace:      default&#13;
Priority:       0&#13;
Node:           kind-worker3/172.20.0.5</pre>&#13;
<p><a contenteditable="false" data-primary="Worker Nodes" data-type="indexterm" id="idm46183197747536"/><a contenteditable="false" data-primary="Kubernetes Worker Nodes" data-type="indexterm" id="idm46183197567280"/>As you can see, each Cassandra node is running on a different Worker Node. If your Kubernetes cluster has at least three Worker Nodes and no other workloads, you’ll likely observe similar behavior. While it is true that this even allocation could happen naturally in a cluster that has an even load across Worker Nodes, this is probably not the case in your production environment. However, to promote maximum availability of your data, we want to try to honor the intent of Cassandra’s architecture to run nodes on different machines in order to promote high availability.</p>&#13;
<p><a contenteditable="false" data-primary="Bitnami Helm" data-type="indexterm" id="idm46183197559920"/>To help guarantee this isolation, the Bitnami Helm chart uses Kubernetes’s affinity capabilities, specifically anti-affinity. If you examine the generated configuration for the Cassandra StatefulSet, you’ll see the following:</p>&#13;
<pre data-type="programlisting">apiVersion: apps/v1&#13;
kind: StatefulSet&#13;
metadata:&#13;
  name: cassandra&#13;
  namespace: default&#13;
  labels: ...&#13;
spec:&#13;
  ...&#13;
  template:&#13;
    metadata:&#13;
      labels: ...&#13;
    spec:&#13;
      ...&#13;
      affinity:&#13;
        podAffinity:&#13;
         &#13;
        podAntiAffinity:&#13;
          preferredDuringSchedulingIgnoredDuringExecution:&#13;
            - podAffinityTerm:&#13;
                labelSelector:&#13;
                  matchLabels:&#13;
                    app.kubernetes.io/name: cassandra&#13;
                    app.kubernetes.io/instance: cassandra&#13;
                namespaces:&#13;
                  - "default"&#13;
                topologyKey: kubernetes.io/hostname&#13;
              weight: 1&#13;
        nodeAffinity:</pre>&#13;
<p>As shown here, the Pod template specification lists three possible types of affinity, with only the <code>podAntiAffinity</code> being defined. What do these concepts mean?</p>&#13;
<dl>&#13;
<dt>Pod affinity</dt>&#13;
<dd><a contenteditable="false" data-primary="Pod affinity" data-type="indexterm" id="idm46183197566176"/>The preference that a Pod is scheduled onto a node where another specific Pod is running. For example, Pod affinity could be used to colocate a web server with its cache.</dd>&#13;
<dt>Pod anti-affinity</dt>&#13;
<dd><a contenteditable="false" data-primary="Pod anti-affinity" data-type="indexterm" id="idm46183197569440"/>The opposite of Pod affinity—that is, a preference that a Pod not be scheduled on a node where another identified Pod is running. This is the constraint used in this example, as we’ll discuss shortly.</dd>&#13;
<dt>Node affinity</dt>&#13;
<dd><a contenteditable="false" data-primary="Node affinity" data-type="indexterm" id="idm46183197568512"/>A preference that a Pod be run on a node with specific characteristics.</dd>&#13;
</dl>&#13;
<p>Each type of affinity can be expressed as either hard or soft constraints. These are known as <code>requiredDuringSchedulingIgnoredDuringExecution</code> and <code>preferredDuringSchedulingIgnoredDuringExecution</code>. The first constraint specifies rules that must be met before a Pod is scheduled on a node, while the second specifies a preference that the scheduler will attempt to meet but may relax if necessary in order to schedule the Pod.</p> &#13;
&#13;
<p><code>IgnoredDuringExcecution</code> implies that the constraints apply only when the Pods are first scheduled. In the future, new <code>RequiredDuringExecution</code> options will be added called <code>requiredDuringSchedulingRequiredDuringExecution</code> and <code>requiredDuringSchedulingRequiredDuringExecution</code>. These will ask Kubernetes to evict Pods (that is, move them to another node) that no longer meet the criteria—for example, by a change in their labels.</p>&#13;
<p>Looking at the preceding example, the Pod template specification for the Cassandra StatefulSet specifies an anti-affinity rule using the labels that are applied to each Cassandra Pod. The net effect is that Kubernetes will try to spread the Pods across the available Worker Nodes.</p>&#13;
<p><a contenteditable="false" data-primary="Bitnami Helm" data-type="indexterm" id="idm46183197989136"/>Those are the highlights of looking at the Bitnami Helm chart for Cassandra. To clean things up, uninstall the Cassandra release:</p>&#13;
<pre data-type="programlisting"><strong>helm uninstall cassandra</strong></pre>&#13;
<p class="pagebreak-before"><a contenteditable="false" data-primary="" data-startref="ac_helm" data-type="indexterm" id="idm46183197871648"/><a contenteditable="false" data-primary="" data-startref="helm_ac" data-type="indexterm" id="idm46183197543168"/>If you don’t want to work with Bitnami Helm charts any longer, you can also remove the repository from your Helm client:</p>&#13;
<pre data-type="programlisting"><strong>helm repo remove bitnami</strong></pre>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>More Kubernetes Scheduling Constraints</h1>&#13;
<p><a contenteditable="false" data-primary="constraints, scheduling" data-type="indexterm" id="idm46183197539616"/><a contenteditable="false" data-primary="scheduling constraints" data-type="indexterm" id="idm46183197538688"/><a contenteditable="false" data-primary="taints and tolerations" data-type="indexterm" id="idm46183197537584"/><a contenteditable="false" data-primary="NodeSelectors" data-type="indexterm" id="idm46183197536480"/>Kubernetes supports additional mechanisms for providing hints to its scheduler about Pod placement. One of the simplest is <a href="https://oreil.ly/05hSU">NodeSelectors</a>, which is very similar to node affinity, but with a less expressive syntax that can match on one or more labels by using AND logic. Since you may or may not have the required privileges to attach labels to Worker Nodes in your cluster, Pod affinity is often a better option. <a href="https://oreil.ly/fbkTB">Taints and tolerations</a> are another mechanism that can be used to configure Worker Nodes to repel specific Pods from being scheduled on those nodes.</p>&#13;
<p>In general, you want to be careful to understand all of the constraints you’re putting on the Kubernetes scheduler from various workloads so as not to overly constrain its ability to place Pods. See the Kubernetes documentation for more information on <a href="https://oreil.ly/aUWsi">scheduling constraints</a>. We’ll also look at how Kubernetes allows you to plug in different schedulers in <a data-type="xref" href="ch09.html#alternative_schedulers_for_kubernetes">“Alternative Schedulers for Kubernetes”</a>.</p>&#13;
</div>&#13;
&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Helm, CI/CD, and Operations" data-type="sect1"><div class="sect1" id="helmcomma_cisoliduscdcomma_and_operatio">&#13;
<h1>Helm, CI/CD, and Operations</h1>&#13;
<p><a contenteditable="false" data-primary="CI/CD" data-secondary="Helm and" data-type="indexterm" id="cicd_helm"/><a contenteditable="false" data-primary="Helm" data-secondary="power of" data-type="indexterm" id="helm_po"/><a contenteditable="false" data-primary="operations, Helm and" data-type="indexterm" id="op_helm"/>Helm is a powerful tool focused on one primary task: deploying complex applications to Kubernetes clusters. To get the most benefit from Helm, you’ll want to consider how it fits into your larger CI/CD toolset:</p>&#13;
<ul>&#13;
<li><p><a contenteditable="false" data-primary="Jenkins" data-type="indexterm" id="idm46183197806624"/>Automation servers such as <a href="https://www.jenkins.io">Jenkins</a> automatically build, test, and deploy software according to scripts known as <em>jobs</em>. These jobs are typically run based on predefined triggers, such as a commit to a source repository. Helm charts can be referenced in jobs to install an application under test and its supporting infrastructure in a Kubernetes cluster.</p></li>&#13;
<li><p><a contenteditable="false" data-primary="VPC (virtual private cloud)" data-type="indexterm" id="idm46183197528704"/><a contenteditable="false" data-primary="Terraform" data-type="indexterm" id="idm46183197597088"/>IaC automation tools such as <a href="https://www.terraform.io">Terraform</a> allow you to define templates and scripts that describe how to create infrastructure in a variety of cloud environments. For example, you could write a Terraform script that automates the creation of a new VPC within a specific cloud provider and the creation of a new Kubernetes cluster within that VPC. The script could then use Helm to install applications within the Kubernetes cluster.</p></li>&#13;
</ul>&#13;
<p class="pagebreak-before">While overlaps certainly occur in the capabilities these tools provide, you’ll want to consider the strengths and limitations of each as you construct your toolset. For this reason, we want to make sure to note that Helm has limitations when it comes to managing the operations of applications that it deploys. To get a good picture of the challenges involved, we spoke to a practitioner who has built assemblies of Helm charts to manage a complex database deployment. This discussion begins to introduce concepts like Kubernetes Custom Resource Definitions (CRDs) and the operator pattern, both of which we’ll cover in depth in <a data-type="xref" href="ch05.html#automating_database_management_on_kuber">Chapter 5</a>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="pushing_helm_to_the_limit">&#13;
<h5>Pushing Helm to the Limit</h5>&#13;
<p><em>With John Sanda, Software Engineer, DataStax</em></p>&#13;
<p><a contenteditable="false" data-primary="K8ssandra/K8ssandra Operator" data-primary-sortas="kassandra" data-secondary="about" data-type="indexterm" id="idm46183197521648"/><a contenteditable="false" data-primary="Sanda, John" data-type="indexterm" id="idm46183197519968"/><a contenteditable="false" data-primary="Helm" data-secondary="limitations of" data-type="indexterm" id="idm46183197517456"/>K8ssandra is a distribution of Apache Cassandra on Kubernetes built from multiple open source components, including a Cassandra operator, known as <a href="https://oreil.ly/jlAZi">Cass Operator</a>;  operational tools for managing anti-entropy repair, known as <a href="https://oreil.ly/SOXkK">Reaper</a>; and backups, known as <a href="https://oreil.ly/LLjBD">Medusa</a>. K8ssandra also includes the Prometheus-Grafana stack for metrics collection and reporting.</p>&#13;
<p>From the start, we used Helm to help manage the installation and configuration of these components. Helm enabled us to quickly bootstrap the project and attract developers in the Cassandra community who didn’t necessarily have much Kubernetes expertise and experience. Many of these folks found it easy to grasp a package management tool and installer like Helm.</p>&#13;
<p>As the project grew, we began to run into some limitations with Helm. While it was pretty straightforward to get the installation of K8ssandra clusters working correctly, we encountered more issues when it came to upgrading and managing clusters:</p>&#13;
<dl>&#13;
<dt>Writing complex logic</dt>&#13;
<dd><a contenteditable="false" data-primary="logic complexity, Helm and" data-type="indexterm" id="idm46183197527952"/>Helm has good support for control flow, with loops and <code>if</code> statements. However, when you start getting multiple levels deep, it’s harder to read and reason through the code, and indentation becomes an issue. In particular, we found that peer-reviewing changes to Helm charts became quite difficult.</dd>&#13;
<dt>Reuse and extensibility</dt>&#13;
<dd><p><a contenteditable="false" data-primary="reuse, Helm and" data-type="indexterm" id="idm46183198073776"/><a contenteditable="false" data-primary="extensibility, Helm and" data-type="indexterm" id="idm46183198072912"/><a contenteditable="false" data-primary="DRY (“don't repeat yourself”) principle" data-type="indexterm" id="idm46183197500416"/>Helm variables are limited to the scope of the template where you declare them, which meant we had to re-create the same variables in multiple templates. This prevented us from following the <a href="https://oreil.ly/71DTv">“don’t repeat yourself” (DRY) principle</a>, which we found to be a source of defects.</p>&#13;
<p>Similarly, Helm has a big library of helper template functions, but that library doesn’t cover every use case, and there is no interface to define your own functions. You can define your own custom templates, which allow for a lot of reuse, but those are not a replacement for functions.</p></dd>&#13;
<dt>Project structure and inheritance</dt>&#13;
<dd><a contenteditable="false" data-primary="project structure, Helm and" data-type="indexterm" id="idm46183197872352"/><a contenteditable="false" data-primary="inheritance, Helm and" data-type="indexterm" id="idm46183197817248"/>We also ran into difficulties as we tried to implement an umbrella chart design pattern, which is a best practice for Helm. We were able to create a top-level K8ssandra Helm chart with subcharts for Cassandra and Prometheus but ran into problems with variable scoping when attempting to create additional subcharts. Our intent was to define authentication settings in the top-level chart and push them down to subcharts, but this functionality is not supported by the Helm inheritance model.</dd>&#13;
<dt>Custom resource management</dt>&#13;
<dd><a contenteditable="false" data-primary="custom resources" data-type="indexterm" id="idm46183197814560"/>Helm can create Kubernetes custom resources, but it doesn’t manage them. This was a deliberate design choice that the Helm developers made for Helm 3. Because the definition of a custom resource is cluster-wide, it can get confusing if multiple Helm installs are trying to work off of different versions of a CRD. This presented us with some difficulties in managing updates to resources like a Cassandra Datacenter within Helm. The workaround was to implement custom Kubernetes jobs labeled as pre-upgrade hooks that Helm would execute on an upgrade. At some point, writing these jobs began to feel like we were writing an operator.</dd>&#13;
<dt>Multicluster deployments</dt>&#13;
<dd><p><a contenteditable="false" data-primary="multiclusters" data-secondary="Helm and multicluster deployments" data-type="indexterm" id="idm46183197662448"/>While we’ve been able to work around these Helm challenges in many cases, the next major feature on our roadmap was implementing Cassandra clusters that spanned multiple Kubernetes clusters. We realized that even without the intricacies of the network configuration, this was going to be a step beyond what we could implement effectively using Helm.</p>&#13;
<p>In the end, we realized that we were trying to make Helm do too much. It’s easy to get into a situation where you learn how to use the hammer and everything looks like a nail, but what you really need is a screwdriver. However, we don’t see Helm and operators as mutually exclusive. These are complementary approaches, and we need to use each one in terms of its strengths. We continue to use Helm to perform basic installation actions including installing operators and setting up the administrator service account used by Cassandra and other components; these are the sort of actions that package managers like Helm do best.</p>&#13;
<p>Note: this sidebar was adapted from the post <a href="https://oreil.ly/2xyX0">“We Pushed Helm to the Limit, Then Built a Kubernetes Operator”</a>.</p></dd>&#13;
</dl>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="Sanda, John" data-type="indexterm" id="idm46183197528448"/><a contenteditable="false" data-primary="" data-startref="cicd_helm" data-type="indexterm" id="idm46183198029280"/><a contenteditable="false" data-primary="" data-startref="helm_po" data-type="indexterm" id="idm46183198027904"/><a contenteditable="false" data-primary="" data-startref="op_helm" data-type="indexterm" id="idm46183198026528"/><a contenteditable="false" data-primary="" data-startref="auto_helm" data-type="indexterm" id="idm46183197481392"/><a contenteditable="false" data-primary="" data-startref="db_helm" data-type="indexterm" id="idm46183197480176"/><a contenteditable="false" data-primary="" data-startref="helm_ch" data-type="indexterm" id="idm46183197478864"/>As John Sanda notes in his commentary, Helm is a powerful tool for scripting the deployment of applications consisting of multiple Kubernetes resources, but can be less effective at managing more complex operational tasks. As you’ll see in the chapters to come, a common pattern used for data infrastructure and other complex applications is to use a Helm chart to deploy an operator, which can then in turn manage both the deployment and lifecycle of the application.</p>&#13;
</div></section>&#13;
<section class="pagebreak-before" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000003">&#13;
<h1 class="less_space">Summary</h1>&#13;
<p>In this chapter, you’ve learned how a package management tool like Helm can help you manage the deployment of applications on Kubernetes, including your database infrastructure. Along the way, you’ve also learned how to use some additional Kubernetes resources like ServiceAccounts, Secrets, and ConfigMaps. Now it’s time to round out our discussion of running databases on Kubernetes. In the next chapter, we’ll take a deeper dive into managing database operations on Kubernetes by using the operator pattern.</p>&#13;
</div></section>&#13;
</div></section></body></html>