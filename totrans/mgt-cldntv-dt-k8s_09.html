<html><head></head><body><section data-pdf-bookmark="Chapter 8. Streaming Data on Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="streaming_data_on_kubernetes">&#13;
<h1><span class="label">Chapter 8. </span>Streaming Data on Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="streaming" data-secondary="about" data-type="indexterm" id="idm46183196351296"/><a contenteditable="false" data-primary="data" data-secondary="streaming on Kubernetes" data-type="indexterm" id="data_str"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="streaming data on" data-type="indexterm" id="kub_str"/><a contenteditable="false" data-primary="streaming" data-secondary="data on Kubernetes" data-type="indexterm" id="str_kub"/>When you think about data infrastructure, persistence is the first thing that comes to mind for many—storing the state of running applications. Accordingly, our focus up to this point has been on databases and storage. It’s now time to consider the other aspects of the cloud native data stack.</p>&#13;
<p>For those of you managing data pipelines, streaming may be your starting point, with other parts of your data infrastructure being of secondary concern. Regardless of your starting place, data movement is a vitally important part of the overall data stack. In this chapter, we’ll examine how to use streaming technologies in Kubernetes to share data securely and reliably in your cloud native applications.</p>&#13;
<section data-pdf-bookmark="Introduction to Streaming" data-type="sect1"><div class="sect1" id="introduction_to_streaming">&#13;
<h1>Introduction to Streaming</h1>&#13;
<p>In <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, we defined <em>streaming</em> as the function of moving data from one point to another and, in some cases, processing data in transit. The history of streaming is almost as long as that of persistence. As data was pooling in various isolated stores, it became evident that moving data reliably was just as important as storing data reliably. In those days, it was called <em>messaging</em>. Data was transferred slowly but <span class="keep-together">deliberately,</span> which resembled something closer to postal mail. Messaging infrastructure put data in a place where it could be read asynchronously, in order, with delivery guarantees. This met a critical need when using more than one computer and is one of the foundations of distributed computing.</p>&#13;
<p>Modern application requirements have evolved from what was known as messaging into today’s definition of streaming. Typically, this means managing large volumes of data that require more immediate processing, which we call <em>near real-time</em>. Ordering and delivery guarantees become a critically important feature in the distributed applications deployed in Kubernetes and in many cases are a key enabler of the scale required. How can adding more infrastructure complexity help scale? By providing an orderly way to manage the flow from the creation of data to where it can be used and stored. Rarely are streams used as the source of truth, but more importantly, they are used as the <em>conduit</em> of truth.</p>&#13;
<p>There is a lot of software and terminology around streaming that can confuse first-time users. As with any complex topic, decomposing the parts can be helpful as we build understanding. There are three areas to evaluate when choosing a streaming system for your use case:</p>&#13;
<ul>&#13;
<li><p>Types of delivery</p></li>&#13;
<li><p>Delivery guarantees</p></li>&#13;
<li><p>Feature scope for streaming</p></li>&#13;
</ul>&#13;
<p>Let’s take a closer look at each of these areas.</p>&#13;
<section data-pdf-bookmark="Types of Delivery" data-type="sect2"><div class="sect2" id="types_of_delivery">&#13;
<h2>Types of Delivery</h2>&#13;
<p><a contenteditable="false" data-primary="delivery" data-type="indexterm" id="idm46183196035920"/><a contenteditable="false" data-primary="streaming" data-secondary="types of delivery" data-type="indexterm" id="idm46183195952032"/>To use streaming in your application, you will need to understand the delivery methods available to you from the long choice list of streaming systems. You will need to understand your application requirements to efficiently plan how data flows from producer to consumer. For example, “Does my consumer need exclusive access?” The answer will drive which system fits the requirements. <a data-type="xref" href="#delivery_types">Figure 8-1</a> shows two of the most common choices in streaming systems: point to point and publish/subscribe:</p>&#13;
&#13;
<dl>&#13;
<dt>Point to point</dt>&#13;
<dd><a contenteditable="false" data-primary="point to point delivery method" data-type="indexterm" id="idm46183196180560"/>In this data flow, data created by the producer is passed through the broker and then to a single consumer in a one-to-one relationship. This is primarily used as a way to decouple direct connections from producer to consumer. It serves as an excellent feature for resilience as consumers can be removed and added with no data loss. At the same time, the broker maintains the order and last message read, addressable by the consumer using an offset.</dd>&#13;
<dt>Publish/subscribe (pub/sub)</dt>&#13;
<dd><a contenteditable="false" data-primary="publish/subscribe delivery method" data-type="indexterm" id="idm46183196163488"/>In this delivery method, the broker serves as a distribution hub for a single producer and one or more consumers in a one-to-many relationship. Consumers subscribe to a topic and receive notifications for any new messages created by the producer—a critical component for reactive or event-driven architectures.</dd>&#13;
</dl>&#13;
&#13;
<figure><div class="figure" id="delivery_types">&#13;
<img alt="Delivery types" src="assets/mcdk_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Delivery types</h6>&#13;
</div></figure>&#13;
&#13;
</div></section>&#13;
<section data-pdf-bookmark="Delivery Guarantees" data-type="sect2"><div class="sect2" id="delivery_guarantee">&#13;
<h2>Delivery Guarantees</h2>&#13;
<p><a contenteditable="false" data-primary="delivery" data-type="indexterm" id="idm46183196002256"/><a contenteditable="false" data-primary="guarantees, delivery" data-type="indexterm" id="idm46183196002608"/><a contenteditable="false" data-primary="streaming" data-secondary="delivery guarantees" data-type="indexterm" id="idm46183196000112"/><a contenteditable="false" data-primary="contract" data-type="indexterm" id="idm46183195998736"/><a contenteditable="false" data-primary="at-least-once delivery" data-type="indexterm" id="idm46183195901552"/><a contenteditable="false" data-primary="at-most-once delivery" data-type="indexterm" id="idm46183195900576"/><a contenteditable="false" data-primary="exactly-once delivery" data-type="indexterm" id="idm46183195899600"/>In conjunction with the delivery types, the broker maintains delivery guarantees from producer to consumer per message type in an agreement called a <em>contract</em>. The typical delivery types are shown in <a data-type="xref" href="#delivery_guarantees">Figure 8-2</a>: at-most-once, at-least-once, and exactly once. The diagram shows the important relationship between when the producer sends a message and the expectation of how the consumer receives the message:</p>&#13;
&#13;
&#13;
<dl>&#13;
<dt>At-most-once</dt>&#13;
<dd>The lowest guarantee is used to avoid any potential data duplication due to transient errors that can happen in distributed systems. For example, the producer could get a timeout on send. However, the message may have just gone through without acknowledgment. In this gray area, the safest choice to avoid duplicate data will be for the producer to not attempt a resend and proceed. The critical downside to understand is that data loss is possible by design.</dd>&#13;
<dt>At-least-once</dt>&#13;
<dd>This guarantee is the opposite side of at-most-once. Data created by the producer is guaranteed to be picked up by a consumer. The added aspect allows for redelivery any number of times after the first. For example, this might be used with a unique key such as a date stamp or ID number that is considered idempotent on the consumer side that multiple processing won’t impact. The consumer will always see data delivered by the producer but could see it numerous times. Your application will need to account for this possibility.</dd>&#13;
<dt>Exactly once</dt>&#13;
<dd>The strictest of the three guarantees, this means that data created by a producer will be delivered only one time to a producer—for example, in exact transactions such as money movement, which require subtractions or additions to be delivered and processed one time to avoid problems. This guarantee puts a more significant burden on the broker to maintain, so you will need to adjust the resources allocated to the broker and your expected throughput.</dd>&#13;
</dl>&#13;
&#13;
<figure><div class="figure" id="delivery_guarantees">&#13;
<img alt="Delivery guarantees" src="assets/mcdk_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Delivery guarantees</h6>&#13;
</div></figure>&#13;
<p>Exercise care in selecting delivery guarantees for each type of message. Delivery guarantees are ones to carefully evaluate as they can have unexpected downstream effects on the consumer if not wholly understood. Questions like “Can my application handle duplicate messages?” need a good answer. “Maybe” is not good enough.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Feature Scope" data-type="sect2"><div class="sect2" id="feature_scope">&#13;
<h2>Feature Scope</h2>&#13;
<p><a contenteditable="false" data-primary="feature scope" data-type="indexterm" id="idm46183195922256"/><a contenteditable="false" data-primary="streaming" data-secondary="feature scope" data-type="indexterm" id="idm46183196519504"/>Many streaming technologies are available, some of which have been around for quite a few years. On the surface, these technologies may appear similar, but each solves a different problem because of new requirements. The majority are open source projects, so each has a community of like-minded individuals who join in and advance the project. Just as many different persistent data stores fit under the large umbrella of “database,” features under the heading of data streaming can vary significantly.</p>&#13;
<p>Feature scope is likely the most important selection criterion when evaluating which streaming technology to use. Still, you should also challenge yourself to add suitability for Kubernetes as a criterion and consider whether more complex features are worth the added resource cost. Fortunately, the price for getting your decision wrong the first time is relatively low. Streaming data systems tend to be some of the easiest to migrate because of their ephemeral nature. The deeper into your feature stack the streaming technology goes, the harder it is to move. The scope of streaming features can be broken into the two large buckets shown in <a data-type="xref" href="#streaming_types">Figure 8-3</a>:</p>&#13;
&#13;
<dl class="less_space pagebreak-before">&#13;
<dt>Message broker</dt>&#13;
<dd><a contenteditable="false" data-primary="message broker" data-type="indexterm" id="idm46183195949664"/>This is the simplest form of streaming technology that facilitates the moving of data from one point to another with one or more of the delivery methods and guarantees listed previously. It’s easy to discount this feature’s simplistic appearance, but it’s the backbone of modern cloud native applications. It’s like saying FedEx is just a package delivery company, but imagine what would happen to the world economy if it stopped for even one day? Example OSS message brokers include Apache Kafka, Apache Pulsar, RabbitMQ, and Apache ActiveMQ.</dd>&#13;
<dt>Stream analytics</dt>&#13;
<dd><a contenteditable="false" data-primary="stream analytics" data-type="indexterm" id="idm46183195926688"/>In some cases, the best or only time to analyze data is while it is moving. Waiting for data to persist and then begin the analysis could be far too late, and the insight’s value is almost useless. Consider fraud detection. The only opportunity to stop the fraudulent activity is when it’s happening; waiting for a report to run the next day just doesn’t work. Example OSS stream analytics systems include the Apache prooducts Spark, Flink, Storm, Kafka Streams, and Pulsar.</dd>&#13;
</dl>&#13;
<figure><div class="figure" id="streaming_types">&#13;
<img alt="Streaming types" src="assets/mcdk_0803.png"/>&#13;
<h6><span class="label">Figure 8-3. </span>Streaming types</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="The Role of Streaming in Kubernetes" data-type="sect1"><div class="sect1" id="the_role_of_streaming_in_kubernetes">&#13;
<h1>The Role of Streaming in Kubernetes</h1>&#13;
<p><a contenteditable="false" data-primary="CRUD (create, read, update, and delete) operations" data-type="indexterm" id="idm46183196327408"/><a contenteditable="false" data-primary="role, of streaming in Kubernetes" data-type="indexterm" id="role_str"/><a contenteditable="false" data-primary="streaming" data-secondary="role of in Kubernetes" data-type="indexterm" id="str_role"/>Now that we have covered the basic terminology, how does streaming fit into a cloud native application running on Kubernetes? Database applications follow the pattern of create, read, update and delete (CRUD). For a developer, the database provides a single location for data. The addition of streaming assumes some sort of motion in the data from one place to another. Data may be short-lived if used to create new data. Some data may be transformed in transit, and some may eventually be persisted. Streaming assumes a distributed architecture, and the way to scale a streaming system is to manage its resource allocation of compute, network, and storage. This is landing right into the sweet spot of cloud native architecture. In the case of stream-driven applications in Kubernetes, you’re managing the reliable flow of data in an environment that can change over time. Allocate what you need when you need it.</p>&#13;
<div data-type="note" epub:type="note">&#13;
<h1>Streaming and Data Engineering</h1>&#13;
<p><a contenteditable="false" data-primary="data engineers/engineering" data-type="indexterm" id="idm46183195866288"/><a contenteditable="false" data-primary="streaming" data-secondary="data engineering and" data-type="indexterm" id="idm46183195865264"/>Data engineering is a relatively new and fast-growing discipline, so we want to be sure to define it. This is especially applicable to the practice of data streaming. Data engineers are concerned with the efficient movement of data in complex environments. The two T’s are important in this case: transport and transformation. The role of the data scientist is to derive meaning and insights from data. In contrast, the data engineer is building the pipeline that collects data from various locations, organizes it, and in most cases, persists to something like a data lake. Data engineers work with application developers and data scientists to make sure application requirements are met in the increasingly distributed nature of data.</p>&#13;
</div>&#13;
<p>The most critical aspect of your speed and agility is how well your tools work together. When developers dream up new applications, how fast can that idea turn into a production deployment? Deploying and managing separate infrastructure (streaming, persistence, microservices) for one application is burdensome and prone to error. When asking why you would want to add streaming into your cloud native stack, you should consider the cost of not integrating your entire stack in terms of technical debt. Creating custom ways of moving data puts a huge burden on application and infrastructure teams. Data streaming tools are built for a specific purpose, with large communities of users and vendors to aid in your success.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="cloud_native_streaming_is_game_changing">&#13;
<h5>Cloud Native Streaming Is Game-Changing, but Remember the Fundamentals</h5>&#13;
<p><em>With Jesse Anderson, Managing Director, Big Data Institute</em></p>&#13;
<p><a contenteditable="false" data-primary="Anderson, Jesse" data-type="indexterm" id="idm46183195932080"/><a contenteditable="false" data-primary="cloud native streaming" data-type="indexterm" id="idm46183195930912"/><a contenteditable="false" data-primary="streaming" data-secondary="fundamentals of" data-type="indexterm" id="idm46183195929808"/>What makes streaming a good fit for Kubernetes? If you think about which component in your system is the most dynamic, it’s probably streaming. Your database won’t have as much need to scale up and down in the course of a day. The typical demand curve in a 24-hour period is going to require more scaling for streaming, especially the processing. If you’re moving to Kubernetes from VMs, you will be tempted to copy your exact environment into Pods and forget about it. By doing this, you are missing the primary value of cloud native for streaming workloads. In my experience, teams pre-provisioning for expected loads typically end up wasting over 50% of resources by over-provisioning. The best way to manage cost is to add resources when needed and release them when you are finished. The real measurement of success is when end users have no idea that infrastructure is coming and going. They get a smooth experience and a consistent service level. On the other hand, artificially constraining your streaming capacity because of costs can reduce response times and degrade service levels. In the worst case, the real-time processing window falls behind without any way to catch up.</p>&#13;
<p><a contenteditable="false" data-primary="Apache Kafka" data-type="indexterm" id="idm46183195928144"/>The challenge in deploying streaming workloads in Kubernetes is one of matching system architectures to balance provisioning and service levels. If the technology wasn’t designed with the idea of dynamic workload matching, it could take a lot of effort to force it to do something it wasn’t designed to accomplish. Kafka is a highly scalable distributed system, but the idea of scaling down wasn’t part of the initial design. A Kafka cluster is designed to maintain the declared operational state. If ten brokers have been deployed and one is lost, Kafka tries to return to the state of ten brokers. While this is a critically important feature for resiliency, it takes a different approach to achieve elasticity. Pulsar is an example of a streaming system that has been designed with cloud native thinking to handle dynamic workloads from day one. Flink is a stream-processing system designed with the same considerations. Used in combination, a deployment will consume compute and storage at different times and in different volumes. That is a closer match to the Kubernetes architecture.</p>&#13;
<p>Storage has been an area of rapid change for the Kubernetes project but one that you should avoid making assumptions about in your streaming deployments. When the data you are streaming needs to be persisted, where is it going? A great resilience question to ask is “What happens if I mistakenly delete my Kubernetes cluster?” I have worked with teams deploying streaming on Kubernetes that were unknowingly using ephemeral storage by mistake. You have to make sure you are thinking about the durability of your storage from the earliest stages of your move to Kubernetes. Streaming requires a higher level of operational excellence. Having five nines of uptime or better isn’t optional. In contrast to a batch system where downtime isn’t a high impact, you can just rerun the job if there is a failure. With streaming, if you are down, you’ve potentially lost data. Having an operational outage due to losing a StatefulSet can be a big deal.</p>&#13;
<p>The final thing to consider is your disaster recovery plan. Do not assume that cloud native deployments eliminate potentially devastating failures. You can mitigate many of them, but in my experience, some amount of failure is inevitable, which is why planning is so important. At a minimum, be ready for the various failures that can happen with infrastructure, such as loss of a Pod, a StatefulSet, or an entire Kubernetes cluster. The most common and impactful failures are due to human error, like purposefully deleting data thinking you are working in a QA environment, or getting a configuration wrong. It happens to everyone, and we just need to plan for it.</p>&#13;
</div></aside>&#13;
<p>For data engineers and site reliability engineers (SREs), your planning and implementation of streaming in Kubernetes can greatly impact your organization. Cloud native data should allow for more agility and speed while squeezing out all the efficiency you can get. As a reader of this book, you are already on your way to thinking differently about your infrastructure. Taking some advice from Jesse Anderson, there are two areas you should be focusing on as you begin your journey into streaming data on Kubernetes:</p>&#13;
<dl>&#13;
<dt>Resource allocation</dt>&#13;
<dd><a contenteditable="false" data-primary="resources" data-secondary="allocation of" data-type="indexterm" id="idm46183195991856"/>Are you planning for peaks as well as the valleys? As you’ll recall from <a data-type="xref" href="ch01.html#introduction_to_cloud_native_data_infra">Chapter 1</a>, elasticity is one of the more challenging aspects of cloud native data to get right. Scaling up is a commonly solved problem in large-scale systems, but scaling down can potentially result in data loss, especially with streaming systems. Traffic to resources needs to be redirected before they are decommissioned, and any data they are managing locally will need to be accounted for in other parts of the system. The risk involved with elasticity is what keeps it from being widely used, and the result is a lot of unused capacity. Commit yourself to the idea that resources should never be idle and build streaming systems that use what they need and no more.</dd>&#13;
<dt>Disaster recovery planning</dt>&#13;
<dd><a contenteditable="false" data-primary="disaster recovery planning" data-type="indexterm" id="idm46183195881024"/>Moving data efficiently is an important problem to solve, but just as important is how to manage inevitable failure. Without understanding your data flows and durability requirements, you can’t just rely on Kubernetes to handle recovery. Disaster recovery is about more than backing up data. How are Pods scheduled so that physical server failure has a reduced impact? Can you benefit from geographic redundancy? Are you clear on where data is persisted and understand the durability of those storage systems? And finally, do you have a clear plan to restore systems after a failure? In all cases, writing down the procedure is the first step, but testing those procedures is the difference between success and failure.</dd>&#13;
</dl>&#13;
<p><a contenteditable="false" data-primary="" data-startref="role_str" data-type="indexterm" id="idm46183195886640"/><a contenteditable="false" data-primary="" data-startref="str_role" data-type="indexterm" id="idm46183195844864"/>We’ve covered the what and why of streaming data on Kubernetes, and it’s time we start looking at the how with a particular focus on cloud native deployments. We’ll give a quick overview of how to install these technologies on Kubernetes and highlight some important details to aid your planning. You’ve already learned in previous chapters how to use many of the Kubernetes resources we’ll need, so we’ll speed up the pace a bit. Let’s get started on the first cloud native streaming technology.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Streaming on Kubernetes with Apache Pulsar" data-type="sect1"><div class="sect1" id="streaming_on_kubernetes_with_apache_pul">&#13;
<h1>Streaming on Kubernetes with Apache Pulsar</h1>&#13;
<p><a contenteditable="false" data-primary="Apache Pulsar" data-type="indexterm" id="ap_ab"/><a contenteditable="false" data-primary="Kubernetes" data-secondary="streaming on, with Apache Pulsar" data-type="indexterm" id="kub_ap"/><a contenteditable="false" data-primary="streaming" data-secondary="with Apache Pulsar" data-type="indexterm" id="str_ap"/>Apache Pulsar is an exciting project to watch for cloud native streaming applications. Streaming software was mostly built in an era before Kubernetes and cloud native architectures. Pulsar was originally developed at Yahoo!, which is no stranger to high-scale cloud native workloads. Donated to the Apache Software Foundation, it was accepted as a top-level project in 2018. Additional projects, like Apache Kafka or RabbitMQ, may suit your application’s needs, but they will require more planning and well-written operators to function at the level of efficiency of Pulsar. In terms of the streaming definitions we covered previously, Pulsar supports the following characteristics:</p>&#13;
<ul class="pagebreak-before">&#13;
<li class="less_space"><p>Types of delivery: one-to-one and pub/sub</p></li>&#13;
<li><p>Delivery guarantees: at-least-once, at-most-once, exactly once</p></li>&#13;
<li><p>Feature scope for streaming: message broker, analytics (through functions)</p></li>&#13;
</ul>&#13;
<p>So what makes Pulsar a good fit for Kubernetes?</p>&#13;
<p>We use Kubernetes to create virtual datacenters to efficiently use compute, network, and storage. Pulsar was designed from the beginning with a separation of compute and storage resource types linked by the network, similar to a microservices <span class="keep-together">architecture.</span></p> &#13;
&#13;
<p>These resources can even span multiple Kubernetes clusters or physical datacenters, as shown in <a data-type="xref" href="#apache_pulsar_architecture">Figure 8-4</a>. Deployment options give operators the flexibility to install and scale a running Pulsar cluster based on use case and workload. Pulsar was also designed with multitenancy in mind, making a big efficiency difference in large deployments. Instead of installing a separate Pulsar instance per application, many applications (tenants) can use one Pulsar instance with guardrails to prevent resource contention. Finally, built-in storage tiering creates automated alternatives for storage persistence as data ages, and lower-cost storage can be utilized.</p>&#13;
&#13;
<figure><div class="figure" id="apache_pulsar_architecture">&#13;
<img alt="Apache Pulsar architecture" src="assets/mcdk_0804.png"/>&#13;
<h6><span class="label">Figure 8-4. </span>Apache Pulsar architecture</h6>&#13;
</div></figure>&#13;
&#13;
<p class="pagebreak-before">Pulsar’s highest level of abstraction is an instance that consists of one or more clusters. We call the local logical administration domain a <em>cluster</em> and deploy in a Kubernetes cluster, where we’ll concentrate our attention. Clusters can share <span class="keep-together">metadata</span> and configuration, allowing producers and consumers to see a single system regardless of location. Each cluster is made of several parts acting in concert that primarily consume either compute or storage. They are:</p>&#13;
<dl>&#13;
<dt>Broker (compute)</dt>&#13;
<dd><a contenteditable="false" data-primary="Brokers (Apache Pulsar)" data-type="indexterm" id="idm46183196286208"/><a contenteditable="false" data-primary="Apache Bookkeeper" data-type="indexterm" id="idm46183195794128"/>Producers and consumers pass messages via the broker, a stateless cluster component. This means it is purely a compute scaling unit and can be dynamically allocated based on the number of tenants and connections. Brokers maintain an HTTP endpoint used for client communication, which presents a few options for network traffic in a Kubernetes deployment. When multiple clusters are used, the brokers support replication between clusters in the instance. Brokers can run in a memory-only configuration, or with Apache BookKeeper (labeled as <em>bookies</em>) when message durability is required.</dd>&#13;
<dt>Apache BookKeeper (storage)</dt>&#13;
<dd>The BookKeeper project provides infrastructure for managing distributed write-ahead logs. In Pulsar, the individual instances used are called <em>bookies</em>. The storage unit is called a <em>ledger</em>; each topic can have one or more ledgers. Multiple bookie instances provide load-balancing and failure protection. They also offer storage tiering functionality, allowing operators to offer a mix of fast and long-term storage options based on use case. When brokers interact with bookies, they read and write to a topic ledger, an append-only data structure. Bookies provide a single reference to the ledger but manage the replication and load balancing behind the primary interface. In a Kubernetes environment, knowing where data is stored is critical for maintaining resilience.</dd>&#13;
<dt>Apache ZooKeeper (compute)</dt>&#13;
<dd><a contenteditable="false" data-primary="Apache ZooKeeper" data-type="indexterm" id="idm46183195797696"/>ZooKeeper is a standalone project used in many distributed systems for coordination, leader election, and metadata management. Pulsar uses ZooKeeper for service coordination, similar to the way etcd is used in a Kubernetes cluster, storing important metadata such as tenants, topics, and cluster configuration state so that the brokers can remain stateless. Bookies use ZooKeeper for ledger metadata and coordination between multiple storage nodes.</dd>&#13;
<dt>Proxy (network)</dt>&#13;
<dd><a contenteditable="false" data-primary="Proxy (Apache Pulsar)" data-type="indexterm" id="idm46183196070000"/>The proxy is a solution for dynamic environments like Kubernetes. Instead of exposing every broker to HTTP traffic, the proxy serves as a gateway and creates an Ingress route to the Pulsar cluster. As brokers are added and removed, the proxy uses service discovery to keep the connections flowing to and from the cluster. When using Pulsar in Kubernetes, the proxy service IP should be the single access for your applications to a running Pulsar cluster.</dd>&#13;
<dt>Functions (compute)</dt>&#13;
<dd><a contenteditable="false" data-primary="Functions (Apache Pulsar)" data-type="indexterm" id="idm46183196065424"/>Since Pulsar Functions operate independently and consume their own compute resources, we chose not to include them in <a data-type="xref" href="#apache_pulsar_architecture">Figure 8-4</a>. However, they’re worth mentioning in this context because Pulsar Functions work in conjunction with the message broker. When deployed, they take data from a topic, alter it with user code, and return it to a different topic. The component added to a Pulsar cluster is the worker, which accepts function runtimes on an ad hoc basis. Operators can deploy Functions as a part of a larger cluster or standalone for more fine-grained resource management.</dd>&#13;
</dl>&#13;
<section data-pdf-bookmark="Preparing Your Environment" data-type="sect2"><div class="sect2" id="preparing_your_environment">&#13;
<h2>Preparing Your Environment</h2>&#13;
<p><a contenteditable="false" data-primary="environments, preparing" data-type="indexterm" id="idm46183195775936"/><a contenteditable="false" data-primary="streaming" data-secondary="preparing environment for" data-type="indexterm" id="idm46183196887056"/>When preparing to do your first installation, you need to make some choices. Since every user will have unique needs, we recommend you check the <a href="https://oreil.ly/KCqT2">official documentation</a> for the most complete and up-to-date information on installing Pulsar in Kubernetes before reading this section. The examples within this section will take a closer look at the choices available and how they pertain to different cloud native application use cases to help inform your decision making.</p>&#13;
<p>To begin, create a local clone directory of the Pulsar Helm chart repository:</p>&#13;
<pre data-type="programlisting"><strong>git clone https://github.com/apache/pulsar-helm-chart</strong></pre>&#13;
<p>This subproject of Pulsar is well documented, with several helpful examples to follow. When using Helm to deploy Pulsar, you will need a <em>values.yaml</em> file that contains all of the options to customize your deployment. You can include as many parameters as you want to change. The Pulsar Helm chart has a complete set of defaults for a typical cluster that might work for you, but you will want to tune the values for your specific environment. The <em>examples</em> directory has various deployment scenarios. If you choose the default installation as described in the <em>values-local-cluster.yaml</em> file, you’ll have a set of resources like that shown in <a data-type="xref" href="#a_simple_pulsar_installation_on_kuberne">Figure 8-5</a>. As you can see, the installation wraps the proxy and brokers in Deployments and presents a unified service endpoint for applications.</p>&#13;
&#13;
<p>Affinity is a mechanism built into Kubernetes to create rules for which Pods can and cannot be colocated on the same physical node (if needed, refer to the more in-depth discussion in <a data-type="xref" href="ch04.html#automating_database_deployment_on_kuber">Chapter 4</a>). Pulsar, being a distributed system, has deployment requirements for maximum resilience. An example is brokers. When multiple brokers are deployed, each Pod should run on a different physical node in case of failure. If all broker Pods were grouped on the same node and the node went down, the Pulsar cluster would be unavailable. Kubernetes would still recover the runtime state and restart the Pods. However, there would be downtime as they came back online.</p> &#13;
&#13;
<figure><div class="figure" id="a_simple_pulsar_installation_on_kuberne">&#13;
<img alt="A Simple Pulsar installation on Kubernetes" src="assets/mcdk_0805.png"/>&#13;
<h6><span class="label">Figure 8-5. </span>A Simple Pulsar installation on Kubernetes</h6>&#13;
</div></figure>&#13;
&#13;
<p>The easiest thing is not allowing Pods of the same type to group together onto the same nodes. When enabled, anti-affinity will keep this from happening. If you are running on a single-node system such as a desktop, disabling it will allow your cluster to start without blocking based on affinity:</p>&#13;
<pre data-type="programlisting">affinity:&#13;
  anti_affinity: true</pre>&#13;
<p>Fine-grained control over Pulsar component replica counts lets you tailor your deployment based on the use case. Each replica Pod consumes resources and should be considered in the application’s lifecycle. For example, starting with a low number of brokers and BookKeeper Pods can manage some level of traffic. Still, more replicas can be added and configuration updated via Helm as traffic increases:</p>&#13;
<pre data-type="programlisting">zookeeper:&#13;
  replicaCount: 1&#13;
&#13;
bookkeeper:&#13;
  replicaCount: 1&#13;
&#13;
broker:&#13;
  replicaCount: 1&#13;
&#13;
proxy:&#13;
  replicaCount: 1</pre>&#13;
<p>You now have a foundational understanding of how to reliably move data to and from applications and outside of your Kubernetes cluster. Pulsar is a great fit for cloud native application deployments because it can scale compute and storage independently. The declarative nature of deployments makes it easy for data engineers and SREs to deploy easily with consistency. Now that we have the means for data communication, let’s take it a step further with the right kind of network security.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Securing Communications by Default with cert-manager" data-type="sect2"><div class="sect2" id="securing_communications_by_default_with">&#13;
<h2>Securing Communications by Default with cert-manager</h2>&#13;
<p><a contenteditable="false" data-primary="cert-manager, securing communications by default with" data-type="indexterm" id="cert_sec"/><a contenteditable="false" data-primary="communications, securing by default with cert-manager" data-type="indexterm" id="comm_sec"/><a contenteditable="false" data-primary="streaming" data-secondary="securing communication by default with cert-manager" data-type="indexterm" id="str_sec"/>An unfortunate reality we face at the end of product development is what gets left to complete: security or documentation. Unfortunately, Kubernetes doesn’t have much in the way of building documentation, but when it comes to security, there has been some great progress on starting earlier without compromise!</p>&#13;
<p><a contenteditable="false" data-primary="TLS (Transport Layer Security)" data-type="indexterm" id="idm46183195866944"/><a contenteditable="false" data-primary="SSL (Secure Socket Layer)" data-type="indexterm" id="idm46183195912672"/><a contenteditable="false" data-primary="CA (certificate authority)" data-type="indexterm" id="idm46183195861680"/><a contenteditable="false" data-primary="ACME (Automated Certificate Management Environment) protocol" data-type="indexterm" id="idm46183196045072"/><a contenteditable="false" data-primary="X.509 certificates" data-type="indexterm" id="idm46183195840400"/>As you can see, installing Pulsar has created a lot of infrastructure and communication between the elements. High traffic volume is a typical situation. When we build out virtual datacenters in Kubernetes, it will create a lot of <a href="https://oreil.ly/YySn7">internode</a> and external network traffic. All traffic should be encrypted with Transport Layer Security (TLS) and Secure Socket Layer (SSL) using <a href="https://oreil.ly/JG794">X.509 certificates</a>. The most important part of this system is the certificate authority (CA). In a public key infrastructure (PKI) arrangement acts as a trusted third party that digitally signs certificates used to create a chain of trust between two entities. Going through the procedure to have a certificate issued by a CA historically has been a manual and arduous process, which unfortunately has led to a lack of secure communications in cloud-based applications.</p>&#13;
<p><em>cert-manager</em> is a tool that uses the Automated Certificate Management Environment (ACME) protocol to add certificate management seamlessly to your Kubernetes infrastructure. We should always use TLS to secure the data moving from one service to another for our streaming application. The cert-manager project is arguably one of the most critical pieces of your Kubernetes infrastructure that you will eventually forget about. That’s the hallmark of a project that fits the moniker of “it just works.”</p>&#13;
&#13;
<p>Adding TLS to your Pulsar deployment has been made incredibly easy with just a few configuration steps. Before installing Pulsar, you’ll need to set up the cert-manager service inside the target Kubernetes cluster. First, add the cert-manager repo to your local Helm installation:</p>&#13;
<pre data-type="programlisting"><strong>helm repo add jetstack https://charts.jetstack.io</strong></pre>&#13;
&#13;
<div data-type="note" epub:type="note">&#13;
<h1>What Is ACME?</h1>&#13;
<p>When working with X.509 certificates, you’ll frequently see references to the Automated Certificate Management Environment (ACME). ACME allows for automated deployment of certificates between user infrastructure and certificate authorities. It was designed by the Internet Security Research Group when it was building its free certificate authority, Let’s Encrypt. It would be putting it lightly to say this fantastic free service has been a game-changer for cloud native infrastructure.</p>&#13;
</div>&#13;
&#13;
<p>The installation process takes a few parameters, which you should make sure to use. First is declaring a separate Namespace to keep the cert-manager neatly organized in your virtual datacenter. The second is installing the CRD assets. This combination allows you to create services that automate your certificate management:</p>&#13;
<pre data-type="programlisting"><strong>helm install \</strong>&#13;
  <strong>cert-manager jetstack/cert-manager \</strong>&#13;
  <strong>--namespace cert-manager \</strong>&#13;
  <strong>--create-namespace \</strong>&#13;
  <strong>--set installCRDs=true</strong></pre>&#13;
<p>After the cert-manager is installed, you’ll then need to configure the certificate issuer that will be called when new certificates are needed. You have many options based on the environment you are operating in, and these are covered quite extensively in the documentation. One of the custom resources created when installing cert-manager is <code>Issuer</code>. The most basic <code>Issuer</code> is the <code>selfsigned-issuer</code> that can create a certificate with a user-supplied private key. You can create a basic <code>Issuer</code> by applying the following YAML configuration:</p>&#13;
<pre data-type="programlisting">apiVersion: cert-manager.io/v1&#13;
kind: Issuer&#13;
metadata:&#13;
  name: selfsigned-issuer&#13;
  namespace: cert-manager&#13;
spec:&#13;
  selfSigned: {}&#13;
---&#13;
apiVersion: cert-manager.io/v1&#13;
kind: ClusterIssuer&#13;
metadata:&#13;
  name: selfsigned-cluster-issuer&#13;
spec:&#13;
  selfSigned: {}&#13;
</pre>&#13;
<p>When installing Pulsar with Helm, you can secure inter-service communication with a few lines of YAML configuration. You can pick which services are secured by setting the TLS <code>enabled</code> to <code>true</code> or <code>false</code> for each service in the YAML that defines your Pulsar cluster. The examples provided by the project are quite large, so for brevity, we’ll look at some key areas:</p>&#13;
<pre data-type="programlisting">tls:&#13;
  # settings for generating certs for proxy&#13;
  proxy:&#13;
    enabled: true&#13;
    cert_name: tls-proxy&#13;
  # settings for generating certs for broker&#13;
  broker:&#13;
    enabled: true&#13;
    cert_name: tls-broker&#13;
  # settings for generating certs for bookies&#13;
  bookie:&#13;
    enabled: false&#13;
    cert_name: tls-bookie&#13;
  # settings for generating certs for zookeeper&#13;
  zookeeper:&#13;
    enabled: false&#13;
    cert_name: tls-zookeeper</pre>&#13;
<p>Alternatively, you can secure the entire cluster with just one command:</p>&#13;
<pre data-type="programlisting">tls:&#13;
  enabled: true</pre>&#13;
<p>Later in your configuration file, you can use self-signing certificates to create TLS connections between components:</p>&#13;
<pre data-type="programlisting"># issue selfsigning certs&#13;
certs:&#13;
  internal_issuer:&#13;
    enabled: true&#13;
    type: selfsigning</pre>&#13;
<p>If you have been involved in securing infrastructure communication any time in the past, you know the toil in working through all the steps and applying TLS. Inside a Kubernetes virtual datacenter, you no longer have an excuse to leave network communication unencrypted. With a few lines of configuration, everything is secured and maintained.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="cert_manager_making_security_easy_left">&#13;
<h5>cert-manager: Making Security Easy (So You’ll Just Use It)</h5>&#13;
<p><em>With Josh van Leeuwen, Software Engineer, Jetstack</em></p>&#13;
<p><a contenteditable="false" data-primary="van Leeuwen, Josh" data-type="indexterm" id="idm46183197466224"/>The cert-manager is a project born of necessity as our cloud native world grows. Previously, you might have a bunch of VMs or bare metal running somewhere in a ringed fence. You could get away with sticking an SSL certificate in the front gateway and moving on. All of that has now changed, with the thousands or even hundreds of thousands of machines that need to be secured in our cloud native systems. With all of these small containers running microservices continually coming and going, automation is the only way to manage the volume of changes. There is no way a human can do that alone. Of course, this opens a new challenge of reliable automation—one that Kubernetes has taken head-on.</p>&#13;
<p>Soon after the ACME protocol was created, custom resources and CRDs became a feature in Kubernetes. cert-manager is a project that joins those two concepts, providing a declarative way to represent what an X.509 certificate should look like inside a Kubernetes Deployment. ACME happened at just the right time for the Kubernetes Ingress use case, and the first use case for cert-manager was for ACME SSL certificates. However, it quickly became apparent that this would not be the only secure networking problem that needed solving in Kubernetes. Those growing numbers of machines all need to talk to each other, and they all need some kind of security in place, which is generally done with TLS. TLS certificates require the concept of an issuer, and cert-manager was expanded to allow for different types of issuers to automate the complete lifecycle further of those certificates.</p>&#13;
<p><a contenteditable="false" data-primary="GPG (GNU Privacy Guard)" data-type="indexterm" id="idm46183196050016"/>Because it emerged so early in the project, cert-manager has become the de facto X.509 provider and certificate manager for Kubernetes. With this comes a responsibility to make securing communications in Kubernetes easy. Security is only as good as it is easy. If security is challenging to implement, it’s practically useless. Many people don’t like GNU Privacy Guard (GPG), for these reasons—not because it’s necessarily flawed security-wise, but because it’s challenging to use. cert-manager should continue to see wide adoption in cloud native applications. It makes everything secure by default, with little intervention or minimal knowledge of how Rivest–<span class="keep-together">Shamir</span>–Adleman (RSA) or TLS works. It’s a project that is easy to use and solves people’s problems by default.</p>&#13;
<p>One thing that has made cert-manager easy for end users is having a well-defined API to describe their application requirements in a simple way. It is a way of abstracting the more complicated questions, such as “What does it mean to have a certificate signed?” or “What is an issuer?” These APIs provide the guardrails to make sure you do the right thing as much as possible. Some things still require planning and thoughtfulness, such as not reusing private key passwords, which is allowed but discouraged.</p>&#13;
<p>Guardrails and standardization are topics that need to become more prevalent in other parts of Kubernetes. The declarative nature and extensibility of Kubernetes are powerful tools, but with great power comes great responsibility. Different people within an organization can make extension points in a Kubernetes cluster. With a single command, you can have an endpoint exposed on the internet without even realizing it. No single pane of glass is available to security professionals for those extensions. Nor are there guardrails to prevent unexpected behaviors. Without proper guardrails in place, it’s too easy to self-own quite badly. As Kubernetes matures, we’ll need more ways to avoid unhappy accidents.</p>&#13;
<p>The cert-manager project is in an excellent state, being vendor-neutral and mature in its current form. If you search the project changelog for the word “feature,” you’ll see a decrease in occurrence in each successive release. This means we have a core API that is useful and stable, which is an excellent place to be for a core security-based project. The bulk of changes happening in the project are focused on taking advantage of this stable core API to add new issuers. This stability ensures that the project stays up-to-date with the latest requirements without a disruptive breaking change.</p>&#13;
<p>As for the future, the cert-manager project will continue to work with the Kubernetes community to continue the path of “default secure” and make security so easy that it’s used universally. There are still some challenges to overcome, like how secrets are stored and how to manage trust chains, and the momentum of Kubernetes practically ensures that those are problems that will be solved shortly. If these are interesting problems, I urge you to get involved in one of the many ways security professionals can impact the future of Kubernetes.</p>&#13;
</div></aside>&#13;
<p><a contenteditable="false" data-primary="" data-startref="cert_sec" data-type="indexterm" id="idm46183195829008"/><a contenteditable="false" data-primary="" data-startref="comm_sec" data-type="indexterm" id="idm46183195990720"/><a contenteditable="false" data-primary="" data-startref="str_sec" data-type="indexterm" id="idm46183195707952"/>cert-manager should be one of the first things you install in a new Kubernetes cluster. The combination of project maturity and simplicity makes security the easy first thing to add to your project instead of the last. This is true not only for Pulsar but for every service you deploy in Kubernetes that requires network communication.</p>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Using Helm to Deploy Apache Pulsar" data-type="sect2"><div class="sect2" id="using_helm_to_deploy_apache_pulsar">&#13;
<h2>Using Helm to Deploy Apache Pulsar</h2>&#13;
<p><a contenteditable="false" data-primary="Helm" data-secondary="deploying Apache Pulsar using" data-type="indexterm" id="idm46183195943904"/><a contenteditable="false" data-primary="streaming" data-secondary="deploying Apache Pulsar using Helm" data-type="indexterm" id="idm46183195733536"/>Now that we have covered how to design a Pulsar cluster to maximize resources, you can use Helm to carry out the deployment into Kubernetes. First, add the Pulsar Helm repository:</p>&#13;
<pre data-type="programlisting"><strong>helm repo add apache https://pulsar.apache.org/charts</strong></pre>&#13;
<p>One of the special requirements for a Helm install of Pulsar is preparing Kubernetes. The Git repository you cloned earlier has a script that will run through all the preparations, such as creating the destination Namespace. The more complicated setup is the roles with associated keys and tokens. These are important for inter-service communication inside the Pulsar cluster. From the docs, you can invoke the prep script by using this example:</p>&#13;
<pre data-type="programlisting"><strong>./scripts/pulsar/prepare_helm_release.sh -n <em>&lt;k8s-namespace&gt;</em> -k <em>&lt;release-name&gt;</em></strong></pre>&#13;
<p>Once the Kubernetes cluster has been prepared for Pulsar, the final installation can be run. At this point, you should have a YAML configuration file with the settings you need for your Pulsar use case as we described earlier. The <code>helm install</code> command will take that config file and direct Kubernetes to meet the desired state you have specified. When creating a new cluster, use <code>initalize=true</code> to create the base metadata configuration in ZooKeeper:</p>&#13;
<pre data-type="programlisting"><strong>helm install \</strong>&#13;
    <strong>--values <em>&lt;config yaml file&gt;</em> \</strong>&#13;
    <strong>--set initialize=true \</strong>&#13;
    <strong>--namespace <em>&lt;namespace from prepare script&gt;</em> \</strong>&#13;
    <strong><em>&lt;pulsar cluster name&gt;</em> apache/pulsar</strong></pre>&#13;
<p><a contenteditable="false" data-primary="" data-startref="ap_ab" data-type="indexterm" id="idm46183195690896"/><a contenteditable="false" data-primary="" data-startref="kub_ap" data-type="indexterm" id="idm46183195689680"/><a contenteditable="false" data-primary="" data-startref="str_ap" data-type="indexterm" id="idm46183195688464"/>In a typical production deployment, you should expect the setup time to take 10 minutes or more. There are a lot of dependencies to walk through as ZooKeeper, bookies, brokers, and finally, proxies are brought online and in order.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Stream Analytics with Apache Flink" data-type="sect1"><div class="sect1" id="stream_analytics_with_apache_flink">&#13;
<h1>Stream Analytics with Apache Flink</h1>&#13;
<p><a contenteditable="false" data-primary="analytics" data-secondary="streaming with Apache Flink" data-type="indexterm" id="ana_af"/><a contenteditable="false" data-primary="Apache Flink" data-secondary="streaming analytics with" data-type="indexterm" id="af_str"/><a contenteditable="false" data-primary="streaming" data-secondary="analytics with Apache Flink" data-type="indexterm" id="str_af"/>Now, let’s look at a different type of streaming project that is quickly gaining popularity in cloud native deployments: Apache Flink. Flink is a system primarily designed to focus on stream analytics at an incredible scale. As we discussed at the beginning of the chapter, streaming systems come in many flavors, and this is a perfect example. Flink has its competencies that overlap very little with other systems; in fact, it’s widespread to see Pulsar and Flink deployed together to complement each other’s strengths in a cloud native application.</p>&#13;
<p>As a streaming system, the following are available in Flink:</p>&#13;
<ul>&#13;
<li><p>Type of delivery: one-to-one</p></li>&#13;
<li><p>Delivery guarantee: exactly once</p></li>&#13;
<li><p>Feature scope for streaming: analytics</p></li>&#13;
</ul>&#13;
<p>The two main components of the Flink architecture are shown in <a data-type="xref" href="#apache_flink_architecture">Figure 8-6</a>—the JobManager and TaskManager:</p>&#13;
&#13;
<dl>&#13;
<dt>JobManager</dt>&#13;
<dd><a contenteditable="false" data-primary="JobManager" data-type="indexterm" id="idm46183195674304"/>This is the control plane for any running Flink application code deployed. A JobManager consumes CPU resources but only to maintain Job control; no actual processing is done on the JobManager. In high availability (HA) mode, which is exclusive to Flink running on Kubernetes, multiple standby JobManagers will be provisioned but remain idle until the primary is no longer available.</dd>&#13;
<dt>TaskManager</dt>&#13;
<dd><a contenteditable="false" data-primary="TaskManager" data-type="indexterm" id="idm46183195909904"/>This is where the work gets done on a running Flink job. The JobManger uses TaskManagers to satisfy the chain of tasks needed in the application. A chain is the order of operation. In some cases, these operations can be run in parallel, and some need to be run in series. The TaskManger will run only one discrete task and pass it on. Resource management can be controlled through the number of TaskManagers in a cluster and execution slots per TaskManager. The current guidance says that you should allocate one CPU to each TaskManager or slot.</dd>&#13;
</dl>&#13;
<figure><div class="figure" id="apache_flink_architecture">&#13;
<img alt="Apache Flink architecture" src="assets/mcdk_0806.png"/>&#13;
<h6><span class="label">Figure 8-6. </span>Apache Flink architecture</h6>&#13;
</div></figure>&#13;
<p>The Flink project is designed for managing stateful computations, which should cause you to immediately think of storage requirements. Every transaction in Flink is guaranteed to be strongly consistent with no single point of failure. These are the features you need when you are trying to build the kind of highly scalable systems that Flink was designed to accomplish. There are two types of streaming, bounded and unbounded:</p>&#13;
<dl>&#13;
<dt>Unbounded streaming</dt>&#13;
<dd><a contenteditable="false" data-primary="unbounded streaming" data-type="indexterm" id="idm46183196186448"/><a contenteditable="false" data-primary="streaming" data-secondary="unbounded" data-type="indexterm" id="idm46183196190624"/>These streaming systems react to new data whenever the data arrives—there is no endpoint where you can stop and analyze the data gathered. Every piece of data received is independent. The use cases for this can be alerting on values or counting when exactness is essential. Reactive processing can be very resource-efficient.</dd>&#13;
<dt>Bounded streaming</dt>&#13;
<dd><a contenteditable="false" data-primary="bounded streaming" data-type="indexterm" id="idm46183195913616"/><a contenteditable="false" data-primary="streaming" data-secondary="bounded" data-type="indexterm" id="idm46183195779872"/>This is also known as <em>batch processing</em> in other systems but is a specific case within Flink. Bounded windows can be marked by time or specific values. In the case of time windows, they can also slide forward, giving the ability to do rolling updates on values. Resource considerations should be given based on the data window size to be processed. The limit of the boundary size is constrained mainly by memory.</dd>&#13;
</dl>&#13;
<p>One of the foundational tenets of Flink is a strong focus on operations. At the scale required for cloud native applications, easy to use and deploy can be the difference between using it or not. This includes core support for continuous deployment workloads in Kubernetes and feature parity with cloud native applications in the areas of reliability and observability:</p>&#13;
<dl>&#13;
<dt>Continuous deployment</dt>&#13;
<dd><a contenteditable="false" data-primary="CI/CD" data-secondary="Apache Flink and" data-type="indexterm" id="idm46183195658736"/>The core unit of work for Flink is called a <em>job</em>. Jobs are Java or Scala programs that define how the data is read, analyzed, and output. Jobs are chained together and compiled into a JAR file to create a Flink application. Flink provides a Docker image that encapsulates the application in a form that makes deployment on Kubernetes an easy task and facilitates continuous deployment.</dd>&#13;
<dt>Reliability</dt>&#13;
<dd><a contenteditable="false" data-primary="reliability, Apache Flink and" data-type="indexterm" id="idm46183195792016"/>Flink also has built-in support for savepoints, which makes updates easier by pausing and resuming jobs before and after system updates. Savepoints can also be used for fast recovery if a processing Pod fails mid-job. Tighter integration with Kubernetes allows Flink to self-heal on failure by restoring Pods and restarting Jobs with savepoints.</dd>&#13;
<dt>Observability</dt>&#13;
<dd><a contenteditable="false" data-primary="observability" data-secondary="Apache Flink and" data-type="indexterm" id="idm46183195790752"/>Cluster metrics are instrumented to output in Prometheus format. Operations teams can keep track of lifecycle events inside the Flink cluster with time-based details. Application developers can expose custom metrics using the <a href="https://oreil.ly/0x0IS">Flink metric system</a> for further integrated observability.</dd>&#13;
</dl>&#13;
<p>Flink provides a way for data teams to participate in the overall cloud native stack while giving operators everything needed to manage the entire deployment. Application developers building microservices can share a CI/CD pipeline with developers building the stream analytics of data generated from the application. As changes occur in any part of the stack, they can be integration tested entirely and deployed as a single unit. Teams can move faster with more confidence knowing there aren’t disconnected requirements that may show up in production. This sort of outcome is a solid argument to employ cloud native methodologies in your entire stack, so it’s time to see how this is done.</p>&#13;
<section data-pdf-bookmark="Deploying Apache Flink on Kubernetes" data-type="sect2"><div class="sect2" id="deploying_apache_flink_on_kubernetes">&#13;
<h2>Deploying Apache Flink on Kubernetes</h2>&#13;
<p><a contenteditable="false" data-primary="Apache Flink" data-secondary="deploying on Kubernetes" data-type="indexterm" id="af_dep"/>When deploying a Flink cluster into a running Kubernetes cluster, there are a few things to consider. The Flink project has gone the route of offering what it calls “Kubernetes Native,” which programmatically installs the required Flink components without <code>kubectl</code> or Helm. These choices may change in the future. Side projects in the Flink ecosystem already bring a more typical experience that Kubernetes operators might expect, including operators and Helm charts. For now, we will discuss the official method endorsed by the project.</p>&#13;
<p><a contenteditable="false" data-primary="JobManager" data-type="indexterm" id="idm46183196018224"/><a contenteditable="false" data-primary="TaskManager" data-type="indexterm" id="idm46183196024208"/>As shown in <a data-type="xref" href="#deploying_flink_on_kubernetes">Figure 8-7</a>, a running Flink cluster has two main components we’ll deploy in Pods: the <em>JobManager</em> and <em>TaskManager</em>. These are the basic units, but choosing which deployment mode is the critical consideration for your use case. They dictate how compute and network resources are utilized. Another thing of note is how to deploy on Kubernetes. As mentioned before, there are no official project <span class="keep-together">operators</span> or Helm charts. The Flink <a href="https://flink.apache.org/downloads.html">distribution</a> contains command-line tools that will deploy into a running Kubernetes cluster based on the mode for your application.</p>&#13;
<figure><div class="figure" id="deploying_flink_on_kubernetes">&#13;
<img alt="Deploying Flink on Kubernetes" src="assets/mcdk_0807.png"/>&#13;
<h6><span class="label">Figure 8-7. </span>Deploying Flink on Kubernetes</h6>&#13;
</div></figure>&#13;
<p><a data-type="xref" href="#apache_flink_modes">Figure 8-8</a> shows the modes available for deploying Flink clusters in Kubernetes: Application Mode and Session Mode. Flink also supports a third mode called Per-Job Mode, but this is not available for Kubernetes deployments, which leaves us with Application Mode and Session Mode.</p>&#13;
&#13;
<p><a contenteditable="false" data-primary="Session Mode (Apache Flink)" data-type="indexterm" id="idm46183195638640"/>The selection of either Application Mode or Session Mode comes down to resource management inside your Kubernetes cluster, so let’s look at both to make an informed decision.</p>&#13;
&#13;
&#13;
&#13;
<p><a contenteditable="false" data-primary="Application Mode (Apache Flink)" data-type="indexterm" id="idm46183195699776"/><em>Application Mode</em> isolates each Flink application into its own cluster. As a reminder, a Flink application JAR can consist of multiple jobs chained together. The startup cost of the cluster can be minimized with a single application initialization and Job graph. Once deployed, resources are consumed for client traffic and execution of the jobs in the application. Network traffic is much more efficient since there is only one JobManager and client traffic can be multiplexed.</p>&#13;
&#13;
<figure><div class="figure" id="apache_flink_modes">&#13;
<img alt="Apache Flink Modes" src="assets/mcdk_0808.png"/>&#13;
<h6><span class="label">Figure 8-8. </span>Apache Flink modes</h6>&#13;
</div></figure>&#13;
&#13;
<p><a contenteditable="false" data-primary="flink command" data-type="indexterm" id="idm46183195767056"/>To start in Application Mode, you invoke the <code>flink</code> command with the target of <code>kubernetes-application</code>. You will need the name of the running Kubernetes cluster accessible via <code>kubectl</code>. The application to be run is contained in a Docker image, and the path to the JAR file supplied in the command line. Once started, the Flink cluster is created, application code is initialized, and will then be ready for client connections:</p>&#13;
<pre data-type="programlisting"><strong>./bin/flink run-application \</strong>&#13;
    <strong>--target kubernetes-application \</strong>&#13;
    <strong>-Dkubernetes.cluster-id=<em>&lt;kubernetes cluster name&gt;</em> \</strong>&#13;
    <strong>-Dkubernetes.container.image=<em>&lt;custom docker image name&gt;</em> \</strong>&#13;
    <strong>local:///opt/flink/usrlib/my-flink-job.jar</strong></pre>&#13;
<p><em>Session Mode</em> changes resource management by creating a single Flink cluster that can accept any number of applications on an ad hoc basis. Instead of having multiple independent clusters running and consuming resources, you may find it more efficient to have a single cluster that can grow and shrink when new applications are submitted. The downside for operators is that you now have a single cluster that will take several applications with it if it fails. Kubernetes will restart the downed Pods, but you will have a recovery time to manage as resources are reallocated. To start in Session Mode, use the <code>kubernetes-session</code> shell file and give it the name of your running Kubernetes cluster. The default is for the command to execute and detach from the cluster. To reattach or remain in an interactive mode with the running cluster, use the <code>execution.attached=true</code> switch:</p>&#13;
<pre data-type="programlisting"><strong>./bin/kubernetes-session.sh \</strong>&#13;
    <strong>-Dkubernetes.cluster-id=<em>&lt;kubernetes cluster name&gt;</em> \</strong>&#13;
    <strong>-Dexecution.attached=true</strong></pre>&#13;
<p><a contenteditable="false" data-primary="Hueske, Fabian, Stream Processing with Apache Flink" data-type="indexterm" id="idm46183195648032"/><a contenteditable="false" data-primary="Kalavri, Vasiliki, Stream Processing with Apache Flink" data-type="indexterm" id="idm46183195648384"/><a contenteditable="false" data-primary="Stream Processing with Apache Flink (Hueske and Kalavri)" data-type="indexterm" id="idm46183195622944"/><a contenteditable="false" data-primary="" data-startref="ana_af" data-type="indexterm" id="idm46183195621968"/><a contenteditable="false" data-primary="" data-startref="af_str" data-type="indexterm" id="idm46183195620752"/><a contenteditable="false" data-primary="" data-startref="str_af" data-type="indexterm" id="idm46183195619536"/><a contenteditable="false" data-primary="" data-startref="af_dep" data-type="indexterm" id="idm46183195618320"/>This was a quick fly-by of a massive topic, but hopefully, it inspires you to look further. One resource we recommend is <a href="https://oreil.ly/Iocv6"><em>Stream Processing with Apache Flink</em></a> by Fabian Hueske and Vasiliki Kalavri (O’Reilly). Adding Flink to your application isn’t just about choosing a platform to perform stream processing. In cloud native applications, we should be thinking holistically about the entire application stack we are attempting to deploy in Kubernetes. Flink uses containers, as encapsulation lends itself to working with other development workflows<a contenteditable="false" data-primary="" data-startref="data_str" data-type="indexterm" id="idm46183195615536"/><a contenteditable="false" data-primary="" data-startref="kub_str" data-type="indexterm" id="idm46183195762960"/><a contenteditable="false" data-primary="" data-startref="str_kub" data-type="indexterm" id="idm46183195761584"/>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="summary-id000007">&#13;
<h1>Summary</h1>&#13;
<p>In this chapter, we have branched out from persistence-oriented data infrastructure into the world of streaming. We defined what streaming is, how to navigate all the terminology, and how it fits into Kubernetes. From there, we took a deeper look into Apache Pulsar and learned how to deploy it into your Kubernetes cluster according to your environment and application needs. As a part of deploying streaming, we took a side trip into default secure communications with cert-manager to see how it works and how to create self-managed encrypted communication. Finally, we looked at Kubernetes deployments of Apache Flink, which is used primarily for high-scale stream analytics.</p>&#13;
<p>As you saw in this chapter with Pulsar and cert-manager, running cloud native data infrastructure on Kubernetes frequently involves the composition of multiple components as part of an integrated stack. We’ll discuss more examples of this in the next chapter and beyond.</p>&#13;
</div></section>&#13;
</div></section></body></html>