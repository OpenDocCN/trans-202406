<html><head></head><body><section data-pdf-bookmark="Chapter 16. Managing State and Stateful Applications" data-type="chapter" epub:type="chapter"><div class="chapter" id="managing_state_and_stateful_application">&#13;
<h1><span class="label">Chapter 16. </span>Managing State and Stateful Applications</h1>&#13;
&#13;
&#13;
<p>In the early days of container orchestration, the targeted workloads were&#13;
usually stateless applications that used external systems to store state when it was needed. The thought was that containers are very temporal, and&#13;
orchestration of the backing storage needed to keep state consistently was difficult at best. Over time, the need for container-based&#13;
workloads that kept state became a reality, and, in select cases, this need might be&#13;
more performant. As more organizations looked to the cloud for computing power and Kubernetes became the de facto container runtime for applications, the impeding factor became the amount of data and performant access to the data, sometimes called “data gravity.” Kubernetes adapted over many iterations. Now, not only does it&#13;
allow for storage volumes mounted into the pod, but it also allows for those volumes&#13;
to be managed by Kubernetes directly. This was an important component in&#13;
orchestration of storage with the workloads that require it.</p>&#13;
&#13;
<p>If the <a data-primary="state management" data-secondary="importance of" data-type="indexterm" id="id1007"/>ability to mount an external volume to the container was enough,&#13;
many more examples of stateful applications running at scale in&#13;
Kubernetes would exist. The reality is that volume mounting is the&#13;
easy component in the grand scheme of stateful applications. The&#13;
majority of applications that require state to be maintained after node&#13;
failure are complicated data-state engines such as relational database&#13;
systems, distributed key/value stores, and complex document&#13;
management systems. This class of applications requires more coordination&#13;
among how members of the clustered application communicate with one another, how the members are identified, and the order in which members&#13;
either appear or disappear in the system.</p>&#13;
&#13;
<p>This chapter focuses on best practices for managing state, from simple&#13;
patterns such as saving a file to a network share, to complex data&#13;
management systems like MongoDB, MySQL, or Kafka. There is a small&#13;
section on a new pattern for complex systems called Operators that&#13;
brings not only Kubernetes primitives, but also allows for business or&#13;
application logic to be added as custom controllers that can help make&#13;
operating complex data management systems easier.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Volumes and Volume Mounts" data-type="sect1"><div class="sect1" id="id238">&#13;
<h1>Volumes and Volume Mounts</h1>&#13;
&#13;
<p>Not every<a data-primary="state management" data-secondary="volumes" data-type="indexterm" id="state-mgmt-volume"/><a data-primary="volumes" data-secondary="state management" data-type="indexterm" id="volume-state-mgmt"/> workload that requires a way to maintain state needs to be a&#13;
complex database or high-throughput data queue service. Often,&#13;
applications that are being moved to containerized workloads expect&#13;
certain directories to exist and to be able to read and write pertinent information&#13;
to those directories. The ability to inject data into a volume that can&#13;
be read by containers in a pod is covered in <a data-type="xref" href="ch04.html#configuration_secrets_and_rbac">Chapter 4</a>; however, data mounted from ConfigMaps or secrets is usually&#13;
read-only, and this section focuses on giving containers volumes that can be&#13;
written to and will survive a container failure or, even better, a pod&#13;
failure.</p>&#13;
&#13;
<p>Every major container runtime, such as Docker, rkt, CRI-O, and even&#13;
Singularity, allows for mounting volumes into a container that is mapped&#13;
to an external storage system. At its&#13;
simplest, external storage can be a memory location, a path on the container’s host, or an external filesystem such as NFS, Glusterfs, CIFS, or Ceph. Why would&#13;
this be needed? A useful example is&#13;
that of a legacy application that was written to log application-specific information to a local filesystem. Many possible solutions, such as updating the application code to&#13;
log out to a <code>stdout</code> or <code>stderr</code> of a sidecar container, can stream log&#13;
data to an outside source via a shared pod volume. Some will take an infrastructure approach by using a host-based&#13;
logging tool that can read a volume for both host logs and container&#13;
application logs by using a&#13;
volume mount in the container using a Kubernetes <code>hostPath</code> mount, as shown in the following:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:alpine</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">volumeMounts</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hostvol</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">mountPath</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/usr/share/nginx/html</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">volumes</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hostvol</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">hostPath</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/home/webcontent</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Volume Best Practices" data-type="sect1"><div class="sect1" id="id239">&#13;
<h1>Volume Best Practices</h1>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Try to<a data-primary="best practices" data-secondary="volumes" data-type="indexterm" id="id1008"/> limit the use of volumes to pods requiring multiple&#13;
containers that need to share data, such as adapter or ambassador-type patterns. Use the <code>emptyDir</code> for those types of sharing patterns.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use <code>hostDir</code> when access to the data is required by node-based agents&#13;
or <span class="keep-together">services.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Try to identify any services that write their critical application&#13;
logs and events to local disk, and if possible change those to <code>stdout</code>&#13;
or <code>stderr</code> and let a true Kubernetes-aware log aggregation system&#13;
stream the logs instead of leveraging the<a data-primary="state management" data-secondary="volumes" data-startref="state-mgmt-volume" data-type="indexterm" id="id1009"/><a data-primary="volumes" data-secondary="state management" data-startref="volume-state-mgmt" data-type="indexterm" id="id1010"/> volume map.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Storage" data-type="sect1"><div class="sect1" id="id373">&#13;
<h1>Kubernetes Storage</h1>&#13;
&#13;
<p>The examples we’ve walked through so far show basic volume mapping into a container in a pod,&#13;
which is just a basic container engine capability. The real key is&#13;
allowing Kubernetes to manage the storage backing the volume mounts.&#13;
This allows for more dynamic scenarios where pods can live and die as&#13;
needed, and the storage backing the pod will transition accordingly to&#13;
wherever the pod may live. Kubernetes manages storage for pods using two&#13;
distinct APIs, the PersistentVolume and PersistentVolumeClaim.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PersistentVolume" data-type="sect2"><div class="sect2" id="id135">&#13;
<h2>PersistentVolume</h2>&#13;
&#13;
<p>It is<a data-primary="state management" data-secondary="storage" data-tertiary="PersistentVolumes resource" data-type="indexterm" id="id1011"/><a data-primary="storage" data-secondary="PersistentVolumes resource" data-type="indexterm" id="id1012"/><a data-primary="PersistentVolumes resource" data-type="indexterm" id="id1013"/> best to think of a PersistentVolume as a disk that will back any&#13;
volumes that are mounted to a pod. A PersistentVolume will have a&#13;
claim policy that will define the scope of life of the volume&#13;
independent of the life cycle of the pod that uses the volume. Kubernetes&#13;
can use either dynamic or statically defined volumes. To allow for&#13;
dynamically created volumes, there must be a StorageClass defined in&#13;
Kubernetes. PersistentVolumes of varying&#13;
types and classes can be created in the cluster, and only when a PersistentVolumeClaim matches&#13;
the PersistentVolume will it actually be assigned to a pod. The volume itself is&#13;
backed by a volume plug-in. Numerous plug-ins are supported directly&#13;
in Kubernetes, and each has different configuration parameters to adjust:</p>&#13;
&#13;
<pre class="less_space pagebreak-before" data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PersistentVolume</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">pv001</code><code class="w"/>&#13;
<code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">tier</code><code class="p">:</code><code class="w"> </code><code class="s">"silver"</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="nt">capacity</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">storage</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5Gi</code><code class="w"/>&#13;
<code class="nt">accessModes</code><code class="p">:</code><code class="w"/>&#13;
<code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">ReadWriteMany</code><code class="w"/>&#13;
<code class="nt">persistentVolumeReclaimPolicy</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Recycle</code><code class="w"/>&#13;
<code class="nt">storageClassName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nfs</code><code class="w"/>&#13;
<code class="nt">mountOptions</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">hard</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">nfsvers=4.1</code><code class="w"/>&#13;
<code class="nt">nfs</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">path</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/tmp</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">server</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">172.17.0.2</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="PersistentVolumeClaims" data-type="sect2"><div class="sect2" id="id136">&#13;
<h2>PersistentVolumeClaims</h2>&#13;
&#13;
<p>PersistentVolumeClaims are<a data-primary="state management" data-secondary="storage" data-tertiary="PersistentVolumeClaims resource" data-type="indexterm" id="id1014"/><a data-primary="storage" data-secondary="PersistentVolumeClaims resource" data-type="indexterm" id="id1015"/><a data-primary="PersistentVolumeClaims resource" data-type="indexterm" id="id1016"/> a way to give Kubernetes a resource&#13;
requirement definition for storage that a pod will use. Pods will&#13;
reference the claim, and then if a <code>persistentVolume</code> that matches the&#13;
claim request exists, it will allocate that volume to that specific pod.&#13;
At minimum, a storage request size and access mode must be defined, but a&#13;
specific StorageClass can also be defined. Selectors can also be used to&#13;
ensure PersistentVolumes that meet a certain criteria will be&#13;
allocated appropriately. In the following example, the label with key <code>tier</code> has a value of <code>"silver"</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">PersistentVolumeClaim</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">my-pvc</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">storageClass</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nfs</code><code class="w"/>&#13;
<code class="w">    </code><code class="l-Scalar-Plain">accessModes</code><code class="p-Indicator">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">ReadWriteMany</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">requests</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">storage</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">5Gi</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">tier</code><code class="p">:</code><code class="w"> </code><code class="s">"silver"</code><code class="w"/></pre>&#13;
&#13;
<p>The claim will match the PersistentVolume created earlier because&#13;
the StorageClass name, the selector match, the size, and the access mode are&#13;
all equal.</p>&#13;
&#13;
<p>Kubernetes will match the PersistentVolume with the claim and bind&#13;
them together. To use the volume, the <code>pod.spec</code> should reference the claim by name, as follows:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Deployment</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">matchLabels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">app</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx-webserver</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nginx:alpine</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">80</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">volumeMounts</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hostvol</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">mountPath</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/usr/share/nginx/html</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">volumes</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">hostvol</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">persistentVolumeClaim</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">            </code><code class="nt">claimName</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">my-pvc</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="StorageClasses" data-type="sect2"><div class="sect2" id="id137">&#13;
<h2>StorageClasses</h2>&#13;
&#13;
<p>Instead of<a data-primary="state management" data-secondary="storage" data-tertiary="StorageClasses resource" data-type="indexterm" id="id1017"/><a data-primary="storage" data-secondary="StorageClasses resource" data-type="indexterm" id="id1018"/><a data-primary="StorageClasses resource" data-type="indexterm" id="id1019"/> manually defining the PersistentVolumes ahead of time,&#13;
administrators might elect to create StorageClass objects, which define&#13;
the volume plug-in to use. They can also create any specific mount options and parameters&#13;
that all PersistentVolumes of that class will use. This then allows&#13;
the claim to be defined with the specific StorageClass to use, and&#13;
Kubernetes will dynamically create the PersistentVolume based on the&#13;
StorageClass parameters and options:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">StorageClass</code><code class="w"/>&#13;
<code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">storage.k8s.io/v1</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">nfs</code><code class="w"/>&#13;
<code class="nt">provisioner</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cluster.local/nfs-client-provisioner</code><code class="w"/>&#13;
<code class="nt">parameters</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">archiveOnDelete</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">True</code><code class="w"/></pre>&#13;
&#13;
<p>Kubernetes also allows operators to create a default storage class using&#13;
the DefaultStorageClass admission plug-in. If this has been enabled on&#13;
the API server, then a default StorageClass can be defined, and any&#13;
PersistentVolumeClaims that do not explicitly define a StorageClass will be assigned to the default class.&#13;
Some cloud providers will include a default storage class to map to the&#13;
cheapest storage allowed by their instances.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Container Storage Interface and FlexVolume" data-type="sect3"><div class="sect3" id="id138">&#13;
<h3>Container Storage Interface and FlexVolume</h3>&#13;
&#13;
<p>Most<a data-primary="state management" data-secondary="storage" data-tertiary="CSI (Container Storage Interface) plug-in" data-type="indexterm" id="id1020"/><a data-primary="storage" data-secondary="CSI (Container Storage Interface) plug-in" data-type="indexterm" id="id1021"/><a data-primary="CSI (Container Storage Interface) drivers" data-type="indexterm" id="id1022"/><a data-primary="state management" data-secondary="storage" data-tertiary="FlexVolume plug-in" data-type="indexterm" id="id1023"/><a data-primary="storage" data-secondary="FlexVolume plug-in" data-type="indexterm" id="id1024"/><a data-primary="FlexVolume plug-in" data-type="indexterm" id="id1025"/> volume plug-ins today need to wait for direct code additions to the Kubernetes codebase. However, the Container Storage Interface (CSI) and FlexVolume, often referred to as “Out-of-Tree” volume plug-ins, enable storage vendors to create custom storage plug-ins without the need to wait for these direct code additions.</p>&#13;
&#13;
<p>The CSI and FlexVolume plug-ins are deployed on Kubernetes clusters as&#13;
extensions by operators and can be updated by the storage vendors when&#13;
needed to expose new functionality.</p>&#13;
&#13;
<p>The CSI states its objective on <a href="https://oreil.ly/AuMgE">GitHub</a> as:</p>&#13;
<blockquote>&#13;
<p>To define an&#13;
industry standard Container Storage Interface that will enable&#13;
storage vendors (SP) to develop a plug-in once and have it work across a&#13;
number of container orchestration (CO) systems.</p>&#13;
</blockquote>&#13;
&#13;
<p>The FlexVolume interface has been the traditional method used to add&#13;
additional features for a storage provider. It does require specific&#13;
drivers to be installed on all the nodes of the cluster that will use&#13;
it. This basically becomes an executable that is installed on the hosts&#13;
of the cluster. This last component is the main detractor to using&#13;
FlexVolumes, especially in managed service providers, because access to the nodes is frowned upon and accessing the control plane is practically impossible. The CSI plug-in solves this by exposing the same functionality and being as easy to use as deploying a pod into the cluster.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Storage Best Practices" data-type="sect2"><div class="sect2" id="id240">&#13;
<h2>Kubernetes Storage Best Practices</h2>&#13;
&#13;
<p>Cloud native <a data-primary="state management" data-secondary="storage" data-tertiary="best practices" data-type="indexterm" id="state-mgmt-storage-best"/><a data-primary="storage" data-secondary="best practices" data-type="indexterm" id="storage-best"/><a data-primary="best practices" data-secondary="storage" data-type="indexterm" id="best-practice-storage"/>application design principles try to enforce stateless application design as much as possible; however, the growing footprint of container-based services has created the need for data storage persistence. These best practices around storage in Kubernetes will help to design an effective approach to providing the required storage implementations to the application design:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>If possible, enable the DefaultStorageClass admission plug-in and define a default storage class. Often, Helm charts for applications&#13;
that require &#13;
<span class="keep-together">PersistentVolumes</span> default to a <code>default</code> storage class for the chart, which allows the application to be installed without too much modification.</p>&#13;
</li>&#13;
<li>&#13;
<p>When designing the architecture of the cluster, either on premises or&#13;
in a cloud provider, take into consideration zone and connectivity&#13;
between the compute and data layers. You will want to use the proper labels for both&#13;
nodes and PersistentVolumes, and use affinity to keep the data and&#13;
workload as close as possible. The last thing you want is a pod on a&#13;
node in zone A trying to mount a volume that is attached to a node in&#13;
zone B.</p>&#13;
</li>&#13;
<li>&#13;
<p>Consider very carefully which workloads require state to be maintained&#13;
on disk. Can that be handled by an outside service like a database&#13;
system? Or, can your instance run in a cloud provider, by a hosted service that is API-consistent with currently used APIs, say a MongoDB or MySQL as a service?</p>&#13;
</li>&#13;
<li>&#13;
<p>Determine how much effort would be involved in modifying the application code to be more stateless.</p>&#13;
</li>&#13;
<li>&#13;
<p>While Kubernetes will track and mount the volumes as workloads are&#13;
scheduled, it does not yet handle redundancy and backup of the data that&#13;
is stored in those volumes. The CSI specification has added an API for&#13;
vendors to plug in native snapshot technologies if the storage backend&#13;
can support it.</p>&#13;
</li>&#13;
<li>&#13;
<p>Verify the proper life cycle of the data that volumes will hold. By&#13;
default the reclaim policy is set for dynamically provisioned PersistentVolumes, which will delete the volume from the backing storage provider when the pod is deleted. Sensitive data or data that can be used for forensic analysis should be set to <a data-primary="state management" data-secondary="storage" data-startref="state-mgmt-storage-best" data-tertiary="best practices" data-type="indexterm" id="id1026"/><a data-primary="storage" data-secondary="best practices" data-startref="storage-best" data-type="indexterm" id="id1027"/><a data-primary="best practices" data-secondary="storage" data-startref="best-practice-storage" data-type="indexterm" id="id1028"/>reclaim.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Stateful Applications" data-type="sect1"><div class="sect1" id="id139">&#13;
<h1>Stateful Applications</h1>&#13;
&#13;
<p>Contrary<a data-primary="state management" data-secondary="stateful applications" data-tertiary="ReplicaSets" data-type="indexterm" id="id1029"/><a data-primary="stateful applications" data-secondary="ReplicaSets" data-type="indexterm" id="id1030"/><a data-primary="ReplicaSet resource" data-type="indexterm" id="id1031"/> to popular belief, Kubernetes has supported stateful&#13;
applications since its infancy, from MySQL, Kafka, and Cassandra to&#13;
other technologies. Those pioneering days, however, were fraught with complexities and were usually only for small workloads with lots of work required to get things like scaling and durability to function.</p>&#13;
&#13;
<p>To fully grasp the critical differences, you must understand how a&#13;
typical ReplicaSet schedules and manages pods, and how this could be&#13;
detrimental to traditional stateful applications:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Pods in a ReplicaSet are scaled out and assigned random names&#13;
when scheduled.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a ReplicaSet are scaled down arbitrarily.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a ReplicaSet are never called directly through their name or&#13;
IP address but through their association with a <code>Service</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a ReplicaSet can be restarted and moved to another node at&#13;
any time.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a ReplicaSet that have a PersistentVolume mapped are&#13;
linked only by the claim, but any new pod with a new name can take over the&#13;
claim if needed when rescheduled.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Those who have only cursory knowledge of cluster data management&#13;
systems can immediately begin to see issues with these characteristics of&#13;
ReplicaSet-based pods. Imagine a pod that has the current writable&#13;
copy of the database just all of a sudden getting deleted! Pure&#13;
pandemonium would ensue for sure.</p>&#13;
&#13;
<p>Most neophytes to the Kubernetes world assume that StatefulSet&#13;
applications are automatically database applications and therefore&#13;
equate the two. This could not be farther from the truth. Kubernetes has no sense of what type of application it is&#13;
deploying. It does not know that your database system requires leader&#13;
election processes, that it can or cannot handle data replication&#13;
between members of the set, or, for that matter, that it is a database&#13;
system at all. This is where StatefulSets come into play.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="StatefulSets" data-type="sect2"><div class="sect2" id="id140">&#13;
<h2>StatefulSets</h2>&#13;
&#13;
<p>What<a data-primary="state management" data-secondary="stateful applications" data-tertiary="StatefulSets" data-type="indexterm" id="state-mgmt-apps-statefulsets"/><a data-primary="stateful applications" data-secondary="StatefulSets" data-type="indexterm" id="state-apps-statefulset"/><a data-primary="StatefulSet resource" data-type="indexterm" id="stateful-set"/> StatefulSets do is make it easier to run application&#13;
systems that expect more reliable node/pod behavior. If we look back at the&#13;
list of typical pod characteristics in a ReplicaSet, StatefulSets offer&#13;
almost the complete opposite. The original spec back in Kubernetes&#13;
version 1.3 called <code>PetSets</code> was introduced to answer some of the&#13;
critical scheduling and management needs for stateful-type applications&#13;
such as complex data management systems:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Pods in a StatefulSet are scaled out and assigned sequential&#13;
names. As the set scales up, the pods get ordinal names, and by default a&#13;
new pod must be fully online (pass its liveness and/or readiness probes)&#13;
before the next pod is added.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a StatefulSet are scaled down in reverse sequence.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a StatefulSet can be addressed individually by name behind a&#13;
headless Service.</p>&#13;
</li>&#13;
<li>&#13;
<p>Pods in a StatefulSet that require a volume mount must use a&#13;
 defined PersistentVolume template. Volumes claimed by pods in a&#13;
StatefulSet are not deleted when the StatefulSet is deleted.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>A StatefulSet specification looks very similar to a Deployment&#13;
except for the Service declaration and the PersistentVolume template.&#13;
The headless Service should be created first, which defines the Service&#13;
that the pods will be addressed with individually. The headless Service&#13;
is the same as a regular Service but does not do the normal load&#13;
balancing:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">v1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">Service</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">port</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">27017</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">targetPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">27017</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">clusterIP</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">None</code><code class="w"> </code><code class="c1">#This creates the headless Service</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">selector</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">role</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo</code><code class="w"/></pre>&#13;
&#13;
<p>The StatefulSet definition will also look exactly like a Deployment&#13;
with a few <a data-primary="state management" data-secondary="stateful applications" data-startref="state-mgmt-apps-statefulsets" data-tertiary="StatefulSets" data-type="indexterm" id="id1032"/><a data-primary="stateful applications" data-secondary="StatefulSets" data-startref="state-apps-statefulset" data-type="indexterm" id="id1033"/><a data-primary="StatefulSet resource" data-startref="stateful-set" data-type="indexterm" id="id1034"/>changes:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">apps/v1beta1</code><code class="w"/>&#13;
<code class="nt">kind</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">StatefulSet</code><code class="w"/>&#13;
<code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo</code><code class="w"/>&#13;
<code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">serviceName</code><code class="p">:</code><code class="w"> </code><code class="s">"mongo"</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">replicas</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">3</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">template</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">labels</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">role</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">environment</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">test</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">terminationGracePeriodSeconds</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">10</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">containers</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo:3.4</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">command</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">mongod</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"--replSet"</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">rs0</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"--bind_ip"</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="l-Scalar-Plain">0.0.0.0</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"--smallfiles"</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="s">"--noprealloc"</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">ports</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">containerPort</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">27017</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">volumeMounts</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo-persistent-storage</code><code class="w"/>&#13;
<code class="w">              </code><code class="nt">mountPath</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">/data/db</code><code class="w"/>&#13;
<code class="w">        </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo-sidecar</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">image</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">cvallance/mongo-k8s-sidecar</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">env</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">            </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">MONGO_SIDECAR_POD_LABELS</code><code class="w"/>&#13;
<code class="w">              </code><code class="nt">value</code><code class="p">:</code><code class="w"> </code><code class="s">"role=mongo,environment=test"</code><code class="w"/>&#13;
<code class="w">  </code><code class="nt">volumeClaimTemplates</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">  </code><code class="p-Indicator">-</code><code class="w"> </code><code class="nt">metadata</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">name</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">mongo-persistent-storage</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">annotations</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">volume.beta.kubernetes.io/storage-class</code><code class="p">:</code><code class="w"> </code><code class="s">"fast"</code><code class="w"/>&#13;
<code class="w">    </code><code class="nt">spec</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">accessModes</code><code class="p">:</code><code class="w"> </code><code class="p-Indicator">[</code><code class="w"> </code><code class="s">"ReadWriteOnce"</code><code class="w"> </code><code class="p-Indicator">]</code><code class="w"/>&#13;
<code class="w">      </code><code class="nt">resources</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">        </code><code class="nt">requests</code><code class="p">:</code><code class="w"/>&#13;
<code class="w">          </code><code class="nt">storage</code><code class="p">:</code><code class="w"> </code><code class="l-Scalar-Plain">2Gi</code><code class="w"/></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Operators" data-type="sect2"><div class="sect2" id="Operators">&#13;
<h2>Operators</h2>&#13;
&#13;
<p>StatefulSets have <a data-primary="state management" data-secondary="stateful applications" data-tertiary="Operators" data-type="indexterm" id="id1035"/><a data-primary="stateful applications" data-secondary="Operators" data-type="indexterm" id="id1036"/><a data-primary="Operators" data-type="indexterm" id="operators"/>been a major factor in introducing complex&#13;
stateful data systems as feasible workloads in Kubernetes. The only real&#13;
issue, as stated earlier, is that Kubernetes does not really understand the&#13;
workload that is running in the StatefulSet. All the other complex&#13;
operations, like backups, failover, leader registration, new replica&#13;
registration, and upgrades, are operations that need to happen regularly and will require some careful consideration when running as&#13;
StatefulSets.</p>&#13;
&#13;
<p>Early on in the growth of Kubernetes, CoreOS site reliability engineers (SREs) created a new class of cloud native software for Kubernetes called Operators. The original intent was to encapsulate domain-specific knowledge of running a specific application into a specific controller that extends Kubernetes. Imagine building up on the StatefulSet controller to be able to deploy, scale, upgrade, back up, and run general maintenance operations on Cassandra or Kafka. Some of the first Operators that were created were for etcd and Prometheus, which uses a time-series database to keep metrics over time. The proper creation, backup, and restore configuration of Prometheus or etcd instances can be handled by an Operator and are new Kubernetes-managed objects just like a pod or Deployment.</p>&#13;
&#13;
<p>Until recently, Operators have been one-off tools created by SREs or by software vendors for their specific application. In mid-2018, Red Hat created the Operator Framework, a set of tools including an SDK life cycle manager and future modules that will enable features such as metering, marketplace, and registry type functions. Operators are not only for stateful applications, but because of their custom controller logic they are definitely more amenable to complex data services and stateful <span class="keep-together">systems.</span></p>&#13;
&#13;
<p>Operators have become a standard way not only of extending the Kubernetes API but also bringing in best practice and operational oversight to complex system processes in Kubernetes. A good place to discover existing Operators published for the Kubernetes ecosystem is <a href="http://operatorhub.io">OperatorHub</a>. They maintain an updated list of curated Operators.</p>&#13;
&#13;
<p>If you are interested in learning how Operators work, <a data-type="xref" href="ch21.html#implementing_an_operator">Chapter 21</a> is new to this edition and will give you a primer on the development of an Operator and best practices to use. Also check out <a class="orm:hideurl" href="https://oreil.ly/zaEEo"><em>Kubernetes Operators</em></a> (O’Reilly)&#13;
by Jason Dobies and Joshua Wood for a more in-depth run-through of building Operators.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="less_space pagebreak-before" data-pdf-bookmark="StatefulSet and Operator Best Practices" data-type="sect2"><div class="sect2" id="id142">&#13;
<h2>StatefulSet and Operator Best Practices</h2>&#13;
&#13;
<p>Large distributed <a data-primary="state management" data-secondary="stateful applications" data-tertiary="best practices" data-type="indexterm" id="state-mgmt-apps-best"/><a data-primary="stateful applications" data-secondary="best practices" data-type="indexterm" id="state-apps-best"/><a data-primary="best practices" data-secondary="stateful applications" data-type="indexterm" id="best-practice-state-apps"/><a data-primary="StatefulSet resource" data-type="indexterm" id="stateful-set-best"/>applications that require state and possibly complicated management and configuration operations benefit from Kubernetes StatefulSets and Operators. Operators are still evolving, but they have the backing of the community at large, so these best practices are based on current capabilities at the time of publication:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The decision to use StatefulSets should be taken judiciously because stateful applications usually require much deeper management that the&#13;
orchestrator cannot manage well yet (read <a data-type="xref" href="#Operators">“Operators”</a> for the possible future answer to this deficiency in Kubernetes).</p>&#13;
</li>&#13;
<li>&#13;
<p>The headless Service for the StatefulSet is not automatically created&#13;
and must be created at deployment time to properly address the pods as&#13;
individual nodes.</p>&#13;
</li>&#13;
<li>&#13;
<p>When an application requires ordinal naming and dependable&#13;
scaling, this does not always mean it requires the assignment of PersistentVolumes.</p>&#13;
</li>&#13;
<li>&#13;
<p>If a node in the cluster becomes unresponsive, any pods that are part&#13;
of a StatefulSet are not automatically deleted; instead they will&#13;
enter a <code>Terminating</code> or <code>Unknown</code> state after a grace period. The only&#13;
ways to clear this pod are to remove the node object from the cluster, the&#13;
kubelet beginning to work again and deleting the pod directly, or an&#13;
Operator force deleting the pod. The force delete should be the last&#13;
option, and great care should be taken that the node that had the deleted&#13;
pod does not come back online, because there will now be two pods with the same&#13;
name in the cluster. You can use <code>kubectl delete pod nginx-0 --grace-period=0 --force</code> to force delete the pod.</p>&#13;
</li>&#13;
<li>&#13;
<p>Even after force deleting a pod, it might stay in an <code>Unknown</code> state, so&#13;
a patch to the API server will delete the entry and cause the StatefulSet&#13;
controller to create a new instance of the deleted pod:&#13;
<code>kubectl patch pod nginx-0 -p '{"metadata":{"finalizers":null}}'</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>If you’re running a complex data system with some type of leader election or&#13;
data replication confirmation processes, use <code>preStop hook</code> to properly&#13;
close any connections, force leader election, or verify data synchronization before&#13;
the pod is deleted using a graceful shutdown process.</p>&#13;
</li>&#13;
<li>&#13;
<p>When the application that requires stateful data is a complex data management system, look to determine whether an Operator exists to help manage the more complicated life-cycle components of the application. If the application is built in-house, it might be worth investigating whether it would be useful to package the application as an Operator to add more manageability to the application. See <a href="https://oreil.ly/gRIej">the CoreOS Operator SDK</a> for an<a data-primary="Operators" data-startref="operators" data-type="indexterm" id="id1037"/> example.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id374">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Most organizations look to containerize their stateless applications and&#13;
leave the stateful applications as is. As more cloud native applications run in cloud provider Kubernetes&#13;
offerings, data gravity becomes an issue. Stateful applications require&#13;
much more due diligence, but the reality of running them in clusters has&#13;
been accelerated by the introduction of StatefulSets and Operators.&#13;
Mapping volumes into containers allows Operators to abstract the storage&#13;
subsystem specifics away from any application development. Managing&#13;
stateful applications such as database systems in Kubernetes is still a&#13;
complex distributed system and needs to be carefully orchestrated using the&#13;
native Kubernetes primitives of pods, ReplicaSets, Deployments, and&#13;
StatefulSets. Using Operators that have specific application&#13;
knowledge built into them as Kubernetes-native APIs may help to elevate&#13;
these systems into production-based clusters.</p>&#13;
</div></section>&#13;
</div></section></body></html>