<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Computing Technology Basics for Life Scientists"><div class="chapter" id="computing_technology_basics_for_life_sc">
<h1><span class="label">Chapter 3. </span>Computing Technology Basics <span class="keep-together">for Life Scientists</span></h1>

<p>In an ideal world, you wouldn’t need to worry too much about computing infrastructure when you’re pursuing your research.<a contenteditable="false" data-primary="computing technology basics" data-type="indexterm" id="ix_cmptch"/> In fact, in later chapters we introduce you to systems that are specifically designed to abstract away the nitty-gritty of computing infrastructure in order to help you focus on your science. However, you will find that a certain amount of terminology and concepts are unavoidable in the real world. Investing some effort into learning them will help you to plan and execute your work more efficiently, address performance challenges, and achieve larger scale with less effort. In this chapter, we review the essential components that form the most common types of computing infrastructure, and we discuss how their strengths and limitations inform our strategies for getting work done efficiently at scale. We also go over key concepts such as parallel computing and pipelining, which are essential in genomics because of the need for automation and reproducibility. Finally, we introduce virtualization and lay out the case for cloud infrastructure.</p>

<p>The first few sections in this chapter are aimed at readers who have not had much training, if any, in informatics, programming, or systems administration. If you are a computational scientist or an IT professional, feel free to skip ahead until you encounter something that you don’t already know. The last two sections, which together cover pipelining, virtualization, and the cloud, are more specifically focused on the problems that we tackle in this book and should be informative for all readers regardless of background.</p>

<section data-type="sect1" data-pdf-bookmark="Basic Infrastructure Components and Performance Bottlenecks"><div class="sect1" id="basic_infrastructure_components_and_per">
<h1>Basic Infrastructure Components and Performance Bottlenecks</h1>

<p>Don’t worry; we’re not going to make you sit through an exhaustive inventory of computer parts. <a contenteditable="false" data-primary="computing technology basics" data-secondary="infrastructure components" data-type="indexterm" id="ix_cmptchinf"/>Rather, we’ve put together a short list of the components, terminology, and concepts that you’re most likely to encounter in the course of your work. In relation to each of these, we’ve summarized the main performance challenges and the strategies that you might need to consider to use them effectively.</p>

<p>Let’s begin with a brief overview of the types of processors that you might come across in scientific computing today.</p>

<section data-type="sect2" data-pdf-bookmark="Types of Processor Hardware: CPU, GPU, TPU, FPGA, OMG"><div class="sect2" id="types_of_processor_hardware_cpucomma_gp">
<h2>Types of Processor Hardware: CPU, GPU, TPU, FPGA, OMG</h2>

<p>At its simplest, a <em>processor</em> is a component in your computer that performs computations.<a contenteditable="false" data-primary="computing technology basics" data-secondary="infrastructure components" data-tertiary="processor hardware, types of" data-type="indexterm" id="idm45625634969784"/><a contenteditable="false" data-primary="processor hardware, types of" data-type="indexterm" id="idm45625634968056"/> There are various types of processors, with the most common being the <em>central processing unit</em> (CPU) that serves as the main processor in general-use computers, including personal computers such as laptops.<a contenteditable="false" data-primary="CPUs (central processing units)" data-type="indexterm" id="idm45625634966184"/> The CPU in your laptop may have multiple cores, subunits that can process operations more or less independently.<a contenteditable="false" data-primary="central processing units" data-see="CPUs" data-type="indexterm" id="idm45625634964808"/></p>

<p>In addition to a CPU, your personal computer <a contenteditable="false" data-primary="GPUs (graphical processing units)" data-type="indexterm" id="idm45625634963128"/>also has a <em>graphical processing unit</em> (GPU) that processes the graphical information for display on your screen. GPUs came into the limelight with the development of modern video games, which require extremely fast processing to ensure smooth visual rendering of game action. In essence, the GPU solution outsources the rather specific type of processing involved in mathematical calculations like matrix and vector operations from the CPU to a secondary processing unit that specializes in handling certain types of calculations that are applied to graphical data very efficiently. As a result, GPUs are also becoming a popular option for certain types of scientific computing applications that involve a lot of matrix or vector operations.</p>

<p>The third type of processor you should<a contenteditable="false" data-primary="FPGAs (field-programmable gate arrays)" data-type="indexterm" id="idm45625634960328"/> know about is called a <em>field-programmable gate array</em> (FPGA), which, despite breaking with the *PU naming convention, is also a type of processing unit; however, it’s unlikely that you’ll find one in your laptop. What’s interesting about FPGAs is that unlike GPUs, FPGAs were not developed for a specific type of application; quite the contrary, they were developed to be adaptable for custom types of computations. Hence “field-programmable” as part of their name.</p>

<p>On GCP, you might also come across something called a <em>tensor processing unit</em> (TPU), which is a kind of processor developed and branded by Google for machine learning applications that involve tensor data.<a contenteditable="false" data-primary="TPUs (tensor processing units)" data-type="indexterm" id="idm45625634956968"/><a contenteditable="false" data-primary="tensor processing units (TPUs)" data-type="indexterm" id="idm45625634955848"/><a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="tensor processing units (TPUs)" data-type="indexterm" id="idm45625634954728"/> A <em>tensor</em> is a mathematical concept used to represent and manipulate multiple layers of data related to vectors and matrices. Consider that a vector is a tensor with one dimension, and a matrix is a tensor with two dimensions; more generally, tensors can have arbitrary numbers of dimensions beyond that, so they are very popular in machine learning applications. TPUs belong to a category of processors called <a href="https://oreil.ly/bz4mv">application-specific integrated circuit</a> (ASIC), which are custom designed for specialized uses rather than general use.<a contenteditable="false" data-primary="ASIC (application-specific integrated circuit)" data-type="indexterm" id="idm45625634951640"/></p>

<p>Now that you have the basic types of processors down, let’s talk about how they are organized in typical high-performance computing setups.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Levels of Compute Organization: Core, Node, Cluster, and Cloud"><div class="sect2" id="levels_of_compute_organization_corecomm">
<h2>Levels of Compute Organization: Core, Node, Cluster, and Cloud</h2>

<p>When you move beyond <a contenteditable="false" data-primary="computing technology basics" data-secondary="infrastructure components" data-tertiary="organization levels, core, node, cluster, and cloud" data-type="indexterm" id="idm45625634948168"/>personal computers and into the world of high-performance computing, you’ll hear people talk about<a contenteditable="false" data-primary="cores" data-type="indexterm" id="idm45625634946152"/> cores, nodes, and either clusters or clouds, as illustrated in <a data-type="xref" href="#levels_of_compute_organization">Figure 3-1</a>. Let’s review what these mean and how they relate to one another.</p>

<figure><div id="levels_of_compute_organization" class="figure"><img alt="Levels of compute organization " src="Images/gitc_0301.png" width="1469" height="613"/>
<h6><span class="label">Figure 3-1. </span>Levels of compute organization.</h6>
</div></figure>

<section data-type="sect3" data-pdf-bookmark="Low level: core"><div class="sect3" id="low_level_core">
<h3>Low level: core</h3>

<p>A <em>core</em> is the smallest indivisible processing unit within a machine’s, or node’s, processor unit, which can comprise one or more cores. If your laptop or desktop is relatively recent, its CPU probably has at least two cores, and is therefore called <em>dual-core</em>. If it has four, it’s a <em>quad-core</em>, and so on. <a contenteditable="false" data-primary="CPUs (central processing units)" data-secondary="cores" data-type="indexterm" id="idm45625634938520"/>High-end consumer machines can have more than that; for example, the latest Mac Pro has a twelve-core CPU (which should be called dodeca-core if we follow the Latin terminology) but the CPUs on professional-grade machines can have tens or hundreds of cores, and GPUs typically have an order of magnitude more, into the thousands. Meanwhile, TPUs have core counts in the single digits like consumer CPUs, and FPGAs break the mold entirely: their cores are defined by how they are programmed, not by how they are built.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Mid level: node/machine"><div class="sect3" id="mid_level_nodesolidusmachine">
<h3>Mid level: node/machine</h3>

<p>A <em>node</em> is really just a computer that is part of a cluster or cloud.<a contenteditable="false" data-primary="nodes/machines" data-type="indexterm" id="idm45625634934072"/> It is analogous to the laptop or desktop computer that most of us interact with primarily in our day-to-day work, except without the dedicated monitor and peripherals we are used to seeing associated with personal computers. A node is also sometimes simply called a <em>machine</em>.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Top level: cluster and cloud"><div class="sect3" id="top_level_cluster_and_cloud">
<h3>Top level: cluster and cloud</h3>

<p>A cluster and a cloud are both a<a contenteditable="false" data-primary="clusters" data-type="indexterm" id="idm45625634929896"/> collection of machines/nodes.</p>

<p>A <em>cluster</em> is an HPC structure composed of nodes networked together to some extent. If you have access to a cluster, the chances are that either it belongs to your institution, or your company is renting time on it. A cluster can also be called a <em>server farm</em> or a <em>load-sharing facility</em>.</p>

<p>A <em>cloud</em> is different from a cluster in that in its resting state, its nodes are not explicitly networked together.<a contenteditable="false" data-primary="cloud computing" data-secondary="cloud, defined" data-type="indexterm" id="idm45625634925608"/> Rather, it is a collection of independent machines that are available to be networked (or not) depending on your needs. We cover that in more detail in the final section of this chapter, along with the concept of virtualization, which gives us virtual machines (VMs), and containerization, which gives us Docker containers.</p>

<p>For now, however, we move on to the very common concern of how to use a given set of computing infrastructure effectively, which typically revolves around identifying and solving key computational bottlenecks. As with the rest of this chapter, an in-depth exploration of this topic would be beyond the scope of this book, so we’re aiming simply to familiarize you with the key concepts and terminology.<a contenteditable="false" data-primary="computing technology basics" data-secondary="infrastructure components" data-startref="ix_cmptchinf" data-type="indexterm" id="idm45625634922936"/></p>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Addressing Performance Bottlenecks"><div class="sect2" id="addressing_performance_bottlenecks">
<h2>Addressing Performance Bottlenecks</h2>

<p>You’ll occasionally find that some computing operations seem slow and you’ll need to figure out how to make them go faster (if possible). <a contenteditable="false" data-primary="performance bottlenecks in computing" data-type="indexterm" id="ix_perfbot"/><a contenteditable="false" data-primary="computing technology basics" data-secondary="performance bottlenecks" data-type="indexterm" id="ix_cmptchprf"/>The solutions available to you will depend on the nature of the bottleneck you’re facing.</p>

<p>At a very high level, following are the main operations that the computer typically has to perform (not necessarily in a linear order):</p>

<ol>
	<li>
	<p>Read some data into memory from the permanent storage where it resides at rest</p>
	</li>
	<li>
	<p>Have the processor execute instructions, transforming data and producing results</p>
	</li>
	<li>
	<p>Write results back to the permanent storage</p>
	</li>
</ol>

<section data-type="sect3" data-pdf-bookmark="Data storage and I/O operations: hard drive versus solid state"><div class="sect3" id="data_storage_and_isoliduso_operations_h">
<h3>Data storage and I/O operations: hard drive versus solid state</h3>

<p>Steps<a contenteditable="false" data-primary="computing technology basics" data-secondary="performance bottlenecks" data-tertiary="data storage and I/O operations" data-type="indexterm" id="idm45625634910152"/> 1 and 3 are called <em>I/O operations</em> (I/O stands for input/output). <a contenteditable="false" data-primary="performance bottlenecks in computing" data-secondary="data storage and I/O operations, hard drive vs. solid state" data-type="indexterm" id="idm45625634907912"/><a contenteditable="false" data-primary="I/O (input/output)" data-secondary="performance bottlenecks" data-type="indexterm" id="idm45625634906424"/>You might hear people describe some software programs as being “I/O-bound,” which means the part of the program that takes the longest is reading and writing data to and from relatively slow storage.<a contenteditable="false" data-primary="data storage" data-secondary="and I/O operations, performance bottlenecks" data-secondary-sortas="I/O" data-type="indexterm" id="idm45625634904696"/> This is typically the case for simple programs that do things like file format conversions, in which you’re just reading in some data and writing it out in a different shape, and you’re not doing any real computing (i.e., there’s little to no math involved). <a contenteditable="false" data-primary="solid-state drives (SSDs)" data-type="indexterm" id="idm45625634902536"/>In those cases, you can speed up operation by using faster storage drives; for example, solid-state drives (SSDs) rather than hard-disk drives (HDDs). <a contenteditable="false" data-primary="hard-disk drives (HDDs)" data-type="indexterm" id="idm45625634901128"/>The key difference between them is that HDDs have physical disks called platters that spin and an armature that reads data from and writes it to the platter via magnetics—like a tiny high-tech turntable—whereas SSDs have no moving parts. That makes SSDs less prone to physical malfunctions and also quite a bit faster at accessing data.</p>

<p>If you’re working with a networked infrastructure in which the storage drives are not directly connected to the computing nodes, you will also be limited by the speed at which data can be transferred over the network connections. That can be determined by hardware factors as pedestrian as the kind of cables used to connect the network parts. Although you might not notice the difference when computing on small files, you definitely will notice it when running on whole genomes; and even on a network with very fast transfer speeds, transferring whole genomes will consume some noticeable time.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Memory: cache or crash"><div class="sect3" id="memory_cache_or_crash">
<h3>Memory: cache or crash</h3>

<p>Step 2 is where your program is taking data and applying some kind of transformation or calculation, aka the interesting part. <a contenteditable="false" data-primary="performance bottlenecks in computing" data-secondary="memory" data-type="indexterm" id="idm45625634896648"/><a contenteditable="false" data-primary="memory, performance bottlenecks and" data-type="indexterm" id="idm45625634895304"/><a contenteditable="false" data-primary="computing technology basics" data-secondary="performance bottlenecks" data-tertiary="memory" data-type="indexterm" id="idm45625634894184"/>For a lot of applications, the calculation requires holding a lot of information in memory. In those cases, if your machine doesn’t have enough memory, it might resort to <em>caching</em>, which is a way of using local storage space as a substitute for real memory. <a contenteditable="false" data-primary="caching" data-type="indexterm" id="idm45625634891784"/>That allows you to keep working, but now your processes become I/O bound because they need to copy data back and forth to slow storage, which takes you back to the first bottleneck. In extreme cases, the program can stall indefinitely, fail to complete, or crash. Sometimes, it’s possible for a developer to rewrite the program to be smarter about the information it needs to see concurrently, but when it’s not, the solution is to simply add more memory. Fortunately, unlike memory in humans, computer memory is just hardware, and it comes relatively cheap.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Specialized hardware and code optimizations: navigating the trade-offs"><div class="sect3" id="specialized_hardware_and_code_optimizat">
<h3>Specialized hardware and code optimizations: navigating the trade-offs</h3>

<p>At times, the nature of the program requires the <a contenteditable="false" data-primary="performance bottlenecks in computing" data-secondary="specialized hardware and code optimizations, trade-offs" data-type="indexterm" id="idm45625634887496"/>processor itself to do a lot of heavy lifting.<a contenteditable="false" data-primary="computing technology basics" data-secondary="performance bottlenecks" data-tertiary="specialized hardware and code optimizations, trade-offs with" data-type="indexterm" id="idm45625634885992"/> For example, in the widely used GATK tool<a contenteditable="false" data-primary="HaplotypeCaller" data-secondary="calculation of genotype likelihoods" data-type="indexterm" id="idm45625634884088"/> <code>HaplotypeCaller</code>, an operation can calculate genotype likelihoods; we need to compute the likelihood of every single sequence read given each candidate allele using a hidden Markov model (HMM) called <em>PairHMM</em> (don’t worry if this sounds like gibberish at the moment—it’s just a bunch of genomics-specific math).<a contenteditable="false" data-primary="PairHMM" data-type="indexterm" id="idm45625634881368"/><a contenteditable="false" data-primary="Hidden Markov Model (HMM)" data-type="indexterm" id="idm45625634880264"/> In some areas of the genome, that leads us to do millions of computations per site across a very large number of sites. We know from performance profiling tests, which record how much time is spent in processing for each operation in the program, that PairHMM is by far the biggest bottleneck for this tool. We can reduce this bottleneck in some surface-level ways; for example, by making the program skip some of the computations for cases in which we can predict they will be unnecessary on uninformative. After all, the fastest way to calculate something is to not calculate it at all.</p>

<p>Being lazy gets<a contenteditable="false" data-primary="hardware optimizations" data-type="indexterm" id="idm45625634878120"/> us only so far, however, so to get to the next level, we need to think about the kind of processor we can (or should) use for the work we need to do. Not just because some processors run faster than others, but also because it’s possible to write program instructions in a way that is very specific to a particular type and architecture of processor. If done well, the program will be extra efficient in that context and therefore run faster.<a contenteditable="false" data-primary="code optimizations" data-type="indexterm" id="idm45625634876392"/> That is what we call <em>code optimization</em>, and more specifically <em>native</em> code optimization because it must be written in a low-level language that the processor understands “natively” without going through additional translation layers.<a contenteditable="false" data-primary="native code optimizations" data-type="indexterm" id="idm45625634874104"/></p>

<p>Within a<a contenteditable="false" data-primary="processor hardware, types of" data-secondary="implementation optimizations tied to" data-type="indexterm" id="idm45625634872568"/> type of processor like CPUs, different manufacturers (e.g., Intel and AMD) develop different <em>architectures</em> for different generations of their products (e.g., Intel Skylake and Haswell), and these different architectures provide opportunities for optimizing the software. <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="code modules optimizing PairHMM calculations for different CPUs" data-type="indexterm" id="idm45625634870296"/>For example, the GATK package includes several code modules corresponding to alternative implementations of the PairHMM algorithm that are optimized for specific Intel processor architectures. The program automatically activates the most appropriate version when it finds itself running on Intel processors, which provides some useful speed gains.</p>

<p>However, the benefits of hardware optimizations are most obvious across processor types; for example, if you compare how certain algorithms perform when implemented to run on FPGAs instead of CPUs. The Illumina DRAGEN toolkit (originally developed by Edico Genome) includes implementations of tools like <code>HaplotypeCaller</code> that are optimized to run on FPGAs and as a result are much faster than the original Java software version.<a contenteditable="false" data-primary="Illumina DRAGEN" data-see="DRAGEN toolkit" data-type="indexterm" id="idm45625634867080"/><a contenteditable="false" data-primary="DRAGEN toolkit" data-secondary="implementation optimizations" data-type="indexterm" id="idm45625634865704"/></p>

<p>The downside of hardware-optimized implementations is that by definition, they require specialized hardware. This can be a big problem for the many research labs that rely on shared institutional computing systems and don’t have access to other hardware. <a contenteditable="false" data-primary="Java" data-secondary="applications written in, portability across platforms" data-type="indexterm" id="idm45625634863400"/>In contrast, applications written in Java, like GATK, can run on a wide range of hardware architectures because the Java Virtual Machine (JVM) translates the application code (called <em>bytecode</em> in the Java world) into instructions appropriate<a contenteditable="false" data-primary="JVM (Java Virtual Machine)" data-type="indexterm" id="idm45625634861192"/> for the machine. <a contenteditable="false" data-primary="separation of concerns (SoC)" data-type="indexterm" id="idm45625634859992"/>This <em>separation of concerns</em> (SoC) between the bytecode of Java and what actually is executed on the machine is called an <em>abstraction layer</em> and it’s <span class="keep-together">incredibly</span> convenient for everyone involved. <a contenteditable="false" data-primary="abstraction layer" data-type="indexterm" id="idm45625634857016"/>Developers don’t need to worry about exactly what kind of processor we have in our laptops, and we don’t need to worry about what kind of processor they had in mind when they wrote the code. It also guarantees that the software can be readily deployed on standard off-the-shelf hardware, which makes it usable by anyone in the world.</p>

<p>Sometimes, you’ll need to choose between different implementations of the same algorithms depending on what is most important to you, including how much you prize speed over portability and interoperability. Other times, you’ll be able to enjoy the best of both worlds.<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="development of DRAGEN-GATK pipelines" data-type="indexterm" id="idm45625634854856"/> For example, the GATK team at the Broad Institute has entered into a collaboration with the DRAGEN team at Illumina, and the two teams are now working together to produce unified DRAGEN-GATK pipelines that will be available both as a free open source version (via Broad) and as a licensed hardware-accelerated version (via Illumina). <a contenteditable="false" data-primary="DRAGEN-GATK pipelines" data-type="indexterm" id="idm45625634852968"/>A key goal of the collaboration is to make the two implementations functionally equivalent—meaning that you could run either version and get the same results within a margin of error considered to be insignificant. This will benefit the research community immensely in that it will be possible to combine samples analyzed by either pipeline into downstream analyses without having to worry about batch effects, which we discussed briefly in the previous chapter.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="why_gatk_is_written_in_java">
<h5>Why GATK Is Written in Java</h5>

<p>For GATK, the analysis toolkit that we work with throughout most of this book, the<a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="choice of Java programming language, reasons for" data-type="indexterm" id="idm45625634849304"/> choice <a contenteditable="false" data-primary="Java" data-secondary="why GATK is written in" data-type="indexterm" id="idm45625634847704"/>of Java was deliberate: it makes development much easier for people who are not software engineers. Considering that the majority of the GATK methods development team are primarily data scientists, it’s more important to enable them to focus on developing algorithms to answer complex scientific questions rather than code optimizations for specific hardware.</p>

<p>But the good news is that we can still take advantage of native code optimizations where it matters! Third-party teams that specialize in the kind of hardcore software engineering involved have contributed libraries of GATK code that are optimized for specific hardware. The GATK engine has a plug-in system that allows us to swap out these code libraries at runtime (i.e., when we run a tool command), so we can, for example, run <code>HaplotypeCaller</code> with a version of the PairHMM algorithm optimized for a specific processor architecture.</p>

<p>For convenience, the library selection is automatically done by default. So as an end user, you don’t need to know what kind of hardware you’re working with. The engine will try to recognize what architecture it is running on, and if it finds a match in the available native code libraries, it switches to the correct one. If it doesn’t find a match, it falls back to the default Java implementation.</p>

<p>Between <a contenteditable="false" data-primary="C++, effort to rewrite GATK in" data-type="indexterm" id="idm45625634843144"/>2013 and 2014, a subset of the GATK team experimented with rewriting the GATK engine and key tools in C++, a more “low-level” language that allows the <span class="keep-together">programmer</span> to specify processor-level instructions. Codenamed Foghorn, the project was motivated on one hand by the emerging need to rewrite GATK with a more streamlined framework, and on the other, by claims that getting rid of the Java layer and using low-level capabilities like cache manipulation would deliver substantial speed improvements. In practice, the developers found that, although they were able to wring some speed improvements out of the C++ implementation, these were not as large as expected. More important, the pace of development was massively slower because the tool developers had to spend a lot more time solving software engineering problems instead of focusing on the data science.</p>

<p>The project was abandoned in late 2014, and the team recommitted to developing GATK in Java going forward. However, the effort spent on Foghorn was not all in vain: many lessons learned in the course of that project went on to serve as foundations for the<a contenteditable="false" data-primary="computing technology basics" data-secondary="performance bottlenecks" data-startref="ix_cmptchprf" data-type="indexterm" id="idm45625634839592"/> development of GATK4.<a contenteditable="false" data-primary="performance bottlenecks in computing" data-startref="ix_perfbot" data-type="indexterm" id="idm45625634837800"/> </p>
</div></aside>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Parallel Computing"><div class="sect1" id="parallel_computing">
<h1>Parallel Computing</h1>

<p>When you can’t go faster, go parallel.<a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-type="indexterm" id="ix_cmptchpar"/><a contenteditable="false" data-primary="parallel computing" data-type="indexterm" id="ix_parcmp"/> In the context of computing, <em>parallel computing</em>, or <em>parallelism</em>, is a way to make a program finish sooner by performing several operations in parallel rather than sequentially (i.e., waiting for each operation to finish before starting the next one). Imagine that you need to cook rice for 64 people, but your rice cooker can make enough rice for only 4 people at a time. If you need to cook all of the batches of rice sequentially, it’s going to take all night. But if you have eight rice cookers that you can use in parallel, you can finish up to eight times sooner.</p>

<p>This is a simple idea but it has a key requirement: you must be able to break the job into smaller tasks that can be performed independently. It’s easy enough to divide portions of rice because rice itself is a collection of discrete units. But you can’t always make that kind of division: for example, it takes one pregnant woman nine months to grow a baby, but you can’t do it in one month by having nine women share the work.</p>

<p>The good news is that most genomic analyses are more like rice than like babies—they essentially consist of a series of many small independent operations that can be parallelized. So how do we get from cooking rice to executing programs?</p>

<section data-type="sect2" data-pdf-bookmark="Parallelizing a Simple Analysis"><div class="sect2" id="parallelizing_a_simple_analysis">
<h2>Parallelizing a Simple Analysis</h2>

<p>Consider that when you run an analysis program, you’re just telling the computer to execute a set of instructions.<a contenteditable="false" data-primary="parallel computing" data-secondary="parallelizing a simple analysis" data-type="indexterm" id="ix_parcmpex"/><a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-tertiary="parallelizing a simple analysis" data-type="indexterm" id="ix_cmptchparanal"/>  Suppose that we have a text file and we want to count the number of lines in it. The set of instructions to do this can be as simple as this:</p>

<blockquote>
<p>Open the file; count the number of lines in it; tell us the number; close the file.</p>
</blockquote>

<p>Note that “tell us the number” can mean writing it to the console or storing it somewhere for use later on—let’s not worry about that right now.</p>

<p>Now suppose that we want to know the number of words on each line. The set of instructions would be as follows:</p>

<blockquote>
<p>Open the file; read the first line; count the number of words; tell us the number; read the second line; count the number of words; tell us the number; read the third line; count the number of words; tell us the number.</p>
</blockquote>

<p>And so on until we’ve read all the lines, and then finally we can close the file. It’s pretty straightforward, but if our file has a lot of lines, it will take a long time, and it will probably not use all the computing power we have available. So, to parallelize this program and save time, we just cut up this set of instructions into separate subsets, like this:</p>

<ul>
	<li>
	<p>Open the file; index the lines.</p>
	</li>
	<li>
	<p>Read the first line; count the number of words; tell us the number.</p>
	</li>
	<li>
	<p>Read the second line; count the number of words; tell us the number.</p>
	</li>
	<li>
	<p>Read the third line; count the number of words; tell us the number.</p>
	</li>
	<li>
	<p>[Repeat for all lines.]</p>
	</li>
	<li>
	<p>Collect final results and close the file.</p>
	</li>
</ul>

<p>Here, the “read the <em>N</em>th line” steps can be performed in parallel because they are all independent operations.</p>

<p>You’ll notice that <a contenteditable="false" data-primary="indexes" data-secondary="precompiled, in parallel computing" data-type="indexterm" id="idm45625634810712"/>we added a step, “index the lines.” That’s a little bit of preliminary work that allows us to perform the “read the <em>N</em>th line” steps in parallel (or in any order we want) because it tells us how many lines there are and, importantly, where to find each one within the file. It makes the entire process much more efficient. As you will see in the following chapters, tools like GATK require index files for the main data files (reference genome, read data and variant calls). The reason is to have that indexing step already done so that we can have the program look up specific chunks of data by their position in the genome.</p>

<p>Anyway, that’s the general principle: you transform your linear set of instructions into several subsets of instructions. There’s usually one subset that has to be run first and one that has to be run last, but all the subsets in the middle can be run at the same time (in parallel) or in whatever order you want.<a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-startref="ix_cmptchparanal" data-tertiary="parallelizing a simple analysis" data-type="indexterm" id="idm45625634807272"/><a contenteditable="false" data-primary="parallel computing" data-secondary="parallelizing a simple analysis" data-startref="ix_parcmpex" data-type="indexterm" id="idm45625634805320"/></p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="From Cores to Clusters and Clouds: Many Levels of Parallelism"><div class="sect2" id="from_cores_to_clusters_and_clouds_many">
<h2>From Cores to Clusters and Clouds: Many Levels of Parallelism</h2>

<p>So how do we go from rice cookers to parallelizing the execution of a genomic analysis program?<a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-tertiary="many levels of parallelism" data-type="indexterm" id="ix_cmptchparlvl"/><a contenteditable="false" data-primary="parallel computing" data-secondary="many levels, from cores to clusters and clouds" data-type="indexterm" id="ix_parcmplvl"/> Overall, the action of parallelizing computing operations consists of sending subsets of the work we want done to multiple cores for processing. We can do that by splitting up the work across the cores of a single multicore machine, or we can dispatch work to other machines if we have access to a cluster or cloud. In fact, we can combine the two ideas and dispatch work to multicore machines, in which the work is further split up among each machine’s cores. Going back to the rice-cooking example, it’s as if instead of cooking the rice yourself, you hired a catering company to do it for you. The company assigns the work to several people, who each have their own cooking station with multiple rice cookers. Now, you can feed a lot more people in the same amount of time! And you don’t even need to clean the dishes.</p>

<p>Whether we want <a contenteditable="false" data-primary="cores" data-secondary="multiple, distributing work across" data-type="indexterm" id="idm45625634796520"/>to distribute the work across multiple cores on a single machine or across multiple machines, we’re going to need a system that splits up the work, dispatches jobs for execution, monitors them for completion, and then compiles the results. Several kinds of systems can do that, falling broadly into two categories: internal or external to the analysis program itself. In the first case, the parallelization happens “inside” the program that we’re running: we run that program’s command line, and the parallelization happens without any additional “wrapping” on our part.<a contenteditable="false" data-primary="multithreading" data-type="indexterm" id="idm45625634794408"/> We call that <em>multithreading</em>. In the second case, we need to use a separate program to run multiple instances of the program’s command line. An example of an external parallelization is writing a script that runs a given tool separately on the data from each chromosome in a genome and then combines the result with an additional merge step. We call that approach <em>scatter-gather</em>. <a contenteditable="false" data-primary="scatter-gather parallelism" data-type="indexterm" id="idm45625634791960"/>We cover that in more detail in the next section when we introduce workflow management systems. In <a data-type="xref" href="#scatter_gather_allows_parallel_executio">Figure 3-2</a>, you can see how we can use multithreading and scatter-gather parallelism in the course of an analysis.<a contenteditable="false" data-primary="multithreading" data-secondary="using with scatter-gather parallelism" data-type="indexterm" id="idm45625634789448"/></p>

<figure><div id="scatter_gather_allows_parallel_executio" class="figure"><img alt="Scatter-gather allows parallel execution of tasks on different CPU cores (on a single machine or multiple machines depending on how it’s implemented)." src="Images/gitc_0302.png" width="1048" height="640"/>
<h6><span class="label">Figure 3-2. </span>Scatter-gather allows parallel execution of tasks on different CPU cores (on a single machine or multiple machines, depending on how it’s implemented).</h6>
</div></figure>

<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="spark_multithreading_on_steroids">
<h5>Spark: Multithreading on Steroids</h5>

<p><a href="https://spark.apache.org">Spark</a> is a powerful open source framework for data analysis that was originally developed at the University of California, Berkeley, and is now maintained by the Apache Software Foundation.<a contenteditable="false" data-primary="Spark framework" data-secondary="internal parallelization system in GATK" data-type="indexterm" id="idm45625634783224"/><a contenteditable="false" data-primary="multithreading" data-secondary="using Spark" data-type="indexterm" id="idm45625634781832"/>  It is a huge topic in and of itself, worthy of (and already covered by) multiple books; here we just want to call out a couple of specific applications for which Spark makes a big difference in terms of speed and scalability of genomic <span class="keep-together">analysis.</span> <a contenteditable="false" data-primary="Genome Analysis Toolkit (GATK)" data-secondary="Spark as internal parallelization system" data-type="indexterm" id="idm45625634779384"/></p>

<p>In GATK, Spark serves as the internal parallelization system that distributes execution across multiple cores. In the simplest case, it will distribute work across local cores of a multicore machine; but it is also capable of distributing work to multiple machines of a Spark-compatible cluster. This functionality is also used in another analysis toolkit called <a href="https://hail.is">Hail</a>, in the context of interactive data analysis that happens mostly downstream of GATK.<a contenteditable="false" data-primary="parallel computing" data-secondary="many levels, from cores to clusters and clouds" data-startref="ix_parcmplvl" data-type="indexterm" id="idm45625634776440"/><a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-startref="ix_cmptchparlvl" data-tertiary="many levels of parallelism" data-type="indexterm" id="idm45625634774664"/><a contenteditable="false" data-primary="Hail toolkit" data-type="indexterm" id="idm45625634772712"/></p>
</div></aside>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Trade-Offs of Parallelism: Speed, Efficiency, and Cost"><div class="sect2" id="trade_offs_of_parallelism_speedcomma_ef">
<h2>Trade-Offs of Parallelism: Speed, Efficiency, and Cost</h2>

<p>Parallelism is a great way to speed up processing on large amounts of data, but it has overhead costs.<a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-tertiary="trade-offs, speed, efficiency, and cost" data-type="indexterm" id="idm45625634769464"/><a contenteditable="false" data-primary="parallel computing" data-secondary="trade-offs, speed, efficiency, and cost" data-type="indexterm" id="idm45625634767784"/> Without getting too technical at this point, let’s just say that parallelized jobs need to be managed, you need to set aside memory for them, regulate file access, collect results, and so on. So it’s important to balance the costs against the benefits, and avoid dividing the overall work into too many small jobs. Going back to our earlier example, you wouldn’t want to use a thousand tiny rice cookers that each boil a single grain of rice. They would take far too much space on your countertop, and the time required to distribute each grain and then collect it when it’s cooked would more than negate any benefits from parallelizing in the first place.</p>

<p>More generally, although it’s tempting to think of parallelism as a way to make things go faster, it’s important to remember that the impression of speed is entirely subjective to the observer. In reality, the computation being run on each piece of data is not going any faster. We’re just running more computations at the same time, and we’re limited by the parallel capacity of our computing setup (typically measured in number of nodes or cores) as well as hardware limitations like I/O and network speeds. It’s more realistic to think of parallelism as a way to optimize available resources in order to finish tasks <em>sooner</em>, rather than making individual tasks run <em>faster</em>.</p>

<p>This distinction might seem pedantic given that, from the point of view of the human at the keyboard, the elapsed time (often called <em>wall-clock time</em>; that is, “the time shown by the clock on the wall”) is shorter. And isn’t that what we all understand as going faster? However, from the point of view of the resources we utilize, if we add up the time spent doing computation by the processor across all the cores we use, we might find that the overall job takes <em>more</em> time to complete compared to purely sequential execution because of the overhead costs.</p>

<p>That brings us to another important question: what is the monetary cost of utilizing those resources? If we’re working with a dedicated machine that is just sitting there with multiple cores and nothing else to do, the parallelization is still absolutely worth it, even with the overhead. We’ll want to parallelize the heck out of everything we do on that machine in order to maximize efficiency. However, when we start working in the cloud environment, as we do in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, and we need to start paying for itemized resources as we go, we’ll want to look more carefully at the trade-offs between minimizing wall-clock time and the size of the bill.<a contenteditable="false" data-primary="computing technology basics" data-secondary="parallel computing" data-startref="ix_cmptchpar" data-type="indexterm" id="idm45625634759752"/><a contenteditable="false" data-primary="parallel computing" data-startref="ix_parcmp" data-type="indexterm" id="idm45625634758072"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Pipelining for Parallelization and Automation"><div class="sect1" id="pipelining_for_parallelization_and_auto">
<h1>Pipelining for Parallelization and Automation</h1>

<p>Many genomic analyses involve<a contenteditable="false" data-primary="computing technology basics" data-secondary="pipelining for parallelization and automation" data-type="indexterm" id="ix_cmptchpipe"/> running a lot of routine data-processing operations that need to be parallelized for efficiency and automated to reduce human error.<a contenteditable="false" data-primary="pipelining" data-type="indexterm" id="ix_pipe"/> We do this by describing the workflow in a machine-readable language, which we then can feed into a workflow management system for execution. We go over how this works in practice in <a data-type="xref" href="ch08.xhtml#automating_analysis_execution_with_work">Chapter 8</a>, but first let’s set the stage by introducing basic concepts, definitions, and key software components. As we go, please keep in mind that this field currently has no such thing as a one-size-fits-all solution, and it’s ultimately up to you to review your needs and available options before picking a particular option. However, we can identify general principles to guide your selection, and we demonstrate these principles in action using the open source pipelining solution that is recommended by the GATK development team and used in production at the Broad Institute. As with most of this book, the goal here is not to prescribe the use of specific software, but to show through working examples how all of this fits together in practice.</p>

<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="scriptscomma_workflowscomma_and_pipelin">
<h5>Scripts, Workflows, and Pipelines</h5>

<p>The terms <em>script</em>, workflow, and <em>pipeline</em> are often used interchangeably but are not entirely equivalent.<a contenteditable="false" data-primary="scripts versus workflows" data-type="indexterm" id="idm45625634746328"/><a contenteditable="false" data-primary="pipelining" data-secondary="terminology, scripts, workflows, and pipelines" data-type="indexterm" id="idm45625634745128"/> Very generically, a script is a set of instructions in written form, whereas a workflow is a kind of script comprising multiple steps that are connected.<a contenteditable="false" data-primary="workflows" data-secondary="scripts, workflows, and pipelines" data-type="indexterm" id="idm45625634743352"/> In this book, we mostly talk about <em>writing and running workflows</em>, though we might occasionally use <em>script</em> or <em>workflow script</em> to refer specifically to the written form.</p>

<p>The term <em>pipeline</em> is often used<a contenteditable="false" data-primary="pipelines" data-type="indexterm" id="idm45625634739752"/> either as a substitute for workflow or as shorthand for the real-world operation of a particular workflow script (or set of scripts that are run in sequence), as in “the Broad’s production pipeline,” which refers to a combination of infrastructure, process, and artifacts involved in executing workflows in a routine, highly automated manner. We limit our usage of the word pipeline to the latter, along with <em>pipelining</em> as a convenient shorthand for the action of producing and running analysis workflows.</p>
</div></aside>

<p>One tricky aspect is that we have dozens of scripting languages and workflow management systems to choose from in the bioinformatics world—likely hundreds if you look at a wider range of fields. It can be difficult to compare them directly because they tend to be developed with a particular type of audience in mind, leading to very different modalities of user experience. They are often tuned for particular use cases and are sometimes optimized to operate on certain classes of infrastructure. We often see one solution that is preferred by one group prove to be particularly difficult or frustrating to use for another. These various solutions are also generally not interoperable, meaning that you can’t take a workflow script written for one workflow management system and run it unmodified on the next one over. <a contenteditable="false" data-primary="standardization, lack of, in workflow management systems" data-type="indexterm" id="idm45625634736248"/>This lack of standardization is a topic of both humor and desperation in just about every field of research known to humankind, as is illustrated in <a data-type="xref" href="#xkcd_comic_on_the_proliferation_of_stan">Figure 3-3</a>.</p>

<figure class="no-frame"><div id="xkcd_comic_on_the_proliferation_of_stan" class="figure"><img alt="XKCD comic on the proliferation of standards." src="Images/gitc_0303.png" width="500" height="283"/>
<h6><span class="label">Figure 3-3. </span>XKCD comic on the proliferation of standards (source: https://xkcd.com/927).</h6>
</div></figure>

<p>In recent years, we have seen some high-profile initiatives such as the Global Alliance GA4GH emerge <a contenteditable="false" data-primary="Global Alliance for Genomics and Health (GA4GH)" data-secondary="standards development and interoperable solutions" data-type="indexterm" id="idm45625634731032"/>with the explicit mission of developing common standards and consolidating efforts around a subset of solutions that have interoperability as a core value.<a contenteditable="false" data-primary="GA4GH Cloud Work Stream" data-type="indexterm" id="idm45625634729192"/> For example, the GA4GH Cloud Work Stream has converged on a small set of workflow languages for its driver projects, including CWL, Nextflow, and WDL, which we use in this book. At the same time, given the recognition that no single language <a contenteditable="false" data-primary="Workflow Description Language (WDL)" data-type="indexterm" id="idm45625634727704"/>is likely to satisfy <a contenteditable="false" data-primary="Common Workflow Language (CWL)" data-type="indexterm" id="idm45625634726456"/>all needs and preferences, several groups are working to increase interoperability by building support for multiple workflow languages into their workflow management systems. The workflow management system we use in this book, Cromwell, supports both WDL and CWL, and it could be extended to support additional languages in the future.<a contenteditable="false" data-primary="Cromwell workflow management system" data-type="indexterm" id="idm45625634724824"/></p>

<section data-type="sect2" data-pdf-bookmark="Workflow Languages"><div class="sect2" id="workflow_languages">
<h2>Workflow Languages</h2>

<p>In principle, we could write our workflows <a contenteditable="false" data-primary="workflow languages" data-type="indexterm" id="idm45625634721800"/>in almost <a contenteditable="false" data-primary="pipelining" data-secondary="workflow languages" data-type="indexterm" id="idm45625634720568"/>any programming language we like; but in practice, some are more amenable than others for describing workflows.<a contenteditable="false" data-primary="computing technology basics" data-secondary="pipelining for parallelization and automation" data-tertiary="workflow languages" data-type="indexterm" id="idm45625634718904"/> Just like natural languages, programming languages also exhibit a fascinating diversity and can be classified in various ways including grammar, mode of execution, and the programming paradigms that they support.</p>

<p>From a practical standpoint, we begin by making a distinction between all-purpose programming languages, which are intended to be usable for a wide range of applications, and <a contenteditable="false" data-primary="domain-specific languages (DSLs)" data-type="indexterm" id="idm45625634716184"/>domain-specific languages (DSLs) that are, as the name indicates, specifically designed for a particular domain or activity. The latter are typically preloaded with things like specially formulated data structures (i.e., ways to represent and manipulate data that “understand” the nature of the underlying information) and convenience functions that act as shortcuts; for example, handling domain-specific file formats, applying common processing actions, and so on. As a result, a DSL can be an attractive option if your needs fit well within the intended scope of the language, especially if your computational background is limited, given that the DSL typically enables you to get your work done without having to learn a lot of programming concepts and syntax.</p>

<p>On the other hand, if your needs are more varied or you are used to having the more expansive toolbox of a general-purpose language at your disposal, you might find yourself uncomfortably constrained by the DSL. In that case, you might prefer to use a general-purpose language, especially one enriched with domain-specific libraries that provide relevant data structures and convenience functions (e.g., Python with Biopython and related libraries). In fact, using a general-purpose language is more likely to enable you to use the same language for writing the data-processing tasks themselves and for managing the flow of operations, which is how many have traditionally done this kind of work. What we’re seeing now in the field, however, is a move toward separation of description and content, which manifests as increased adoption of DSLs specifically designed to describe workflows as well as of specialized workflow management systems. This evolution is strongly associated with the push for interoperability and portability.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Popular Pipelining Languages for Genomics"><div class="sect2" id="popular_pipelining_languages_for_genomi">
<h2>Popular Pipelining Languages for Genomics</h2>

<p>When we look at the cross-section of people who find themselves at the intersection of bioinformatics and genomics, we see a wide range of backgrounds, computational experience, and needs.<a contenteditable="false" data-primary="computing technology basics" data-secondary="pipelining for parallelization and automation" data-tertiary="popular pipelining languages for genomics" data-type="indexterm" id="idm45625634710408"/><a contenteditable="false" data-primary="pipelining" data-secondary="popular languages for genomics" data-type="indexterm" id="idm45625634708664"/> Some come from a software engineering background and prefer languages that are full featured and highly structured, offering great power at the cost of accessibility.<a contenteditable="false" data-primary="Bash, sed, and awk" data-type="indexterm" id="idm45625634706968"/> Some come from systems administration and believe every problem can be solved with judicious application of Bash, sed, and awk, the duct tape of the Unix-verse.<a contenteditable="false" data-primary="R language" data-type="indexterm" id="idm45625634705560"/><a contenteditable="false" data-primary="Python" data-type="indexterm" id="idm45625634704456"/> On the “bio” side of the fence, the more computationally trained tend to feel most at home with analyst favorites like Python and R, which have been gaining ground over old-time classics Perl and <a contenteditable="false" data-primary="Perl" data-type="indexterm" id="idm45625634703000"/><a contenteditable="false" data-primary="MATLAB" data-type="indexterm" id="idm45625634701896"/>MATLAB; some also tend to gravitate toward DSLs. Meanwhile wetlab-trained researchers might find themselves baffled by all of this, on initial contact at least. (Author’s note and disclaimer: Geraldine identifies as one of the initially baffled, having trained as a traditional microbiologist and eventually learned the rudiments of Perl and Python in a desperate bid to escape the wetlab workbench. Spoiler: it worked!)</p>

<p>Based on recent polling, some of the languages that are most popular with workflow authors in the genomics space are SnakeMake and Nextflow. <a contenteditable="false" data-primary="Nextflow" data-type="indexterm" id="idm45625634699784"/><a contenteditable="false" data-primary="SnakeMake" data-type="indexterm" id="idm45625634698680"/>Both are noted for their high degree of flexibility and ease of use.<a contenteditable="false" data-primary="Common Workflow Language (CWL)" data-type="indexterm" id="idm45625634697448"/><a contenteditable="false" data-primary="Workflow Description Language (WDL)" data-type="indexterm" id="idm45625634696296"/> Likewise, CWL and WDL are picking up steam because of their focus on portability and computational reproducibility. Of the two, CWL is more frequently preferred by people who have a technical background and enjoy its high level of abstraction and expressiveness. In contrast, WDL is generally considered to be more accessible to a wide audience.</p>

<p>At the end of the day, when it comes to picking a workflow language, we look at four main criteria: what kind of data structures the language supports (i.e., how we can represent and pass around information), how it enables us to control the flow of operations, how accessible it is to read and write for the intended audience, and how it affects our ability to collaborate with others. Whatever we choose, it’s unlikely that we can satisfy everyone’s requirements. However, if we were to boil all this down to just one recommendation, it would be this: if you want your workflow scripts to be widely used and understood in your area of research, pick a language that is open and accessible enough to newcomers yet scales well enough to the ambitions of the more advanced. And, of course, try to pick a language that you can run across different workflow management systems and computing platforms, because you never know what environment you or your collaborators might find yourselves in next.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Workflow Management Systems"><div class="sect2" id="workflow_management_systems">
<h2>Workflow Management Systems</h2>

<p>Many workflow management systems exist, but in general they follow the same basic pattern.<a contenteditable="false" data-primary="computing technology basics" data-secondary="pipelining for parallelization and automation" data-tertiary="workflow management systems" data-type="indexterm" id="idm45625634691304"/><a contenteditable="false" data-primary="workflow management systems" data-seealso="Cromwell" data-type="indexterm" id="idm45625634689544"/><a contenteditable="false" data-primary="pipelining" data-secondary="workflow management systems" data-type="indexterm" id="idm45625634688152"/> First, the workflow engine reads and interprets the instructions laid out in the workflow script, translating the instruction calls into executable jobs that are associated with a list of inputs (including data and parameters). <a contenteditable="false" data-primary="job schedulers" data-type="indexterm" id="idm45625634686392"/>It then sends out each job with its list of inputs to another program, generally called a <em>job scheduler</em>, that is responsible for orchestrating the actual execution of the work on the designated computing environment. Finally, it retrieves any outputs produced when the job is done. Most workflow management systems have some built-in logic for controlling the flow of execution; that is, the order in which they dispatch jobs for execution and for determining how they deal with errors and communicate with the compute <span class="keep-together">infrastructure.</span></p>

<aside class="pagebreak-before less_space" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45625634683512">
<h5>Examples of Job Schedulers</h5>

<p>If you have used on-premises HPC before, you might have used a job scheduler such as Sun GridEngine (SGE), Slurm, or HTCondor. On GCP, we’ll use an alternative called Pipelines API (PAPI) to marshal cloud resources with minimal effort. On AWS, there is a similar system called AWS Batch.<a contenteditable="false" data-primary="Amazon Web Services (AWS)" data-secondary="AWS Batch job scheduler" data-type="indexterm" id="idm45625634681624"/><a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="Pipelines API (PAPI) job scheduler" data-type="indexterm" id="idm45625634680088"/></p>
</div></aside>

<p>Another important advance for increasing portability and interoperability of analyses is the adoption of container technology, which we cover in detail in the last section of this chapter. For now, assume that a container is a mechanism that allows you to encapsulate all software requirements for a particular task, from the deepest levels of the operating system (OS) all the way to library imports, environment variables, and accessory configuration files.<a contenteditable="false" data-primary="pipelining" data-startref="ix_pipe" data-type="indexterm" id="idm45625634677656"/><a contenteditable="false" data-primary="computing technology basics" data-secondary="pipelining for parallelization and automation" data-startref="ix_cmptchpipe" data-type="indexterm" id="idm45625634676280"/></p>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Virtualization and the Cloud"><div class="sect1" id="virtualization_and_the_cloud">
<h1>Virtualization and the Cloud</h1>

<p>Up to this point, we have been assuming that whether you’re working <a contenteditable="false" data-primary="virtualization and the cloud" data-type="indexterm" id="ix_vrtl"/>with a single computer or<a contenteditable="false" data-primary="computing technology basics" data-secondary="virtualization and the cloud" data-type="indexterm" id="ix_cmptchvirt"/> a cluster, you’re dealing with “real” physical machines that are each set up with a given OS and software stack, as represented in <a data-type="xref" href="#representation_of_aright_parenthesis_th">Figure 3-4</a> A. Unfortunately, interacting with that kind of system has several disadvantages, especially in a shared environment like an institutional cluster. As an end user, you typically don’t have a choice regarding the OS, environment, and installed software packages. If you need to use something that isn’t available, you can ask an administrator to install it, but they might decline your request or the package you want might not be compatible with existing software. For the system administrators on the other side of the helpdesk, it can be a headache to keep track of what users want, manage versions, and deal with compatibility issues. Such systems take effort to update and scale.</p>

<p>That is why most modern systems use various degrees of <em>virtualization</em>, which is basically a clever bit of abstraction that makes it possible to run multiple different software configurations on top of the same hardware through virtual machines (VMs) and containers as represented in <a data-type="xref" href="#representation_of_aright_parenthesis_th">Figure 3-4</a> B and C respectively. These constructs can be utilized in many contexts, including optionally on local systems (you can even use containers on your laptop!), but they are absolutely essential for cloud <span class="keep-together">infrastructure.</span></p>

<figure><div id="representation_of_aright_parenthesis_th" class="figure"><img alt="Representation of A) The software stack installed on a physical machine; B) a server providing clients with access to individual VMs; C) VMs and containers side by side comparison." src="Images/gitc_0304.png" width="1446" height="993"/>
<h6><span class="label">Figure 3-4. </span>A) The software stack installed on a physical machine; B) a system hosting multiple VMs; C) a system hosting multiple containers.</h6>
</div></figure>

<section data-type="sect2" data-pdf-bookmark="VMs and Containers"><div class="sect2" id="vms_and_containers">
<h2>VMs and Containers</h2>

<p>A VM is an infrastructure-level construct that includes its own OS. The VM sits on top of a virtualization layer that runs on the actual OS of the underlying physical machine(s). In the simplest case, VMs can be run on a single physical machine, with the effect of turning that physical machine into multiple servers that share the underlying resources. However, the most robust systems utilize multiple physical machines to support the layer of VMs, with a complex layer between them that manages the allocation of physical resources. The good news is that for end users, this should not make any difference—all you need to know is that you can interact with a particular VM in isolation without worrying about what it’s sitting on.</p>

<p>A container is similar in principle to a VM, but it is an application-level construct that is much lighter and more mobile, meaning that it can be deployed easily to different sites, whereas VMs are typically tied to a particular location’s infrastructure. Containers are intended to bundle all the software required to run a particular program or set of programs. This makes it a lot easier to reproduce the same analysis on any infrastructure that supports running the container, from your laptop to a cloud platform, without having to go through the pain of identifying and installing all the software dependencies involved. You can even have multiple containers running on the same machine, so you can easily switch between different environments if you need to run programs that have incompatible system requirements.</p>

<p>If you’re thinking, “These both sound great; which one should I use?” here’s some good news: you can use both in combination, as illustrated in <a data-type="xref" href="#representation_of_a_system_with_three_v">Figure 3-5</a>.</p>

<figure class="no-frame"><div id="representation_of_a_system_with_three_v" class="figure"><img alt="Representation of a system with three VMs, of which two are running containers." src="Images/gitc_0305.png" width="820" height="557"/>
<h6><span class="label">Figure 3-5. </span>A system with three VMs: the one on the left is running two containers, serving App #1 and App #2; the middle is running a single container, serving App #3; the right is serving App #4 directly (no container).</h6>
</div></figure>

<p>There are several registries for sharing and obtaining containers, including <a href="https://hub.docker.com">Docker Hub</a>, <a href="https://quay.io">Quay.io</a>, and <a href="https://cloud.google.com/container-registry">GCR</a>, Google’s general-purpose container registry in GCP. In the registry, the container is packaged as an <em>image</em>. Note that this has nothing to do with pictures; here the word <em>image</em> is used in the same software-specific way that refers to a special type of file. You know how sometimes when you need to install new software on your computer, the download file is called a <em>disk image</em>? That’s because the file you download is in a format that your OS is going to treat as if it were a physical disk on your machine. This is basically the same thing. To use a container, you first tell the Docker program to download, or <em>pull</em>, a container image file from a registry—for example, Docker Hub (more on Docker shortly)—and then you tell it to initialize the container, which is conceptually equivalent to booting up a VM. And after the container is running, you can run any software within it that is installed on its system. You can also install additional packages or perform additional configurations as needed. <a data-type="xref" href="#the_relationship_between_containercomma">Figure 3-6</a> illustrates the relationship between container, image, and registry.</p>

<figure><div id="the_relationship_between_containercomma" class="figure"><img alt="The relationship between container, image, and registry." src="Images/gitc_0306.png" width="1107" height="550"/>
<h6><span class="label">Figure 3-6. </span>The relationship between registry, image, and container.</h6>
</div></figure>

<p>The most widely used brand of container systems is Docker, produced by the company of the same name. As a result of Docker’s ubiquitousness, people will often say “a docker” instead of “a container,” much like when “xerox” became a replacement for “copy machines” (in the US at least) because of the dominance of the Xerox company. However, <code>docker</code> with a lowercase <em>d</em> is also the command-line program that you install on your machine to run Docker containers. Similarly, although the action of bundling a software tool, package, or analysis into a Docker container should rightly be called “containerizing,” people often call it “dockerizing,” as in, “I dockerized my Python script.” Dockerizing a tool involves writing a script called a Dockerfile that describes all installations and environment configurations necessary to <em>build</em> the Docker image, as demonstrated in <a data-type="xref" href="#the_process_for_creating_a_docker_conta">Figure 3-7</a>.</p>

<figure><div id="the_process_for_creating_a_docker_conta" class="figure"><img alt="The process for creating a Docker container." src="Images/gitc_0307.png" width="1049" height="550"/>
<h6><span class="label">Figure 3-7. </span>The process for creating a Docker image.</h6>
</div></figure>

<p>As noted earlier, it is possible to use containers in various contexts, including local machines, HPC, and the cloud. One important restriction is that Docker specifically is usually not allowed in shared environments like most institutions’ HPCs, because it requires a very high level of access permissions called <em>root</em>. In that kind of setting, <span class="keep-together">system</span> administrators will prefer <em>Singularity</em>, an alternative system that achieves the same results. Fortunately, it is possible to run Docker containers within a Singularity system.</p>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Introducing the Cloud"><div class="sect2" id="introducing_the_cloud">
<h2>Introducing the Cloud</h2>

<p>Finally, we get to the topic many of you have been waiting for: what is this cloud thing anyway? <a contenteditable="false" data-primary="computing technology basics" data-secondary="virtualization and the cloud" data-tertiary="introduction to the cloud" data-type="indexterm" id="ix_cmptchvirtcld"/><a contenteditable="false" data-primary="virtualization and the cloud" data-secondary="introduction to the cloud" data-type="indexterm" id="ix_vrtlcld"/><a contenteditable="false" data-primary="cloud computing" data-secondary="introduction to" data-type="indexterm" id="ix_cldintr"/>The surprisingly easy answer is that the cloud is a bunch of computers that you can rent. In practice, that means that as a user, you can easily launch a VM and select how much RAM, storage, and what CPUs you want.<a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="launching and selecting RAM, storage, and CPUs" data-type="indexterm" id="idm45625634630296"/> You want a VM with <span class="keep-together">1 TB</span> of RAM and 32 CPUs for genome assembly? No problem! Most of the VMs in the cloud are running some form of Linux as the OS, which you get to choose when you launch it, and are typically accessed using a remote shell via Secure Shell (SSH).<a contenteditable="false" data-primary="SSH (Secure Shell)" data-type="indexterm" id="idm45625634627784"/><a contenteditable="false" data-primary="Linux OS on cloud VMs" data-type="indexterm" id="idm45625634626680"/></p>

<p>Although some VMs include free storage, this is typically ephemeral and will go away when you stop and start your VM.<a contenteditable="false" data-primary="data storage" data-secondary="in the cloud" data-type="indexterm" id="idm45625634625160"/> Instead, use block storage (a persistent device) to store data, scripts, and so on on your VM. You can think of these very much like a USB thumb drive that you can have “plugged” into your VM whenever you like. Even when you terminate your VM, files on block storage will be OK and safely saved. Files can also be stored in object store—think of this more like Google Drive or Dropbox, where files can be read and written by multiple VMs at the same time, but you don’t typically use these as a normal filesystem. <a contenteditable="false" data-primary="SSH File Transfer Protocol (SFTP)" data-type="indexterm" id="idm45625634623112"/>Instead, they are more akin to an SSH File Transfer Protocol (SFTP) server for sharing files between VMs, where you transfer files through a utility to and from the object storage system. The final basic component of a cloud is networking.<a contenteditable="false" data-primary="networking (cloud)" data-type="indexterm" id="idm45625634621656"/> With a virtual networking service, you can control who has access to your VMs, locking it down tightly to ensure that only you (or others you trust) have access.</p>

<section data-type="sect3" data-pdf-bookmark="Clouds are not fluffy"><div class="sect3" id="clouds_are_not_fluffy">
<h3>Clouds are not fluffy</h3>

<p>When you think about clouds, they are fluffy, distant, and elusive, not at all concrete, real things that you can touch, feel, and capture.<a contenteditable="false" data-primary="cloud computing" data-secondary="introduction to" data-tertiary="abstraction of infrastructure concerns" data-type="indexterm" id="idm45625634618360"/> Unlike their namesake, the cloud infrastructure that most of us use directly (or indirectly) today ultimately is composed of real, physical computers racked up and blinking away in huge datacenters. What makes it different, though, from previous models for compute (and rings true to their name) is its ephemeral nature. Just like clouds coming and going—popping up, dumping their rain, and then blowing away—cloud computing is transient for the end user. The cloud allows you as a researcher, developer, or analyst to request computational infrastructure when you need it, to use it for computing as long as you need it, and then you can release all the resources when you’re done.</p>

<p>This approach is great because it saves time and money insomuch as you can spin up a lot of resources at once, get your work done, and spin these back down, saving on the costs or running hardware continuously. You don’t need to think too much about where the servers are racked, how they are configured, the health of the hardware, power consumption, or myriad other infrastructure concerns. These are all <em>abstracted away</em> from you and are taken care of without you having to think about it too much. What you focus on, instead, is the computational work that you need to perform, the resources you need to do it, and how to most effectively use these resources both from a time and money perspective.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Evolution of cloud infrastructure and services"><div class="sect3" id="evolution_of_cloud_infrastructure_and_s">
<h3>Evolution of cloud infrastructure and services</h3>

<p>Amazon launched the first widely successful commercial public cloud service in 2006, but the basic idea has been around for a long time. <a contenteditable="false" data-primary="cloud computing" data-secondary="introduction to" data-tertiary="evolution of cloud infrastructure and services" data-type="indexterm" id="idm45625634612040"/>Mainframes in the 1960s were often rented for use, which made a ton of sense, given the massive costs of buying and operating them. Regardless of the invention of the personal computer, the idea of renting computing infrastructure has cropped up again over and over in the intervening decades. In academic groups and industry, the concept of shared grid computing in the 1990s and 2000s was the more modern equivalent of rented mainframe time. Groups banded together to build cheap but powerful Linux-based HPC clusters that were often centrally managed and allocated out to multiple groups based on some sort of financial split.</p>

<p>Today’s public clouds are different, though, in the level of abstraction. Hence, the adoption of the fluffy, amorphous name to reflect the fact that an understanding of the underlying details is not required in order to run large-scale analysis on clouds. When working with a given cloud, you might know the general region of the world that hosts your infrastructure (e.g., North Virginia for AWS <code>us-east-1</code>), but many of the details are hidden from you:<a contenteditable="false" data-primary="virtual machines (VMs)" data-type="indexterm" id="idm45625634608280"/> how many people are using the underlying hardware of your VM, where the datacenter is really located, how the network is set up, and so on. What you do know are key details that affect service cost and job execution time, like how many CPUs are available, how much RAM the VM has, the uptime guarantees of the file storage system, and the regulations the system conforms to.</p>

<p>There are <a contenteditable="false" data-primary="public cloud providers" data-seealso="cloud computing" data-type="indexterm" id="idm45625634606248"/>now many public cloud providers—clouds available to anyone who can pay for the service. <a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-type="indexterm" id="idm45625634604600"/><a contenteditable="false" data-primary="Microsoft Azure" data-type="indexterm" id="idm45625634603480"/><a contenteditable="false" data-primary="Amazon Web Services (AWS)" data-type="indexterm" id="idm45625634602376"/>The most dominant currently in the Western hemisphere are AWS, Microsoft Azure, and GCP. Each provides a similar mix of services that range from simple VMs rentable by the hour (or minute), file storage services, and networking to more specialized services such as Google’s Cloud TPU service, which allows you to perform accelerated machine learning operations. The important feature, though, is that these resources are provided as services: you use what you need per hour, minute, second, or API call, and are charged accordingly.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Pros and cons of the cloud"><div class="sect3" id="pros_and_cons_of_the_cloud">
<h3>Pros and cons of the cloud</h3>

<p>One of the major advantages that many people point to when discussing the cloud is cost. <a contenteditable="false" data-primary="cloud computing" data-secondary="introduction to" data-tertiary="cloud pros and cons" data-type="indexterm" id="idm45625634598632"/>When building a datacenter, the fixed costs are enormous. You must hire people to rack and maintain physical servers, monitor the network for intrusion, deal with power fluctuations, backups, air conditioning, and so on. Honestly, it is a lot of work! For a datacenter that supports hundreds of users, the costs associated with maintaining the infrastructure can be worth it. But many researchers, developers, analysts, and others are realizing that they don’t need to have hundreds of computers always available and running, just waiting for a task. Instead, it makes a lot more sense to use a cloud environment in which you can do local work on your laptop, without extensive resources, and then, when your analysis is ready, you can scale up to hundreds or thousands of machines. Commercial public clouds allow you to easily <em>burst</em> your capacity and do a huge analysis when you need to, as opposed to waiting weeks, months, or even years for a dedicated local cluster to finish your tasks. Likewise, you don’t need to pay for the maintenance of local infrastructure for all the time you spend developing your algorithms and perfecting your analysis locally.</p>

<p>Finally, as a public cloud user, you have full control of your environment. Need a specific version of Python? Do you have a funky library that compiles only if very specific tool chains are installed? No problem! The cloud lets you have full control over your VMs, something that a shared, local infrastructure would never allow. Even with this control, when they are set up following cloud vendor best practices, public cloud solutions are invariably more secure than on-premises infrastructure because of the vast amount of resources dedicated to security services in these environments and the isolation between users afforded by virtualization.</p>

<p>Although the public cloud platforms are amazing, powerful, flexible and, in many cases, can be used effectively to save a ton of money in the long run, there are some disadvantages to look out for. If you are looking to always process a fixed number of genomes produced by your sequencing group per month, the public cloud might be less attractive and it would make more sense to build a small local compute environment for this very predictable workload of data produced locally. This is assuming, of course, that you have IT professionals who can act as administrators. Another consideration is expertise.<a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="expertise needed in use of" data-type="indexterm" id="idm45625634593112"/> Using the cloud demands a certain level of expertise, and an unsuspecting novice user might accidentally use VMs with weak passwords, set up data storage buckets with weak security, share credentials in an insecure way, or just be totally lost in the process of managing a fleet of Linux VMs. Even these potential downfalls, though, are generally outweighed by the benefits of working flexibly<a contenteditable="false" data-primary="computing technology basics" data-secondary="virtualization and the cloud" data-startref="ix_cmptchvirtcld" data-tertiary="introduction to the cloud" data-type="indexterm" id="idm45625634591128"/> on commercial cloud <a contenteditable="false" data-primary="virtualization and the cloud" data-secondary="introduction to the cloud" data-startref="ix_vrtlcld" data-type="indexterm" id="idm45625634589032"/>environments for many people.<a contenteditable="false" data-primary="cloud computing" data-secondary="introduction to" data-startref="ix_cldintr" data-type="indexterm" id="idm45625634587192"/></p>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Categories of Research Use Cases for Cloud Services"><div class="sect2" id="categories_of_research_use_cases_for_cl">
<h2>Categories of Research Use Cases for Cloud Services</h2>

<p>The basic components of the cloud described in the previous section are really just the tip of the iceberg. <a contenteditable="false" data-primary="computing technology basics" data-secondary="virtualization and the cloud" data-tertiary="categories of use cases for cloud services" data-type="indexterm" id="ix_cmptchvirtuse"/><a contenteditable="false" data-primary="virtualization and the cloud" data-secondary="categories of research use cases for cloud services" data-type="indexterm" id="ix_vrtlresuse"/><a contenteditable="false" data-primary="cloud computing" data-secondary="categories of research use cases for cloud services" data-type="indexterm" id="ix_cldcatuse"/>Many more services are available on the main commercial cloud platforms. In fact, there are far too many services, some universal and some unique to a particular cloud, than we can describe here. But let’s take a look at how researchers might use the services or the cloud most commonly. <a data-type="xref" href="#an_overview_of_the_types_of_usage_of_cl">Table 3-1</a> provides an overall summary.</p>

<table class="border" id="an_overview_of_the_types_of_usage_of_cl">
	<caption><span class="label">Table 3-1. </span>An overview of the types of usage of cloud infrastructure</caption>
	<thead>
		<tr>
			<th>Usage type</th>
			<th>Cloud environment</th>
			<th>Description</th>
			<th>Positives</th>
			<th>Negatives</th>
		</tr>
	</thead>
	<tbody>
		<tr>
			<td>Lightweight development</td>
			<td>Google Cloud Shell</td>
			<td>Using a simple-to-launch free VM for editing code and scripts</td>
			<td>
			<ul>
				<li>
				<p>Free</p>
				</li>
				<li>
				<p>Extremely easy to launch and log in to</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Extremely limited VM</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>Intermediate analysis and development</td>
			<td>Single VM</td>
			<td>Launching a single VM, logging in, performing development and analysis work</td>
			<td>
			<ul>
				<li>
				<p>Can control the resources on the VM launched</p>
				</li>
				<li>
				<p>VMs can be powerful enough to perform realistic analysis</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Launching a VM requires more configuration</p>
				</li>
				<li>
				<p>Larger VMs have increased costs</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>Batch analysis</td>
			<td>Multiple VMs via batch system</td>
			<td>Using a system like AWS Batch or Google Cloud Pipelines API to launch many VMs and analyze data in parallel</td>
			<td>
			<ul>
				<li>
				<p>Allows for parallel, scaled-up analysis</p>
				</li>
				<li>
				<p>Workflow management systems like Cromwell support these with little effort</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Increased costs and complexity</p>
				</li>
			</ul>
			</td>
		</tr>
		<tr>
			<td>Framework analysis</td>
			<td>Multiple VMs via a framework</td>
			<td>Using Spark, Hadoop, or other framework for data analysis</td>
			<td>
			<ul>
				<li>
				<p>These frameworks allow for specialty analysis</p>
				</li>
			</ul>
			</td>
			<td>
			<ul>
				<li>
				<p>Increased costs and complexity</p>
				</li>
			</ul>
			</td>
		</tr>
	</tbody>
</table>

<section data-type="sect3" data-pdf-bookmark="Lightweight development: Google Cloud Shell"><div class="sect3" id="lightweight_development_google_cloud_sh">
<h3>Lightweight development: Google Cloud Shell</h3>

<p>The cloud is a fantastic place for software development.<a contenteditable="false" data-primary="lightweight development using cloud services" data-type="indexterm" id="idm45625634543816"/><a contenteditable="false" data-primary="cloud computing" data-secondary="categories of research use cases for cloud services" data-tertiary="lightweight development, Google Cloud Shell" data-type="indexterm" id="idm45625634542536"/> Even though many researchers will want to use their own laptops or workstation for development, there can be some really compelling reasons for using the cloud as a primary development environment, especially for testing.<a contenteditable="false" data-primary="Google Cloud Platform (GCP)" data-secondary="Google Cloud Shell" data-type="indexterm" id="idm45625634540456"/> On GCP, for example, you can use the Google Cloud Shell from the Google Cloud Console for light development and testing. This is a free (yes, <em>free</em>!) VM with one virtual CPU core and 5 GB of storage that you can use just by clicking the terminal icon in the web console. This is a fantastic environment for some light coding and testing; just remember to copy code off of your free instance (using Git, for example) because there are quotas for total runtime per week, and, if you don’t use the service for a while, your 5 GB volume might get cleaned out. Still, this is a great option for quickly getting started with the cloud and performing lightweight tasks. You just need a web browser, and the GCP tools are all preinstalled and configured for you. Many other tools that you might want to work with are already installed as well, including Git and Docker, along with languages like Java and Python. You’ll have a chance to try it out early on in the next chapter.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Intermediate development and analysis: single VM"><div class="sect3" id="intermediate_development_and_analysis_s">
<h3>Intermediate development and analysis: single VM</h3>

<p>Although the Google Cloud Shell<a contenteditable="false" data-primary="cloud computing" data-secondary="categories of research use cases for cloud services" data-tertiary="intermediate development and analysis, using single VM" data-type="indexterm" id="idm45625634535448"/> is great for many purposes, easy to use, and free, sometimes <a contenteditable="false" data-primary="intermediate development and analysis using cloud services" data-type="indexterm" id="idm45625634533544"/>you might <a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="intermediate development and analysis using single cloud VM" data-type="indexterm" id="idm45625634532184"/>need a bit more power, especially if you want to test your code or analysis at the next scale up, so you spin up your own dedicated VM. This is perhaps the most commonly used option because of the mix of flexibility and simplicity it offers: you can customize your VM, ensuring you have enough CPU cores, RAM, and local storage to accomplish your goal. Unlike the Google Cloud Shell, you must pay for each hour or minute you run this VM; however, you have full control over the nature of the VM. You might use this for software or algorithm development, testing your analysis approach, or spinning up a small fleet of these VMs to perform analysis on multiple VMs simultaneously. Keep in mind, however, that if you are manually launching these VMs, fewer tools will be preinstalled on them and ready to go for you. <a contenteditable="false" data-primary="Docker" data-type="indexterm" id="idm45625634529768"/><a contenteditable="false" data-primary="Git" data-type="indexterm" id="idm45625634528664"/>That makes using utilities such as Git and Docker very helpful for moving your analysis tasks from VM to VM. You’ll have a chance to use this extensively in <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a> through <a data-type="xref" href="ch07.xhtml#gatk_best_practices_for_somatic_variant">Chapter 7</a>.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Batch analysis: multiple VMs via batch services"><div class="sect3" id="batch_analysis_multiple_vms_via_batch_s">
<h3>Batch analysis: multiple VMs via batch services</h3>

<p>This approach is really the sweet spot for most users who are aware of it. Although you might use<a contenteditable="false" data-primary="batch analysis using multiple VMs via cloud batch services" data-type="indexterm" id="idm45625634523176"/> your laptop<a contenteditable="false" data-primary="cloud computing" data-secondary="categories of research use cases for cloud services" data-tertiary="batch analysis, using multiple VMs via batch services" data-type="indexterm" id="idm45625634521896"/> or Google Cloud Shell for software and script development, and one or more VMs for testing them on appropriately sized hardware, you ultimately don’t want to manually manage VMs if your goal is to scale up your analysis.<a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="batch analysis with multiple VMs via batch cloud services" data-type="indexterm" id="idm45625634519720"/> Imaging running 10,000 genome alignments at the same time; you need systems that can batch up the work, provision VMs automatically for you, and turn the VMs off when your work is done. Batch systems are designed just for this task; Google Cloud, for example, offers the Google Cloud Pipelines API, which you can use to submit a large batch of multiple jobs simultaneously. The service will take care of spinning up numerous VMs to perform your analysis and then automatically clean them up after collecting the output files. This is extremely convenient if you need to perform noninteractive analysis on a ton of samples. You’ll see in <a data-type="xref" href="ch08.xhtml#automating_analysis_execution_with_work">Chapter 8</a> through <a data-type="xref" href="ch11.xhtml#running_many_workflows_conveniently_in">Chapter 11</a> that workflow engines like Cromwell are designed to take advantage of these batch services, which take care of all the details of launching batch jobs. That makes it much easier for you to focus on the details of the analysis you’re performing rather than on the infrastructure involved.</p>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Framework analysis: multiple VMs via framework services"><div class="sect3" id="framework_analysis__multiple_vms_via_fr">
<h3>Framework analysis: multiple VMs via framework services</h3>

<p>The final approach that many researchers will use involves interactive, iterative analysis. <a contenteditable="false" data-primary="cloud computing" data-secondary="categories of research use cases for cloud services" data-tertiary="framework analysis with multiple VMs via framework services" data-type="indexterm" id="idm45625634513208"/><a contenteditable="false" data-primary="framework analysis, using multiple VMs via cloud framework services" data-type="indexterm" id="idm45625634511416"/><a contenteditable="false" data-primary="virtual machines (VMs)" data-secondary="framework analysis with multiple VMs via cloud framework services" data-type="indexterm" id="idm45625634510264"/>In genomics, you can use a batch system to perform large-scale alignment and variant calling but, after you have VCF files for your variants, you might choose to move to a Spark cluster, RStudio, Jupyter Notebook, or any of a large number of analytical environments for subsequent analysis.<a contenteditable="false" data-primary="RStudio" data-type="indexterm" id="idm45625634508408"/><a contenteditable="false" data-primary="Spark framework" data-secondary="framework analysis via cloud framework services" data-type="indexterm" id="idm45625634507304"/><a contenteditable="false" data-primary="Jupyter Notebook" data-type="indexterm" id="idm45625634505896"/> In <a data-type="xref" href="ch12.xhtml#interactive_analysis_in_jupyter_noteboo">Chapter 12</a>, we explore how this works in Terra, which you can use to easily create a custom environment for data <span class="keep-together">processing</span> with a Jupyter interface for interactive analysis, generating plots for your publications, and sharing results with others.<a contenteditable="false" data-primary="computing technology basics" data-secondary="virtualization and the cloud" data-startref="ix_cmptchvirtuse" data-tertiary="categories of use cases for cloud services" data-type="indexterm" id="idm45625634502728"/><a contenteditable="false" data-primary="virtualization and the cloud" data-secondary="categories of research use cases for cloud services" data-startref="ix_vrtlresuse" data-type="indexterm" id="idm45625634500728"/><a contenteditable="false" data-primary="cloud computing" data-secondary="categories of research use cases for cloud services" data-startref="ix_cldcatuse" data-type="indexterm" id="idm45625634499032"/><a contenteditable="false" data-primary="virtualization and the cloud" data-startref="ix_vrtl" data-type="indexterm" id="idm45625634497352"/><a contenteditable="false" data-primary="computing technology basics" data-secondary="virtualization and the cloud" data-startref="ix_cmptchvirt" data-type="indexterm" id="idm45625634495960"/></p>
</div></section>
</div></section>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Wrap-Up and Next Steps"><div class="sect1" id="wrap_up_and_next_step">
<h1>Wrap-Up and Next Steps</h1>

<p>In this chapter, we completed the primer topics, which gave you a background on genomics (<a data-type="xref" href="ch02.xhtml#genomics_in_a_nutshell_a_primer_for_new">Chapter 2</a>) and computing technologies (this chapter). We delved into the nitty-gritty details of computer hardware, parallel computing, and virtualization and gave you a glimpse of the power of using workflow execution systems to scale out your analysis on the cloud. In <a data-type="xref" href="ch04.xhtml#first_steps_in_the_cloud">Chapter 4</a>, we take our first baby steps to the cloud environment and show you how to get started with your own VMs running in GCP.<a contenteditable="false" data-primary="computing technology basics" data-startref="ix_cmptch" data-type="indexterm" id="idm45625634490136"/></p>
</div></section>
</div></section></div>



  </body></html>