- en: Chapter 5\. Automating Database Management on Kubernetes with Operators
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章。使用 Operator 在 Kubernetes 上自动化数据库管理。
- en: In this chapter, we’ll continue our exploration of running databases on Kubernetes,
    but shift our focus from installation to operations. It’s not enough just to know
    how the elements of a database application map onto the primitives provided by
    Kubernetes for an initial deployment. You also need to know how to maintain that
    infrastructure over time in order to support your business-critical applications.
    In this chapter, we’ll take a look at the Kubernetes approach to operations so
    that you can keep databases running effectively.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探讨在 Kubernetes 上运行数据库的话题，但将焦点从安装转向操作。仅仅了解数据库应用程序的各个元素如何映射到 Kubernetes
    提供的原语以进行初始部署是不够的。您还需要知道如何随着时间推移维护该基础设施，以支持业务关键的应用程序。在本章中，我们将详细研究 Kubernetes 的操作方法，以便您可以有效地保持数据库的运行。
- en: 'Operations for databases and other data infrastructure consist of a common
    list of “day two” tasks, including the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库和其他数据基础设施的操作包括一个常见的“第二天”任务列表，包括以下内容：
- en: Scaling capacity up and down, including reallocating workload across resized
    clusters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展容量的上下调整，包括在调整大小的集群中重新分配工作负载。
- en: Monitoring database health and replacing failed (or failing) instances
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控数据库健康状态并替换失败（或正在失败）的实例。
- en: Performing routine maintenance tasks, such as repair operations in Apache Cassandra
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行常规维护任务，如在 Apache Cassandra 中进行修复操作。
- en: Updating and patching software
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新和打补丁软件。
- en: Maintaining secure access keys and other credentials that may expire over time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护安全访问密钥和其他可能随时间过期的凭证。
- en: Performing backups, and using them to restore data in disaster recovery
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行备份，并在灾难恢复中使用它们来恢复数据。
- en: 'While the details of how these tasks are performed may vary among technologies,
    the common concern is how we can use automation to reduce the workload on human
    operators and enable us to operate infrastructure at larger and larger scales.
    How can we incorporate the knowledge that human operators have built up around
    these tasks? While traditional cloud operations have used scripting tools that
    run externally to your cloud infrastructure, a more cloud native approach is to
    have this database control logic running directly within your Kubernetes clusters.
    The question we’ll explore in this chapter is: what is the Kubernetes-friendly
    way to represent this control logic?'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管执行这些任务的具体细节在不同技术之间可能会有所不同，但共同关注的是如何利用自动化来减少人工操作员的工作量，并使我们能够在越来越大的规模上操作基础设施。我们如何整合人工操作员在这些任务中积累的知识？传统的云操作使用在云基础设施外部运行的脚本工具，而更符合云原生的方法是在您的
    Kubernetes 集群中直接运行这些数据库控制逻辑。本章我们将探讨的问题是：如何用符合 Kubernetes 的方式来表示这种控制逻辑？
- en: Extending the Kubernetes Control Plane
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展 Kubernetes 控制平面。
- en: The good news is that the designers of Kubernetes aren’t surprised at all by
    this question. In fact, the Kubernetes control plane and API are designed to be
    extensible. Kelsey Hightower and others have referred to Kubernetes as [“a platform
    for building platforms”](https://oreil.ly/pFRDO).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，Kubernetes 的设计者们对这个问题并不感到意外。事实上，Kubernetes 的控制平面和 API 设计为可扩展的。Kelsey Hightower
    和其他人称 Kubernetes 为 [“构建平台的平台”](https://oreil.ly/pFRDO)。
- en: Kubernetes provides multiple extension points, primarily related to its control
    plane. [Figure 5-1](#kubernetes_control_plane_and_extension) includes the [Kubernetes
    core components](https://oreil.ly/hsxFY) such as the API server, scheduler, Kubelet
    and `kubectl`, along with indications of the [extension points](https://oreil.ly/UXbo0)
    they support.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供多个扩展点，主要与其控制平面相关。[图 5-1](#kubernetes_control_plane_and_extension)
    包括 [Kubernetes 核心组件](https://oreil.ly/hsxFY) 如 API 服务器、调度器、Kubelet 和 `kubectl`，以及它们支持的
    [扩展点](https://oreil.ly/UXbo0) 的指示。
- en: '![Kubernetes control plane and extension points](assets/mcdk_0501.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes 控制平面和扩展点](assets/mcdk_0501.png)'
- en: Figure 5-1\. Kubernetes control plane and extension points
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. Kubernetes 控制平面和扩展点。
- en: Now let’s examine the details of extending the Kubernetes control plane, starting
    with components on your local client and those within the Kubernetes cluster.
    Many of these extension points are relevant to databases and data infrastructure.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们详细讨论扩展 Kubernetes 控制平面的细节，从本地客户端上的组件开始，到 Kubernetes 集群内部的组件。这些扩展点中许多与数据库和数据基础设施相关。
- en: Extending Kubernetes Clients
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 Kubernetes 客户端。
- en: The `kubectl` command-line tool is the primary interface for many users for
    interacting with Kubernetes. You can extend `kubectl` with [plug-ins](https://oreil.ly/kbh4u)
    that you download and make available on your system’s `PATH`, or use [Krew](https://krew.dev),
    a package manager that maintains a [list of `kubectl` plug-ins](https://oreil.ly/iw93T).
    Plug-ins perform tasks such as bulk actions across [multiple resources](https://oreil.ly/AllQX)
    or even [multiple clusters](https://oreil.ly/3Bsj2), or assessing the state of
    a cluster and making [security](https://oreil.ly/BWbpn) or [cost](https://oreil.ly/Exxjp)
    recommendations. More particularly to our focus in this chapter, several plug-ins
    are available to manage operators and custom resources.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl` 命令行工具是许多用户与 Kubernetes 交互的主要接口。您可以通过下载并将插件放置在系统的 `PATH` 中，或使用 [Krew](https://krew.dev)，一个维护
    [kubectl 插件列表](https://oreil.ly/iw93T) 的包管理器来扩展 `kubectl`。插件执行诸如跨 [多个资源](https://oreil.ly/AllQX)
    或甚至 [多个集群](https://oreil.ly/3Bsj2) 的批量操作，或评估集群的状态并做出 [安全性](https://oreil.ly/BWbpn)
    或 [成本](https://oreil.ly/Exxjp) 建议。在本章的特定焦点下，有几个插件可用于管理操作符和自定义资源。'
- en: Extending Kubernetes Control Plane Components
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 Kubernetes 控制平面组件
- en: 'The core of the Kubernetes control plane consists of several [control plane
    components](https://oreil.ly/ZnxfB) including the API server, scheduler, controller
    manager, Cloud Controller Manager, and etcd. While these components can be run
    on any node within a Kubernetes cluster, they are typically assigned to a dedicated
    node which does not run any user application Pods. The components are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面的核心包括几个 [控制平面组件](https://oreil.ly/ZnxfB)，包括 API 服务器、调度器、控制器管理器、云控制器管理器和
    etcd。虽然这些组件可以在 Kubernetes 集群中的任何节点上运行，但通常会分配给专用节点，该节点不运行任何用户应用 Pod。这些组件如下：
- en: API server
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: API 服务器
- en: 'This is the primary interface for external and internal clients of a Kubernetes
    cluster. It exposes RESTful interfaces via an HTTP API. The API server performs
    a coordination role, routing requests from clients to other components to implement
    imperative and declarative instructions. The API server supports two types of
    extensions: custom resources and API aggregation. CRDs allow you to add new types
    of resources and are managed through `kubectl` without further extension. API
    aggregation allows you to extend the Kubernetes API with additional REST endpoints,
    which the API server will delegate to a separate API server provided as a plug-in.
    Custom resources are the more commonly used extension mechanism and will be a
    major focus throughout the remainder of the book.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Kubernetes 集群的外部和内部客户端的主要接口。它通过 HTTP API 暴露 RESTful 接口。API 服务器扮演协调角色，将客户端的请求路由到其他组件以实现命令式和声明式指令。API
    服务器支持两种类型的扩展：自定义资源和 API 聚合。CRD 允许您添加新类型的资源，并通过 `kubectl` 进行管理，无需进一步扩展。API 聚合允许您通过额外的
    REST 端点扩展 Kubernetes API，API 服务器将这些请求委派给作为插件提供的独立 API 服务器。自定义资源是更常用的扩展机制，在本书的后续部分将是重点讨论的内容。
- en: Scheduler
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器
- en: This determines the assignment of Pods to Worker Nodes, considering factors
    including the load on each Worker Node, as well as affinity rules, taints, and
    tolerations (as discussed in [Chapter 4](ch04.html#automating_database_deployment_on_kuber)).
    The scheduler can be extended with plug-ins that override default behavior at
    multiple points in its decision-making process. For example, a *scheduling plug-in*
    could filter out nodes for a specific type of Pod or set the relative priority
    of nodes by assigning a score. *Binding plug-ins* can customize the logic that
    prepares a node for running a scheduled Pod, such as mounting a network volume
    the Pod needs. Data infrastructure such as Apache Spark that relies on running
    a lot of short-lived tasks may benefit from this ability to exercise more fine-grained
    control over scheduling decisions, as we’ll discuss in [“Alternative Schedulers
    for Kubernetes”](ch09.html#alternative_schedulers_for_kubernetes).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这决定了将 Pod 分配给工作节点的方式，考虑因素包括每个工作节点的负载，以及亲和性规则、污点和容忍性（如 [第 4 章](ch04.html#automating_database_deployment_on_kuber)
    中讨论的）。调度器可以通过插件进行扩展，在其决策过程的多个点上覆盖默认行为。例如，*调度插件* 可以过滤出特定类型的 Pod 的节点，或者通过分配分数设置节点的相对优先级。*绑定插件*
    可以自定义准备节点以运行调度 Pod 的逻辑，例如挂载调度 Pod 需要的网络卷。依赖于运行大量短期任务的 Apache Spark 等数据基础设施可能会从这种对调度决策更细粒度控制的能力中受益，我们将在
    [“Kubernetes 的替代调度器”](ch09.html#alternative_schedulers_for_kubernetes) 中讨论这一点。
- en: etcd
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: etcd
- en: This distributed key-value store is used by the API server to persist information
    about the cluster’s configuration and status. As resources are added, removed
    and updated, the API server updates the metadata in etcd accordingly, so that
    if the API server crashes or needs to be restarted, it can easily recover its
    state. As a strongly consistent data store that supports high availability, etcd
    is frequently used by other data infrastructure that runs on Kubernetes, as we’ll
    see frequently throughout the book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分布式键值存储由 API 服务器用于持久化集群配置和状态信息。当资源被添加、删除和更新时，API 服务器相应地更新 etcd 中的元数据，因此如果
    API 服务器崩溃或需要重新启动，它可以轻松恢复其状态。作为一个支持高可用性的强一致性数据存储，etcd 经常被其他运行在 Kubernetes 上的数据基础设施频繁使用，正如我们将在整本书中经常看到的那样。
- en: Controller manager and Cloud Controller Manager
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器管理器和云控制器管理器
- en: The controller manager and Cloud Controller Manager incorporate multiple control
    loops called *controllers*. These managers contain multiple logically separate
    controllers compiled into a single executable to simplify the ability of Kubernetes
    to manage itself. The controller manager includes controllers which manage built-in
    resource types such as Pods, StatefulSets, and more. The Cloud Controller Manager
    includes controllers that differ among Kubernetes providers to enable the management
    of platform-specific resources such as load balancers or VMs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器管理器和云控制器管理器集成了称为*控制器*的多个控制循环。这些管理器包含了多个逻辑上独立的控制器，编译成单个可执行文件，以简化 Kubernetes
    管理自身的能力。控制器管理器包含管理内置资源类型（如 Pod、StatefulSets 等）的控制器。云控制器管理器包含了不同于 Kubernetes 提供者的控制器，以便管理特定于平台的资源，例如负载均衡器或虚拟机（VMs）。
- en: Extending Kubernetes Worker Node Components
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 Kubernetes 工作节点组件
- en: 'Some elements of the Kubernetes control plane run on every node in the cluster.
    These [Worker Node components](https://oreil.ly/KmmkS) include the Kubelet, kube-proxy,
    and container runtime:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面的某些元素在集群中的每个节点上运行。这些[工作节点组件](https://oreil.ly/KmmkS)包括 Kubelet、kube-proxy
    和容器运行时：
- en: Kubelet
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet
- en: This manages the Pods running on a node assigned by the scheduler, including
    the containers that run within a Pod. The Kubelet restarts containers when needed,
    provides access to container logs, and more.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这管理由调度程序分配给节点的 Pod，包括运行在 Pod 内的容器。Kubelet 在需要时重新启动容器，提供对容器日志的访问等功能。
- en: Compute, network, and storage plug-ins
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 计算、网络和存储插件
- en: The Kubelet can be extended with plug-ins that take advantage of unique compute,
    networking, and storage capabilities provided by the underlying environment on
    which it is running. Compute plug-ins include container runtimes, and [device
    plug-ins](https://oreil.ly/cdqrT) that expose specialized hardware capabilities
    such as GPUs or field-programmable gate arrays (FPGA). [Network plug-ins](https://oreil.ly/aMdKH),
    including those that comply with the Container Network Interface (CNI), can provide
    features beyond Kubernetes built-in networking, such as bandwidth management or
    network policy management. We’ve previously discussed storage plug-ins in [“Kubernetes
    Storage Architecture”](ch02.html#kubernetes_storage_architecture), including those
    that conform to the CSI.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet 可以通过插件进行扩展，利用底层环境提供的独特计算、网络和存储能力。计算插件包括容器运行时，以及[设备插件](https://oreil.ly/cdqrT)，这些插件暴露了专用硬件功能，如
    GPU 或可编程门阵列（FPGA）。[网络插件](https://oreil.ly/aMdKH)，包括符合容器网络接口（CNI）的插件，可以提供超出 Kubernetes
    内置网络的功能，如带宽管理或网络策略管理。我们之前在[“Kubernetes 存储架构”](ch02.html#kubernetes_storage_architecture)中讨论了存储插件，包括符合
    CSI 的插件。
- en: Kube-proxy
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Kube-proxy
- en: This maintains network routing for the Pods running on a Worker Node so that
    they can communicate with other Pods running inside your Kubernetes cluster, or
    clients and services running outside of the cluster. Kube-proxy is part of the
    implementation of Kubernetes Services, providing the mapping of virtual IPs to
    individual Pods on a Worker Node.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这维护运行在工作节点上的 Pod 的网络路由，以便它们可以与集群内运行的其他 Pod、或者集群外的客户端和服务进行通信。Kube-proxy 是 Kubernetes
    服务实现的一部分，提供将虚拟 IP 映射到工作节点上单个 Pod 的功能。
- en: Container runtime
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时
- en: The Kubelet uses the container runtime to execute containers on the worker’s
    operating system. Supported container runtimes for Linux include [containerd](https://oreil.ly/6ylhH)
    and [CRI-O](https://oreil.ly/8fk2x). [Docker](https://oreil.ly/col9R) runtime
    support was deprecated in Kubernetes 1.20 and removed entirely in 1.24.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet 使用容器运行时在工作节点的操作系统上执行容器。Linux 支持的容器运行时包括[containerd](https://oreil.ly/6ylhH)
    和 [CRI-O](https://oreil.ly/8fk2x)。在 Kubernetes 1.20 中，[Docker](https://oreil.ly/col9R)
    运行时支持已被弃用，并在 1.24 中完全移除。
- en: Custom controllers and operators
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义控制器和操作者
- en: These controllers are responsible for managing applications installed on a Kubernetes
    cluster using custom resources. Although these controllers are extensions to the
    Kubernetes control plane, they can run on any Worker Node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些控制器负责使用自定义资源管理安装在 Kubernetes 集群上的应用程序。尽管这些控制器是 Kubernetes 控制平面的扩展，它们可以运行在任何工作节点上。
- en: The Operator Pattern
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作者模式
- en: 'With this context, we’re ready to examine one of the most common patterns for
    extending Kubernetes: the *operator pattern*. This pattern combines custom resources
    with controllers that operate on those resources. Let’s examine each of these
    concepts in more detail to see how they apply to data infrastructure, and then
    you’ll be ready to dig into an example operator for MySQL.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，我们现在可以研究 Kubernetes 中最常见的扩展模式之一：*操作者模式*。这种模式结合了自定义资源和对这些资源进行操作的控制器。让我们更详细地看看每个概念如何适用于数据基础设施，然后您将准备好深入研究一个
    MySQL 操作者的示例。
- en: Controllers
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制器
- en: The concept of a controller originates from the domain of electronics and electrical
    engineering, in which a controller is a device that operates in a continuous loop.
    On each iteration through the loop, the device receives an input signal, compares
    that with a set point value, and generates an output signal intended to produce
    a change in the environment that can be detected in future inputs. A simple example
    is a thermostat, which powers up your air conditioner or heater when the temperature
    in a space is too high or low.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器的概念源自电子和电气工程领域，其中控制器是在连续循环中运行的设备。在每次循环迭代中，设备接收一个输入信号，将其与设定值进行比较，并生成一个输出信号，旨在产生环境变化，未来的输入信号可以检测到这种变化。一个简单的例子是恒温器，当空间内的温度过高或过低时，它会启动空调或加热器。
- en: 'A *Kubernetes controller* implements a similar [control loop](https://oreil.ly/LKefh),
    consisting of the following steps:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kubernetes 控制器* 实现了类似的[控制循环](https://oreil.ly/LKefh)，包括以下步骤：'
- en: Reading the current state of resources
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取资源的当前状态
- en: Making changes to the state of resources
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改资源的状态
- en: Updating the status of resources
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新资源的状态
- en: Repeat
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复
- en: 'These steps are embodied both by Kubernetes built-in controllers that run in
    the controller manager and Cloud Controller Manager, as well as *custom controllers*
    that are provided to run applications on top of Kubernetes. Let’s look at some
    examples of what these steps might entail for controllers that manage data infrastructure:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤既由运行在控制器管理器中的 Kubernetes 内置控制器和云控制器管理器实现，也由提供给 Kubernetes 上运行应用程序的*自定义控制器*
    实现。让我们看一些管理数据基础设施的控制器可能涉及的示例：
- en: Reading the current state of resources
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 读取资源的当前状态
- en: A controller tracks the state of one or more resource types, including built-in
    resources like Pods, PersistentVolumes, and Services, as well as custom resources
    (which we discuss in the next section). Controllers are driven asynchronously
    by notification from the API server. The API server sends [*watch events*](https://oreil.ly/UvOJY)
    to controllers to notify them of changes in state for resource types for which
    they have registered interest, such as the creation or deletion of a resource,
    or an event occurring on the resource.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一个控制器跟踪一个或多个资源类型的状态，包括内置资源如 Pods、PersistentVolumes 和 Services，以及自定义资源（我们将在下一节中讨论）。控制器通过
    API 服务器发送的通知驱动异步工作。API 服务器向控制器发送[*观察事件*](https://oreil.ly/UvOJY)，以通知它们资源状态的变化，例如资源的创建或删除，或者资源上发生的事件。
- en: For data infrastructure, these changes could include a change in the number
    of requested replicas for a cluster, or a notification that a Pod containing a
    database replica has died. Because many such updates could be occurring in a large
    cluster, controllers frequently use caching.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据基础设施，这些变更可能包括集群请求副本数量的变更，或者包含数据库副本的 Pod 已经死亡的通知。由于在大型集群中可能会发生许多此类更新，控制器经常使用缓存。
- en: Making changes to the state of resources
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 修改资源状态
- en: This is the core business logic of a controller—comparing the state of resources
    to their desired state and executing actions to change the state to the desired
    state. In the Kubernetes API, the current state is captured in `.status` fields
    of resources, and the desired state is expressed in terms of the `.spec` field.
    The changes could include invocations of the Kubernetes API to modify other resources,
    administrative actions on the application being managed, or even interactions
    outside of the Kubernetes cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这是控制器的核心业务逻辑——比较资源的状态与其期望状态，并执行操作将状态更改为期望状态。在 Kubernetes API 中，当前状态以资源的`.status`字段表示，期望状态则以`.spec`字段表示。更改可能包括调用
    Kubernetes API 以修改其他资源，对被管理应用程序的管理操作，甚至与 Kubernetes 集群外的交互。
- en: For example, consider a controller managing a distributed database with multiple
    replicas. When the database controller receives a notification that the desired
    number of replicas has increased, the controller could scale an underlying Deployment
    or StatefulSet that it is using to manage replicas. Later, when receiving a notification
    that a Pod has been created to host a new replica, the controller could initiate
    an action on one or more replicas in order to rebalance the workload across those
    replicas.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个管理具有多个副本的分布式数据库的控制器。当数据库控制器收到副本数量增加的通知时，控制器可以扩展正在使用的 Deployment 或 StatefulSet
    以管理副本。随后，当收到关于创建用于托管新副本的 Pod 的通知时，控制器可以在一个或多个副本上启动操作，以重新平衡这些副本之间的工作负载。
- en: Updating the status of resources
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 更新资源的状态
- en: In the final step of the control loop, the controller updates the `.status`
    fields of the resource using the API server, which in turn updates that state
    in etcd. You’ve viewed the status of resources like Pods and PersistentVolumes
    in previous chapters using the `kubectl get` and `kubectl describe` commands.
    For example, the status of a Pod includes its overall state (`Pending`, `Running`,
    `Succeeded`, `Failed`, etc.), the most recent time at which various conditions
    were noted (`PodScheduled`, `ContainersReady`, `Initialized`, `Ready`), as well
    as the state of each of its containers (`Waiting`, `Running`, `Terminated`). Custom
    resources can define their own status fields as well. For example, a custom resource
    representing a cluster might have status values reflecting the overall availability
    of the cluster and its current topology.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在控制循环的最后一步中，控制器使用 API 服务器更新资源的`.status`字段，进而在 etcd 中更新该状态。在之前的章节中，您使用 `kubectl
    get` 和 `kubectl describe` 命令查看过诸如 Pods 和 PersistentVolumes 的状态。例如，Pod 的状态包括其总体状态（`Pending`、`Running`、`Succeeded`、`Failed`等）、最近记录各种条件的时间（`PodScheduled`、`ContainersReady`、`Initialized`、`Ready`）以及其每个容器的状态（`Waiting`、`Running`、`Terminated`）。自定义资源也可以定义自己的状态字段。例如，表示集群的自定义资源可能具有反映集群整体可用性及其当前拓扑的状态值。
- en: Events
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 事件
- en: A controller can also produce *events* via the Kubernetes API for consumption
    by human operators or other applications. These are distinct from the watcher
    events described previously that the Kubernetes API uses to notify controllers
    of changes, which are not exposed to other clients.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器还可以通过 Kubernetes API 生成*事件*，供人类操作员或其他应用程序消费。这些事件与之前描述的监视器事件不同，Kubernetes
    API 使用这些事件来通知控制器进行更改，但不会暴露给其他客户端。
- en: Writing a Custom Controller
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写自定义控制器
- en: While you may not ever need to write your own controller, being familiar with
    the concepts involved is helpful. [*Programming Kubernetes*](https://oreil.ly/Ad4Ga)
    is a great resource for those interested in digging deeper.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可能永远不需要编写自己的控制器，但熟悉涉及的概念是有帮助的。[*编程 Kubernetes*](https://oreil.ly/Ad4Ga) 对于那些有兴趣深入了解的人来说是一个很好的资源。
- en: The [controller-runtime project](https://oreil.ly/VjP9w) provides a common set
    of libraries to help aid the process of writing controllers, including registering
    for notifications from the API server, caching resource status, implementing reconciliation
    loops, and more. Controller-runtime libraries are implemented in the Go programming
    language, so it’s no surprise that most controllers are implemented in Go.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '[controller-runtime 项目](https://oreil.ly/VjP9w) 提供了一组通用的库，帮助编写控制器，包括从 API 服务器注册通知、缓存资源状态、实现协调循环等。Controller-runtime
    库是用 Go 编程语言实现的，因此大多数控制器也是用 Go 实现的，这一点并不令人惊讶。'
- en: '[Go](https://go.dev) was first developed at Google in 2007 and used in many
    cloud native applications including Borg, the predecessor to Kubernetes, and then
    in Kubernetes itself. Go is a strongly typed, compiled language (as opposed to
    interpreted languages like Java and JavaScript) with a high value on usability
    and developer productivity (in reaction to the higher learning curve of C/C++).'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[Go](https://go.dev) 最初于 2007 年在 Google 开发，并被用于包括 Borg（Kubernetes 的前身）在内的许多云原生应用中。Go
    是一种强类型的编译语言（与解释性语言如 Java 和 JavaScript 相对），注重可用性和开发者生产力（作为对 C/C++ 学习曲线较高的反应）。'
- en: If you’ve ever misconfigured a Pod specification and observed a `CrashLoopBackOff`
    status, you may have encountered events. Using the `kubectl describe pod` command,
    you can observe events such as a container being started and failing, followed
    by a backoff period, followed by the container restarting. Events expire from
    the API server in an hour, but common Kubernetes monitoring tools provide capabilities
    to track them. Controllers can also create events for custom resources.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经错误配置过 Pod 规范并观察到 `CrashLoopBackOff` 状态，你可能已经遇到了事件。使用 `kubectl describe
    pod` 命令，你可以观察到事件，例如容器启动失败，随后是回退周期，然后容器重新启动。事件在 API 服务器中的有效期为一小时，但常见的 Kubernetes
    监控工具提供了跟踪它们的功能。控制器还可以为自定义资源创建事件。
- en: Custom Resources
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义资源
- en: As we’ve discussed, controllers can operate on built-in Kubernetes resources
    as well as [custom resources](https://oreil.ly/62uQj). We’ve briefly mentioned
    this concept, but let’s take this opportunity to define what custom resources
    are and how they extend the Kubernetes API.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器可以操作内置的 Kubernetes 资源以及[自定义资源](https://oreil.ly/62uQj)。我们已经简要提到这个概念，但现在让我们利用这个机会定义一下什么是自定义资源以及它们如何扩展
    Kubernetes API。
- en: 'Fundamentally, a *custom resource* is a piece of configuration data that Kubernetes
    recognizes as part of its API. While a custom resource is similar to a ConfigMap,
    it has a structure similar to built-in resources: metadata, specification, and
    status. The specific attributes of a particular custom resource type are defined
    in a CRD. A CRD is itself a Kubernetes resource that is used to describe a custom
    resource.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，*自定义资源*是 Kubernetes 认可为其 API 的一部分的配置数据。虽然自定义资源类似于 ConfigMap，但其结构类似于内置资源：元数据、规范和状态。特定自定义资源类型的属性在
    CRD 中定义。CRD 本身是一个用于描述自定义资源的 Kubernetes 资源。
- en: In this book, we’ve been discussing how Kubernetes enables you to move beyond
    managing VMs and containers to managing virtual datacenters. CRDs provide the
    flexibility that helps make this a practical reality. Instead of being limited
    to the resources that Kubernetes provides off the shelf, you can create additional
    abstractions to extend Kubernetes for your own purposes. This is a critical component
    in a fast-moving ecosystem.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经讨论了 Kubernetes 如何使您能够超越管理 VM 和容器，进而管理虚拟数据中心。CRD 提供了灵活性，有助于实现这一实际需求。您不再仅限于
    Kubernetes 提供的现成资源，可以创建额外的抽象来扩展 Kubernetes 以满足您自己的需求。这是快速发展生态系统中的关键组成部分。
- en: 'Let’s see what you can learn about CRDs from the command line. Use `kubectl
    api-resources` to get a listing of all of the resources defined in your cluster:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从命令行了解一下 CRD 的相关知识。使用 `kubectl api-resources` 命令列出集群中定义的所有资源：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As you look through the output, you’ll see many resource types introduced in
    previous chapters, along with their short names: StorageClass (`sc`), PersistentVolumes
    (`pv`), Pods (`po`), StatefulSets (`sts`), and so on. The API versions provide
    some clues as to the origins of each resource type. For example, resources with
    version `v1` are core Kubernetes resources. Other versions such as `apps/v1`,
    `networking.k8s.io/v1`, or `storage.k8s.io/v1` indicate resources that are defined
    by various Kubernetes SIGs.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当你查看输出时，你会看到在前几章介绍过的许多资源类型及其简称：StorageClass (`sc`)、PersistentVolumes (`pv`)、Pods
    (`po`)、StatefulSets (`sts`) 等等。API 版本提供了每种资源类型来源的一些线索。例如，版本为 `v1` 的资源是核心 Kubernetes
    资源。其他版本如 `apps/v1`、`networking.k8s.io/v1` 或 `storage.k8s.io/v1` 表示由不同的 Kubernetes
    SIGs 定义的资源。
- en: Depending on the configuration of the Kubernetes cluster you are using, you
    may have some CRDs defined already. If any are present, they will appear in the
    output of the `kubectl api-resources` command. They’ll stand out by their API
    version, which will typically include a path other than `k8s.io`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的 Kubernetes 集群的配置，可能已经定义了一些 CRD。如果有任何 CRD 存在，它们将出现在 `kubectl api-resources`
    命令的输出中。它们将通过其 API 版本显著突出，通常包含除 `k8s.io` 之外的路径。
- en: 'Since a CRD is itself a Kubernetes resource, you can also use the command `kubectl
    get crd` to list custom resources installed in your Kubernetes cluster. For example,
    after installing the Vitess Operator referenced in the following section, you
    would see several CRDs:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CRD 本身是 Kubernetes 资源，您也可以使用命令`kubectl get crd`列出安装在 Kubernetes 集群中的自定义资源。例如，在安装了下一节中提到的
    Vitess Operator 后，您会看到几个 CRD：
- en: '[PRE1]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We’ll introduce the usage of these custom resources later, but for now let’s
    focus on the mechanics of a specific CRD to see how it extends Kubernetes. You
    use the `kubectl describe crd` or `kubectl get crd` commands to see the definition
    of a CRD. For example, to get a YAML-formatted description for the `vitesskeyspace`
    custom resource, you could run this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的其余部分介绍这些自定义资源的用法，但现在让我们专注于特定 CRD 的机制，看看它如何扩展 Kubernetes。您可以使用`kubectl
    describe crd`或`kubectl get crd`命令查看 CRD 的定义。例如，要获取`vitesskeyspace`自定义资源的 YAML
    格式描述，您可以运行以下命令：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Looking at the [original YAML configuration](https://oreil.ly/5ml3q) for this
    CRD, you’ll see something like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个 CRD 的[原始 YAML 配置](https://oreil.ly/5ml3q)，您会看到类似于这样的内容：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: From this part of the definition, you can see the declaration of the custom
    resource’s name or kind and `shortName`. The `scope` designation of `Namespaced`
    means that custom resources of this type are confined to a single Namespace.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从这部分定义中，您可以看到自定义资源的名称或种类以及`shortName`的声明。`scope`指定为`Namespaced`意味着此类型的自定义资源限定在单个命名空间中。
- en: The longest part of the definition is the `validation` section, which we’ve
    omitted due to its considerable size. Kubernetes supports the definition of attributes
    within custom resource types, and the ability to define legal values for these
    types using the [OpenAPI v3 schema](https://oreil.ly/b13qP) (which is used to
    document RESTful APIs, which in turn uses [JSON schema](http://json-schema.org)
    to describe rules used to validate JSON objects). Validation rules ensure that
    when you create or update custom resources, the definitions of the objects are
    valid and can be understood by the Kubernetes control plane. The validation rules
    are used to generate the documentation you use as you define instances of these
    custom resources in your application.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 定义中最长的部分是`validation`部分，由于其相当大，我们已经省略了。Kubernetes 支持在自定义资源类型中定义属性，并使用 [OpenAPI
    v3 schema](https://oreil.ly/b13qP)（用于记录 RESTful API，进而使用 [JSON schema](http://json-schema.org)
    描述用于验证 JSON 对象的规则）来定义这些类型的合法值。验证规则确保当您创建或更新自定义资源时，对象的定义是有效的，并且可以被 Kubernetes 控制平面理解。验证规则用于生成您在应用程序中定义这些自定义资源实例时使用的文档。
- en: Once a CRD has been installed in your Kubernetes cluster, you can create and
    interact with the resources using `kubectl`. For example, `kubectl get vitesskeyspaces`
    will return a list of Vitess keyspaces. You create an instance of a Vitess keyspace
    by providing a compliant YAML definition to the `kubectl apply` command.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 CRD 安装在您的 Kubernetes 集群中，您可以使用`kubectl`创建和与资源交互。例如，`kubectl get vitesskeyspaces`将返回一个
    Vitess keyspace 列表。您可以通过向`kubectl apply`命令提供符合规范的 YAML 定义来创建 Vitess keyspace 的实例。
- en: Operators
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作员
- en: Now that you’ve learned about custom controllers and custom resources, let’s
    tie these threads back together. An [*operator*](https://oreil.ly/BXc35) is a
    combination of custom resources and custom controllers that maintain the state
    of those resources and manage an application (or *operand)* in Kubernetes.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了自定义控制器和自定义资源，让我们将这些线索重新联系在一起。[*操作员*](https://oreil.ly/BXc35)是自定义资源和自定义控制器的组合，用于维护这些资源的状态并管理
    Kubernetes 中的应用程序（或*操作数）。
- en: As we’ll see in examples throughout the rest of the book, this simple definition
    can cover a pretty wide range of implementations. The recommended pattern is to
    provide a custom controller for each custom resource, but beyond that, the details
    vary. A simple operator might consist of a single resource and controller, while
    a more complex operator might have multiple resources and controllers. Those multiple
    controllers might run in the same process space or be broken into separate Pods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本书的其余部分中看到的示例一样，这个简单的定义可以涵盖相当广泛的实现。推荐的模式是为每个自定义资源提供一个自定义控制器，但除此之外，细节会有所不同。一个简单的操作员可能由单个资源和控制器组成，而一个更复杂的操作员可能有多个资源和控制器。这些多个控制器可能在同一进程空间中运行，也可能被分成单独的
    Pod。
- en: Controllers Versus Operators
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 控制器与操作员
- en: While technically operators and controllers are distinct concepts in Kubernetes,
    the terms are frequently used interchangeably. It’s common to refer to a deployed
    controller or collection of controllers as “the operator,” and you’ll see this
    usage reflected both in this book and the community in general.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在技术上来说，Kubernetes 中的运算符和控制器是不同的概念，但这两个术语经常被互换使用。习惯上会将部署的控制器或一组控制器称为“运算符”，在本书和社区中经常可以看到这种用法。
- en: To unpack this pattern and see how the different elements of an operator and
    the Kubernetes control plane work together, let’s consider the interactions of
    a notional operator, the DBCluster operator, as shown in [Figure 5-2](#interaction_between_kubernetes_controll).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解开这种模式并查看运算符的不同元素以及 Kubernetes 控制平面如何协同工作，请考虑一个概念上的运算符，即 DBCluster 运算符的交互，如[图 5-2](#interaction_between_kubernetes_controll)所示。
- en: After an administrator installs the DBCluster operator and `db-cluster` custom
    resource in the cluster, users can then create instances of the `db-cluster` resource
    using `kubectl` (1), which registers the resource with the API server (2), which
    in turns stores the state in etcd (3) to ensure high availability (other interactions
    with etcd are omitted from this sequence for brevity).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理员安装了 DBCluster 运算符和集群中的 `db-cluster` 自定义资源后，用户可以使用 `kubectl` 创建 `db-cluster`
    资源的实例（1），该操作会将资源注册到 API 服务器（2），后者将状态存储在 etcd 中（3），以确保高可用性（其他与 etcd 的交互由于篇幅问题未列出）。
- en: '![Interaction between Kubernetes controllers and operators](assets/mcdk_0502.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![Kubernetes 控制器与运算符之间的交互](assets/mcdk_0502.png)'
- en: Figure 5-2\. Interaction between Kubernetes controllers and operators
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. Kubernetes 控制器与运算符之间的交互
- en: The DBCluster controller (part of the operator) is notified of the new `db-cluster`
    resource (4) and creates additional Kubernetes resources using the API server
    (5), which could include StatefulSets, Services, PersistentVolumes, PersistentVolumeClaims,
    and more, as we’ve seen in previous examples of deploying databases on Kubernetes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: DBCluster 控制器（作为运算符的一部分）会被通知新的 `db-cluster` 资源（4）的创建，并使用 API 服务器（5）创建额外的 Kubernetes
    资源，这可能包括 StatefulSets、Services、PersistentVolumes、PersistentVolumeClaims 等，正如我们在之前的数据库部署示例中所见。
- en: Focusing on the StatefulSet path, the StatefulSet controller running as part
    of the Kubernetes controller manager is notified of a new StatefulSet (6) and
    creates new Pod resources (7). The API server asks the scheduler to assign each
    Pod to a Worker Node (8) and communicates with the Kubelet on the chosen Worker
    Nodes (9) to start each of the required Pods (10).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关注 StatefulSet 路径，作为 Kubernetes 控制器管理器的一部分运行的 StatefulSet 控制器会被通知新的 StatefulSet（6）的创建，并创建新的
    Pod 资源（7）。API 服务器请求调度程序将每个 Pod 分配给工作节点（8），并与所选工作节点上的 Kubelet（9）通信以启动每个所需的 Pod（10）。
- en: 'As you see, creating a `db-cluster` resource sets off a chain of interactions
    as various controllers are notified of changes to Kubernetes resources and initiate
    changes to bring the state of the cluster in line with the desired state. The
    sequence of interactions appears complex from a user perspective, but the design
    demonstrates strong encapsulation: the responsibilities of each controller are
    well bounded and independent of other controllers. This separation of concerns
    is what makes the Kubernetes control plane so extensible.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，创建 `db-cluster` 资源会触发一系列交互，因为各种控制器被通知 Kubernetes 资源的变化，并启动变更以使集群的状态符合期望的状态。从用户的角度来看，这些交互序列看起来复杂，但设计展示了强大的封装性：每个控制器的责任范围都明确定界，独立于其他控制器。这种关注点的分离是使
    Kubernetes 控制平面如此可扩展的关键。
- en: Managing MySQL in Kubernetes Using the Vitess Operator
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Vitess 运算符在 Kubernetes 中管理 MySQL
- en: 'Now that you understand how operators, custom controllers, and custom resources
    work, it’s time to get some hands-on experience with an operator for the database
    we’ve been using as our primary relational database example: MySQL. MySQL examples
    in previous chapters were confined to simple deployments of a single primary replica
    and a couple of secondary replicas. While this could provide a sufficient amount
    of storage for many cloud applications, managing a larger cluster can quickly
    become quite complex, whether it runs on bare-metal servers or as a containerized
    application in Kubernetes.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了操作员、自定义控制器和自定义资源的工作原理，是时候亲自体验一下我们一直作为主要关系数据库示例使用的数据库操作员了：MySQL。在前几章中，MySQL
    的示例仅限于简单部署一个主副本和几个次要副本。虽然这可以为许多云应用程序提供足够的存储空间，但管理更大的集群可能会迅速变得非常复杂，无论它运行在裸机服务器上还是作为
    Kubernetes 中的容器化应用程序。
- en: Vitess Overview
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vitess 概述
- en: '[*Vitess*](https://oreil.ly/7I0vO) is an open source project started at YouTube
    in 2010\. Before the company was acquired by Google, YouTube was running on MySQL,
    and as YouTube scaled up, it reached a point of daily outages. Vitess was created
    as a layer to abstract application access to databases by making multiple instances
    appear to be a single database, routing application requests to the appropriate
    instances using a sharding approach. Before we explore deploying Vitess on Kubernetes,
    let’s take some time to explore its architecture. We’ll start with the high-level
    concepts shown in [Figure 5-3](#vitess_cluster_topology_cellscomma_keys): cells,
    keyspaces, shards, and primary and replica tablets.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[*Vitess*](https://oreil.ly/7I0vO) 是一个自 2010 年起在 YouTube 开始的开源项目。在该公司被 Google
    收购之前，YouTube 运行在 MySQL 上，并且随着 YouTube 的扩展，每天都会出现故障。Vitess 被创建为一个层，通过使用分片方法，将应用程序对数据库的访问抽象化为多个实例看起来像是单个数据库，将应用程序请求路由到适当的实例。在我们探索在
    Kubernetes 上部署 Vitess 之前，让我们花些时间了解其架构。我们将从图 5-3 中显示的高层概念开始：单元、键空间、分片以及主和副本表格。'
- en: '![Vitess cluster topology: cells, keyspaces, and shards](assets/mcdk_0503.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![Vitess 集群拓扑结构：单元、键空间和分片](assets/mcdk_0503.png)'
- en: 'Figure 5-3\. Vitess cluster topology: cells, keyspaces, and shards'
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. Vitess 集群拓扑结构：单元、键空间和分片
- en: 'At a high level, a Vitess cluster consists of multiple MySQL instances called
    *tablets* which may be spread across multiple datacenters, or *cells*. Each MySQL
    instance takes on a role as either a primary or replica, and may be dedicated
    to a specific slice of a database known as a *shard*. Let’s consider the implications
    of each of these concepts for reading and writing data in Vitess:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，Vitess 集群由称为*表格*的多个 MySQL 实例组成，可以分布在多个数据中心或*单元*中。每个 MySQL 实例承担主或副本的角色，并可能专用于称为*分片*的数据库特定片段。让我们考虑每个概念对在
    Vitess 中读写数据的影响：
- en: Cell
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 单元
- en: A typical production deployment of Vitess is spread across multiple failure
    domains in order to provide high availability. Vitess refers to each of these
    failure domains as a [*cell*](https://oreil.ly/2VDke). The recommended topology
    is a cell per datacenter or cloud provider zone. While writes and replication
    involve communication across cell boundaries, Vitess reads are confined to the
    local cell to optimize performance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess 的典型生产部署跨多个故障域，以提供高可用性。Vitess 将这些故障域称为[*单元*](https://oreil.ly/2VDke)。推荐的拓扑结构是每个数据中心或云提供商区域一个单元。虽然写入和复制涉及跨单元边界的通信，但
    Vitess 的读取限制在本地单元以优化性能。
- en: Keyspace
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 键空间
- en: This is a logical database consisting of one or more tables. Each keyspace in
    a cluster can be *sharded* or *unsharded*. An unsharded keyspace has a primary
    cell where a MySQL instance designated as the *primary* will reside, while other
    cells will contain *replicas*. In the unsharded keyspace shown on the left side
    of [Figure 5-3](#vitess_cluster_topology_cellscomma_keys), writes from client
    applications are routed to the primary and replicated to the replica nodes in
    the background. Reads can be served from the primary or replica nodes.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个逻辑数据库，由一个或多个表组成。集群中的每个键空间可以是*分片*或*非分片*的。非分片的键空间有一个主单元，其中一个被指定为*主数据库*的 MySQL
    实例将驻留在那里，而其他单元将包含*副本*。在左侧显示的非分片键空间中的 [图 5-3](#vitess_cluster_topology_cellscomma_keys)，来自客户端应用程序的写入操作将被路由到主节点，并在后台复制到副本节点。读取可以从主节点或副本节点提供服务。
- en: Shard
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 分片
- en: The real power of Vitess comes from its ability to scale by spreading the contents
    of a keyspace across multiple replicated MySQL databases known as *shards*, while
    providing the abstraction of a single database to client applications. The client
    on the right side of [Figure 5-3](#vitess_cluster_topology_cellscomma_keys) is
    not aware of how data is sharded. On writes, Vitess determines what shards are
    involved and then routes the data to the appropriate primary instances. On reads,
    Vitess gathers data from primary or replica nodes in the local cell.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess真正的强大之处在于其通过将keyspace的内容分布在多个复制的MySQL数据库（称为*shards*）上来实现扩展能力，同时为客户端应用程序提供单个数据库的抽象。右侧图中的客户端不知道数据如何分片。在写入时，Vitess确定涉及哪些分片，然后将数据路由到适当的主实例。在读取时，Vitess从本地单元的主节点或副本节点中收集数据。
- en: The sharding rules for a keyspace are specified in a [Vitess Schema (VSchema)](https://oreil.ly/wDmQa),
    an object that contains the sharding key (known in Vitess as the *keyspace ID*)
    used for each table. To provide maximum flexibility over the way data is sharded,
    Vitess allows you to specify which columns in a table are used to calculate the
    keyspace ID, as well as the algorithm (or *VIndex*) used to make the calculation.
    Tables can also have secondary VIndexes to support more-efficient queries across
    multiple keyspace IDs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: keyspace的分片规则在[Vitess Schema（VSchema）](https://oreil.ly/wDmQa)中指定，这是一个包含每个表所用的分片键（在Vitess中称为*keyspace
    ID*）的对象。为了在数据分片方式上提供最大的灵活性，Vitess允许您指定表中用于计算keyspace ID的列，以及用于进行计算的算法（或*VIndex*）。表还可以具有辅助VIndex，以支持跨多个keyspace
    ID的更高效查询。
- en: To understand how Vitess manages shards and how it routes queries to the various
    MySQL instances, you’ll want to get to know the components of a Vitess cluster
    shown in [Figure 5-4](#vitess_architecture_including_vtgatecom), including VTGate,
    VTTablet, and the Topology Service.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解Vitess如何管理分片以及如何将查询路由到各个MySQL实例，您需要了解图5-4（#vitess_architecture_including_vtgatecom）中显示的Vitess集群组件，包括VTGate、VTTablet和拓扑服务。
- en: '![Vitess architecture including VTGate, VTTablets, Topology Service](assets/mcdk_0504.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![Vitess架构包括VTGate、VTTablets、拓扑服务](assets/mcdk_0504.png)'
- en: Figure 5-4\. Vitess architecture including VTGate, VTTablets, and the Topology
    Service
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-4\. Vitess架构包括VTGate、VTTablets和拓扑服务
- en: 'Let’s walk through these components to learn what they do and how they interact:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步了解这些组件，了解它们的作用及其如何交互：
- en: VTGate
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: VTGate
- en: A Vitess gateway (VTGate) is a proxy server that provides the SQL binary endpoint
    used by client applications, making the Vitess cluster appear as a single database.
    Vitess clients generally connect to a VTGate running in the same cell (datacenter).
    The VTGate parses each incoming read or write query and uses its knowledge of
    the VSchema and cluster topology to create a query execution plan. The VTGate
    executes queries for each shard, assembles the result set, and returns it to the
    client. The VTGate can detect and limit queries that will impact memory or CPU
    utilization, providing high reliability and helping to ensure consistent performance.
    Although VTGate instances do cache cluster metadata, they are stateless, so you
    can increase the reliability and scalability of your cluster by running multiple
    VTGate instances per cell.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess网关（VTGate）是一个代理服务器，为客户端应用程序提供SQL二进制端点，使Vitess集群看起来像是一个单一的数据库。通常，Vitess客户端连接到运行在同一单元（数据中心）中的VTGate。VTGate解析每个传入的读取或写入查询，并利用其对VSchema和集群拓扑的了解创建查询执行计划。VTGate为每个分片执行查询，组装结果集并将其返回给客户端。VTGate可以检测和限制可能影响内存或CPU利用率的查询，提供高可靠性并帮助确保一致的性能。尽管VTGate实例会缓存集群元数据，但它们是无状态的，因此通过在每个单元中运行多个VTGate实例可以提高集群的可靠性和可扩展性。
- en: VTTablet
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: VTTablet
- en: 'A Vitess tablet (VTTablet) is an agent that runs on the same compute instance
    as a single MySQL database, managing access to it and monitoring its health. Each
    VTTablet takes on a specific role in the cluster, such as the primary for a shard,
    or one of its replicas. There are two types of replica: those that can be promoted
    to replace a primary and those that cannot. The latter are typically used to provide
    additional capacity for read-intensive use cases such as analytics. The VTTablet
    exposes a gRPC interface, which the VTGate uses to send queries and control commands
    that the VTTablet then turns into SQL commands on the MySQL instance. VTTablets
    maintain a pool of long-lived connections to the MySQL node, leading to improved
    throughput, reduced latency, and reduced memory pressure.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess 表t（VTTablet）是一个在与单个 MySQL 数据库相同计算实例上运行的代理，负责管理其访问和监控其健康情况。每个 VTTablet
    承担集群中特定角色，如分片的主要节点或其副本之一。有两种类型的副本：可晋升以替换主要节点的副本和不能晋升的副本。后者通常用于提供额外容量，支持读密集型用例，如分析。VTTablet
    提供 gRPC 接口，VTGate 使用该接口发送查询和控制命令，VTTablet 将其转换为 MySQL 实例上的 SQL 命令。VTTablet 维护到
    MySQL 节点的长连接池，从而提高吞吐量、减少延迟和减少内存压力。
- en: Topology Service
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Topology 服务
- en: 'Vitess requires a strongly consistent data store to maintain a small amount
    of metadata describing the cluster topology, including the definition of keyspaces
    and their VSchema, what VTTablets exist for each shard, and which VTTablet is
    the primary. Vitess uses a pluggable interface called the Topology Service, with
    three implementations provided by the project: etcd (the default), ZooKeeper,
    and Consul. VTGates and VTTablets interface with the Topology Service in the background
    in order to maintain awareness of the topology, and do not interact with the Topology
    Service on the query path to avoid performance impact. For multicell clusters,
    Vitess incorporates both cell-local Topology Services and a global Topology Service
    with instances in multiple cells that maintains knowledge of the entire cluster.
    This design provides high availability of topology information across the cluster.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess 需要一个强一致的数据存储来维护描述集群拓扑的少量元数据，包括 keyspace 的定义及其 VSchema、每个分片存在的 VTTablets，以及哪个
    VTTablet 是主要节点。Vitess 使用名为 Topology Service 的可插拔接口，项目提供了三种实现：etcd（默认）、ZooKeeper
    和 Consul。VTGate 和 VTTablet 在后台与 Topology Service 接口，以维护对拓扑的感知，并避免在查询路径上与 Topology
    Service 交互以避免性能影响。对于多单元格集群，Vitess 结合了单元格本地的 Topology Service 和一个全局的 Topology Service，后者在多个单元格中维护整个集群的知识，以提供拓扑信息的高可用性。
- en: '`vtctld` and `vtctlclient`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`vtctld` 和 `vtctlclient`'
- en: 'The Vitess control daemon `vtctld` and its client, `vtctlclient`, provide the
    control plane used to configure and manage Vitess clusters. `vtctld` is deployed
    on one or more of the cells in the cluster, while `vtctlclient` is deployed on
    the client machine of the user administering the cluster. `vtctld` uses a declarative
    approach similar to Kubernetes to perform its work: it updates the cluster metadata
    in the Topology Service, and the VTGates and VTTablets pick up changes and respond
    accordingly.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess 控制守护程序 `vtctld` 及其客户端 `vtctlclient` 提供用于配置和管理 Vitess 集群的控制平面。`vtctld`
    部署在集群中一个或多个单元格上，而 `vtctlclient` 部署在管理集群的用户客户端机器上。`vtctld` 使用类似 Kubernetes 的声明式方法执行其工作：更新
    Topology 服务中的集群元数据，VTGate 和 VTTablet 捕捉更改并相应地响应。
- en: Now that you understand the Vitess architecture and basic concepts, let’s discuss
    how they are mapped into a Kubernetes environment. This is an important consideration
    for any application, but especially for a complex piece of data infrastructure
    like Vitess.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了 Vitess 架构和基本概念，让我们讨论如何将它们映射到 Kubernetes 环境中。这对于任何应用程序都是重要考虑因素，尤其是对于像
    Vitess 这样复杂的数据基础设施。
- en: PlanetScale Vitess Operator
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PlanetScale Vitess Operator
- en: Over time, Vitess has evolved in a couple of key aspects. First, it can now
    run additional MySQL-compatible database engines such as Percona. Second, and
    more important for our investigations, PlanetScale has packaged Vitess as a containerized
    application that can be deployed to Kubernetes.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，Vitess 在几个关键方面有了发展。首先，它现在可以运行其他与 MySQL 兼容的数据库引擎，如 Percona。其次，对于我们的研究来说更为重要的是，PlanetScale
    将 Vitess 打包为可部署到 Kubernetes 的容器化应用程序。
- en: Evolving Options For Running Vitess in Kubernetes
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中运行 Vitess 的演变选项
- en: The state of the art for running Vitess in Kubernetes has evolved over time.
    While Vitess once included a Helm chart, this was [deprecated in the 7.0 release](https://oreil.ly/xhUt4)
    in mid-2020\. The Vitess project also hosted an operator which was [deprecated](https://oreil.ly/4RPMj)
    around the same time. Both of these options were retired in favor of the PlanetScale
    operator we examine in this section.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中运行 Vitess 的最新技术已随时间演变。尽管 Vitess 曾包含一个 Helm 图表，在 2020 年中期的 7.0
    版本中已经 [弃用](https://oreil.ly/xhUt4)。Vitess 项目还托管了一个运算符，大约在同一时间被 [弃用](https://oreil.ly/4RPMj)。这两个选项都被
    PlanetScale 运算符取代，我们在本节中进行详细讨论。
- en: Let’s see how easy it is to deploy a multinode MySQL cluster using the [PlanetScale
    Vitess Operator](https://oreil.ly/W5Dc2). Since the Vitess project has adopted
    the PlanetScale Vitess Operator as its officially supported operator, you can
    reference the [Get Started guide](https://oreil.ly/Nl7e2) in the Vitess project
    documentation. We’ll walk through a portion of this guide here to get an understanding
    of the operator’s contents and how it works.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 看看使用 [PlanetScale Vitess 运算符](https://oreil.ly/W5Dc2)轻松部署多节点 MySQL 集群有多简单。由于
    Vitess 项目已将 PlanetScale Vitess 运算符作为其官方支持的运算符，您可以参考 Vitess 项目文档中的 [入门指南](https://oreil.ly/Nl7e2)。我们将在这里部分地遵循该指南，以了解运算符的内容及其工作原理。
- en: Examples Require Kubernetes Clusters with More Resources
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例需要具备更多资源的 Kubernetes 集群
- en: The examples in previous chapters have not required a large amount of compute
    resources, and we encouraged you to run them on local distributions such as kind
    or K3s. Beginning in this chapter, the examples become more complex and may require
    more resources than you have available on your desktop or laptop. For these cases,
    we will provide references to documentation or scripts for creating Kubernetes
    clusters with sufficient resources.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章的示例不需要大量计算资源，我们建议您在诸如 kind 或 K3s 的本地发行版上运行它们。从本章开始，示例变得更加复杂，可能需要比您的台式机或笔记本电脑更多的资源。对于这些情况，我们将提供文档或脚本的参考，以创建具有足够资源的
    Kubernetes 集群。
- en: Installing the Vitess Operator
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Vitess 运算符
- en: 'You can find the source code used in this section in [this book’s code repository](https://github.com/data-on-k8s-book/examples).
    The files are copied for convenience from their original source in the [Vitess
    GitHub repo](https://oreil.ly/Kq7dm). First, install the operator using the provided
    configuration file:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[这本书的代码仓库](https://github.com/data-on-k8s-book/examples)中找到本节使用的源代码。文件为了方便起见从它们在[Vitess
    GitHub 仓库](https://oreil.ly/Kq7dm)的原始来源复制而来。首先，使用提供的配置文件安装运算符：
- en: '[PRE4]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you’ll see in the output of the `kubectl apply` command, this configuration
    creates several CRDs, as well as a Deployment managing a single instance of the
    operator. [Figure 5-5](#vitess_operator_and_custom_resource_def) shows many of
    the elements you’ve just installed, in order to highlight a few interesting details
    that will not be obvious at first glance:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将在 `kubectl apply` 命令的输出中看到的那样，此配置将创建多个 CRD，以及一个管理运算符单个实例的 Deployment。[图 5-5](#vitess_operator_and_custom_resource_def)
    展示了您刚刚安装的许多元素，以突出一些乍一看不明显的有趣细节：
- en: The operator contains a controller corresponding to each CRD. If you’re interested
    in seeing what this looks like in the operator source code in Go, compare the
    [controller implementations](https://oreil.ly/ABID9) with the [custom resource
    specifications](https://oreil.ly/tUD9z) that are used to generate the CRD configurations
    introduced in [“Building Operators”](#building_operators).
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运算符包含与每个 CRD 对应的控制器。如果您有兴趣查看这在 Go 中的运算符源代码是什么样子，请将 [控制器实现](https://oreil.ly/ABID9)
    与用于生成 CRD 配置的 [自定义资源规格](https://oreil.ly/tUD9z) 进行比较，这些配置是在 [“构建运算符”](#building_operators)
    中介绍的。
- en: The figure depicts a hierarchy of CRDs representing their relationships and
    intended usage, as described in the operator’s [API reference](https://oreil.ly/25qhN).
    To use the Vitess Operator, you define a VitessCluster resource which contains
    the definitions of VitessCells and VitessKeyspaces. VitessKeyspaces, in turn,
    contain definitions of VitessShards. While you can view the status of each VitessCell,
    VitessKeyspace, and VitessShard independently, you must update them in the context
    of the parent VitessCluster resource.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图中显示了一组 CRD 的层次结构，代表它们之间的关系和预期的使用方法，如运算符的[API 参考](https://oreil.ly/25qhN)所述。要使用
    Vitess 运算符，您需要定义一个 VitessCluster 资源，其中包含 VitessCells 和 VitessKeyspaces 的定义。VitessKeyspaces
    又包含 VitessShards 的定义。虽然您可以单独查看每个 VitessCell、VitessKeyspace 和 VitessShard 的状态，但必须在父级
    VitessCluster 资源的上下文中更新它们。
- en: Currently, the Vitess Operator supports only etcd as the Topology Service implementation.
    The EtcdLockserver CRD is used to configure these etcd clusters.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前，Vitess Operator 仅支持将 etcd 作为拓扑服务实现。EtcdLockserver CRD 用于配置这些 etcd 集群。
- en: '![Vitess Operator and Custom Resource Definitions](assets/mcdk_0505.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![Vitess Operator 和自定义资源定义](assets/mcdk_0505.png)'
- en: Figure 5-5\. Vitess Operator and Custom Resource Definitions
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. Vitess Operator 和自定义资源定义
- en: Roles and RoleBindings
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 角色和角色绑定
- en: 'As shown toward the bottom of [Figure 5-5](#vitess_operator_and_custom_resource_def),
    installing the operator caused the creation of a ServiceAccount, along with two
    new resources we have not discussed previously: a Role and a RoleBinding. These
    additional resources allow the ServiceAccount to access specific resources on
    the Kubernetes API. First, examine the configuration of the `vitess-operator`
    Role from the file that you used to [install the operator](https://oreil.ly/q52Iq)
    (you can search for `kind: Role` to locate the pertinent code):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '正如在[图 5-5](#vitess_operator_and_custom_resource_def)底部所示，安装操作员会创建一个 ServiceAccount，并创建两个我们之前未讨论过的新资源：一个
    Role 和一个 RoleBinding。这些额外的资源允许 ServiceAccount 访问 Kubernetes API 上的特定资源。首先，查看你用来[安装操作员](https://oreil.ly/q52Iq)的文件中
    `vitess-operator` Role 的配置（可以搜索 `kind: Role` 来定位相关代码）：'
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This first portion of the Role definition identifies resources that are part
    of the core Kubernetes distribution, which may be designated by passing the empty
    string as the `apiGroup` instead of `k8s.io`. The `verbs` correspond to operations
    the Kubernetes API provides on resources, including `get`, `list`, `watch`, `create`,
    `update`, `patch`, and `delete`. This Role is given access to all operations using
    the wildcard `*`. If you follow the URL in the example and examine more of the
    code, you’ll also see how the Role is given access to other resources, including
    Deployments and ReplicaSets, and resources in the `apiGroup planetscale.com`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 角色定义的第一部分标识了核心 Kubernetes 分布中的资源，可以通过将 `apiGroup` 参数设为空字符串而不是 `k8s.io` 来指定。`verbs`
    对应于 Kubernetes API 在资源上提供的操作，包括 `get`、`list`、`watch`、`create`、`update`、`patch`
    和 `delete`。这个 Role 被赋予使用通配符 `*` 访问所有操作的权限。如果你访问示例中的 URL 并查看更多代码，还会看到如何授予这个 Role
    访问其他资源的权限，包括 Deployments 和 ReplicaSets，以及 `apiGroup planetscale.com` 中的资源。
- en: 'The RoleBinding associates the ServiceAccount with the Role:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: RoleBinding 将 ServiceAccount 与 Role 关联起来：
- en: '[PRE6]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Least Privilege for Operators
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运营商的最小特权
- en: As a creator or consumer of operators, exercise care in choosing which permissions
    are granted to operators, and be conscious of the implications for what an operator
    is allowed to do.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 作为操作员的创建者或消费者，在选择授予操作员哪些权限时要小心，并意识到操作员被允许做什么可能会产生的影响。
- en: PriorityClasses
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优先级类
- en: 'Another detail is not depicted in [Figure 5-4](#vitess_architecture_including_vtgatecom):
    installing the operator created two PriorityClass resources. [PriorityClasses](https://oreil.ly/dlkRe)
    provide input to the Kubernetes scheduler to indicate the relative priority of
    Pods. The priority is an integer value, where higher values indicate higher priority.
    Whenever a Pod resource is created and is ready to be assigned to a Worker Node,
    the Scheduler takes the Pod’s priority into account as part of its decisions.
    When multiple Pods are awaiting scheduling, higher-priority Pods are assigned
    before lower-priority Pods. When a cluster’s nodes are running low on compute
    resources, lower-priority Pods may be stopped or *evicted* in order to make room
    for higher-priority Pods, a process known as *preemption*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个细节没有显示在[图 5-4](#vitess_architecture_including_vtgatecom)中：安装操作员创建了两个 PriorityClass
    资源。[PriorityClasses](https://oreil.ly/dlkRe) 提供给 Kubernetes 调度器的输入，指示 Pod 的相对优先级。优先级是一个整数值，较高的值表示较高的优先级。每当创建一个
    Pod 资源并准备分配给工作节点时，调度器会考虑 Pod 的优先级作为决策的一部分。当有多个 Pod 等待调度时，较高优先级的 Pod 会先被分配。当集群的节点资源不足时，可能会停止或*驱逐*较低优先级的
    Pod，以为较高优先级的 Pod 腾出空间，这个过程称为*抢占*。
- en: 'A PriorityClass is a convenient way to set a priority value referenced by multiple
    Pods or other workload resources such as Deployments and StatefulSets. The Vitess
    Operator creates two PriorityClasses: `vitess-operator-control-plane` defines
    a higher priority used for the operator and `vtctld` Deployments, while the `vitess`
    class is used for the data plane components such as the VTGate and VTTablet Deployments.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PriorityClass 是一种方便的方式，用于为多个 Pod 或其他工作负载资源（如 Deployments 和 StatefulSets）设置优先级值。Vitess
    Operator 创建了两个 PriorityClasses：`vitess-operator-control-plane` 用于操作者和 `vtctld`
    Deployments 的较高优先级，而 `vitess` 类别用于数据平面组件，例如 VTGate 和 VTTablet Deployments。
- en: Kubernetes Scheduling Complexity
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 调度复杂性
- en: Kubernetes provides multiple constraints that influence Pod scheduling, including
    prioritization and preemption, affinity and anti-affinity, and scheduler extensions,
    as discussed in [“Extending Kubernetes Clients”](#extending_kubernetes_clients).
    The interaction of these constraints may not be predictable, especially in large
    clusters shared across multiple teams. As resources in a cluster become scarce,
    Pods can be preempted or fail to be scheduled in ways you don’t expect. It’s a
    best practice to maintain awareness of the various scheduling needs and constraints
    across the workloads in your cluster to avoid surprises.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了多个影响 Pod 调度的约束条件，包括优先级和抢占、亲和性和反亲和性，以及调度器扩展，如 [“扩展 Kubernetes 客户端”](#extending_kubernetes_clients)
    中讨论的那样。这些约束条件的交互可能是不可预测的，特别是在跨多个团队共享的大型集群中。当集群中的资源变得稀缺时，Pods 可能会被抢占或以你意想不到的方式无法调度。保持对集群中工作负载的各种调度需求和约束条件的意识是一种最佳实践，以避免出现意外情况。
- en: Creating a VitessCluster
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 VitessCluster
- en: 'Now let’s create a VitessCluster and put the operator to work. The code sample
    contains a configuration file defining a very simple cluster named `example`,
    with a VitessCell `zone1`, keyspace `commerce`, and single shard, which the operator
    gives the name `x-x`:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个 VitessCluster 并让操作者开始工作。代码示例包含一个配置文件，定义了一个名为 `example` 的非常简单的集群，具有
    VitessCell `zone1`，keyspace `commerce` 和单个分片，操作者给它命名为 `x-x`：
- en: '[PRE7]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output of the command indicates a couple of items that are created directly.
    But more is going on behind the scenes, as the operator detects the creation of
    the VitessCluster and begins provisioning other resources, as summarized in [Figure 5-6](#resources_managed_by_the_vitesscluster).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出指示直接创建了几个项目。但在幕后还有更多工作，因为操作者检测到 VitessCluster 的创建并开始为其它资源提供支持，正如 [Figure 5-6](#resources_managed_by_the_vitesscluster)
    中总结的那样。
- en: '![Resources managed by the VitessCluster example](assets/mcdk_0506.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![VitessCluster example 管理的资源](assets/mcdk_0506.png)'
- en: Figure 5-6\. Resources managed by the VitessCluster `example`
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. VitessCluster `example` 管理的资源
- en: 'By comparing the configuration script with [Figure 5-6](#resources_managed_by_the_vitesscluster),
    you can make several observations about this simple VitessCluster. First, the
    top-level configuration allows you to specify the name of the cluster and the
    container images that will be used for the various components:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 通过与 [Figure 5-6](#resources_managed_by_the_vitesscluster) 的配置脚本进行比较，您可以对这个简单的
    VitessCluster 做出几点观察。首先，顶层配置允许您指定集群的名称以及将用于各个组件的容器镜像：
- en: '[PRE8]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, the VitessCluster configuration provides a definition of the VitessCell
    `zone1`. The values provided for `gateway` specify a single VTGate instance to
    be allocated for this cell, with specific compute resource limits:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，VitessCluster 配置提供了 VitessCell `zone1` 的定义。提供给 `gateway` 的值指定了为该 cell 分配的单个
    VTGate 实例，具有特定的计算资源限制：
- en: '[PRE9]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The Vitess Operator uses this information to create a VTGate Deployment prefixed
    with `example-zone1-vtgate` containing a single replica, and a Service that provides
    access. The access credentials for the VTGate instance are provided in the `example-cluster-config`
    Secret. This Secret is used to secure other configuration values, as you’ll see.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Vitess Operator 使用这些信息创建了一个以 `example-zone1-vtgate` 为前缀的 VTGate Deployment，包含一个副本，并提供访问服务。VTGate
    实例的访问凭据存储在 `example-cluster-config` Secret 中。此 Secret 用于保护其他配置值，正如您将看到的那样。
- en: 'The next section of the VitessCluster configuration specifies the creation
    of a single `vtctld` instance (a *dashboard*) with permission to control `zone1`.
    The Vitess Operator uses this information to create a Deployment to manage the
    dashboard using the specified resource limits, and a Service to provide access
    to the VTGate:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: VitessCluster 配置的下一部分指定创建单个 `vtctld` 实例（一个 *仪表盘*），具有控制 `zone1` 的权限。Vitess Operator
    使用这些信息创建一个 Deployment 来管理指定资源限制的仪表盘，并创建一个 Service 来提供对 VTGate 的访问：
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The VitessCluster also defines the `commerce` keyspace, which contains a single
    shard (essentially, an unsharded keyspace). This single shard has a pool of two
    VTTablets in the cell `zone1`, each of which will be allocated 10 GB of storage:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: VitessCluster 还定义了 `commerce` keyspace，其中包含一个单独的分片（实质上是一个未分片的 keyspace）。这个单独的分片在
    `zone1` 区域有两个 VTTablets 的池，每个分片将被分配 10 GB 的存储空间：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As shown in [Figure 5-6](#resources_managed_by_the_vitesscluster), the Vitess
    Operator manages a Pod for each VTTablet and creates a Service to manage access
    across the tablets. The operator does not use a StatefulSet because the VTTablets
    have distinct roles, with one as the primary and the other as a replica. Each
    VTTablet Pod contains multiple containers, including the `vttablet` sidecar which
    configures and controls the `mysql` container. The `vttablet` sidecar initializes
    the `mysql` instance using a script contained in the `example-cluster-config`
    Secret.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 5-6](#resources_managed_by_the_vitesscluster) 所示，Vitess Operator 管理每个 VTTablet
    的 Pod，并创建一个 Service 来管理对这些 tablet 的访问。操作员不使用 StatefulSet，因为 VTTablets 有不同的角色，一个是主要的，另一个是副本。每个
    VTTablet Pod 包含多个容器，包括 `vttablet` sidecar，它配置和控制 `mysql` 容器。`vttablet` sidecar
    使用 `example-cluster-config` Secret 中包含的脚本初始化 `mysql` 实例。
- en: While this configuration doesn’t specifically include details about etcd, the
    Vitess Operator uses its default settings to create a three-node etcd cluster
    to serve as the Topology Service for the VitessCluster. Because of the shortcomings
    of the StatefulSets, the operator manages each Pod and PersistentVolumeClaim individually.
    This points to the possibility for future improvements as Kubernetes and the operator
    mature; perhaps the Kubernetes API server can one day serve the role of the Topology
    Service in the Vitess architecture.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然此配置没有具体包括 etcd 的详细信息，但 Vitess Operator 使用其默认设置创建了一个三节点的 etcd 集群，作为 VitessCluster
    的拓扑服务。由于 StatefulSets 的不足，操作员单独管理每个 Pod 和 PersistentVolumeClaim。这指向 Kubernetes
    和操作员成熟后未来改进的可能性；也许 Kubernetes API 服务器有一天可以在 Vitess 架构中扮演拓扑服务的角色。
- en: At this point, you have a VitessCluster with all of its infrastructure provisioned
    in Kubernetes. The next steps are to create the database schema and configure
    your applications to access the cluster using the VTGate Service. You can follow
    the steps in Alkin Tezuysal’s 2020 blog post [“Vitess Operator for Kubernetes”](https://oreil.ly/543Y8),
    which also describes other use cases for managing a Vitess installation on Kubernetes,
    including schema migration, backup, and restore.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您在 Kubernetes 中已经配置了一个 VitessCluster 及其所有基础设施。接下来的步骤是创建数据库模式并配置应用程序以使用
    VTGate Service 访问集群。您可以按照 Alkin Tezuysal 在其 2020 年的博客文章 [“Vitess Operator for
    Kubernetes”](https://oreil.ly/543Y8) 中描述的步骤进行操作，该文章还描述了在 Kubernetes 上管理 Vitess
    安装的其他用例，包括模式迁移、备份和恢复。
- en: The backup/restore capabilities leverage VitessBackupStorage and VitessBackup
    CRDs, which you may have noticed during installation. VitessBackupStorage resources
    represent locations where backups can be stored. After you configure the backup
    section of a VitessCluster and point to a backup location, the operator creates
    VitessBackup resources as a record of each backup it performs. When you add additional
    replicas to a VitessCluster, the operator initializes their data by performing
    a restore from the most recent backup.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 备份/恢复功能利用 VitessBackupStorage 和 VitessBackup CRD，在安装过程中可能已经注意到。VitessBackupStorage
    资源表示备份可以存储的位置。在配置 VitessCluster 的备份部分并指向备份位置后，操作员创建 VitessBackup 资源作为其执行的每个备份的记录。当向
    VitessCluster 添加额外的副本时，操作员通过从最近的备份进行恢复来初始化它们的数据。
- en: Visualizing Larger Kubernetes Applications
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化更大的 Kubernetes 应用程序
- en: It’s a good exercise to use the `kubectl get` and `kubectl` describe commands
    to explore all of the resources that were created when you installed the operator
    and created a cluster. However, you may find it easier to use a tool such as [Lens](https://github.com/lensapp/lens),
    which offers a friendly graphical interface enabling you to click through the
    resources more quickly, or [K9s](https://k9scli.io), which provides a command-line
    interface.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl get` 和 `kubectl describe` 命令来探索在安装运算符和创建集群时创建的所有资源是一个很好的练习。但是，您可能会发现使用像
    [Lens](https://github.com/lensapp/lens) 这样的工具更容易，它提供了一个友好的图形界面，让您可以更快速地浏览资源，或者像
    [K9s](https://k9scli.io) 这样的工具，提供命令行界面。
- en: 'Resharding is another interesting use case, which you might need to perform
    when a cluster becomes unbalanced and one or more shards run out of capacity more
    quickly than others. You’ll need to modify the VSchema using `vtctlclient`, and
    then [update the VitessCluster resource](https://oreil.ly/i0n5S) with additional
    VitessShards so that the operator provisions the required infrastructure. This
    highlights the division of responsibility: the Vitess Operator manages Kubernetes
    resources, while the Vitess control daemon (`vtctld`) provides more application-specific
    behavior.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 重新分片是另一个有趣的用例，当集群变得不平衡且一个或多个分片比其他分片更快地耗尽容量时，您可能需要执行此操作。您将需要使用 `vtctlclient`
    修改 VSchema，然后通过添加额外的 VitessShards 来[更新 VitessCluster 资源](https://oreil.ly/i0n5S)，以便运算符配置所需的基础设施。这突显了责任的分工：Vitess
    运算符管理 Kubernetes 资源，而 Vitess 控制守护进程 (`vtctld`) 提供更多的应用特定行为。
- en: A Growing Ecosystem of Operators
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运算符生态系统的不断增长
- en: The operator pattern has become quite popular in the Kubernetes community, aided
    in part by the development of the [Operator Framework](https://operatorframework.io),
    an ecosystem for creating and distributing operators. In this section, we’ll examine
    the Operator Framework and related open source projects.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符模式在 Kubernetes 社区中变得非常流行，部分得益于[运算符框架](https://operatorframework.io)的发展，这是一个用于创建和分发运算符的生态系统。在本节中，我们将检查运算符框架及其相关的开源项目。
- en: Choosing Operators
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择运算符
- en: While we’ve focused in this chapter on Vitess as an example database operator,
    operators are clearly relevant to all of the elements of your data stack. In all
    aspects of cloud native data, we see a growing number of maturing, open source
    operators to use in your deployments, and we’ll be looking at additional operators
    as we examine how to run different types of data infrastructure on Kubernetes
    in upcoming chapters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在本章中我们着重介绍了 Vitess 作为示例数据库运算符，但运算符显然与您数据栈的所有元素相关。在云原生数据的各个方面，我们看到越来越多成熟的开源运算符可用于您的部署，而我们将在未来的章节中查看更多不同类型数据基础设施在
    Kubernetes 上运行时的其他运算符。
- en: You should consider multiple aspects in choosing an operator. What are its features?
    How much does it automate? How well supported is it? Is it proprietary or open
    source? The Operator Framework provides a great resource, the [Operator Hub](https://operatorhub.io),
    which you should consider as your first stop when looking for an operator. Operator
    Hub is a well-organized list of various operators that cover every aspect of cloud
    native software. It does rely on maintainers to submit their operators for listing,
    which means that many existing operators may not be listed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择运算符时，您应该考虑多个方面。它有哪些特性？它自动化程度如何？它的支持情况如何？它是专有的还是开源的？运算符框架提供了一个重要资源，[运算符中心](https://operatorhub.io)，这是您在寻找运算符时应该首先考虑的地方。运算符中心是一个井然有序的列表，涵盖了云原生软件的各个方面。它依赖于维护者提交其运算符以进行列表化，这意味着许多现有的运算符可能未列出。
- en: The Operator Framework also contains the Operator Lifecycle Manager (OLM), an
    operator for installing and managing other operators in your cluster. You can
    curate your own custom catalog of operators that are permitted in your environment,
    or use catalogs provided by others. For example, Operator Hub can itself be [treated
    as a catalog](https://oreil.ly/ble8P).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符框架还包括运算符生命周期管理器（OLM），这是一个用于安装和管理集群中其他运算符的运算符。您可以管理您环境中允许的运算符的自定义目录，或使用其他人提供的目录。例如，运算符中心本身可以被视为一个[目录](https://oreil.ly/ble8P)。
- en: Part of the curation the Operator Hub provides is rating the capability of each
    operator according to the [Operator Capability Model](https://oreil.ly/eYVEA).
    The levels in this capability model are summarized in [Table 5-1](#operator_capability_levels_applied_to_d),
    with additional commentary we’ve added to highlight considerations for database
    operators. The examples are not prescriptive but indicate the type of capabilities
    expected at each level.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符中心提供的部分典藏包括根据[运算符能力模型](https://oreil.ly/eYVEA)评级每个运算符的能力。这个能力模型的级别在[表格 5-1](#operator_capability_levels_applied_to_d)总结，我们还添加了额外的评论来强调数据库运算符的考虑事项。这些示例并非指导性建议，而是指出每个级别预期的能力类型。
- en: Table 5-1\. Operator capability levels applied to databases
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5-1\. 数据库运算符能力级别应用
- en: '| Capability level | Characteristics | Database operator examples | Tools |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 能力级别 | 特征 | 数据库运算符示例 | 工具 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Level 1: Basic install | Installation and configuration of Kubernetes and
    workloads | The operator uses custom resources to provide a central point of configuration
    for a database cluster. The operator deploys the database by creating resources
    such as Deployments, ServiceAccounts, RoleBindings, PersistentVolumeClaims, and
    Secrets, and helps initialize the database schema. | Helm, Ansible, Go |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Level 1: 基本安装 | Kubernetes 和工作负载的安装和配置 | 运算符使用自定义资源为数据库集群提供集中的配置点。运算符通过创建部署、服务账户、角色绑定、持久卷索赔和机密等资源来部署数据库，并帮助初始化数据库架构。
    | Helm, Ansible, Go |'
- en: '| Level 2: Seamless updates | Upgrade of the managed workload and operator
    | The operator can update an existing database to a newer version without data
    loss (or, hopefully, downtime). The operator can be replaced with a newer version
    of itself. | Helm, Ansible, Go |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Level 2: 无缝更新 | 管理工作负载和运算符的升级 | 运算符可以将现有数据库升级到更新的版本，无需数据丢失（或希望没有停机）。运算符可以用更新的版本替换自身。
    | Helm, Ansible, Go |'
- en: '| Level 3: Full lifecycle | Ability to create and restore from backups, ability
    to fail over or replace portions of a clustered application, ability to scale
    the application | The operator provides a way to create a consistent backup across
    multiple data nodes and the ability to use those backups to restore or replace
    failed database nodes. The operator can respond to a configuration change to add
    or remove database nodes or perhaps even datacenters. | Ansible, Go |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Level 3: 完整生命周期 | 能够创建和从备份中恢复，能够故障转移或替换部分集群应用程序，能够扩展应用程序 | 运算符提供一种方法在多个数据节点上创建一致的备份，并能够使用这些备份来恢复或替换失败的数据库节点。运算符可以响应配置更改以添加或删除数据库节点，甚至数据中心。
    | Ansible, Go |'
- en: '| Level 4: Deep insights | Providing capabilities including alerting, monitoring,
    events, or metering | The operator monitors metrics and logging output by the
    database software and uses this information to implement health and readiness
    checks. The operator pushes metrics and alerts to other infrastructure. | Ansible,
    Go |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Level 4: 深入洞察 | 提供包括警报、监控、事件或计量在内的能力 | 运算符监视数据库软件的度量和日志输出，并使用这些信息执行健康和准备检查。运算符将度量和警报推送到其他基础设施。
    | Ansible, Go |'
- en: '| Level 5: Auto-pilot | Providing capabilities including auto-scaling, auto-healing,
    auto-tuning | The operator auto-scales the number of database nodes in the cluster
    up or down to meet performance requirements. The operator might also dynamically
    resize PVs or change the StorageClass used for various database nodes. The operator
    automatically performs database maintenance such as rebuilding indexes to improve
    slow response times.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '| Level 5: 自动驾驶 | 提供包括自动扩展、自动修复、自动调优在内的能力 | 运算符根据性能需求自动调整集群中数据库节点的数量。运算符还可以动态调整持久卷的大小或更改用于各个数据库节点的存储类。运算符自动执行数据库维护，如重建索引以改善响应速度慢的问题。'
- en: The operator detects abnormal workload patterns and takes action such as resharding
    to balance workloads. | Ansible, Go |
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 运算符检测到异常的工作负载模式，并采取行动，如重新分片以平衡工作负载。 | Ansible, Go |
- en: These levels are useful both for evaluating operators you might want to use,
    and for providing targets for operator developers to aim for. They also provide
    an opinionated view on what Helm-based operators can accomplish, limiting them
    to Level 2\. For full lifecycle management and automation, more direct involvement
    with the Kubernetes control plane is needed. For a Level 5 operator, the goal
    is a complete hands-off Deployment.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些级别既有助于评估您可能希望使用的运算符，也为运算符开发人员提供了目标。它们还提供了基于 Helm 的运算符能够完成的任务的一个具有见解的观点，将其限制在第
    2 级。对于全生命周期管理和自动化，需要更直接地与 Kubernetes 控制平面进行交互。对于第 5 级运算符，目标是完全无需手动干预的部署。
- en: 'Let’s take a quick look at a few of the available operators for popular open
    source databases:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速浏览几种流行开源数据库的可用运算符：
- en: Cass Operator
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: Cass Operator
- en: 'In 2021, several companies in the Cassandra community that had developed their
    own operators [came together](https://oreil.ly/AS16G) in support of an operator
    built by DataStax, known primarily by its nickname: [Cass Operator](https://oreil.ly/ueyGZ).
    Cass Operator was inspired by the best features of the community operators as
    well as DataStax experience running Astra, a Cassandra-based database as a service
    (DBaaS). The operator has been donated to the [K8ssandra project](https://k8ssandra.io),
    where it is part of a larger ecosystem for deploying Cassandra on Kubernetes.
    We’ll take a deeper look at K8ssandra and Cass Operator in [Chapter 7](ch07.html#the_kubernetes_native_database).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2021 年，几家 Cassandra 社区的公司，他们各自开发了自己的运算符，在支持由 DataStax 开发的运算符时[汇聚在一起](https://oreil.ly/AS16G)，该运算符以
    [Cass Operator](https://oreil.ly/ueyGZ) 的昵称而闻名。Cass Operator 受到社区运算符最佳特性以及 DataStax
    在运行基于 Cassandra 的数据库服务（DBaaS）Astra 方面的经验的启发。该运算符已经捐赠给了[K8ssandra 项目](https://k8ssandra.io)，它是在
    Kubernetes 上部署 Cassandra 的更大生态系统的一部分。我们将在[第 7 章](ch07.html#the_kubernetes_native_database)深入了解
    K8ssandra 和 Cass Operator。
- en: PostgreSQL operators
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: PostgreSQL 运算符
- en: Several operators are available for PostgreSQL, which is not surprising given
    that it is the second most popular open source database after MySQL. Two of the
    most popular operators are the [Zalando Postgres Operator](https://oreil.ly/i6Us5),
    and [PGO](https://oreil.ly/A40Qf) (which stands for Postgres Operator) from Crunchy
    Data. Read Nikolay Bogdanov’s blog post [“Comparing Kubernetes Operators for PostgreSQL”](https://oreil.ly/AQcvB)
    for a helpful comparison of these and other operators.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 PostgreSQL，提供了几种运算符，这并不奇怪，因为它是继 MySQL 之后最受欢迎的开源数据库。其中两种最受欢迎的运算符是[Zalando
    PostgreSQL Operator](https://oreil.ly/i6Us5)，以及来自 Crunchy Data 的[PGO](https://oreil.ly/A40Qf)（代表
    Postgres Operator）。阅读 Nikolay Bogdanov 的博文[“比较用于 PostgreSQL 的 Kubernetes Operators”](https://oreil.ly/AQcvB)，对比这些以及其他运算符会很有帮助。
- en: MongoDB Kubernetes Operator
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB Kubernetes Operator
- en: MongoDB is the most popular document database, beloved by developers for its
    ease of use. The MongoDB Community Kubernetes Operator provides basic support
    for creating and managing MongoDB ReplicaSets, scaling up and down, and upgrades.
    This operator is available on [GitHub](https://oreil.ly/dAhuV) but not yet listed
    on Operator Hub, possibly because MongoDB also offers a separate operator for
    its enterprise version.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: MongoDB 是最受欢迎的文档数据库，开发者喜爱它的易用性。MongoDB Community Kubernetes Operator 提供了基本支持，用于创建和管理
    MongoDB ReplicaSets，扩展和缩减规模，以及升级操作。这个运算符可以在[GitHub](https://oreil.ly/dAhuV)上找到，但尚未列入
    Operator Hub，可能是因为 MongoDB 还为其企业版本提供了单独的运算符。
- en: Redis Operator
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Redis Operator
- en: Redis is an in-memory key-value store that has a broad set of use cases. Application
    developers typically use Redis as an adjunct to other data infrastructure when
    ultra-low latency is required. It excels at caching, counting, and shared data
    structures. The [Redis Operator](https://oreil.ly/SKLSz) covers the basic install
    and upgrade but also manages harder operations such as cluster failover and recovery.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 是一种内存中的键值存储，具有广泛的用例。应用开发者通常在需要超低延迟时将 Redis 作为其他数据基础设施的附属品使用。它擅长缓存、计数和共享数据结构。[Redis
    Operator](https://oreil.ly/SKLSz) 覆盖了基本的安装和升级，同时还管理了更复杂的操作，如集群故障转移和恢复。
- en: As you can see, operators are available for many popular open source databases,
    although it’s unfortunate that some vendors have tended to think of Kubernetes
    operators primarily as a feature differentiator for paid enterprise versions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到，为许多流行的开源数据库提供了运算符，尽管有些供应商倾向于将 Kubernetes 运算符主要作为付费企业版本的特性差异化。
- en: Building Operators
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据库运算符
- en: 'While there is broad consensus in the Kubernetes community that you should
    *use* operators for distributed data infrastructure whenever possible, there are
    a variety of opinions about who exactly should be *building* operators. If you
    don’t happen to work for a data infrastructure vendor, this can be a challenging
    question. Mary Branscombe’s blog post [“When to Use, and When to Avoid, the Operator
    Pattern”](https://oreil.ly/hd8FE) provides some excellent questions to consider,
    which we’ll summarize here:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Kubernetes 社区普遍认为，尽可能*使用*操作符来管理分布式数据基础设施，但对于*构建*操作符的具体人员意见不一。如果你不恰好在一个数据基础设施供应商工作，这可能是一个具有挑战性的问题。玛丽·布兰斯科姆在她的博客文章[“何时使用及避免操作符模式”](https://oreil.ly/hd8FE)提供了一些需要考虑的优秀问题，我们将在这里总结：
- en: What is the scale of the deployment? If you’re deploying only a single instance
    of the database application, building and maintaining an operator might not be
    cost-effective.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署的规模是多大？如果你只部署一个数据库应用程序的实例，构建和维护操作符可能不是一种成本有效的方法。
- en: Do you have the expertise in the database? The best operators tend to be built
    by companies running databases at scale in production, including vendors that
    are providing DBaaS solutions.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否具备数据库方面的专业知识？最好的操作符通常由在生产环境中运行数据库的公司构建，包括提供 DBaaS 解决方案的供应商。
- en: Do you need higher levels of application awareness and automation, or would
    deployment with a Helm chart and standard Kubernetes resources be sufficient?
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否需要更高水平的应用程序意识和自动化，或者使用 Helm 图表和标准的 Kubernetes 资源部署已经足够？
- en: Are you trying to make the operator manage resources that are external to Kubernetes?
    Consider a solution that runs closer to the resources being managed with an API
    you can access from your Kubernetes application.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否尝试让操作符管理 Kubernetes 外部的资源？考虑一个解决方案，它更接近于被管理的资源，并且具有一个你可以从你的 Kubernetes 应用程序访问的
    API。
- en: Have you considered security implications? Since operators are extensions of
    the Kubernetes control plane, you’ll want to carefully manage what resources your
    operator can access.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否考虑了安全性问题？由于操作符是 Kubernetes 控制平面的扩展，你需要仔细管理你的操作符可以访问的资源。
- en: 'If you decide to write an operator, several great tools and resources are available:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定编写一个操作符，有几个出色的工具和资源可供使用：
- en: '[Operator SDK](https://oreil.ly/HtSZt)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '[Operator SDK](https://oreil.ly/HtSZt)'
- en: This software development kit, included in the Operator Framework, contains
    tools to build, test, and package operators. Operator SDK uses templates to autogenerate
    new operator projects and provides APIs and abstractions to simplify common aspects
    of building operators, especially interactions with the Kubernetes API. The SDK
    supports the creation of operators using Go, Ansible, or Helm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这个软件开发工具包包含在操作符框架中，用于构建、测试和打包操作符。Operator SDK 使用模板自动生成新的操作符项目，并提供 API 和抽象来简化构建操作符的常见方面，特别是与
    Kubernetes API 的交互。SDK 支持使用 Go、Ansible 或 Helm 创建操作符。
- en: '[Kubebuilder](https://book.kubebuilder.io)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kubebuilder](https://book.kubebuilder.io)'
- en: This toolkit for building operators is managed by the Kubernetes API Machinery
    SIG. Similarly to Operator SDK, Kubebuilder provides tools for project generation,
    testing, and publishing controllers and operators. Both Kubebuilder and Operator
    SDK are built on the Kubernetes [controller-runtime](https://oreil.ly/XR9y4),
    a set of Go libraries for building controllers. Wei Tei’s blog post [“Kubebuilder
    vs. Operator SDK”](https://oreil.ly/NKC8d) provides a concise summary of the differences
    between these toolkits.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用于构建操作符的工具包由 Kubernetes API 机器人 SIG 管理。与 Operator SDK 类似，Kubebuilder 提供了用于项目生成、测试和发布控制器和操作符的工具。Kubebuilder
    和 Operator SDK 都构建在 Kubernetes [controller-runtime](https://oreil.ly/XR9y4) 上，这是一组用于构建控制器的
    Go 库。魏泰在他的博客文章[“Kubebuilder vs. Operator SDK”](https://oreil.ly/NKC8d)提供了这些工具包之间差异的简洁总结。
- en: '[Kubernetes Universal Declarative Operator (KUDO)](https://kudo.dev)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kubernetes 通用声明性操作符 (KUDO)](https://kudo.dev)'
- en: This operator allows you to create operators declaratively using YAML files.
    This is an attractive approach for some developers as it eliminates the need to
    write Go. Dmytro Vedetskyi’s blog post [“How to Deploy Your First App with Kudo
    Operator on K8S”](https://oreil.ly/K71fK) provides a helpful introduction to using
    KUDO and discusses some of the pros and cons of the declarative approach.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作符允许你使用 YAML 文件声明性地创建操作符。对于一些开发者来说，这是一种有吸引力的方法，因为它消除了编写 Go 代码的需要。德米特罗·韦德茨基在他的博客文章[“如何在
    K8S 上使用 KUDO 操作符部署你的第一个应用”](https://oreil.ly/K71fK)介绍了如何使用 KUDO，并讨论了声明性方法的一些优缺点。
- en: Finally, the O’Reilly books [*Kubernetes Operators*](https://oreil.ly/rWIM0)
    by Jason Dobies and Joshua Wood and [*Programming Kubernetes*](https://oreil.ly/iczyv)
    are great resources for understanding the operator ecosystem and getting into
    the details of writing operators and controllers in Go.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Jason Dobies 和 Joshua Wood 的 [*Kubernetes Operators*](https://oreil.ly/rWIM0)
    和 [*Programming Kubernetes*](https://oreil.ly/iczyv) 等 O’Reilly 出版的书籍，是理解运算符生态系统并深入编写
    Go 中运算符和控制器细节的重要资源。
- en: As you can see, the state of the art in Kubernetes operators is continuing to
    mature. Whether the goal is to build a unified operator or just to make it easier
    to build database-specific operators, it’s clear that great progress can be made
    as multiple communities begin to collaborate on common CRDs to address problems
    like cluster membership, topology awareness, and leader election.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Kubernetes 运算符的最新技术正在不断成熟。无论目标是构建统一的运算符还是使构建特定于数据库的运算符更加容易，可以明显看出，多个社区开始合作解决如集群成员资格、拓扑感知和领导选举等问题的共同
    CRD，取得了重大进展。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you’ve learned about several ways of extending the Kubernetes
    control plane, especially operators and custom resources. The operator pattern
    provides the critical breakthrough that enables us to simplify database operations
    in Kubernetes through automation. While you should definitely be using operators
    to run distributed databases in Kubernetes, think carefully before starting to
    write your own operator. If building an operator is the right course for you,
    there are plenty of resources and frameworks to help you along the way. There
    are certainly ways in which Kubernetes itself could improve to make writing operators
    easier, as you’ve learned from the experts we spoke to in this chapter.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了几种扩展 Kubernetes 控制平面的方法，尤其是运算符和自定义资源。运算符模式提供了重要突破，使我们能够通过自动化简化 Kubernetes
    中的数据库操作。虽然你肯定应该使用运算符在 Kubernetes 中运行分布式数据库，但在开始编写自己的运算符之前，请认真考虑。如果构建运算符是你的正确选择，那么有很多资源和框架可以帮助你一路前行。正如本章中专家们所讨论的那样，Kubernetes
    本身也有改进的空间，以使编写运算符更加容易。
- en: While we’ve spent the past couple of chapters focusing primarily on running
    databases on Kubernetes, let’s expand our focus to consider how those databases
    interact with other infrastructure.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们主要专注于在 Kubernetes 上运行数据库，现在让我们扩展焦点，考虑这些数据库与其他基础设施的交互方式。
