<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 15. Service Meshes" data-type="chapter" epub:type="chapter"><div class="chapter" id="service_mesh">
<h1><span class="label">Chapter 15. </span>Service Meshes</h1>
<p>Perhaps second only to containers, the term <em>service mesh</em> has become synonymous with cloud native development.<a data-primary="service meshes" data-type="indexterm" id="ix_sermsh"/> However, just like containers, service mesh is a broad term that encompasses a variety of open source projects as well as commercial products. Understanding the general role of a service mesh in a cloud native architecture is useful.<a data-primary="cloud native development" data-secondary="service meshes" data-type="indexterm" id="idm45664074083856"/> This chapter will show you what a service mesh is, how different software projects implement them, and finally (and most importantly) when it makes sense to incorporate a service mesh, versus a less complex architecture, into your application.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In many abstract cloud native architecture diagrams, it seems that a service mesh is necessary for a cloud native architecture. This is very much not true. When considering adopting a service mesh, you have to balance the complexity of adding a new component (generally provided by a third party) to your list of dependencies. In many cases, it is easier and more reliable to simply depend on the existing Kubernetes resources, if they meet the needs of your application.</p>
</div>
<p>We have previously discussed other networking primitives in Kubernetes like Services and Ingress. <a data-primary="networking" data-secondary="Services and Ingress" data-type="indexterm" id="idm45664074080368"/>Given the presence of these networking capabilities in the core of Kubernetes, why is there a need to inject additional capabilities (and complexities) into the networking layer? Fundamentally it comes down to the needs of the software application that is using these networking primitives.</p>
<p>Networking in the core of Kubernetes is really only aware of the application as a destination. Both Service and Ingress resources have label selectors that route traffic to a particular set of Pods, but beyond that there is comparatively little in the way of additional capabilities that these resources bring. As an HTTP load balancer, Ingress goes a little beyond this, but the challenge of defining a common API that fits a wide variety of different existing implementations limits the capabilities in the Ingress API.<a data-primary="routing" data-secondary="cloud native HTTP-routing API" data-type="indexterm" id="idm45664074078576"/> How can a truly “cloud native” HTTP-routing API be compatible with load balancers and proxies ranging from bare-metal networking devices through to public cloud APIs
that were built without thinking about cloud native development?<a data-primary="cloud native development" data-secondary="cloud native HTTP-routing API" data-type="indexterm" id="idm45664074077248"/></p>
<p>In a very real way, the development of service mesh APIs outside the core of Kubernetes is a result of this challenge. The Ingress APIs bring HTTP(S) traffic from the outside world into a cloud native application. Within a cloud native application in Kubernetes, freed of the need to be compatible with existing infrastructure, the service mesh APIs provide additional cloud native networking capabilities.<a data-primary="networking" data-secondary="cloud native capabilities with service meshes" data-type="indexterm" id="idm45664074075888"/> So what are these capabilities? There are three general capabilities provided by most service mesh implementations: network encryption and authorization, traffic shaping, and observability. The following sections look at each of these in turn.</p>
<section data-pdf-bookmark="Encryption and Authentication with Mutal TLS" data-type="sect1"><div class="sect1" id="idm45664074074304">
<h1>Encryption and Authentication with Mutal TLS</h1>
<p>Encryption of network traffic between Pods is a key component to security in a
microservice architecture. <a data-primary="service meshes" data-secondary="encryption and authentication with mTLS" data-type="indexterm" id="idm45664074072688"/><a data-primary="authentication" data-secondary="using mTLS" data-type="indexterm" id="idm45664074071744"/><a data-primary="mTLS (Mutual Transport Layer Security)" data-type="indexterm" id="idm45664074070800"/><a data-primary="encryption with mTLS" data-type="indexterm" id="idm45664074070112"/><a data-primary="TLS (Transport Layer Security)" data-secondary="Mutual TLS (mTLS)" data-type="indexterm" id="idm45664074069440"/>Encryptions provided by Mutual Transport Layer Security, or <em>mTLS</em>, is one of the most popular use cases for a service mesh. While it is possible for developers to implement this encryption themselves, certificate handling and traffic encryption is complicated and hard to get right. Leaving the implementation of encryption to individual development teams leads to developers forgetting to add encryption at all, or doing it poorly. When poorly implemented, encryption can negatively impact both reliability and, in the worst case, provide no real security. By contrast, installing a service mesh on your Kubernetes cluster automatically provides encryption to network traffic between every Pod in the cluster. The service mesh adds a sidecar container to every Pod, which transparently intercepts all network communication. <a data-primary="certificates" data-secondary="client certificates in mTLS" data-type="indexterm" id="idm45664074067888"/>In addition to securing the communication, mTLS adds identity to the encryption using client certificates so your application securely knows the identity of every
network client.</p>
</div></section>
<section data-pdf-bookmark="Traffic Shaping" data-type="sect1"><div class="sect1" id="idm45664074066448">
<h1>Traffic Shaping</h1>
<p>When you first think about your application design, it is typically a clean diagram with a single box for each microservice or layer in the system (e.g., the frontend service, user preferences service, etc). <a data-primary="service meshes" data-secondary="traffic shaping" data-type="indexterm" id="idm45664074064784"/>When implemented in practice, there are actually often multiple instances of any particular microservice running within the application. For example, when you are doing a rollout from version X of your service to version Y, there is a point in the middle of the rollout when you are simultaneously running two different versions of that service. While the middle of a rollout is a temporary state, there are many times when you need to create a longer-running experiment that persists for an extended period. A common model used in the software industry is “dog-fooding” your own software, meaning that a new version of the software is tried internally before anywhere else. In a dog-fooding model, you may run version Y of your service for a day to a week (or longer) for a subset of users before you roll it out broadly to your full set of users.</p>
<p>Such experiments require the ability to do <em>traffic shaping</em>, or routing of requests to different service implementations based on the characteristics of the request.<a data-primary="routing" data-secondary="traffic shaping" data-type="indexterm" id="idm45664074062688"/><a data-primary="traffic shaping" data-type="indexterm" id="idm45664074061712"/> In this example, you would create an experiment where all traffic from your company’s internal users went to service Y, while all traffic from the rest of the world still went to service X.</p>
<p>Experiments are useful for a variety of scenarios, including development, where a programmer can send a limited set (typically 1% or less) of real-world traffic to an experimental backend, or you can run an A/B experiment where 50% of users get one experience and 50% of users get another, so that you can build statistical models of which approach is more effective. Experiments are an incredibly useful way to add reliability, agility, and insight to your application, but they are often difficult to implement in practice and thus not used as often as they might otherwise be.</p>
<p>Service meshes change this by building experimentation into the mesh itself. Instead of writing code to implement your experiment, or deploying an entirely new copy of your application on new infrastructure, you declaratively define the parameters of the experiment (10% of traffic to version Y, 90% of traffic to version X), and the service mesh implements it for you. Although, as a developer, you are involved in defining the experiment, the implementation is transparent and automatic, meaning that many more experiments will be run with a corresponding increase in reliability, agility, and insight.</p>
</div></section>
<section data-pdf-bookmark="Introspection" data-type="sect1"><div class="sect1" id="idm45664074059152">
<h1>Introspection</h1>
<p>If you are like most programmers, once you write a program, you repeatedly debug it as new errors manifest themselves.<a data-primary="introspection" data-secondary="capabilities provided by service meshes" data-type="indexterm" id="idm45664074057696"/><a data-primary="service meshes" data-secondary="introspection" data-type="indexterm" id="idm45664074056752"/><a data-primary="debugging" data-secondary="introspection capabilities in service meshes" data-type="indexterm" id="idm45664074055808"/> Finding errors in your code is a large part of how most developers spend their days. Debugging is even more difficult when applications are spread across multiple microservices. It is hard to stitch together a single request when it spans multiple Pods. The information needed for debugging must be stitched back together from multiple sources, assuming that the relevant information was collected in the first place.</p>
<p>Automatic introspection is another important capability provided by a service mesh. Because it is involved in all communication between Pods, the service mesh knows where requests were routed, and it can keep track of the information required to put a complete request trace back together. Instead of seeing a flurry of requests to a bunch of different microservices, the developer can see a single aggregate request that defines the user experience of their complete application. Furthermore, the service mesh is implemented once for an entire cluster. This means that the same request tracing works no matter which team developed the service. The monitoring data is entirely consistent across all of the different services joined together by a cluster-wide service mesh.</p>
</div></section>
<section data-pdf-bookmark="Do You Really Need a Service Mesh?" data-type="sect1"><div class="sect1" id="idm45664074053568">
<h1>Do You Really Need a Service Mesh?</h1>
<p>The advantages described here may have you eager to start installing a service mesh on your cluster. <a data-primary="service meshes" data-secondary="deciding need for" data-type="indexterm" id="idm45664074052160"/>However, before you do that, it’s worth considering whether a service mesh is really necessary for your application. A service mesh is a distributed system that adds complexity to your application design. The service mesh is deeply integrated into the core communication of your microservices. When a service mesh fails, your entire application stops working. Before you adopt a service mesh, you must be confident that you can fix problems when they occur. You must also be ready to monitor the software releases for the service mesh to make sure that you pick up the latest security and bug fixes, and, of course, when fixes become available, you must also be ready to roll out the new version without impacting your application. This additional operational overhead means that for many small applications, a service mesh is an unnecessary complexity.</p>
<p>If you are using Kubernetes provided as a managed service that also provides a service mesh, it is much easier to use that mesh, knowing that the cloud provider will provide the support, debugging, and seamless new releases for the service mesh. <a data-primary="cloud" data-secondary="service meshes from providers" data-type="indexterm" id="idm45664074049552"/>But even with a service mesh supplied by a cloud provider, there is additional complexity for your developers to learn. Ultimately, weighing the costs versus benefits of a service mesh is something each application or platform team needs to do at a cluster level. To maximize the benefits of a service mesh, it’s helpful for all microservices in the cluster to adopt it at the same time.</p>
</div></section>
<section data-pdf-bookmark="Introspecting a Service Mesh Implementation" data-type="sect1"><div class="sect1" id="idm45664074048128">
<h1>Introspecting a Service Mesh Implementation</h1>
<p>There are many different service mesh projects and implementations in the cloud native ecosystem, but most share many of the same design patterns and technology.<a data-primary="service meshes" data-secondary="introspecting an implementation" data-type="indexterm" id="idm45664074046640"/><a data-primary="introspection" data-secondary="introspecting a service mesh implementation" data-type="indexterm" id="idm45664074045696"/> Because the service mesh is transparently intercepting network traffic from your application Pod, modifying it and rerouting it through the cluster, a part of the service mesh needs to be present within every one of your Pods. Forcing developers to add something to each container image would introduce significant friction as well as make it much more difficult to centrally manage the service mesh version. <a data-primary="Pods" data-secondary="in service meshes, sidecar container added to" data-secondary-sortas="service" data-type="indexterm" id="idm45664074044096"/>As a result, most service mesh implementations add a sidecar container to every Pod in the mesh.<a data-primary="sidecar containers" data-secondary="added to each Pod in service meshes" data-type="indexterm" id="idm45664074042608"/> Because the sidecar sits in the same network stack as the application Pod, it can use tools like <code>iptables</code> or, more recently, <code>eBPF</code> to introspect and intercept network traffic coming from your application container and process it into the service mesh.</p>
<p>Of course, requiring every developer to add a container image to their Pod definition introduces nearly as much friction as requiring them to modify their container image. To address this, most service mesh implementations depend on a mutating admission controller to automatically add the service mesh sidecar to all Pods that are created in a particular cluster. Any REST API request to create a Pod is first routed to this admission controller. The service mesh admission controller modifies the Pod definition by adding the sidecar. Because this admission controller is installed by the cluster administrator, it transparently and consistently implements a service mesh for an entire cluster.</p>
<p>But the service mesh isn’t just about modifying the Pod network. You also need to be able to control how the service mesh behaves; for example, by defining routing rules for experiments or access restrictions for services in the mesh. Like everything else in Kubernetes, these resource definitions are declaratively specified via JSON or YAML object definitions that you create using <code>kubectl</code> or other tools that communicate with the Kubernetes API server. Service mesh implementations take advantages of <em>custom resource definitions</em> (CRDs) to add specialized resources to your Kubernetes cluster that are not part of the default installation.<a data-primary="custom resource definitions (CRDs)" data-secondary="service mesh implementations using" data-type="indexterm" id="idm45664074038512"/> In most cases, the specific custom resources are tightly tied to the service mesh itself. An ongoing effort in the CNCF is defining a standard vendor-neutral Service Mesh Interface (SMI) that many different service meshes can implement.<a data-primary="Service Mesh Interface (SMI), vendor-neutral" data-type="indexterm" id="idm45664074037104"/></p>
</div></section>
<section data-pdf-bookmark="Service Mesh Landscape" data-type="sect1"><div class="sect1" id="idm45664074036144">
<h1>Service Mesh Landscape</h1>
<p>The most daunting aspect of the service mesh landscape may be figuring out which mesh to choose. <a data-primary="service meshes" data-secondary="landscape" data-type="indexterm" id="idm45664074034736"/>So far, no one mesh has emerged as the de facto standard. Though concrete statistics are hard to come by, the most popular service mesh is likely the Istio project. In addition to Istio, there are many other open source meshes, including Linkerd, Consul Connect, Open Service Mesh, and others. There are also proprietary meshes like AWS App Mesh. We expect efforts to standardize these interfaces to continue in the coming years in the cloud native community.</p>
<p>How is a developer or cluster administrator to choose? The truth is that the best service mesh for you is likely the one that your cloud provider supplies for you. Adding operating a service mesh to the already complicated duties of your cluster operators is generally unnecessary. It is much better to let a cloud provider manage it for you.</p>
<p>If that isn’t an option for you, do your research. Don’t be drawn in by flashy demos and promises of functionality. A service mesh lives deep within your infrastructure, and any failures can significantly impact the availability of your application. Additionally, because service mesh APIs tend to be implementation specific, it is difficult to change your choice of service mesh once you have spent time developing applications around it. In the end, you may find that the right mesh for you is no mesh at all.<a data-primary="service meshes" data-startref="ix_sermsh" data-type="indexterm" id="idm45664074032000"/></p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45664074030768">
<h1>Summary</h1>
<p>Service meshes contain powerful functionality that adds security and flexibility to your application. At the same time, a service mesh adds complexity to the operations of your cluster and is another potential source of outages for your application. Carefully consider the pros and cons of adding service meshes to your infrastructure. If you have the choice, use a managed service mesh where someone else takes responsibility for the operational details while enabling your applications access to service mesh capabilities.</p>
</div></section>
</div></section></div></body></html>