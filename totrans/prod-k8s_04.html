<html><head></head><body><section data-pdf-bookmark="Chapter 3. Container Runtime" data-type="chapter" epub:type="chapter"><div class="chapter" id="container_runtime_chapter">&#13;
<h1><span class="label">Chapter 3. </span>Container Runtime</h1>&#13;
&#13;
&#13;
<p>Kubernetes is a container orchestrator. Yet, Kubernetes itself does not know how to create, start, and stop containers.<a data-primary="container runtimes" data-type="indexterm" id="ix_cntrrun"/> Instead, it delegates these operations to a pluggable component called the <em>container runtime</em>. The container runtime is a piece of software that creates and manages containers on a cluster node.<a data-primary="Linux" data-secondary="container runtime" data-type="indexterm" id="idm45612000560104"/> In Linux, the container runtime uses a set of kernel primitives such as control groups (cgroups) and namespaces to spawn a process from a container image. In essence, Kubernetes, and more specifically, the kubelet, works together with the container runtime to run containers.</p>&#13;
&#13;
<p>As we discussed in <a data-type="xref" href="ch01.html#chapter1">Chapter 1</a>, organizations building platforms on top of Kubernetes are faced with multiple choices. Which container runtime to use is one such choice. Choice is great as it lets you customize the platform to your needs, enabling innovation and advanced use cases that might otherwise not be possible. However, given the fundamental nature of a container runtime, why does Kubernetes not provide an implementation? Why does it choose to provide a pluggable interface and offload the responsibility to another component?</p>&#13;
&#13;
<p>To answer these questions, we will look back and briefly review the history of containers and how we got here. We will first discuss the advent of containers and how they changed the software development landscape. After all, Kubernetes would probably not exist without them. We will then discuss the Open Container Initiative (OCI), which arose from the need for standardization around container runtimes, images, and other tooling. We will review the OCI specifications and how they pertain to Kubernetes. After OCI, we will discuss the Kubernetes-specific Container Runtime Interface (CRI). The CRI is the bridge between the kubelet and the container runtime. It specifies the interface that the container runtime must implement to be compatible with Kubernetes. Finally, we will discuss how to choose a runtime for your platform and review the available options in the Kubernetes ecosystem.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Advent of Containers" data-type="sect1"><div class="sect1" id="idm45612000555512">&#13;
<h1>The Advent of Containers</h1>&#13;
&#13;
<p>Control groups (cgroups) and namespaces are the primary ingredients necessary to implement containers.<a data-primary="control groups (cgroups)" data-type="indexterm" id="idm45612000553864"/><a data-primary="Namespaces" data-type="indexterm" id="idm45612000553144"/><a data-primary="containers" data-secondary="history of" data-type="indexterm" id="idm45612000552472"/> Cgroups impose limits on the amount of resources a process can use (e.g., CPU, memory, etc.), while namespaces control what a process can see (e.g., mounts, processes, network interfaces, etc.). Both these primitives have been in the Linux kernel since 2008. Even earlier in the case of namespaces. So why did containers, as we know them today, become popular years later?</p>&#13;
&#13;
<p>To answer this question, we first need to consider the environment surrounding the software and IT industry at the time. An initial factor to think about is the complexity of applications. Application developers built applications using service-oriented architectures and even started to embrace microservices. These architectures brought various benefits to organizations, such as maintainability, scalability, and productivity. However, they also resulted in an explosion in the number of components that made up an application. Meaningful applications could easily involve a dozen services, potentially written in multiple languages. As you can imagine, developing and shipping these applications was (and continues to be) complex. Another factor to remember is that software quickly became a business differentiator. The faster you could ship new features, the more competitive your offerings. Having the ability to deploy software in a reliable manner was key to a business. Finally, the emergence of the public cloud as a hosting environment is another important factor. Developers and operations teams had to ensure that applications behaved the same across all environments, from a developer’s laptop to a production server running in someone else’s datacenter.</p>&#13;
&#13;
<p>Keeping these challenges in mind, we can see how the environment was ripe for innovation.<a data-primary="Docker" data-type="indexterm" id="idm45612000548808"/> Enter Docker. Docker made containers accessible to the masses. They built an abstraction that enabled developers to build and run containers with an easy-to-use CLI. Instead of developers having to know the low-level kernel constructs needed to leverage container technology, all they had to do was type <code>docker run</code> in their &#13;
<span class="keep-together">terminal</span>.</p>&#13;
&#13;
<p>While not the answer to all our problems, containers improved many stages of the software development life cycle. First, containers and container images allowed developers to codify the application’s environment. Developers no longer had to wrestle with missing or mismatched application dependencies. Second, containers impacted testing by providing reproducible environments for testing applications. Lastly, containers made it easier to deploy software to production. As long as there was a Docker Engine in the production environment, the application could be deployed with minimum friction. Overall, containers helped organizations to ship software from zero to production in a more repeatable and efficient manner.</p>&#13;
&#13;
<p>The advent of containers also gave birth to an abundant ecosystem full of different tools, container runtimes, container image registries, and more. This ecosystem was well received but introduced a new challenge: How do we make sure that all these container solutions are compatible with each other? After all, the encapsulation and portability guarantees are one of the main benefits of containers. To solve this challenge and to improve the adoption of containers, the industry came together and collaborated on an open source specification under the umbrella of the Linux Foundation: the Open Container Initiative.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Open Container Initiative" data-type="sect1"><div class="sect1" id="idm45612000544184">&#13;
<h1>The Open Container Initiative</h1>&#13;
&#13;
<p>As containers continued to gain popularity across the industry, it became clear that standards and specifications were required to ensure the success of the container movement.<a data-primary="OCI (Open Container Initiative)" data-type="indexterm" id="idm45612000542616"/> The Open Container Initiative (OCI) is an open source project established in 2015 to collaborate on specifications around containers.<a data-primary="Docker" data-secondary="Open Container Initiative" data-type="indexterm" id="idm45612000541576"/> Notable founders of this initiative included Docker, which donated runc to the OCI, and CoreOS, which pushed the needle on container runtimes with rkt.</p>&#13;
&#13;
<p>The OCI includes three specifications: the OCI runtime specification, the OCI image specification, and the OCI distribution specification. These specs enable development and innovation around containers and container platforms such as Kubernetes. Furthermore, the OCI aims to allow end users to use containers in a portable and interoperable manner, enabling them to move between products and solutions more easily when necessary.</p>&#13;
&#13;
<p>In the following sections, we will explore the runtime and image specifications. We will not dig into the distribution specification, as it is primarily concerned with container image registries.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="OCI Runtime Specification" data-type="sect2"><div class="sect2" id="idm45612000538728">&#13;
<h2>OCI Runtime Specification</h2>&#13;
&#13;
<p>The OCI runtime specification determines how to instantiate and run containers in an OCI-compatible fashion.<a data-primary="OCI (Open Container Initiative)" data-secondary="runtime specification" data-type="indexterm" id="ix_OCIrun"/><a data-primary="container runtimes" data-secondary="OCI runtime specification" data-type="indexterm" id="ix_cntrrunOCI"/> First, the specification describes the schema of a container’s configuration. The schema includes information such as the container’s root filesystem, the command to run, the environment variables, the user and group to use, resource limits, and more. The following snippet is a trimmed example of a container configuration file obtained from the OCI runtime specification:</p>&#13;
&#13;
<pre data-type="programlisting">{&#13;
    "ociVersion": "1.0.1",&#13;
    "process": {&#13;
        "terminal": true,&#13;
        "user": {&#13;
            "uid": 1,&#13;
            "gid": 1,&#13;
            "additionalGids": [&#13;
                5,&#13;
                6&#13;
            ]&#13;
        },&#13;
        "args": [&#13;
            "sh"&#13;
        ],&#13;
        "env": [&#13;
            "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin",&#13;
            "TERM=xterm"&#13;
        ],&#13;
        "cwd": "/",&#13;
        ...&#13;
    },&#13;
    ...&#13;
    "mounts": [&#13;
        {&#13;
            "destination": "/proc",&#13;
            "type": "proc",&#13;
            "source": "proc"&#13;
        },&#13;
    ...&#13;
    },&#13;
    ...&#13;
}</pre>&#13;
&#13;
<p>The runtime specification also determines the operations that a container runtime must support. These operations include create, start, kill, delete, and state (which provides information about the container’s state). In addition to the operations, the runtime spec describes the life cycle of a container and how it progresses through different stages. The life cycle stages are (1) <code>creating</code>, which is active when the container runtime is creating the container; (2) <code>created</code>, which is when the runtime has completed the <code>create</code> operation; (3) <code>running</code>, which is when the container process has started and is running; and (4) <code>stopped</code>, which is when the container process has finished.</p>&#13;
&#13;
<p>The OCI project also houses <em>runc</em>, a low-level container runtime that implements the OCI runtime specification.<a data-primary="runc" data-type="indexterm" id="idm45612000528856"/> Other higher-level container runtimes such as Docker, containerd, and CRI-O use runc to spawn containers according to the OCI spec, as shown in <a data-type="xref" href="#docker_engine_containerd_and_other_runtimes_use_runc_to_spawn">Figure 3-1</a>. Leveraging runc enables container runtimes to focus on higher-level features such as pulling images, configuring networking, handling storage, and so on while conforming to the OCI runtime spec.</p>&#13;
&#13;
<figure><div class="figure" id="docker_engine_containerd_and_other_runtimes_use_runc_to_spawn">&#13;
<img alt="prku 0301" src="assets/prku_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Docker Engine, containerd, and other runtimes use runc to spawn containers according to the OCI spec.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="OCI Image Specification" data-type="sect2"><div class="sect2" id="idm45612000524648">&#13;
<h2>OCI Image Specification</h2>&#13;
&#13;
<p>The OCI image specification focuses on the container image.<a data-primary="container runtimes" data-secondary="OCI runtime specification" data-startref="ix_cntrrunOCI" data-type="indexterm" id="idm45612000523240"/><a data-primary="OCI (Open Container Initiative)" data-secondary="runtime specification" data-startref="ix_OCIrun" data-type="indexterm" id="idm45612000521848"/><a data-primary="container images" data-secondary="OCI image specification" data-type="indexterm" id="ix_cntrimg"/><a data-primary="OCI (Open Container Initiative)" data-secondary="image specification" data-type="indexterm" id="ix_OCIimg"/> The specification defines a manifest, an optional image index, a set of filesystem layers, and a configuration. The image manifest describes the image. It includes a pointer to the image’s configuration, a list of image layers, and an optional map of annotations. The following is an example manifest obtained from the OCI image specification:</p>&#13;
&#13;
<pre data-type="programlisting">{&#13;
  "schemaVersion": 2,&#13;
  "config": {&#13;
    "mediaType": "application/vnd.oci.image.config.v1+json",&#13;
    "size": 7023,&#13;
    "digest": "sha256:b5b2b2c507a0944348e0303114d8d93aaaa081732b86451d9bce1f4..."&#13;
  },&#13;
  "layers": [&#13;
    {&#13;
      "mediaType": "application/vnd.oci.image.layer.v1.tar+gzip",&#13;
      "size": 32654,&#13;
      "digest": "sha256:9834876dcfb05cb167a5c24953eba58c4ac89b1adf57f28f2f9d0..."&#13;
    },&#13;
    {&#13;
      "mediaType": "application/vnd.oci.image.layer.v1.tar+gzip",&#13;
      "size": 16724,&#13;
      "digest": "sha256:3c3a4604a545cdc127456d94e421cd355bca5b528f4a9c1905b15..."&#13;
    },&#13;
    {&#13;
      "mediaType": "application/vnd.oci.image.layer.v1.tar+gzip",&#13;
      "size": 73109,&#13;
      "digest": "sha256:ec4b8955958665577945c89419d1af06b5f7636b4ac3da7f12184..."&#13;
    }&#13;
  ],&#13;
  "annotations": {&#13;
    "com.example.key1": "value1",&#13;
    "com.example.key2": "value2"&#13;
  }&#13;
}</pre>&#13;
&#13;
<p>The <em>image index</em> is a top-level manifest that enables the creation of multiplatform container images.<a data-primary="image index" data-type="indexterm" id="idm45612000515272"/> The image index contains pointers to each of the platform-specific manifests. The following is an example index obtained from the specification. Notice how the index points to two different manifests, one for <code>ppc64le/linux</code> and another for <code>amd64/linux</code>:</p>&#13;
&#13;
<pre data-type="programlisting">{&#13;
  "schemaVersion": 2,&#13;
  "manifests": [&#13;
    {&#13;
      "mediaType": "application/vnd.oci.image.manifest.v1+json",&#13;
      "size": 7143,&#13;
      "digest": "sha256:e692418e4cbaf90ca69d05a66403747baa33ee08806650b51fab...",&#13;
      "platform": {&#13;
        "architecture": "ppc64le",&#13;
        "os": "linux"&#13;
      }&#13;
    },&#13;
    {&#13;
      "mediaType": "application/vnd.oci.image.manifest.v1+json",&#13;
      "size": 7682,&#13;
      "digest": "sha256:5b0bcabd1ed22e9fb1310cf6c2dec7cdef19f0ad69efa1f392e9...",&#13;
      "platform": {&#13;
        "architecture": "amd64",&#13;
        "os": "linux"&#13;
      }&#13;
    }&#13;
  ],&#13;
  "annotations": {&#13;
    "com.example.key1": "value1",&#13;
    "com.example.key2": "value2"&#13;
  }&#13;
}</pre>&#13;
&#13;
<p>Each OCI image manifest references a container image configuration. The configuration includes the image’s entry point, command, working directory, environment variables, and more. The container runtime uses this configuration when instantiating a container from the image. The following snippet shows the configuration of a container image, with some fields removed for brevity:</p>&#13;
&#13;
<pre data-type="programlisting">{&#13;
  "architecture": "amd64",&#13;
  "config": {&#13;
    ...&#13;
    "ExposedPorts": {&#13;
      "53/tcp": {},&#13;
      "53/udp": {}&#13;
    },&#13;
    "Tty": false,&#13;
    "OpenStdin": false,&#13;
    "StdinOnce": false,&#13;
    "Env": [&#13;
      "PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"&#13;
    ],&#13;
    "Cmd": null,&#13;
    "Image": "sha256:7ccecf40b555e5ef2d8d3514257b69c2f4018c767e7a20dbaf4733...",&#13;
    "Volumes": null,&#13;
    "WorkingDir": "",&#13;
    "Entrypoint": [&#13;
      "/coredns"&#13;
    ],&#13;
    "OnBuild": null,&#13;
    "Labels": null&#13;
  },&#13;
  "created": "2020-01-28T19:16:47.907002703Z",&#13;
  ...</pre>&#13;
&#13;
<p>The OCI image spec also describes how to create and manage container image layers. <a data-primary="layers (container image)" data-type="indexterm" id="idm45612000509384"/>Layers are essentially TAR archives that include files and directories.<a data-primary="media types for container image layers" data-type="indexterm" id="idm45612000508536"/> The specification defines different media types for layers, including uncompressed layers, gzipped layers, and nondistributable layers. Each layer is uniquely identified by a digest, usually a SHA256 sum of the contents of the layer. As we discussed before, the container image manifest references one or more layers. The references use the SHA256 digest to point to a specific layer. The final container image filesystem is the result of applying each of the layers, as listed in the manifest.</p>&#13;
&#13;
<p>The OCI image specification is crucial because it ensures that container images are portable across different tools and container-based platforms. The spec enables the development of different <a data-primary="build tools for container images" data-type="indexterm" id="idm45612000506536"/>image build tools, such as kaniko and Buildah for userspace container builds, Jib for Java-based containers, and Cloud Native Buildpacks for streamlined and automated builds. (We will explore some of these tools in <a data-type="xref" href="ch15.html#chapter15">Chapter 15</a>). Overall, this specification ensures that Kubernetes can run container images regardless of the tooling used to build them.<a data-primary="OCI (Open Container Initiative)" data-secondary="image specification" data-startref="ix_OCIimg" data-type="indexterm" id="idm45612000504520"/><a data-primary="container images" data-secondary="OCI image specification" data-startref="ix_cntrimg" data-type="indexterm" id="idm45612000503288"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Container Runtime Interface" data-type="sect1"><div class="sect1" id="idm45612000501944">&#13;
<h1>The Container Runtime Interface</h1>&#13;
&#13;
<p>As we’ve discussed in prior chapters, Kubernetes offers many extension points that allow you to build a bespoke application platform. <a data-primary="Container Runtime Interface (CRI)" data-type="indexterm" id="ix_CRI"/>One of the most critical extension points is the Container Runtime Interface (CRI). The CRI was introduced in Kubernetes v1.5 as an effort to enable the growing ecosystem of container runtimes, which included rkt by CoreOS and hypervisor-based runtimes such as Intel’s Clear Containers, which later became Kata Containers.</p>&#13;
&#13;
<p>Prior to CRI, adding support for a new container runtime required a new release of Kubernetes and intimate knowledge of the Kubernetes code base. Once the CRI was established, container runtime developers could simply adhere to the interface to ensure compatibility of the runtime with Kubernetes.</p>&#13;
&#13;
<p>Overall, the goal of the CRI was to abstract the implementation details of the container runtime away from Kubernetes, more specifically the kubelet. This is a classic example of the dependency inversion principle. The kubelet evolved from having container runtime–specific code and if-statements scattered throughout to a leaner implementation that relied on the interface. Thus, the CRI reduced the complexity of the kubelet implementation while also making it more extensible and testable. These are all important qualities of well-designed software.</p>&#13;
&#13;
<p>The CRI is implemented using gRPC and Protocol Buffers. The interface defines two services: the <em>RuntimeService</em> and the <em>ImageService</em>. <a data-primary="RuntimeService" data-type="indexterm" id="idm45612000495688"/><a data-primary="ImageService" data-type="indexterm" id="idm45612000494952"/>The kubelet leverages these services to interact with the container runtime. The RuntimeService is responsible for all the Pod-related operations, including creating Pods, starting and stopping containers, deleting Pods, and so on. The ImageService is concerned with container image operations, including listing, pulling, and removing container images from the node.</p>&#13;
&#13;
<p>While we could detail the APIs of both the RuntimeService and ImageService in this chapter, it is more useful to understand the flow of perhaps the most important operation in Kubernetes: starting a Pod on a node. Thus, let’s explore the interaction between the kubelet and the container runtime through CRI in the following section.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Starting a Pod" data-type="sect2"><div class="sect2" id="idm45612000492904">&#13;
<h2>Starting a Pod</h2>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>The following descriptions are based on Kubernetes v1.18.2 and containerd v1.3.4. These components use the v1alpha2 version of the CRI.</p>&#13;
</div>&#13;
&#13;
<p>Once the Pod is scheduled onto a node, the kubelet works together with the container runtime to start the Pod.<a data-primary="kubelet" data-secondary="starting Pods" data-type="indexterm" id="idm45612000489768"/> As mentioned, the kubelet interacts with the container runtime through the CRI. In this case, we will explore the interaction between the kubelet and the containerd CRI plug-in.</p>&#13;
&#13;
<p>The containerd CRI plug-in <a data-primary="containerd" data-secondary="CRI plug-in" data-type="indexterm" id="idm45612000487864"/>starts a gRPC server that listens on a Unix socket. <a data-primary="Unix socket" data-secondary="for containerd CRI plug-in" data-type="indexterm" id="idm45612000486760"/>By default, this socket is located at <em>/run/containerd/containerd.sock</em>. The kubelet is configured to interact with containerd through this socket with the <code>container-runtime</code> and <code>container-runtime-endpoint</code> command-line flags:</p>&#13;
&#13;
<pre data-type="programlisting">/usr/bin/kubelet&#13;
  --container-runtime=remote&#13;
  --container-runtime-endpoint=/run/containerd/containerd.sock&#13;
  ... other flags here ...</pre>&#13;
&#13;
<p>To start the Pod, the kubelet first creates a Pod sandbox using the <code>RunPodSandbox</code> method of the RuntimeService.<a data-primary="sandbox (Pod), creating" data-type="indexterm" id="idm45612000482648"/> Because a Pod is composed of one or more containers, the sandbox must be created first to establish the Linux network namespace (among other things) for all containers to share. When calling this method, the kubelet sends metadata and configuration to containerd, including the Pod’s name, unique ID, Kubernetes Namespace, DNS configuration, and more. Once the container runtime creates the sandbox, the runtime responds with a Pod sandbox ID that the kubelet uses to create containers in the sandbox.</p>&#13;
&#13;
<p>Once the sandbox is available, the kubelet checks whether the container image is present on the node using the <code>ImageStatus</code> method of the ImageService. The <code>ImageStatus</code> method returns information about the image. When the image is not present, the method returns null and the kubelet proceeds to pull the image. The kubelet uses the <code>PullImage</code> method of the ImageService to pull the image when necessary. Once the runtime pulls the image, it responds with the image SHA256 digest, which the kubelet then uses to create the container.</p>&#13;
&#13;
<p>After creating the sandbox and pulling the image, the kubelet creates the containers in the sandbox using the <code>CreateContainer</code> method of the RuntimeService. The kubelet provides the sandbox ID and the container configuration to the container runtime. The container configuration includes all the information you might expect, including the container image digest, command and arguments, environment variables, volume mounts, etc. During the creation process, the container runtime generates a container ID that it then passes back to the kubelet. This ID is the one you see in the Pod’s status field under container statuses:</p>&#13;
&#13;
<pre data-type="programlisting">containerStatuses:&#13;
  - containerID: containerd://0018556b01e1662c5e7e2dcddb2bb09d0edff6cf6933...&#13;
    image: docker.io/library/nginx:latest</pre>&#13;
&#13;
<p>The kubelet then proceeds to start the container using the <code>StartContainer</code> method of the RuntimeService. When calling this method, it uses the container ID it received from the container runtime.</p>&#13;
&#13;
<p>And that’s it! In this section, we’ve learned how the kubelet interacts with the container runtime using the CRI. We specifically looked at the gRPC methods invoked when starting a Pod, which include those on the ImageService and the RuntimeService. Both of these CRI services provide additional methods that the kubelet uses to complete other tasks. Besides the Pod and container management (i.e., CRUD) methods, the CRI also defines methods to execute a command inside a container (<code>Exec</code> and <code>ExecSync</code>), attach to a container (<code>Attach</code>), forward a specific container port (<code>PortForward</code>), and others.<a data-primary="Container Runtime Interface (CRI)" data-startref="ix_CRI" data-type="indexterm" id="idm45612000473000"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Choosing a Runtime" data-type="sect1"><div class="sect1" id="idm45612000492344">&#13;
<h1>Choosing a Runtime</h1>&#13;
&#13;
<p>Given the availability of the CRI, platform teams get the flexibility of choice when it comes to container runtimes.<a data-primary="container runtimes" data-secondary="choosing a runtime" data-type="indexterm" id="ix_cntrrunch"/> The reality, however, is that over the last couple of years the container runtime has become an implementation detail. If you are using a Kubernetes distribution or leveraging a managed Kubernetes service, the container runtime will most likely be chosen for you. This is the case even for community projects such as Cluster API, which provide prebaked node images that include a container runtime.</p>&#13;
&#13;
<p>With that said, if you do have the option to choose a runtime or have a use case for a specialized runtime (e.g., VM-based runtimes), you should be equipped with information to make that decision.<a data-primary="virtual machines (VMs)" data-secondary="VM-based runtimes" data-type="indexterm" id="idm45612000467592"/> In this section, we will discuss considerations you should make when choosing a container runtime.</p>&#13;
&#13;
<p>The first question we like to ask when helping organizations in the field is which container runtime they have experience with. <a data-primary="Docker" data-secondary="using as container runtime" data-type="indexterm" id="idm45612000465848"/>In most cases, organizations that have a long history with containers are using Docker and are familiar with Docker’s toolchain and user experience. While Kubernetes supports Docker, we discourage its use as it has an extended set of capabilities that Kubernetes does not need, such as building images, creating container networks, and so on. In other words, the fully fledged Docker daemon is too heavy or bloated for the purposes of Kubernetes. The good news is that Docker uses containerd under the covers, one of the most prevalent container runtimes in the community.<a data-primary="containerd" data-secondary="CRI plug-in" data-type="indexterm" id="idm45612000464136"/> The downside is that platform operators have to learn the containerd CLI.</p>&#13;
&#13;
<p>Another consideration to make is the availability of support. Depending on where you are getting Kubernetes from, you might get support for the container runtime. Kubernetes distributions such as VMware’s Tanzu Kubernetes Grid, RedHat’s OpenShift, and others usually ship a specific container runtime. You should stick to that choice unless you have an extremely compelling reason not to. In that case, ensure that you understand the support implications of using a different container runtime.</p>&#13;
&#13;
<p>Closely related to support <a data-primary="testing" data-secondary="conformance testing of container runtime" data-type="indexterm" id="idm45612000461752"/>is conformance testing of the container runtime. The Kubernetes project, specifically the Node Special Interest Group (sig-node), defines a set of CRI validation tests and node conformance tests to ensure container runtimes are compatible and behave as expected.<a data-primary="Node Special Interest Group" data-type="indexterm" id="idm45612000460280"/> These tests are part of every Kubernetes release, and some runtimes might have more coverage than others. As you can imagine, the more test coverage the better, as any issues with the runtime are caught during the Kubernetes release process.<a data-primary="Kubernetes Test Grid" data-type="indexterm" id="idm45612000459160"/> The community makes all tests and results available through the <a href="https://k8s-testgrid.appspot.com">Kubernetes Test Grid</a>. When choosing a runtime, you should consider the container runtime’s conformance tests and more broadly, the runtime’s relationship with the overall Kubernetes project.</p>&#13;
&#13;
<p>Lastly, you should determine if your workloads need stronger isolation guarantees than those provided by Linux containers.<a data-primary="workloads" data-secondary="isolation guarantees" data-type="indexterm" id="idm45612000456856"/><a data-primary="virtual machines (VMs)" data-secondary="VM-level isolation for workloads" data-type="indexterm" id="idm45612000455880"/> While less common, there are use cases that require VM-level isolation for workloads, such as executing untrusted code or running applications that require strong multitenancy guarantees. In these cases, you can leverage specialized runtimes such as Kata Containers.</p>&#13;
&#13;
<p>Now that we have discussed the considerations you should make when choosing a runtime, let’s review the most prevalent container runtimes: Docker, containerd, and CRI-O. We will also explore Kata Containers to understand how we can run Pods in VMs instead of Linux Containers. Finally, while not a container runtime or component that implements CRI, we will learn about Virtual Kubelet, as it provides another way to run workloads on Kubernetes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Docker" data-type="sect2"><div class="sect2" id="idm45612000453528">&#13;
<h2>Docker</h2>&#13;
&#13;
<p>Kubernetes supports the Docker Engine as a container runtime through a CRI shim called the <em>dockershim</em>. <a data-primary="container runtimes" data-secondary="choosing a runtime" data-tertiary="Docker" data-type="indexterm" id="idm45612000451272"/><a data-primary="dockershim" data-type="indexterm" id="idm45612000449992"/><a data-primary="Docker" data-secondary="using as container runtime" data-type="indexterm" id="idm45612000449320"/><a data-primary="kubelet" data-secondary="dockershim" data-type="indexterm" id="idm45612000448360"/>The shim is a component that’s built into the kubelet. Essentially, it is a gRPC server that implements the CRI services we described earlier in this chapter. The shim is required because the Docker Engine does not implement the CRI. Instead of special-casing all the kubelet code paths to work with both the CRI and the Docker Engine, the dockershim serves as a facade that the kubelet can use to communicate with Docker via the CRI. The dockershim handles the translation between CRI calls to Docker Engine API calls. <a data-type="xref" href="#interaction_between_the_kubelet_and_docker_engine_via_the_dockershim">Figure 3-2</a> depicts how the kubelet interacts with Docker through the shim.</p>&#13;
&#13;
<figure><div class="figure" id="interaction_between_the_kubelet_and_docker_engine_via_the_dockershim">&#13;
<img alt="prku 0302" src="assets/prku_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>Interaction between the kubelet and Docker Engine via the dockershim.</h6>&#13;
</div></figure>&#13;
&#13;
<p>As we mentioned earlier in the chapter, Docker leverages containerd under the hood.<a data-primary="containerd" data-secondary="use by Docker" data-type="indexterm" id="idm45612000443336"/> Thus, the incoming API calls from the kubelet are eventually relayed to containerd, which starts the containers. In the end, the spawned container ends up under containerd and not the Docker daemon:</p>&#13;
&#13;
<pre data-type="programlisting">systemd&#13;
  └─containerd&#13;
      └─containerd-shim -namespace moby -workdir ...&#13;
          └─nginx&#13;
              └─nginx</pre>&#13;
&#13;
<p>From a troubleshooting perspective, you can use the Docker CLI to list and inspect containers running on a given node.<a data-primary="CLIs (command-line interfaces)" data-secondary="using Docker CLI to inspect containers" data-type="indexterm" id="idm45612000440456"/> While Docker does not have the concept of Pods, the dockershim encodes the Kubernetes Namespace, Pod name, and Pod ID into the name of containers.<a data-primary="Pods" data-secondary="inspecting using Docker CLI" data-type="indexterm" id="idm45612000439208"/> For example, the following listing shows the containers that belong to a Pod called <code>nginx</code> in the <code>default</code> namespace. The Pod infrastructure container (aka, pause container) is the one with the <code>k8s_POD_</code> prefix in the name:</p>&#13;
&#13;
<pre data-type="programlisting">$ docker ps --format='{{.ID}}\t{{.Names}}' | grep nginx_default&#13;
3c8c01f47424	k8s_nginx_nginx_default_6470b3d3-87a3-499c-8562-d59ba27bced5_3&#13;
c34ad8d80c4d	k8s_POD_nginx_default_6470b3d3-87a3-499c-8562-d59ba27bced5_3</pre>&#13;
&#13;
<p>You can also<a data-primary="CLIs (command-line interfaces)" data-secondary="containerd CLI (ctr), using to inspect containers" data-type="indexterm" id="idm45612000435432"/><a data-primary="containerd" data-secondary="using ctr CLI to inspect containers" data-type="indexterm" id="idm45612000434488"/><a data-primary="command-line interfaces" data-see="CLIs" data-type="indexterm" id="idm45612000433528"/> use the containerd CLI, <code>ctr</code>, to inspect containers, although the output is not as user friendly as the Docker CLI output. The Docker Engine uses a containerd namespace called <code>moby</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ ctr --namespace moby containers list&#13;
CONTAINER                    IMAGE    RUNTIME&#13;
07ba23a409f31bec7f163a...    -        io.containerd.runtime.v1.linux&#13;
0bfc5a735c213b9b296dad...    -        io.containerd.runtime.v1.linux&#13;
2d1c9cb39c674f75caf595...    -        io.containerd.runtime.v1.linux&#13;
...</pre>&#13;
&#13;
<p>Finally, you <a data-primary="crictl command-line tool" data-type="indexterm" id="idm45612000429736"/>can use <code>crictl</code> if available on the node.<a data-primary="CLIs (command-line interfaces)" data-secondary="crictl, using to inspect containers" data-type="indexterm" id="idm45612000428440"/> The <code>crictl</code> utility is a command-line tool developed by the Kubernetes community. It is a CLI client for interacting with container runtimes over the CRI.<a data-primary="Unix socket" data-secondary="dockershim, using crictl with" data-type="indexterm" id="idm45612000426680"/> Even though Docker does not implement the CRI, you can use <code>crictl</code> with the dockershim Unix socket:</p>&#13;
&#13;
<pre data-type="programlisting">$ crictl --runtime-endpoint unix:///var/run/dockershim.sock ps --name nginx&#13;
CONTAINER ID   IMAGE                CREATED        STATE    NAME   POD ID&#13;
07ba23a409f31  nginx@sha256:b0a...  3 seconds ago  Running  nginx  ea179944...</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="containerd" data-type="sect2"><div class="sect2" id="idm45612000452904">&#13;
<h2>containerd</h2>&#13;
&#13;
<p>containerd is perhaps the most common container runtime we encounter when building Kubernetes-based platforms in the field.<a data-primary="container runtimes" data-secondary="choosing a runtime" data-tertiary="containerd" data-type="indexterm" id="idm45612000422456"/><a data-primary="containerd" data-type="indexterm" id="idm45612000421208"/> At the time of writing, containerd is the default container runtime in Cluster API-based node images and is available across various managed Kubernetes offerings (e.g., AKS, EKS, and GKE).</p>&#13;
&#13;
<p>The containerd container runtime implements the CRI through the containerd CRI plug-in.<a data-primary="Container Runtime Interface (CRI)" data-secondary="containerd plug-in" data-type="indexterm" id="idm45612000419816"/> The CRI plug-in is a native containerd plug-in that is available since containerd v1.1 and is enabled by default. containerd exposes its gRPC APIs over a Unix socket at <em>/run/containerd/containerd.sock</em>. The<a data-primary="kubelet" data-secondary="interaction with containerd" data-type="indexterm" id="idm45612000418072"/> kubelet uses this socket to interact with containerd when it comes to running Pods, as depicted in <a data-type="xref" href="#interaction_between_the_kubelet_and_containerd_through_the_containerd_cri_plugin">Figure 3-3</a>.</p>&#13;
&#13;
<figure><div class="figure" id="interaction_between_the_kubelet_and_containerd_through_the_containerd_cri_plugin">&#13;
<img alt="prku 0303" src="assets/prku_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>Interaction between the kubelet and containerd through the containerd CRI plug-in.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The process tree of spawned containers looks exactly the same as the process tree when using the Docker Engine. This is expected, as the Docker Engine uses containerd to manage containers:</p>&#13;
&#13;
<pre data-type="programlisting">systemd&#13;
  └─containerd&#13;
      └─containerd-shim -namespace k8s.io -workdir ...&#13;
          └─nginx&#13;
              └─nginx</pre>&#13;
&#13;
<p>To inspect <a data-primary="CLIs (command-line interfaces)" data-secondary="containerd CLI (ctr), using to inspect containers" data-type="indexterm" id="idm45612000412024"/>containers on a node, you can use <code>ctr</code>, the containerd CLI. As opposed to Docker, the containers managed by Kubernetes are in a containerd namespace called <code>k8s.io</code> instead of <code>moby</code>:</p>&#13;
&#13;
<pre data-type="programlisting">$ ctr --namespace k8s.io containers ls | grep nginx&#13;
c85e47fa...    docker.io/library/nginx:latest    io.containerd.runtime.v1.linux</pre>&#13;
&#13;
<p>You can also<a data-primary="crictl command-line tool" data-secondary="using to inspect containers" data-type="indexterm" id="idm45612000408152"/> use the <code>crictl</code> CLI to interact with containerd through the containerd unix socket:</p>&#13;
&#13;
<pre data-type="programlisting">$ crictl --runtime-endpoint unix:///run/containerd/containerd.sock ps&#13;
  --name nginx&#13;
CONTAINER ID    IMAGE          CREATED         STATE      NAME   POD ID&#13;
c85e47faf3616   4bb46517cac39  39 seconds ago  Running    nginx  73caea404b92a</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CRI-O" data-type="sect2"><div class="sect2" id="idm45612000423704">&#13;
<h2>CRI-O</h2>&#13;
&#13;
<p>CRI-O is a container runtime specifically designed for Kubernetes.<a data-primary="CRI-O" data-type="indexterm" id="idm45612000403960"/><a data-primary="container runtimes" data-secondary="choosing a runtime" data-tertiary="CRI-O" data-type="indexterm" id="idm45612000403176"/> As you can probably tell from the name, it is an implementation of the CRI. Thus, in contrast to Docker and containerd, it does not cater to uses outside of Kubernetes. At the time of writing, one of the primary consumers of the CRI-O container runtime is the RedHat OpenShift platform.</p>&#13;
&#13;
<p>Similar to containerd, CRI-O exposes the CRI over a Unix socket.<a data-primary="Unix socket" data-secondary="CRI-O exposing the CRI over" data-type="indexterm" id="idm45612000401144"/><a data-primary="kubelet" data-secondary="interaction with CRI-O using CRI APIs" data-type="indexterm" id="idm45612000400120"/> The kubelet uses the socket, typically located at <em>/var/run/crio/crio.sock</em>, to interact with CRI-O. <a data-type="xref" href="#interaction_between_the_kubelet_and_crio_using_the_cri_apis">Figure 3-4</a> depicts the kubelet interacting directly with CRI-O through the CRI.</p>&#13;
&#13;
<figure><div class="figure" id="interaction_between_the_kubelet_and_crio_using_the_cri_apis">&#13;
<img alt="prku 0304" src="assets/prku_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Interaction between the kubelet and CRI-O using the CRI APIs.</h6>&#13;
</div></figure>&#13;
&#13;
<p>When spawning containers, CRI-O instantiates a process called <em>conmon</em>. Conmon is a container monitor.<a data-primary="conmon process" data-type="indexterm" id="idm45612000394664"/> It is the parent of the container process and handles multiple concerns, such as exposing a way to attach to the container, storing the container’s STDOUT and STDERR streams to logfiles, and handling container termination:</p>&#13;
&#13;
<pre class="pagebreak-before" data-type="programlisting">systemd&#13;
  └─conmon -s -c ed779... -n k8s_nginx_nginx_default_e9115... -u8cdf0c...&#13;
      └─nginx&#13;
          └─nginx</pre>&#13;
&#13;
<p>Because CRI-O was designed as a low-level component for Kubernetes, the CRI-O project does not provide a CLI.<a data-primary="CLIs (command-line interfaces)" data-secondary="using crictl with CRI-O" data-type="indexterm" id="idm45612000391976"/><a data-primary="crictl command-line tool" data-secondary="using with CRI-O" data-type="indexterm" id="idm45612000390936"/> With that said, you can use <code>crictl</code> with CRI-O as with any other container runtime that implements the CRI:</p>&#13;
&#13;
<pre data-type="programlisting">$ crictl --runtime-endpoint unix:///var/run/crio/crio.sock ps --name nginx&#13;
CONTAINER   IMAGE                 CREATED         STATE     NAME    POD ID&#13;
8cdf0c...   nginx@sha256:179...   2 minutes ago   Running   nginx   eabf15237...</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kata Containers" data-type="sect2"><div class="sect2" id="idm45612000388408">&#13;
<h2>Kata Containers</h2>&#13;
&#13;
<p>Kata Containers is an open source, specialized runtime that uses lightweight VMs instead of containers to run workloads.<a data-primary="container runtimes" data-secondary="choosing a runtime" data-tertiary="Kata Containers" data-type="indexterm" id="idm45612000386664"/><a data-primary="Kata Containers" data-type="indexterm" id="idm45612000385416"/><a data-primary="virtual machines (VMs)" data-secondary="use by Kata Containers" data-type="indexterm" id="idm45612000384744"/> The project has a rich history, resulting from the merge of two prior VM-based runtimes: Clear Containers from Intel and RunV from Hyper.sh.</p>&#13;
&#13;
<p>Due to the use of VMs, Kata provides stronger isolation guarantees than Linux containers. If you have security requirements that prevent workloads from sharing a Linux kernel or resource guarantee requirements that cannot be met by cgroup isolation, Kata Containers can be a good fit. For example, a common use case for Kata containers is to run multitenant Kubernetes clusters that run untrusted code. Cloud providers such as <a href="https://oreil.ly/btDL9">Baidu Cloud</a> and <a href="https://oreil.ly/Mzarh">Huawei Cloud</a> use Kata Containers in their cloud infrastructure.</p>&#13;
&#13;
<p>To use Kata Containers with Kubernetes, there <a data-primary="kubelet" data-secondary="interaction with Kata Containers through containerd" data-type="indexterm" id="idm45612000380632"/>is still a need for a pluggable container runtime to sit between the kubelet and the Kata runtime, as shown in <a data-type="xref" href="#interaction_between_the_kubelet_and_kata_containers_through_containerd">Figure 3-5</a>. The reason is that Kata Containers does not implement the CRI. Instead, it leverages existing container runtimes such as containerd to handle the interaction with Kubernetes.<a data-primary="containerd" data-secondary="handling interaction between kubelet and Kata Containers" data-type="indexterm" id="idm45612000378264"/> To integrate with containerd, the Kata Containers project implements the containerd runtime API, specifically the <a href="https://oreil.ly/DxGyZ">v2 containerd-shim API</a>.</p>&#13;
&#13;
<figure><div class="figure" id="interaction_between_the_kubelet_and_kata_containers_through_containerd">&#13;
<img alt="prku 0305" src="assets/prku_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Interaction between the kubelet and Kata Containers through containerd.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Because containerd is required and available on the nodes, it is possible to run Linux container Pods and VM-based Pods on the same node.<a data-primary="container runtimes" data-secondary="configuring and running multiple" data-type="indexterm" id="idm45612000373656"/> Kubernetes provides a mechanism to configure and run multiple container runtimes called Runtime Class.<a data-primary="RuntimeClass API" data-type="indexterm" id="idm45612000372424"/> Using the RuntimeClass API, you can offer different runtimes in the same Kubernetes platform, enabling developers to use the runtime that better fits their needs. The following snippet is an example RuntimeClass for the Kata Containers runtime:</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">node.k8s.io/v1beta1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">RuntimeClass</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">kata-containers</code>&#13;
<code class="nt">handler</code><code class="p">:</code> <code class="l-Scalar-Plain">kata</code></pre>&#13;
&#13;
<p>To run a Pod under the <code>kata-containers</code> runtime, developers must specify the runtime class name in their Pod’s specification:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">kata-example</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">containers</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
  <code class="nt">runtimeClassName</code><code class="p">:</code> <code class="l-Scalar-Plain">kata-containers</code></pre>&#13;
&#13;
<p>Kata Containers supports<a data-primary="hypervisors" data-secondary="Kata Containers support for" data-type="indexterm" id="idm45612000329208"/> different hypervisors to run workloads, including <a href="https://www.qemu.org">QEMU</a>, <a href="https://github.com/intel/nemu">NEMU</a>, and <a href="https://firecracker-microvm.github.io">AWS Firecracker</a>. When using QEMU, for example, we can see a QEMU process after launching a Pod that uses the <code>kata-containers</code> runtime class:</p>&#13;
&#13;
<pre data-type="programlisting">$ ps -ef | grep qemu&#13;
root     38290     1  0 16:02 ?        00:00:17&#13;
   /snap/kata-containers/690/usr/bin/qemu-system-x86_64&#13;
   -name sandbox-c136a9addde4f26457901ccef9de49f02556cc8c5135b091f6d36cfc97...&#13;
   -uuid aaae32b3-9916-4d13-b385-dd8390d0daf4&#13;
   -machine pc,accel=kvm,kernel_irqchip&#13;
   -cpu host&#13;
   -m 2048M,slots=10,maxmem=65005M&#13;
   ...</pre>&#13;
&#13;
<p>While Kata Containers provides interesting capabilities, we consider it a niche and have not seen it used in the field. With that said, if you need VM-level isolation guarantees in your Kubernetes cluster, Kata Containers is worth looking into.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Virtual Kubelet" data-type="sect2"><div class="sect2" id="idm45612000387784">&#13;
<h2>Virtual Kubelet</h2>&#13;
&#13;
<p><a href="https://github.com/virtual-kubelet/virtual-kubelet">Virtual Kubelet</a> is an open source project that behaves like a kubelet but offers a pluggable API on the backend. <a data-primary="container runtimes" data-secondary="choosing a runtime" data-tertiary="Virtual Kubelet, using to surface alternative runtimes" data-type="indexterm" id="idm45612000303944"/><a data-primary="Virtual Kubelet" data-type="indexterm" id="idm45612000302664"/>While not a container runtime per se, its main purpose is to surface alternative runtimes to run Kubernetes Pods. <a data-primary="Pods" data-secondary="Virtual Kubelet running Pods" data-type="indexterm" id="idm45612000301736"/>Because of the Virtual Kubelet’s extensible architecture, these alternative runtimes can essentially be any systems that can run an application, such as serverless frameworks, edge frameworks, etc. For example, as shown in <a data-type="xref" href="#virtual_kubelet_running_pods_on_a_cloud_service_such_as_azure_container_instances_aws_fargate_etc">Figure 3-6</a>, the Virtual Kubelet can launch Pods on a cloud service such as Azure Container Instances or AWS Fargate.</p>&#13;
&#13;
<figure><div class="figure" id="virtual_kubelet_running_pods_on_a_cloud_service_such_as_azure_container_instances_aws_fargate_etc">&#13;
<img alt="prku 0306" src="assets/prku_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>Virtual Kubelet running Pods on a cloud service, such as Azure Container Instances, AWS Fargate, etc.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The Virtual Kubelet community offers a variety of providers that you can leverage if they fit your need, including AWS Fargate, Azure Container Instances, HashiCorp Nomad, and others. If you have a more specific use case, you can implement your own provider as well. Implementing a provider involves writing a Go program using the Virtual Kubelet libraries to handle the integration with Kubernetes, including node registration, running Pods, and exporting APIs expected by Kubernetes.</p>&#13;
&#13;
<p>Even though Virtual Kubelet enables interesting scenarios, we have yet to run into a use case that needed it in the field. With that said, it is useful to know that it exists, and you should keep it in your Kubernetes toolbox.<a data-primary="container runtimes" data-secondary="choosing a runtime" data-startref="ix_cntrrunch" data-type="indexterm" id="idm45612000295752"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45612000471336">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>The container runtime is a foundational component of a Kubernetes-based platform. After all, it is impossible to run containerized workloads without a container runtime. As we learned in this chapter, Kubernetes uses the Container Runtime Interface (CRI) to interact with the container runtime. One of the main benefits of the CRI is its pluggable nature, which allows you to use the container runtime that best fits your needs. To give you an idea of the different container runtimes available in the ecosystem, we discussed those that we typically see in the field, such as Docker, containerd, etc. Learning about the different options and further exploring their capabilities should help you select the container runtime that satisfies the requirements of your application platform.<a data-primary="container runtimes" data-startref="ix_cntrrun" data-type="indexterm" id="idm45612000292408"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>