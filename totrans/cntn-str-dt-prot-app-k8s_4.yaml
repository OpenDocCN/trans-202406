- en: Chapter 3\. Kubernetes Performance and Security
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Kubernetes deployments become larger and more prevalent, the enterprise is
    turning its focus to performance and security, which are important for both business
    continuity and cost control. Performance helps save cost by maximizing infrastructure
    usage and preventing unnecessary resource scaling. Security helps control cost
    and ensure business continuity by protecting assets, including customer data and
    other proprietary information.
  prefs: []
  type: TYPE_NORMAL
- en: 'What Hasn’t Changed: Performance, Availability, and Security Requirements'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As applications have evolved from monolithic on-premises deployments, to virtual
    machines, to software as a service (SaaS) and platform as a service (PaaS) offerings
    running in the cloud, to modern containerized cloud native applications, business
    requirements haven’t changed. Enterprise applications running on Kubernetes must
    meet nonnegotiable business requirements such as high availability, data protection,
    and strict performance metrics, all while operating across hybrid clouds or multiple
    data centers.
  prefs: []
  type: TYPE_NORMAL
- en: Performance requires using resources appropriately, ensuring that there is enough
    capability to serve the needs of the business without overprovisioning or overspending.
    Availability involves taking hardware failures and network latency in stride,
    and meeting SLAs without blinking an eye. Security means protecting sensitive
    resources and data through encryption, access controls, and defense in depth while
    making them available to people and processes with a legitimate business case
    for access.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional availability mostly meant uptime monitoring and alerts. Making sure
    the application frontend was running was not only sufficient to identify downtime,
    it was often the only available sign of a problem. Fortunately, monolithic software
    was comparatively simple, with predictable user interactions. End-to-end request
    monitoring was unimportant, and might have seemed absurd to IT teams at the time.
    Performance was less of a concern; as long as applications were up and behaving
    properly, IT and site reliability teams were happy.
  prefs: []
  type: TYPE_NORMAL
- en: Performance, availability, and security must now be managed at the container
    level.
  prefs: []
  type: TYPE_NORMAL
- en: The services that make up today’s containerized applications are highly interdependent,
    meaning that the failure of one can take down part or all of a highly complex,
    mission-critical application. Keeping applications running properly involves maintaining
    multiple instances of critical services, load-balancing among them, and moving
    or restarting them when failures occur.
  prefs: []
  type: TYPE_NORMAL
- en: High Availability at a Global Scale
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Global high availability is a nonnegotiable business requirement for the enterprise.
    This must go beyond keeping individual clusters running when a few control plane
    nodes or worker nodes fail. Application state, including configuration and distributed
    data, must be available without interruption everywhere it’s expected. The bottom
    line is that cloud native high availability requires awareness of each application’s
    persistent data, metadata, configuration, and running state.
  prefs: []
  type: TYPE_NORMAL
- en: High availability within a cluster can be as simple as replicating containers,
    volumes, and Kubernetes services so that at least a minimum number of instances
    of each is available continually. The goal is to ensure that there is no single
    point of failure. High availability across local or geographical areas involves
    replicating data and applications among separate data centers, either synchronously
    or asynchronously. These strategies permit the enterprise to choose strategies
    that balance the cost of hardware or cloud resources against the goals of the
    business.
  prefs: []
  type: TYPE_NORMAL
- en: The key to high availability, whether local or global, is that the storage and
    data replication mechanisms are application consistent, meaning that they capture
    all the data and metadata necessary to keep services running or to restart them
    without interruption to the business flow. This includes not only persistent data
    but application configuration and running state as well. Simply backing up VMs
    or making copies of volumes is not enough, because they only capture part of the
    data any given service or application needs to continue running. Furthermore,
    because individual volumes and containers aren’t guaranteed to last, there must
    be a way to identify applications and data so that the replacement instances can
    be associated with each other correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Data Mobility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One key to high availability is *data mobility*, defined as the ability to move
    or replicate data quickly between clusters in a single data center or cloud, or
    between clouds. This capability supports not only high availability, but also
    upgrades, migrations, scaling, and disaster recovery.
  prefs: []
  type: TYPE_NORMAL
- en: As the enterprise embraces the cloud, the importance of data mobility becomes
    more and more apparent. It’s clear that a single deployment environment or provider
    is no longer sufficient for all business needs. Organizations need the flexibility
    to make trade-offs between public and private clouds, on-premises servers, and
    edge data centers as needed to increase agility, meet regulatory requirements
    for data locality, deliver better service, and keep costs under control.
  prefs: []
  type: TYPE_NORMAL
- en: As Kubernetes comes into play, it’s crucial to be able to move large volumes
    of data among diverse multicloud and hybrid cloud environments. While Kubernetes
    makes it easy to deploy applications anywhere, it has long been more difficult
    to move the underlying data. What is needed is an approach that makes it as easy
    to move persistent volumes as it is to migrate application containers.
  prefs: []
  type: TYPE_NORMAL
- en: Container-native storage is half the battle. By providing data and storage management
    across infrastructure types and providers, a container-native storage pool provides
    a home for data in any deployment environment. But traditional data migration
    and replication approaches, which are time-consuming and operationally complex,
    aren’t up to the task of migrating data when required. What’s needed is a container-aware
    data orchestration layer that can provide the declarative automation for state
    that Kubernetes provides for applications themselves. In cases where an application
    moves, it’s often not practical to move all application data on short notice.
    Data mobility must include the capability of moving the most immediately important
    data first, to allow the application to start working in its new location quickly
    by minimizing the amount of data that must be moved.
  prefs: []
  type: TYPE_NORMAL
- en: Data mobility enables the enterprise to improve operations in a number of ways.
    By moving low-priority applications to auxiliary clusters, the enterprise can
    free up capacity on critical clusters, making room for additional data or replication.
    Automating the movement of data makes it easier to test new versions, promoting
    workloads from development to staging clusters or maintaining two or more live
    environments. When moving applications and data among environments or taking a
    cluster offline to perform hardware maintenance and upgrades, data mobility is
    key to continuous availability.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning Kubernetes Data for Enterprise-Scale Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As the enterprise drives cloud native applications to greater and greater scale,
    tuning Kubernetes data is increasingly important to balance performance, cost,
    and availability. Although there are many factors that affect the performance
    of a Kubernetes cluster, one of the most important is to make the right choices
    about data. There are two significant considerations: overall storage configuration
    and data placement.'
  prefs: []
  type: TYPE_NORMAL
- en: Storage configuration means determining what kind of storage hardware you need
    and how you set it up. You can set up a single storage pool or multiple pools
    that each define their own StorageClass, for example. You’ll need to think about
    cost versus capacity and speed, determine how much memory and CPU to allow for
    use by the storage layer, and how many different types of storage you will need.
    These questions are especially important in on-premises data center environments,
    where the decisions have ramifications that can last for years. In cloud or hybrid
    environments, there is more flexibility to start small and grow or change as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Data placement*, or *data topology*, refers to strategies for managing where
    data is stored, usually to meet either performance or availability goals. For
    maximum performance, one strategy is *hyperconvergence*, which keeps a workload
    and its data together in a single node ([Figure 3-1](#fig_1_hyperconverged_topology)).
    If the node goes down, the data is lost, but that might be acceptable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hyperconverged topology](Images/csdp_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Hyperconverged topology
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In hyperconverged topologies, storage is usually a software-defined layer, often
    on commodity servers shared by the compute nodes. Hyperconvergence is very flexible,
    allowing storage to scale in step with the compute workload. However, because
    the failure of a single node affects not only the workload on that server but
    the storage system as well, hyperconverged topologies can be more difficult to
    manage from the point of view of both operations and security. Storing additional
    replicas of the data on other nodes allows a new instance of the workload to recover
    to a true hyperconverged topology after a failure, starting from the most recent
    data written to the replicas.
  prefs: []
  type: TYPE_NORMAL
- en: For higher availability, data can be separated from workloads. Keeping compute
    and storage nodes separate (called *disaggregation*) provides slower performance
    but prevents loss in the event of a node failure. For a very dynamic environment,
    where the number of compute nodes increases and decreases in response to workload
    demand, one strategy is to separate compute and storage into their own clusters
    ([Figure 3-2](#fig_2_disaggregated_topology)). This way, scaling and management
    operations in the compute cluster don’t interfere with the storage cluster, and
    vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '![Disaggregated topology](Images/csdp_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Disaggregated topology
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Keeping the Cluster Secure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Modern security practices use a *defense in depth* strategy, which means that
    multiple layers of security controls work together, providing redundancy so that
    a breach in one layer doesn’t grant access to a critical system. Cloud native
    security is no different: protections at the levels of the cloud, the cluster,
    the container, and the code itself build upon each other to protect applications
    and data from unauthorized access.'
  prefs: []
  type: TYPE_NORMAL
- en: The environment is the first layer of defense. If the data center or cloud is
    secure, it is easier to protect the cluster and its services. The cluster, in
    turn, must provide security mechanisms at the level of the physical node, the
    VM, the container, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: At the container level, services must be able to communicate securely with SDS.
    Kubernetes provides mechanisms for controlling the access privileges of containers,
    limiting resource usage, and preventing containers from taking dangerous or unwanted
    actions. The storage layer must do its part by encrypting data both at rest and
    in motion, restricting access to specific clients, and using application and API
    knowledge to permit only sensible data transactions.
  prefs: []
  type: TYPE_NORMAL
