<html><head></head><body><section data-pdf-bookmark="Chapter 14. Application Considerations" data-type="chapter" epub:type="chapter"><div class="chapter" id="application_considerations_chapter">&#13;
<h1><span class="label">Chapter 14. </span>Application Considerations</h1>&#13;
&#13;
&#13;
<p>Kubernetes is rather flexible when it comes to the type of applications it can run and manage.<a data-primary="applications on Kubernetes" data-type="indexterm" id="ix_appsK"/> Barring operating system and processor type limitations, Kubernetes can essentially run anything. Large monoliths, distributed microservices, batch workloads, you name it. The only requirement that Kubernetes imposes on workloads is that they are distributed as container images. With that said, there are certain steps you can take to make your applications better Kubernetes citizens.</p>&#13;
&#13;
<p>In this chapter, we will pivot our discussions to focus on the application instead of the platform. If you are part of a platform team, don’t skip this chapter. While you might think it only applies to developers, it also applies to you. As a platform team member, you will most likely get to build applications to provide custom services on your platform. Even if you don’t, the discussions in this chapter will help you better align with development teams consuming the platform, and even educate those teams that might be unfamiliar with container-based platforms.</p>&#13;
&#13;
<p>This chapter covers various considerations you should make when running applications on Kubernetes.<a data-primary="applications on Kubernetes" data-secondary="considerations" data-type="indexterm" id="idm45611973322376"/> Mainly:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Deploying applications onto the platform, and mechanisms to manage deployment manifests, such as templating and packaging.</p>&#13;
</li>&#13;
<li>&#13;
<p>Approaches to configure applications, such as using Kubernetes APIs (ConfigMaps/Secrets), and integrating with external systems for config and secret &#13;
<span class="keep-together">management</span>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Kubernetes features that improve the availability of your workloads, such as pre-stop container hooks, graceful termination, and scheduling constraints.</p>&#13;
</li>&#13;
<li>&#13;
<p>State probes, a feature of Kubernetes that enables you to surface application health information to the platform.</p>&#13;
</li>&#13;
<li>&#13;
<p>Resource requests and limits, which are critical to ensure your applications run properly on the platform.</p>&#13;
</li>&#13;
<li>&#13;
<p>Logs, metrics, and tracing as mechanisms to debug, troubleshoot, and operate your workloads effectively.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deploying Applications to Kubernetes" data-type="sect1"><div class="sect1" id="idm45611973440376">&#13;
<h1>Deploying Applications to Kubernetes</h1>&#13;
&#13;
<p>Once your application is containerized and available in a container image registry, you are ready to deploy it onto Kubernetes.<a data-primary="deployments" data-secondary="deploying applications to Kubenetes" data-type="indexterm" id="ix_depapps"/><a data-primary="applications on Kubernetes" data-secondary="deploying applications to Kubernetes" data-type="indexterm" id="ix_appsKdep"/><a data-primary="resources" data-secondary="required to run apps, manifests for" data-type="indexterm" id="idm45611973436136"/> In most cases, deploying the application involves writing YAML manifests that describe the Kubernetes resources required to run the app, such as Deployments, Services, ConfigMaps, CRDs, etc.<a data-primary="API server" data-secondary="application deployments and" data-type="indexterm" id="idm45611973434840"/> Then, you send the manifests to the API server, and Kubernetes takes care of the rest. Using raw YAML manifests is a great way to get started, but it can quickly become impractical, especially when deploying the application onto different clusters or environments.<a data-primary="deployments" data-secondary="deploying applications to Kubenetes" data-tertiary="common questions about" data-type="indexterm" id="idm45611973433464"/> You will most likely encounter questions similar to the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How do I provide different credentials when running in staging versus &#13;
<span class="keep-together">production</span>?</p>&#13;
</li>&#13;
<li>&#13;
<p>How can I use a different image registry when deploying in various datacenters?</p>&#13;
</li>&#13;
<li>&#13;
<p>How do I set different replica counts in development versus production?</p>&#13;
</li>&#13;
<li>&#13;
<p>How can I ensure all port numbers match up across the different manifests?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The list goes on and on. And while you could have multiple sets of manifests to solve for each of these concerns, the permutations make it rather challenging to manage. In this section, we will discuss approaches you can take to address the issue of manifest management. Mainly, we will cover templating manifests and packaging applications for Kubernetes. We will not, however, discuss the gamut of tools available in the community. More often than not, we find that teams get stuck in analysis paralysis when considering the different options. Our advice is to choose <em>something</em> and move on to solving higher-value concerns.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Templating Deployment Manifests" data-type="sect2"><div class="sect2" id="idm45611973425960">&#13;
<h2>Templating Deployment Manifests</h2>&#13;
&#13;
<p>Templating involves introducing placeholders in your deployment manifests. Instead of hardcoding values<a data-primary="deployments" data-secondary="deploying applications to Kubenetes" data-tertiary="templating deployment manifests" data-type="indexterm" id="idm45611973424552"/> in the manifests, the placeholders provide a mechanism for you to inject values as necessary. For example, the following templated manifest enables you to set replica counts to different values. Perhaps you need one replica in development, but five in production.</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">apps/v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Deployment</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">labels</code><code class="p">:</code>&#13;
    <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">replicas</code><code class="p">:</code> <code class="p-Indicator">{{</code> <code class="nv">.Values.replicaCount</code> <code class="p-Indicator">}}</code>&#13;
  <code class="nt">selector</code><code class="p">:</code>&#13;
    <code class="nt">matchLabels</code><code class="p">:</code>&#13;
      <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
  <code class="nt">template</code><code class="p">:</code>&#13;
    <code class="nt">metadata</code><code class="p">:</code>&#13;
      <code class="nt">labels</code><code class="p">:</code>&#13;
        <code class="nt">app</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
    <code class="nt">spec</code><code class="p">:</code>&#13;
      <code class="nt">containers</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code>&#13;
        <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">nginx</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Packaging Applications for Kubernetes" data-type="sect2"><div class="sect2" id="idm45611973421432">&#13;
<h2>Packaging Applications for Kubernetes</h2>&#13;
&#13;
<p>Creating self-contained software packages is another mechanism you can use to deploy your application while addressing the manifest management.<a data-primary="deployments" data-secondary="deploying applications to Kubenetes" data-tertiary="packaging applications for Kubernetes" data-type="indexterm" id="idm45611973372856"/><a data-primary="packaging applications for Kubernetes" data-type="indexterm" id="idm45611973371704"/> Packaging solutions usually build upon templating, but they introduce additional functionality that can be useful, such as the ability to push the package to OCI-compatible registries, life cycle management hooks, and more.</p>&#13;
&#13;
<p>Packages are a great mechanism to consume software maintained by a third party or deliver software to third parties.<a data-primary="Helm" data-type="indexterm" id="idm45611973370184"/> If you’ve used Helm to install software into a Kubernetes cluster, you’ve already leveraged the benefits of packaging. If you are unfamiliar with Helm, the following snippet gives you an idea of what it takes to install a &#13;
<span class="keep-together">package</span>:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting"><code class="nv">$ </code>helm repo add hashicorp https://helm.releases.hashicorp.com&#13;
<code class="s2">"hashicorp"</code> has been added to your repositories&#13;
&#13;
<code class="nv">$ </code>helm install vault hashicorp/vault</pre>&#13;
&#13;
<p>As you can see, packages can be a great way to deploy and manage software on Kubernetes. With that said, packages can fall short when it comes to complex applications that require advanced life cycle management. For such applications, we find operators to be a better solution. We discuss operators extensively in <a data-type="xref" href="ch02.html#deployment_models">Chapter 2</a>. Even though the chapter focuses on platform services, the concepts discussed apply when building operators for complex applications.<a data-primary="deployments" data-secondary="deploying applications to Kubenetes" data-startref="ix_depapps" data-type="indexterm" id="idm45611973142328"/><a data-primary="applications on Kubernetes" data-secondary="deploying applications to Kubenetes" data-startref="ix_appsKdep" data-type="indexterm" id="idm45611973140280"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ingesting Configuration and Secrets" data-type="sect1"><div class="sect1" id="idm45611973138904">&#13;
<h1>Ingesting Configuration and Secrets</h1>&#13;
&#13;
<p>Applications typically have configuration that tells them how to behave at runtime.<a data-primary="applications on Kubernetes" data-secondary="ingesting configuration and secrets" data-type="indexterm" id="ix_appsKcfgsec"/> Configuration commonly includes logging levels, hostnames of dependencies (e.g., DNS record for a database), timeouts, and more. Some of these settings can contain sensitive information, such as passwords, that we usually call secrets. In this section, we will discuss the different methods you can use to configure applications on a Kubernetes-based platform. First, we will review the ConfigMap and Secret APIs available in core Kubernetes. Then, we will explore an alternative to the Kubernetes API, mainly integrating with an external system. Finally, we will provide guidance on these approaches based on what we’ve seen work best in the field.</p>&#13;
&#13;
<p>Before digging in, it is worth mentioning that you should avoid bundling configuration or secrets inside your application’s container image. The tight coupling between the application binary and its configuration defeats the purpose of runtime configuration. Furthermore, it poses a security problem in the case of secrets, as the image might be accessible to actors that should otherwise not have access to the secrets. Instead of including config in the image, you should leverage platform features to inject configuration at runtime.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes ConfigMaps and Secrets" data-type="sect2"><div class="sect2" id="idm45611973112488">&#13;
<h2>Kubernetes ConfigMaps and Secrets</h2>&#13;
&#13;
<p>ConfigMaps and Secrets are core resources in the Kubernetes API that enable you to configure your applications at runtime.<a data-primary="secrets" data-secondary="consuming Kubernetes Secrets in applications" data-type="indexterm" id="ix_seccns"/><a data-primary="ConfigMaps" data-secondary="consuming in applications" data-type="indexterm" id="ix_CfgM"/><a data-primary="API server" data-secondary="application configurations" data-tertiary="ConfigMaps and Secrets" data-type="indexterm" id="idm45611973108152"/> As with any other resource in Kubernetes, they are created via the API server and are usually declared in YAML, such as the following example:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">ConfigMap</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-config</code>&#13;
<code class="nt">data</code><code class="p">:</code>&#13;
  <code class="nt">debug</code><code class="p">:</code> <code class="s">"false"</code></pre>&#13;
&#13;
<p>Let’s discuss how you can consume ConfigMaps and Secrets in your applications.</p>&#13;
&#13;
<p>The first method is to mount ConfigMaps and Secrets as files in the Pod’s filesystem.<a data-primary="secrets" data-secondary="consuming Kubernetes Secrets in applications" data-tertiary="mounting as files in Pod filesystem" data-type="indexterm" id="idm45611973070616"/><a data-primary="ConfigMaps" data-secondary="consuming in applications" data-tertiary="mounting as files in Pod filesystem" data-type="indexterm" id="idm45611973069528"/> When building your Pod specification, you can add volumes that reference ConfigMaps or Secrets by name and mount them into containers at specific locations. For example, the following snippet defines a Pod that mounts the ConfigMap named <code>my-config</code> into the container named <code>my-app</code> at <code>/etc/my-app/config.json</code>:</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">containers</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
    <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app:v0.1.0</code>&#13;
    <code class="nt">volumeMounts</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-config</code>&#13;
      <code class="nt">mountPath</code><code class="p">:</code> <code class="l-Scalar-Plain">/etc/my-app/config.json</code>&#13;
  <code class="nt">volumes</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-config</code>&#13;
    <code class="nt">configMap</code><code class="p">:</code>&#13;
      <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-config</code></pre>&#13;
&#13;
<p>Leveraging volume mounts is the preferred method when it comes to consuming ConfigMaps and Secrets. The reason is that the files in the Pod are dynamically updated, which allows you to reconfigure applications without restarting the app or re-creating the Pod. With that said, this is something that the application must support. The application must watch the configuration files on disk and apply new configuration when the files change. Many libraries and frameworks make it easy to implement this functionality. When this is not possible, you can introduce a sidecar container that watches the config files and signals the main process (with a SIGHUP, for example) when new configuration is available.</p>&#13;
&#13;
<p>Consuming ConfigMaps and Secrets via environment variables is another method you can use.<a data-primary="environment variables" data-secondary="consuming ConfigMaps and Secrets via" data-type="indexterm" id="idm45611973017960"/><a data-primary="ConfigMaps" data-secondary="consuming in applications" data-tertiary="using environment variables" data-type="indexterm" id="idm45611972985752"/><a data-primary="secrets" data-secondary="consuming Kubernetes Secrets in applications" data-tertiary="using environment variables" data-type="indexterm" id="idm45611972984632"/> If your application expects configuration through environment variables, this is the natural approach to follow. Environment variables can also be helpful if you need to provide settings via command-line flags. In the following example, the Pod sets the <code>DEBUG</code> environment variable using a ConfigMap named <code>my-config</code>, which has a key called <code>debug</code> that contains the value:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">containers</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
      <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app:v0.1.0</code>&#13;
      <code class="nt">env</code><code class="p">:</code>&#13;
        <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">DEBUG</code>&#13;
          <code class="nt">valueFrom</code><code class="p">:</code>&#13;
            <code class="nt">configMapKeyRef</code><code class="p">:</code>&#13;
              <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-config</code>&#13;
              <code class="nt">key</code><code class="p">:</code> <code class="l-Scalar-Plain">debug</code></pre>&#13;
&#13;
<p class="pagebreak-before">One of the downsides of using environment variables is that changes to the ConfigMaps or Secrets are not reflected in the running Pod until it restarts. This might not be a problem for some applications, but you should keep it in mind. Another downside, mainly for Secrets, is that some applications or frameworks may dump the environment details into logs during startup or when they crash. This poses a security risk as secrets can be leaked into logfiles inadvertently.</p>&#13;
&#13;
<p>These first two ConfigMap and Secret consumption methods rely on Kubernetes injecting the configuration into the workload. Another option is for the application to communicate with the Kubernetes API to get its configuration. Instead of using config files or environment variables, the application reads ConfigMaps and Secrets straight from the Kubernetes API server. The app can also watch the API so that it can act whenever the configuration changes. Developers can use one of the many Kubernetes libraries or SDKs to implement this functionality or leverage application frameworks that support this capability, such as Spring Cloud Kubernetes.<a data-primary="Spring Cloud Kubernetes" data-type="indexterm" id="idm45611972920744"/></p>&#13;
&#13;
<p>While leveraging the Kubernetes API for application configuration can be convenient, we <a data-primary="ConfigMaps" data-secondary="consuming in applications" data-tertiary="downsides" data-type="indexterm" id="idm45611972919640"/><a data-primary="secrets" data-secondary="consuming Kubernetes Secrets in applications" data-tertiary="downsides" data-type="indexterm" id="idm45611972918424"/><a data-primary="API server" data-secondary="application configurations" data-tertiary="getting from the server" data-type="indexterm" id="idm45611972917112"/>find that there are important downsides you should consider. First, the need to connect to the API server to get configuration creates a tight coupling between the application and the Kubernetes platform. This coupling raises some interesting questions. What happens if the API server goes down? Will your application experience downtime when your platform team upgrades the API server?</p>&#13;
&#13;
<p>Second, for the application to get its configuration from the API, it needs credentials, and it needs to have the right permissions. These requirements increase your deployment complexity, as you now have to provide a Service Account and define RBAC roles for your workload.</p>&#13;
&#13;
<p>Last, the more applications using this method to get config, the more undue load is imposed on the API server. Since the API server is a critical component of the cluster’s control plane, this approach to app configuration can be at odds with the overall scalability of the cluster.</p>&#13;
&#13;
<p>Overall, when it comes to consuming ConfigMaps and Secrets, we prefer using volume mounts and environment variables over integrating with the Kubernetes API directly. In this way, the applications remain decoupled from the underlying platform.<a data-primary="ConfigMaps" data-secondary="consuming in applications" data-startref="ix_CfgM" data-type="indexterm" id="idm45611972913336"/></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45611972911992">&#13;
<h5>Injecting Workload Metadata</h5>&#13;
<p>There are certain scenarios where workloads need to get information about themselves.<a data-primary="metadata" data-secondary="injecting workload metadata" data-type="indexterm" id="idm45611972910728"/><a data-primary="workloads" data-secondary="injecting workload metadata" data-type="indexterm" id="idm45611972909784"/> Perhaps they need the Namespace they’re running in, their labels, or their resource limits. Kubernetes provides the Downward API, which allows you to inject Pod metadata without the workload having to interact or know about Kubernetes.<a data-primary="Downward API" data-type="indexterm" id="idm45611972908488"/> Similar to ConfigMaps and Secrets, you can provide the metadata via environment variables or volume mounts.<a data-primary="environment variables" data-secondary="providing workload metadata via" data-type="indexterm" id="idm45611972907560"/></p>&#13;
&#13;
<p>The following example shows the Downward API in action. In this case, the Pod needs to <a data-primary="memory consumption" data-secondary="memory limit for a Pod" data-type="indexterm" id="idm45611972906216"/>know its memory limit, which is made available as an environment variable named <code>MEM_LIMIT</code>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">apiVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">v1</code>&#13;
<code class="nt">kind</code><code class="p">:</code> <code class="l-Scalar-Plain">Pod</code>&#13;
<code class="nt">metadata</code><code class="p">:</code>&#13;
  <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">containers</code><code class="p">:</code>&#13;
    <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
      <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app:0.1.0</code>&#13;
      <code class="nt">command</code><code class="p">:</code> <code class="p-Indicator">[</code> <code class="s">"my-app"</code> <code class="p-Indicator">]</code>&#13;
      <code class="nt">env</code><code class="p">:</code>&#13;
        <code class="p-Indicator">-</code> <code class="nt">name</code><code class="p">:</code> <code class="l-Scalar-Plain">MEM_LIMIT</code>&#13;
          <code class="nt">valueFrom</code><code class="p">:</code>&#13;
            <code class="nt">resourceFieldRef</code><code class="p">:</code>&#13;
              <code class="nt">containerName</code><code class="p">:</code> <code class="l-Scalar-Plain">my-app</code>&#13;
              <code class="nt">resource</code><code class="p">:</code> <code class="l-Scalar-Plain">limits.memory</code></pre>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Obtaining Configuration from External Systems" data-type="sect2"><div class="sect2" id="idm45611973111864">&#13;
<h2>Obtaining Configuration from External Systems</h2>&#13;
&#13;
<p>ConfigMaps and Secrets can be convenient when it comes to configuring applications.<a data-primary="applications on Kubernetes" data-secondary="ingesting configuration and secrets" data-tertiary="configuration from external systems" data-type="indexterm" id="idm45611972832104"/><a data-primary="secrets" data-secondary="obtaining for applications via external systems" data-type="indexterm" id="idm45611972830968"/> They are built into the Kubernetes API and are readily available for you to consume. With that said, configuration and secrets have been a concern that application developers had faced well before Kubernetes existed. While Kubernetes provides features to solve this concern, nothing is stopping you from using external systems instead.</p>&#13;
&#13;
<p>One of the most prevalent examples of an external configuration or secrets management<a data-primary="Vault" data-type="indexterm" id="idm45611972829064"/> system we run into in the field is <a href="https://www.vaultproject.io">HashiCorp Vault</a>. Vault provides advanced secret management functionality that is unavailable in Kubernetes Secrets. For example, Vault provides dynamic secrets, secret rotation, time-based tokens, and more. If your application is already leveraging Vault, you can continue to do so when running your application on Kubernetes. Even if not yet using Vault, it is worth evaluating as a more robust alternative to Kubernetes Secrets. We discussed secret management considerations and the Vault integration with Kubernetes extensively in <a data-type="xref" href="ch07.html#chapter7">Chapter 7</a>. If you want to learn more about secret management in Kubernetes and the lower-level details of the Vault integration, we recommend you check out that chapter.</p>&#13;
&#13;
<p>When leveraging an external system for configuration or secrets, we find that offloading the integration (as much as possible) to the platform is beneficial. Integrations with external systems such as Vault can be offered as a platform service to expose Secrets as volumes or environment variables in Pods. The platform service abstracts the external system and enables your application to consume the Secret without &#13;
<span class="keep-together">worrying</span> about the implementation details of the integration. Overall, leveraging such a platform service reduces the application’s complexity and results in standardization across your applications.<a data-primary="secrets" data-secondary="consuming Kubernetes Secrets in applications" data-startref="ix_seccns" data-type="indexterm" id="idm45611972824264"/><a data-primary="applications on Kubernetes" data-secondary="ingesting configuration and secrets" data-startref="ix_appsKcfgsec" data-type="indexterm" id="idm45611972822984"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Handling Rescheduling Events" data-type="sect1"><div class="sect1" id="idm45611972821480">&#13;
<h1>Handling Rescheduling Events</h1>&#13;
&#13;
<p>Kubernetes is a highly dynamic environment where workloads are moved around for different reasons.<a data-primary="rescheduling events, handling" data-type="indexterm" id="ix_resch"/><a data-primary="applications on Kubernetes" data-secondary="handling rescheduling events" data-type="indexterm" id="ix_appsKresch"/> Cluster nodes can come and go; they can run out of resources or even fail. Platform teams can drain, cordon, or remove nodes to perform cluster life cycle operations (e.g., upgrades). These are examples of situations in which your workload might be killed and rescheduled, and there are many others.</p>&#13;
&#13;
<p>Regardless of the reason, the dynamic nature of Kubernetes can impact your application’s availability and operation. Even though the application’s architecture has the highest bearing in determining the impact of disturbances, there are features in Kubernetes you can leverage to minimize that impact. We will explore these features in this section. First, we will dig into pre-stop container life cycle hooks. As indicated by the name, these hooks enable you to act before Kubernetes stops your containers. We will then discuss how you can shut down containers gracefully, which involves handling signals from within the application in response to shutdown events. Finally, we will review Pod anti-affinity rules, a mechanism you can use to spread your application across failure domains. As mentioned before, these mechanisms can help <em>minimize</em> the impact of disturbances but cannot eliminate the potential for failure. Keep that in mind as you read through this section.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pre-stop Container Life Cycle Hook" data-type="sect2"><div class="sect2" id="idm45611972815240">&#13;
<h2>Pre-stop Container Life Cycle Hook</h2>&#13;
&#13;
<p>Kubernetes can terminate workloads for any number of reasons. If you need to perform an action before your container is terminated, you can leverage the pre-stop container life cycle hook.<a data-primary="containers" data-secondary="pre-stop container life cycle hook" data-type="indexterm" id="idm45611972813272"/><a data-primary="applications on Kubernetes" data-secondary="handling rescheduling events" data-tertiary="pre-stop container life cycle hook" data-type="indexterm" id="idm45611972812280"/><a data-primary="rescheduling events, handling" data-secondary="pre-stop container life cycle hook" data-type="indexterm" id="idm45611972811016"/> Kubernetes provides two types of hooks. The <code>exec</code> life cycle hook runs a command within the container, while the <code>HTTP</code> life cycle hook issues an HTTP request against an endpoint you specify (typically the container itself). Which hook to use depends on your specific requirements and what you are trying to &#13;
<span class="keep-together">achieve</span>.</p>&#13;
&#13;
<p>The pre-stop hook in the <a href="https://projectcontour.io">Contour</a> Ingress controller is a great example that showcases the power of pre-stop hooks. To avoid dropping in-flight client requests, Contour includes<a data-primary="Contour Ingress controller" data-secondary="container pre-stop hook" data-type="indexterm" id="idm45611972807032"/> a container pre-stop hook that tells Kubernetes to execute a command before stopping the container. The following snippet from the Contour Deployment YAML file shows the pre-stop hook configuration:</p>&#13;
&#13;
<pre class="pagebreak-before" data-code-language="yaml" data-type="programlisting"><code class="c1"># &lt;... snip ...&gt;</code>&#13;
    <code class="nt">spec</code><code class="p">:</code>&#13;
      <code class="nt">containers</code><code class="p">:</code>&#13;
      <code class="p-Indicator">-</code> <code class="nt">command</code><code class="p">:</code>&#13;
        <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">/bin/contour</code>&#13;
        <code class="nt">args</code><code class="p">:</code>&#13;
          <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">envoy</code>&#13;
          <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">shutdown-manager</code>&#13;
        <code class="nt">image</code><code class="p">:</code> <code class="l-Scalar-Plain">docker.io/projectcontour/contour:main</code>&#13;
        <code class="nt">lifecycle</code><code class="p">:</code>&#13;
          <code class="nt">preStop</code><code class="p">:</code>&#13;
            <code class="nt">exec</code><code class="p">:</code>&#13;
              <code class="nt">command</code><code class="p">:</code>&#13;
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">/bin/contour</code>&#13;
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">envoy</code>&#13;
                <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">shutdown</code>&#13;
<code class="c1"># &lt;... snip ...&gt;</code></pre>&#13;
&#13;
<p>Container pre-stop hooks enable you to take action before Kubernetes stops your container. They allow you to run commands or scripts that exist within the container but are otherwise not part of the running process. One key consideration to keep in mind is that these hooks are executed only in the face of planned life cycle or re-scheduling events. The hooks will not run if a node fails, for example. Furthermore, any action performed as part of the pre-stop hook is governed by the Pod’s graceful shutdown period, which we will discuss next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Graceful Container Shutdown" data-type="sect2"><div class="sect2" id="idm45611972743272">&#13;
<h2>Graceful Container Shutdown</h2>&#13;
&#13;
<p>After executing pre-stop hooks (when provided), Kubernetes initiates the container shutdown process by sending a SIGTERM signal to the workload.<a data-primary="containers" data-secondary="graceful shutdown" data-type="indexterm" id="idm45611972726760"/><a data-primary="applications on Kubernetes" data-secondary="handling rescheduling events" data-tertiary="graceful container shutdown" data-type="indexterm" id="idm45611972725816"/><a data-primary="rescheduling events, handling" data-secondary="graceful container shutdown" data-type="indexterm" id="idm45611972724696"/> This signal lets the container know that it is being stopped. It also starts running down the clock of the termination shutdown period, which is 30 seconds by default. You can tune this period using the <code>terminationGracePeriodSeconds</code> field of the Pod specification.</p>&#13;
&#13;
<p>During the graceful termination period, the application can complete any necessary actions before shutting down. Depending on the application, these actions can be persisting data, closing open connections, flushing files to disk, etc. Once done, the application should exit with a successful exit code. The graceful termination is illustrated in <a data-type="xref" href="#application_termination_in_kubernetes">Figure 14-1</a>, where we can see the kubelet sending the SIGTERM signal and waiting for the container(s) to terminate within the grace period.</p>&#13;
&#13;
<p>If the application shuts down within the termination period, Kubernetes completes the shutdown process and moves on. Otherwise, it forcefully stops the process by sending a SIGKILL signal. <a data-type="xref" href="#application_termination_in_kubernetes">Figure 14-1</a> also shows this forceful termination toward the bottom right of the diagram.</p>&#13;
&#13;
<figure><div class="figure" id="application_termination_in_kubernetes">&#13;
<img alt="prku 1401" src="assets/prku_1401.png"/>&#13;
<h6><span class="label">Figure 14-1. </span>Application termination in Kubernetes. The kubelet first sends a SIGTERM signal to the workload and waits up to the configured graceful termination period. If the process is still running after the period expires, the kubelet sends a SIGKILL to terminate the process.</h6>&#13;
</div></figure>&#13;
&#13;
<p>For your application to terminate gracefully, it must handle the SIGTERM signal.<a data-primary="SIGTERM signal, application handling of" data-type="indexterm" id="idm45611972717048"/> Each programming language or framework has its own way of configuring signal handlers. Some application frameworks might even take care of it for you. The following snippet shows a Go application that configures a SIGTERM signal handler, which stops the application’s HTTP server upon receipt of the signal:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code> <code class="nx">main</code><code class="p">()</code> <code class="p">{</code>&#13;
	<code class="c1">// App initialization code here...</code>&#13;
	<code class="nx">httpServer</code> <code class="o">:=</code> <code class="nx">app</code><code class="p">.</code><code class="nx">NewHTTPServer</code><code class="p">()</code>&#13;
&#13;
	<code class="c1">// Make a channel to listen for an interrupt or terminate signal</code>&#13;
	<code class="c1">// from the OS.</code>&#13;
&#13;
	<code class="c1">// Use a buffered channel because the signal package requires it.</code>&#13;
	<code class="nx">shutdown</code> <code class="o">:=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="nx">os</code><code class="p">.</code><code class="nx">Signal</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
	<code class="nx">signal</code><code class="p">.</code><code class="nx">Notify</code><code class="p">(</code><code class="nx">shutdown</code><code class="p">,</code> <code class="nx">os</code><code class="p">.</code><code class="nx">Interrupt</code><code class="p">,</code> <code class="nx">syscall</code><code class="p">.</code><code class="nx">SIGTERM</code><code class="p">)</code>&#13;
&#13;
	<code class="c1">// Start the application and listen for errors</code>&#13;
	<code class="nx">errors</code> <code class="o">:=</code> <code class="nb">make</code><code class="p">(</code><code class="kd">chan</code> <code class="kt">error</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>&#13;
	<code class="k">go</code> <code class="nx">httpServer</code><code class="p">.</code><code class="nx">ListenAndServe</code><code class="p">(</code><code class="nx">errors</code><code class="p">)</code>&#13;
&#13;
	<code class="c1">// Block main and waiting for shutdown.</code>&#13;
	<code class="k">select</code> <code class="p">{</code>&#13;
	<code class="k">case</code> <code class="nx">err</code> <code class="o">:=</code> <code class="o">&lt;-</code><code class="nx">errors</code><code class="p">:</code>&#13;
		<code class="nx">log</code><code class="p">.</code><code class="nx">Fatalf</code><code class="p">(</code><code class="s">"http server error: %v"</code><code class="p">,</code> <code class="nx">err</code><code class="p">)</code>&#13;
&#13;
	<code class="k">case</code> <code class="o">&lt;-</code><code class="nx">shutdown</code><code class="p">:</code>&#13;
        <code class="nx">log</code><code class="p">.</code><code class="nx">Printf</code><code class="p">(</code><code class="s">"shutting down http server"</code><code class="p">)</code>&#13;
		<code class="nx">httpServer</code><code class="p">.</code><code class="nx">Shutdown</code><code class="p">()</code>&#13;
	<code class="p">}</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>When running your applications on Kubernetes, we recommend you configure signal handlers for the SIGTERM signal. Even if there are no shutdown actions to take, handling the signal makes your workload a better Kubernetes citizen, as it reduces the time it takes to stop the application and thus free up the resources for other &#13;
<span class="keep-together">workloads</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Satisfying Availability Requirements" data-type="sect2"><div class="sect2" id="idm45611972590936">&#13;
<h2>Satisfying Availability Requirements</h2>&#13;
&#13;
<p>Container pre-stop hooks and graceful termination are concerned with a single instance or replica of your application.<a data-primary="availability requirements" data-type="indexterm" id="idm45611972589336"/><a data-primary="applications on Kubernetes" data-secondary="handling rescheduling events" data-tertiary="satisfying availability requirements" data-type="indexterm" id="idm45611972588616"/> If your application is horizontally scalable, you will most likely have multiple replicas running in the cluster to satisfy availability requirements. Running more than one instance of your workload can provide increased fault tolerance.<a data-primary="fault tolerance for applications" data-type="indexterm" id="idm45611972586968"/><a data-primary="failures" data-secondary="increasing application tolerance for" data-type="indexterm" id="idm45611972586280"/> For example, if a cluster node fails and takes one of the application instances with it, the other replicas can pick up the work. With that said, having multiple replicas does not help if they are running in the same failure domain.</p>&#13;
&#13;
<p>One way to ensure your Pods are spread across failure domains is by using Pod anti-affinity rules.<a data-primary="Pods" data-secondary="spreading across failure domains" data-tertiary="anti-affinity rules" data-type="indexterm" id="idm45611972584440"/> With Pod anti-affinity rules, you tell the Kubernetes scheduler that you want to schedule your Pods according to constraints you define in the Pod definition. More specifically, you ask the scheduler to avoid placing your Pod on nodes that are already running a replica of your workload. Consider a web server that has three replicas. To ensure the three replicas are not placed in the same failure domain, you can use Pod anti-affinity as in the following snippet. In this case, the anti-affinity rule tells the scheduler that it should prefer placing Pods across zones, as determined by the <code>zone</code> label on cluster nodes:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="c1"># ... &lt;snip&gt; ...</code>&#13;
      <code class="nt">affinity</code><code class="p">:</code>&#13;
        <code class="nt">PodAntiAffinity</code><code class="p">:</code>&#13;
          <code class="nt">preferredDuringSchedulingIgnoredDuringExecution</code><code class="p">:</code>&#13;
            <code class="p-Indicator">-</code> <code class="nt">labelSelector</code><code class="p">:</code>&#13;
                <code class="nt">matchExpressions</code><code class="p">:</code>&#13;
                  <code class="p-Indicator">-</code> <code class="nt">key</code><code class="p">:</code> <code class="s">"app"</code>&#13;
                    <code class="nt">operator</code><code class="p">:</code> <code class="l-Scalar-Plain">In</code>&#13;
                    <code class="nt">values</code><code class="p">:</code>&#13;
                    <code class="p-Indicator">-</code> <code class="l-Scalar-Plain">my-web-server</code>&#13;
              <code class="nt">topologyKey</code><code class="p">:</code> <code class="s">"zone"</code>&#13;
<code class="c1"># ... &lt;snip&gt; ...</code></pre>&#13;
&#13;
<p>In addition to Pod anti-affinity, Kubernetes provides Pod Topology Spread Constraints, which are an improvement to Pod anti-affinity rules when it comes to spreading Pods across failure domains.<a data-primary="Pods" data-secondary="spreading across failure domains" data-tertiary="Topology Spread Constraints" data-type="indexterm" id="idm45611972486600"/> The problem with anti-affinity rules is that there is no way to guarantee Pods are spread <em>evenly</em> across the domains. You can either “prefer” scheduling them based on the topology key, or you can guarantee a single replica per failure domain.</p>&#13;
&#13;
<p>The Pod Topology Spread Constraints provide a way for you to tell the scheduler to spread your workload. Similar to Pod anti-affinity rules, they are only evaluated against new Pods that need scheduling, and thus they are not retroactively enforced. The following snippet shows an example Pod Topology Spread Constraint that results in Pods spread across zones (based on the <code>zone</code> label of nodes). If the constraint cannot be satisfied, Pods will not be scheduled.</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="c1"># ... &lt;snip&gt; ...</code>&#13;
<code class="nt">spec</code><code class="p">:</code>&#13;
  <code class="nt">topologySpreadConstraints</code><code class="p">:</code>&#13;
  <code class="p-Indicator">-</code> <code class="nt">maxSkew</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code>&#13;
    <code class="nt">topologyKey</code><code class="p">:</code> <code class="l-Scalar-Plain">zone</code>&#13;
    <code class="nt">whenUnsatisfiable</code><code class="p">:</code> <code class="l-Scalar-Plain">DoNotSchedule</code>&#13;
    <code class="nt">labelSelector</code><code class="p">:</code>&#13;
      <code class="nt">matchLabels</code><code class="p">:</code>&#13;
        <code class="nt">foo</code><code class="p">:</code> <code class="l-Scalar-Plain">bar</code>&#13;
<code class="c1"># ... &lt;snip&gt; ...</code></pre>&#13;
&#13;
<p>When running multiple instances of an application, you should leverage these Pod placement features to improve the application’s tolerance of infrastructure failure. Otherwise, you risk Kubernetes scheduling your workload in a way that does not achieve the failure tolerance you are looking for.<a data-primary="rescheduling events, handling" data-startref="ix_resch" data-type="indexterm" id="idm45611972548280"/><a data-primary="applications on Kubernetes" data-secondary="handling rescheduling events" data-startref="ix_appsKresch" data-type="indexterm" id="idm45611972547432"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="State Probes" data-type="sect1"><div class="sect1" id="idm45611972546168">&#13;
<h1>State Probes</h1>&#13;
&#13;
<p>Kubernetes uses many signals to determine the state and health of applications running on the platform.<a data-primary="state probes" data-type="indexterm" id="ix_stprb"/><a data-primary="applications on Kubernetes" data-secondary="state probes" data-type="indexterm" id="ix_appsKst"/> When it comes to health, Kubernetes treats workloads as opaque boxes. It knows whether the process is up or not. While this information is helpful, it is typically not enough to run and manage applications effectively. This is where probes come in. Probes provide Kubernetes with increased visibility of the application’s condition.</p>&#13;
&#13;
<p class="pagebreak-before">Kubernetes provides three probe types: liveness, readiness, and startup probes. Before discussing each type in detail, let’s review the different probing mechanisms<a data-primary="state probes" data-secondary="probing mechanisms common to" data-type="indexterm" id="idm45611972541048"/> that are common to all probe types:</p>&#13;
<dl>&#13;
<dt>Exec</dt>&#13;
<dd>&#13;
<p>The kubelet executes a command inside the container.<a data-primary="Exec probing mechanism" data-type="indexterm" id="idm45611972464936"/> The probe is deemed successful if the command returns a zero exit code. Otherwise, the kubelet considers the container unhealthy.</p>&#13;
</dd>&#13;
<dt>HTTP</dt>&#13;
<dd>&#13;
<p>The kubelet sends an HTTP request to an endpoint in the Pod.<a data-primary="HTTP, use as probing mechanism" data-type="indexterm" id="idm45611972462680"/> As long as the HTTP response code is greater than or equal to 200 and less than 400, the probe is deemed successful.</p>&#13;
</dd>&#13;
<dt>TCP</dt>&#13;
<dd>&#13;
<p>The kubelet establishes a TCP connection with the container on a configurable port. <a data-primary="TCP" data-secondary="using as probing mechanism" data-type="indexterm" id="idm45611972460424"/>The container is deemed healthy if the connection is established &#13;
<span class="keep-together">successfully</span>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In addition to sharing the probing mechanisms, all probes have a common set of parameters that you can use to tune the probe according to your workload. These parameters include success and failure thresholds, timeout periods, and others. The Kubernetes documentation describes each setting in detail, so we will not dive into them here.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Liveness Probes" data-type="sect2"><div class="sect2" id="idm45611972457592">&#13;
<h2>Liveness Probes</h2>&#13;
&#13;
<p>Liveness probes help Kubernetes understand the health of Pods on the cluster. At the <a data-primary="liveness probes" data-type="indexterm" id="idm45611972456264"/><a data-primary="state probes" data-secondary="liveness probes" data-type="indexterm" id="idm45611972455560"/><a data-primary="applications on Kubernetes" data-secondary="state probes" data-tertiary="liveness probes" data-type="indexterm" id="idm45611972454616"/>node level, the kubelet continuously probes Pods that have the liveness probe configured. When the liveness probe exceeds the failure threshold, the kubelet deems the Pod unhealthy and restarts it. <a data-type="xref" href="#flowchart_that_shows_an_http_based_liveness_probe">Figure 14-2</a> shows a flowchart that depicts an HTTP liveness probe. The kubelet probes the container every 10 seconds.<a data-primary="HTTP, use as probing mechanism" data-secondary="liveness probe" data-type="indexterm" id="idm45611972452136"/> If the kubelet finds that the last 10 probes have failed, it restarts the container.</p>&#13;
&#13;
<figure><div class="figure" id="flowchart_that_shows_an_http_based_liveness_probe">&#13;
<img alt="prku 1402" src="assets/prku_1402.png"/>&#13;
<h6><span class="label">Figure 14-2. </span>Flowchart that shows an HTTP-based liveness probe with a period of 10 seconds. If the probe fails 10 times consecutively, the Pod is deemed unhealthy and the kubelet restarts it.</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Given that liveness probe failures result in container restarts, we typically suggest that liveness probe implementations should not check for the workload’s external dependencies. By keeping the liveness probe local to your workload and not checking external dependencies, you prevent cascading failures that could otherwise occur. For example, a service that interacts with a database should not perform a “database availability” check as part of its liveness probe, as restarting the workload will most likely not fix the problem. If the app detects an issue with the database, the app can enter a read-only mode or gracefully disable the functionality that depends on the database. Another option is for the app to fail its readiness probe, which we discuss next.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Readiness Probes" data-type="sect2"><div class="sect2" id="idm45611972446712">&#13;
<h2>Readiness Probes</h2>&#13;
&#13;
<p>The readiness probe is perhaps the most common and most important probe type in Kubernetes, especially for services that handle requests.<a data-primary="readiness probes" data-type="indexterm" id="idm45611972425752"/><a data-primary="state probes" data-secondary="readiness probes" data-type="indexterm" id="idm45611972425144"/><a data-primary="applications on Kubernetes" data-secondary="state probes" data-tertiary="readiness probes" data-type="indexterm" id="idm45611972424296"/> Kubernetes uses readiness probes to control whether to route Service traffic to Pods. Thus, readiness probes provide a mechanism for the application to tell the platform that they’re ready to accept requests.</p>&#13;
&#13;
<p>As with liveness probes, the kubelet is responsible for probing the application and updating the Pod’s status according to the probe results. When the probe fails, the platform removes the failing Pod from the list of available endpoints, effectively diverting traffic to other replicas that are ready. <a data-type="xref" href="#flowchart_that_shows_an_http_based_readiness">Figure 14-3</a> shows a flowchart that explains an HTTP-based readiness probe. The probe has a 5-second initial delay and a probing period of 10 seconds.<a data-primary="HTTP, use as probing mechanism" data-secondary="readiness probe" data-type="indexterm" id="idm45611972421256"/> On startup, the application begins receiving traffic only when the readiness probe succeeds. Then, the platform stops sending traffic to the Pod if the probe fails twice consecutively.</p>&#13;
&#13;
<p>When deploying service-type workloads, make sure that you configure a readiness probe to avoid sending requests to replicas that cannot handle them. The readiness probe is not only critical when the Pod is starting up but also important during the lifetime of the Pod to prevent routing clients to replicas that have become unready.</p>&#13;
&#13;
<figure><div class="figure" id="flowchart_that_shows_an_http_based_readiness">&#13;
<img alt="prku 1403" src="assets/prku_1403.png"/>&#13;
<h6><span class="label">Figure 14-3. </span>Flowchart that shows an HTTP-based readiness probe with a period of 10 seconds. If the probe fails twice consecutively, the Pod is deemed not ready and it is taken out of the set of ready endpoints.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Startup Probes" data-type="sect2"><div class="sect2" id="idm45611972417256">&#13;
<h2>Startup Probes</h2>&#13;
&#13;
<p>Liveness and readiness probes have been available since the first version of Kubernetes.<a data-primary="startup probes" data-type="indexterm" id="idm45611972415768"/><a data-primary="state probes" data-secondary="startup probes" data-type="indexterm" id="idm45611972415160"/><a data-primary="applications on Kubernetes" data-secondary="state probes" data-tertiary="startup probes" data-type="indexterm" id="idm45611972414312"/> As the system gained popularity, the community identified a need to implement an additional probe, the startup probe. The startup probe provides extra time for slow-starting applications to initialize.<a data-primary="liveness probes" data-secondary="startup probes and" data-type="indexterm" id="idm45611972412872"/> Similar to liveness probes, failed startup probes result in the container being restarted. Unlike liveness probes, however, startup probes are executed only until they succeed, at which point the liveness and readiness probes take over.</p>&#13;
&#13;
<p>If you are wondering why a liveness probe is not enough, let’s consider an application that takes, on average, 300 seconds to initialize. You could indeed use a liveness probe that waits 300 seconds before stopping the container. During startup, this liveness probe would work. But what about later on when the application is running? If the application entered an unhealthy state, the platform would wait 300 seconds before restarting it! This is the problem that the startup probe solves. It looks after the workload during startup but then gets out of the way. <a data-type="xref" href="#flowchart_that_shows_an_http_based_startup_probe_with_a_period_of_10_seconds">Figure 14-4</a> shows a flowchart that walks through a startup probe like the one we just discussed.<a data-primary="HTTP, use as probing mechanism" data-secondary="startup probe" data-type="indexterm" id="idm45611972409944"/> It has a failure threshold of 30 times and a probing period of 10 seconds.</p>&#13;
&#13;
<figure><div class="figure" id="flowchart_that_shows_an_http_based_startup_probe_with_a_period_of_10_seconds">&#13;
<img alt="prku 1404" src="assets/prku_1404.png"/>&#13;
<h6><span class="label">Figure 14-4. </span>Flowchart that shows an HTTP-based startup probe with a period of 10 seconds. If the probe returns a successful response, the startup probe is disabled and the liveness/readiness probes are enabled. Otherwise, if the probe fails 30 times consecutively, the kubelet restarts the Pod.</h6>&#13;
</div></figure>&#13;
&#13;
<p>While startup probes can be useful for certain applications, we usually recommend avoiding them unless absolutely necessary. We find liveness and readiness probes to be appropriate most of the time.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Implementing Probes" data-type="sect2"><div class="sect2" id="idm45611972406200">&#13;
<h2>Implementing Probes</h2>&#13;
&#13;
<p>Now that we have covered the different probe types, let’s dive into how you should approach them in your application, specifically liveness and readiness probes.<a data-primary="state probes" data-secondary="implementing" data-type="indexterm" id="idm45611972404648"/><a data-primary="applications on Kubernetes" data-secondary="state probes" data-tertiary="implementing" data-type="indexterm" id="idm45611972403800"/> We know that failed liveness probes result in the platform restarting the Pod, while failed readiness probes prevent traffic from being routed to the Pod.<a data-primary="liveness probes" data-secondary="failed" data-type="indexterm" id="idm45611972402408"/> Given these different outcomes, we find that most applications that leverage both liveness and readiness probes should configure different probe endpoints or commands.</p>&#13;
&#13;
<p>Ideally, the liveness probe fails only when there is a problem that requires a restart, such as a deadlock or some other condition that permanently prevents the app from making progress. Applications that expose an HTTP server commonly implement a liveness endpoint that unconditionally returns a 200 status code. As long as the HTTP server is healthy and the app can respond, there’s no need to restart it.</p>&#13;
&#13;
<p>In contrast<a data-primary="readiness probes" data-type="indexterm" id="idm45611972400040"/> to the liveness endpoint, the readiness endpoint can check for different conditions within the application. For example, if the application warms internal caches on startup, the readiness endpoint can return false unless the caches are warm. Another example is service overload, a condition under which the app can fail the readiness probe as a mechanism to shed load. As you can probably imagine, the checked conditions vary from one application to the next. In general, however, they are temporary conditions that resolve with the passing of time.</p>&#13;
&#13;
<p class="pagebreak-before">To summarize, we typically recommend using readiness probes for workloads that handle requests, given that readiness probes are meaningless in other application types, such as controllers, jobs, etc.<a data-primary="liveness probes" data-secondary="recommended use" data-type="indexterm" id="idm45611972397912"/> When it comes to liveness probes, we recommend considering them only when restarting the application would help fix the problem. Lastly, we tend to avoid startup probes unless absolutely necessary.<a data-primary="state probes" data-startref="ix_stprb" data-type="indexterm" id="idm45611972396728"/><a data-primary="applications on Kubernetes" data-secondary="state probes" data-startref="ix_appsKst" data-type="indexterm" id="idm45611972395880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Pod Resource Requests and Limits" data-type="sect1"><div class="sect1" id="idm45611972394664">&#13;
<h1>Pod Resource Requests and Limits</h1>&#13;
&#13;
<p>One of Kubernetes’ primary functions is to schedule applications across cluster nodes.<a data-primary="applications on Kubernetes" data-secondary="Pod resource requests and limits" data-type="indexterm" id="ix_appsKresreq"/><a data-primary="resources" data-secondary="requests and limits" data-type="indexterm" id="ix_resreq"/><a data-primary="Pods" data-secondary="resource requests and limits" data-type="indexterm" id="ix_Podsres"/> The scheduling process involves, among other things, finding candidate nodes that have enough resources to host the workload. To place workloads effectively, the Kubernetes scheduler first needs to know the resource needs of your application. Typically, these resources encompass CPU and memory, but can also include other resource types such as ephemeral storage and even custom or extended resources.</p>&#13;
&#13;
<p>In addition to scheduling your applications, Kubernetes also needs resource information to guarantee those resources at runtime. After all, the platform has limited resources that are shared across applications. Providing resource requirements is critical to your application’s ability to <em>use</em> those resources.</p>&#13;
&#13;
<p>In this section, we will discuss resource requests and resource limits, and how they can impact your application. We will not dig into the details of how the platform implements resource requests and limits, as we have already discussed it in &#13;
<span class="keep-together"><a data-type="xref" href="ch12.html#multi_tenancy_chapter">Chapter 12</a></span>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Requests" data-type="sect2"><div class="sect2" id="idm45611972386424">&#13;
<h2>Resource Requests</h2>&#13;
&#13;
<p>Resource requests specify the minimum amount of resources your application needs to run. In most cases, you should specify resource requests when deploying applications to Kubernetes. By doing so, you ensure your workload will have access to the requested resources at runtime. If you don’t specify resource requests, you might find your application’s performance diminishes significantly when resources on the node come under contention. You even risk the possibility of your application being terminated if the node needs to reclaim memory for other workloads. <a data-type="xref" href="#pod_1_and_pod_2_share_the_nodes_memory">Figure 14-5</a> shows the termination of an application because another workload with memory requests starts consuming additional memory.</p>&#13;
&#13;
<figure><div class="figure" id="pod_1_and_pod_2_share_the_nodes_memory">&#13;
<img alt="prku 1405" src="assets/prku_1405.png"/>&#13;
<h6><span class="label">Figure 14-5. </span>Pod 1 and Pod 2 share the node’s memory. Each Pod is initially consuming 200 MiB out of the total 500 MiB. Pod 2 is terminated when Pod 1 needs to consume additional memory, as Pod 2 does not have memory requests in its specification. Pod 2 enters a crash loop as it cannot allocate enough memory to start up.</h6>&#13;
</div></figure>&#13;
&#13;
<p>One of the main challenges with resource requests is finding the right numbers to use. If you are deploying an existing application, you might already have data you can analyze to determine the app’s resource requests, such as the application’s actual utilization over time or perhaps the size of the VMs hosting it. When you don’t have historical data, you will have to use an educated guess and gather data over time. Another option is to use the Vertical Pod Autoscaler (VPA), which can suggest values for CPU and memory requests and even adjust those values over time. For more information about the VPA, see <a data-type="xref" href="ch13.html#autoscaling_chapter">Chapter 13</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Resource Limits" data-type="sect2"><div class="sect2" id="idm45611972379752">&#13;
<h2>Resource Limits</h2>&#13;
&#13;
<p>Resource limits allow you to specify the maximum amount of resources your workload can consume. You might be wondering why you would impose an artificial limit. After all, the more resources available, the better. While this is true for some workloads, having unbound access to resources can result in unpredictable performance, as the Pod will have access to extra resources when available but will not when other Pods need the resources on the node. It gets even worse with memory. Given that memory is an incompressible resource, the platform has no other choice but to kill Pods when it needs to reclaim memory that was opportunistically consumed.</p>&#13;
&#13;
<p>An important consideration to make when setting resource limits is whether you need to propagate those limits to the workload itself.<a data-primary="Java applications, configuring memory limits for" data-type="indexterm" id="idm45611972377176"/> Java applications are a good example. If the application uses an older version of Java (JDK version 8u131 or earlier), you need to propagate the memory limit down to the Java Virtual Machine (JVM). Otherwise, the JVM remains unaware of the limit and attempts to consume more memory than allowed. In the case of Java, you can configure the memory settings of the JVM using the <code>JAVA_OPTIONS</code> environment variable. Another option, although not always feasible, is to update the version of the JVM, as more recent versions gained the ability to detect the memory limits within containers. If you are deploying an application that leverages a runtime, consider whether you need to propagate the resource limits for the application to understand them.</p>&#13;
&#13;
<p>Limits are also important if you are trying to run performance tests or benchmarks against your workload. As you can imagine, it is likely that each test run will execute against Pods scheduled on different nodes at different times. If resource limits are not enforced on the workload, the test results can be highly variable as the workload under test can burst above its resource requests when the nodes have idle resources.</p>&#13;
&#13;
<p>Usually, you should set your resource limits equal to your resource requests, which ensures your application will always have the same amount of resources, no matter what’s happening with other Pods running beside it.<a data-primary="applications on Kubernetes" data-secondary="Pod resource requests and limits" data-startref="ix_appsKresreq" data-type="indexterm" id="idm45611972373832"/><a data-primary="resources" data-secondary="requests and limits" data-startref="ix_resreq" data-type="indexterm" id="idm45611972372744"/><a data-primary="Pods" data-secondary="resource requests and limits" data-startref="ix_Podsres" data-type="indexterm" id="idm45611972371656"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Application Logs" data-type="sect1"><div class="sect1" id="idm45611972370440">&#13;
<h1>Application Logs</h1>&#13;
&#13;
<p>Application logs are critical to troubleshoot and debug applications both during development and in production.<a data-primary="logging" data-secondary="application logs" data-type="indexterm" id="ix_loggapp"/><a data-primary="applications on Kubernetes" data-secondary="application logs" data-type="indexterm" id="ix_appsKlog"/> Applications running on Kubernetes should, as much as possible, log to the standard out and standard error streams (STDOUT/STDERR). This not only removes complexity in the application but also is the least complex solution from the platform’s perspective when it comes to shipping logs to a central location. We covered this concern in <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a>, where we also discussed different log processing strategies, systems, and tools. In this section, we will touch on some of the considerations to make when thinking about application logs. The first thing we’ll talk about is what you should log in the first place. Then, we will discuss unstructured versus structured logs. Finally, we will touch on improving the usefulness of logs by including contextual information in log messages.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What to Log" data-type="sect2"><div class="sect2" id="idm45611972364968">&#13;
<h2>What to Log</h2>&#13;
&#13;
<p>One of the first things to figure out when it comes to application logs is <em>what</em> to include in the logs. While development teams typically have their own philosophy, we have found that they tend to go overboard with logs. If you log too much, you run the risk of having too much noise and missing out on important information. On the flip side, if you log too little, it can become difficult to troubleshoot your application effectively. As with most things, there’s a balance to strike here.</p>&#13;
&#13;
<p>While working with application teams, we have found that a good rule of thumb that helps determine whether to log something is to ask the question, Is this log message actionable? If the answer is yes, this is a good indicator that it is worth logging that message. Otherwise, it is an indicator that the log message might not be useful.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Unstructured Versus Structured Logs" data-type="sect2"><div class="sect2" id="idm45611972362024">&#13;
<h2>Unstructured Versus Structured Logs</h2>&#13;
&#13;
<p>Application logs can be categorized as either unstructured or structured. Unstructured logs are, as the name suggests, strings of text that lack a specific format. They are arguably the most prevalent, as there is zero upfront planning that teams need to make. While the team might have generic guidelines, developers get to log messages in whatever format they like.</p>&#13;
&#13;
<p>Structured logs, on the other hand, have predetermined fields that must be provided when logging events. They are typically formatted as JSON lines or key-value lines (e.g., <code>time="2015-08-09T03:41:12-03:21" msg="hello world!" thread="13" batchId="5"</code>). The main benefit of structured logs is that they are written in a machine-readable format, making them easier to query and analyze. With that said, structured logs tend to be harder for humans to read, so you must carefully consider this trade-off as you implement logging in your application.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Contextual Information in Logs" data-type="sect2"><div class="sect2" id="idm45611972358728">&#13;
<h2>Contextual Information in Logs</h2>&#13;
&#13;
<p>The primary purpose of logs is to provide insight into what happened within your application at a certain point in time.<a data-primary="contextual information in logs" data-type="indexterm" id="idm45611972357448"/> Perhaps you are troubleshooting a production issue in a live application, or maybe you are performing a root cause analysis to understand why something happened. To be able to complete such tasks, you typically need contextual information in log messages, in addition to what happened.</p>&#13;
&#13;
<p>Let’s consider a payment application as an example. When the application request serving pipeline encounters an error, in addition to logging the error itself, try to include the context surrounding the error as well. For example, if an error occurs because the payee was not found, include the payee name or ID, the user ID attempting to make the payment, the payment amount, etc. Such contextual information will improve your experience troubleshooting issues and will help you prevent such problems in the future. Having said that, avoid including sensitive information in your logs. You don’t want to leak a user’s password or credit card information.<a data-primary="applications on Kubernetes" data-secondary="application logs" data-startref="ix_appsKlog" data-type="indexterm" id="idm45611972355352"/><a data-primary="logging" data-secondary="application logs" data-startref="ix_loggapp" data-type="indexterm" id="idm45611972354264"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exposing Metrics" data-type="sect1"><div class="sect1" id="idm45611972352920">&#13;
<h1>Exposing Metrics</h1>&#13;
&#13;
<p>In addition to logs, metrics provide critical insight about how your application is behaving.<a data-primary="metrics" data-secondary="application" data-type="indexterm" id="ix_mtrcapp"/><a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-type="indexterm" id="ix_appsKmtrc"/> Once you have application metrics, you can configure alerts to let you know when your application needs attention. Furthermore, by aggregating metrics over time, you can discover trends, improvements, and regressions as you roll out new versions of your software. This section discusses application instrumentation and some of the metrics you can capture, including RED (Rate, Errors, Duration), USE (Utilization, Saturation, Errors), and app-specific metrics.<a data-primary="app-specific metrics" data-type="indexterm" id="idm45611972348648"/><a data-primary="USE (Utilization, Saturation, Errors) method" data-type="indexterm" id="idm45611972348040"/><a data-primary="RED (Rate, Errors, Duration) method" data-type="indexterm" id="idm45611972347432"/> If you are interested in the platform components that enable monitoring and more additional discussions on metrics, check out <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Instrumenting Applications" data-type="sect2"><div class="sect2" id="idm45611972345688">&#13;
<h2>Instrumenting Applications</h2>&#13;
&#13;
<p>In most cases, the platform can measure and surface metrics about your application’s externally visible behavior.<a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-tertiary="instrumenting applications" data-type="indexterm" id="ix_appSKmtrcins"/> Metrics such as CPU usage, memory usage, disk IOPS, and others are readily available from the node that is running your application. While these metrics are useful, instrumenting your application to expose key metrics from within is worthwhile.</p>&#13;
&#13;
<p>Prometheus is one of the most popular monitoring systems for Kubernetes-based platforms that we run into in the field.<a data-primary="Prometheus" data-secondary="instrumenting applications for" data-type="indexterm" id="ix_Prominst"/> We have extensively covered Prometheus and its components in <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a>. In this section, we will focus our discussions on instrumenting apps for Prometheus.</p>&#13;
&#13;
<p>Prometheus pulls metrics from your application using an HTTP request on a configurable endpoint (typically <code>/metrics</code>). This means that your application must expose this endpoint for Prometheus to scrape. More importantly, the endpoint’s response must contain Prometheus-formatted metrics. Depending on the type of software you want to monitor, there are two approaches you can take to expose metrics:</p>&#13;
<dl>&#13;
<dt>Native instrumentation</dt>&#13;
<dd>&#13;
<p>This option involves instrumenting your application using the Prometheus client libraries so that metrics are exposed from within the application process. This is an excellent approach when you have control over the source code of the &#13;
<span class="keep-together">application</span>.</p>&#13;
</dd>&#13;
<dt>Out-of-process exporter</dt>&#13;
<dd>&#13;
<p>This is an additional process running beside <a data-primary="exporters" data-type="indexterm" id="idm45611972335304"/>your workload that transforms preexisting metrics and exposes them in a Prometheus-compatible format. This approach is best suited for off-the-shelf software that you cannot instrument directly and is typically implemented using the sidecar container pattern. Examples include the <a href="https://oreil.ly/g0ZCt">NGINX Prometheus Exporter</a> and the <a href="https://oreil.ly/SJOka">MySQL Server Exporter</a>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The Prometheus instrumentation libraries supports four metric types: Counters, Gauges, Histograms, and Summaries. Counters are metrics that can only increase, while Gauges are metrics that can go up or down. Histograms and Summaries are more advanced metrics than Counters and Gauges. Histograms place observations into configurable buckets that you can then use to compute quantiles (e.g., 95th percentile) on the Prometheus server. Summaries are similar to Histograms, except that they compute quantiles on the client side over a sliding time window. The <a href="https://oreil.ly/epvwC">Prometheus documentation</a> explains the metric types in more depth.</p>&#13;
&#13;
<p>There are three primary things you must do to instrument an application with the Prometheus libraries. Let’s work through an example of instrumenting a Go service. First, you need to start an HTTP server to expose the metrics for Prometheus to scrape. The library provides an HTTP handler that takes care of encoding the metrics into the Prometheus format. Adding the handler would look something like this:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code> <code class="nx">main</code><code class="p">()</code> <code class="p">{</code>&#13;
  <code class="c1">// app code...</code>&#13;
&#13;
  <code class="nx">http</code><code class="p">.</code><code class="nx">Handle</code><code class="p">(</code><code class="s">"/metrics"</code><code class="p">,</code>&#13;
    <code class="nx">promhttp</code><code class="p">.</code><code class="nx">HandlerFor</code><code class="p">(</code>&#13;
      <code class="nx">prometheus</code><code class="p">.</code><code class="nx">DefaultGatherer</code><code class="p">,</code>&#13;
      <code class="nx">promhttp</code><code class="p">.</code><code class="nx">HandlerOpts</code><code class="p">{},</code>&#13;
  <code class="p">))</code>&#13;
&#13;
  <code class="nx">log</code><code class="p">.</code><code class="nx">Fatal</code><code class="p">(</code><code class="nx">http</code><code class="p">.</code><code class="nx">ListenAndServe</code><code class="p">(</code><code class="s">"localhost:8080"</code><code class="p">,</code> <code class="kc">nil</code><code class="p">))</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>Next, you need to create and register metrics. For example, if you wanted to expose a Counter metric called <code>items_handled_total</code>, you would use code similar to the &#13;
<span class="keep-together">following</span>:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="c1">// create the counter</code>&#13;
<code class="kd">var</code> <code class="nx">totalItemsHandled</code> <code class="p">=</code> <code class="nx">prometheus</code><code class="p">.</code><code class="nx">NewCounter</code><code class="p">(</code>&#13;
  <code class="nx">prometheus</code><code class="p">.</code><code class="nx">CounterOpts</code><code class="p">{</code>&#13;
    <code class="nx">Name</code><code class="p">:</code> <code class="s">"items_handled_total"</code><code class="p">,</code>&#13;
    <code class="nx">Help</code><code class="p">:</code> <code class="s">"Total number of queue items handled."</code><code class="p">,</code>&#13;
  <code class="p">},</code>&#13;
<code class="p">)</code>&#13;
&#13;
<code class="c1">// register the counter</code>&#13;
<code class="nx">prometheus</code><code class="p">.</code><code class="nx">MustRegister</code><code class="p">(</code><code class="nx">totalItemsHandled</code><code class="p">)</code></pre>&#13;
&#13;
<p>Finally, you need to update the metric according to what’s happening in the application. Continuing the Counter example, you would use the <code>Inc()</code> method of the Counter to increment it:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code> <code class="nx">handleItem</code><code class="p">(</code><code class="nx">item</code> <code class="nx">Item</code><code class="p">)</code> <code class="p">{</code>&#13;
&#13;
  <code class="c1">// item handling code...</code>&#13;
&#13;
  <code class="c1">// increment the counter as we handle items</code>&#13;
  <code class="nx">totalItemsHandled</code><code class="p">.</code><code class="nx">Inc</code><code class="p">()</code>&#13;
<code class="p">}</code></pre>&#13;
&#13;
<p>Instrumenting an application using the Prometheus libraries is relatively simple. The more complicated task is to determine the metrics that your application should expose. In the following sections, we will discuss different methods or philosophies you can use as a starting point to select metrics.<a data-primary="Prometheus" data-secondary="instrumenting applications for" data-startref="ix_Prominst" data-type="indexterm" id="idm45611972209208"/><a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-startref="ix_appSKmtrcins" data-tertiary="instrumenting applications" data-type="indexterm" id="idm45611972208232"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="USE Method" data-type="sect2"><div class="sect2" id="idm45611972345096">&#13;
<h2>USE Method</h2>&#13;
&#13;
<p>The USE method, proposed by <a href="http://www.brendangregg.com/usemethod.html">Brendan Gregg</a>, focuses on system resources.<a data-primary="USE (Utilization, Saturation, Errors) method" data-type="indexterm" id="idm45611972163096"/><a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-tertiary="USE method" data-type="indexterm" id="idm45611972162360"/> When using this method, you capture Utilization, Saturation, and Errors (USE) for each of the resources your application uses. These resources typically include CPU, memory, disk, etc. They can also include resources that exist within the application software, such as queues, thread pools, etc.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="RED Method" data-type="sect2"><div class="sect2" id="idm45611972202072">&#13;
<h2>RED Method</h2>&#13;
&#13;
<p>In contrast to the USE method, the RED method focuses more on the services themselves instead of the underlying resources.<a data-primary="RED (Rate, Errors, Duration) method" data-type="indexterm" id="idm45611972200744"/><a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-tertiary="RED method" data-type="indexterm" id="idm45611972200024"/> Initially proposed by <a href="https://oreil.ly/sW3al">Tom Wilkie</a>, the RED method captures the Rate, Errors, and Durations of requests that the service handles. The RED method can be better suited for online services, as the metrics provide insight into your users’ experience and how they perceive the service from their standpoint.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Four Golden Signals" data-type="sect2"><div class="sect2" id="idm45611972197432">&#13;
<h2>The Four Golden Signals</h2>&#13;
&#13;
<p>Another philosophy you can adopt is to measure the four golden signals, as proposed <a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-tertiary="four golden signals" data-type="indexterm" id="idm45611972196104"/><a data-primary="four golden signals" data-type="indexterm" id="idm45611972194840"/>by Google in <a href="https://oreil.ly/iv1bJ"><em>Site Reliability Engineering</em></a> (O’Reilly). Google suggests you measure four critical signals for every service: Latency, Traffic, Errors, and Saturation. You might notice that these are somewhat similar to the metrics captured as part of the RED method, with the addition of Saturation.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="App-Specific Metrics" data-type="sect2"><div class="sect2" id="idm45611972192664">&#13;
<h2>App-Specific Metrics</h2>&#13;
&#13;
<p>The USE method, RED method, and four golden signals capture generic metrics that are applicable across most if not all applications.<a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-tertiary="app-specific metrics" data-type="indexterm" id="idm45611972191256"/> There is an additional class of metrics that surface app-specific information. For example, how long does it take to add an item to the shopping cart? Or how much time does it take to connect a customer with an agent? Typically, these metrics are correlated with business key performance indicators (KPIs).</p>&#13;
&#13;
<p>Regardless of the method you choose, exporting metrics from your application is critical to its success. Once you have access to those metrics, you can build dashboards to visualize the behavior of your system, set up alerts to notify your on-call teams when something goes wrong, and perform trend analysis to derive business intelligence that can advance your organization.<a data-primary="metrics" data-secondary="application" data-startref="ix_mtrcapp" data-type="indexterm" id="idm45611972188728"/><a data-primary="applications on Kubernetes" data-secondary="exposing metrics" data-startref="ix_appsKmtrc" data-type="indexterm" id="idm45611972187480"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Instrumenting Services for Distributed Tracing" data-type="sect1"><div class="sect1" id="idm45611972185992">&#13;
<h1>Instrumenting Services for Distributed Tracing</h1>&#13;
&#13;
<p>Distributed tracing enables you to analyze applications that are composed of multiple services.<a data-primary="applications on Kubernetes" data-secondary="instrumenting for distributed tracing" data-type="indexterm" id="ix_appsKDT"/><a data-primary="distributed tracing" data-secondary="instrumenting services for" data-type="indexterm" id="ix_dsttrcins"/> They provide visibility into the execution flow of a request as it traverses the different services that make up the application. As discussed in <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a>, Kubernetes-based platforms can offer distributed tracing as a platform service using systems such as <a href="https://www.jaegertracing.io">Jaeger</a> or <a href="https://zipkin.io">Zipkin</a>. However, similar to monitoring and metrics, you must instrument services to take advantage of distributed tracing.<a data-primary="Zipkin" data-type="indexterm" id="idm45611972137528"/><a data-primary="OpenTracing" data-type="indexterm" id="idm45611972136824"/><a data-primary="Jaeger" data-type="indexterm" id="idm45611972136152"/> In this section, we will explore how to instrument services using Jaeger and <a href="https://opentracing.io">OpenTracing</a>. First, we will discuss how to initialize a tracer. Then, we will dive into how to create spans within a service.  A <em>span</em> is a named, timed operation that is the building block of a distributed trace.  Finally, we will explore how to propagate tracing context from one service to another. We will use Go and the Go libraries for examples, but the concepts are applicable to other programming languages.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Initializing the Tracer" data-type="sect2"><div class="sect2" id="idm45611972133848">&#13;
<h2>Initializing the Tracer</h2>&#13;
&#13;
<p>Before being able to create spans within the service, you must initialize the tracer. <a data-primary="distributed tracing" data-secondary="instrumenting services for" data-tertiary="initializing the tracer" data-type="indexterm" id="idm45611972132344"/><a data-primary="applications on Kubernetes" data-secondary="instrumenting for distributed tracing" data-tertiary="initializing the tracer" data-type="indexterm" id="idm45611972131128"/><a data-primary="Jaeger, initializing the tracer using" data-type="indexterm" id="idm45611972129880"/>Part of the initialization involves configuring the tracer according to the environment the application is running. The tracer needs to know the service name, the URL to send trace information, etc. For these settings, we recommend using the Jaeger client library environment variables. For example, you can set the service name using the <code>JAEGER_SERVICE_NAME</code> environment variable.</p>&#13;
&#13;
<p>In addition to configuring the tracer, you can integrate the tracer with your metrics and logging libraries as you initialize the tracer. The tracer uses the metrics library to emit metrics about what’s happening with the tracer, such as the number of traces and spans sampled, the number of successfully reported spans, and others. On the other hand, the tracer leverages the logging libraries to emit logs when it encounters errors. You can also configure the tracer to log spans, which is rather useful in development.</p>&#13;
&#13;
<p>To initialize a Jaeger tracer in a Go service, you would add code to your application similar to the following. In this case, we are using Prometheus as the metrics library and Go’s standard logging library:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">package</code><code> </code><code class="nx">main</code><code>&#13;
</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="p">(</code><code>&#13;
</code><code>	</code><code class="s">"log"</code><code>&#13;
</code><code>&#13;
</code><code>	</code><code class="nx">jaeger</code><code> </code><code class="s">"github.com/uber/jaeger-client-go"</code><code>&#13;
</code><code>	</code><code class="s">"github.com/uber/jaeger-client-go/config"</code><code>&#13;
</code><code>	</code><code class="s">"github.com/uber/jaeger-lib/metrics/prometheus"</code><code>&#13;
</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="kd">func</code><code> </code><code class="nx">main</code><code class="p">(</code><code class="p">)</code><code> </code><code class="p">{</code><code>&#13;
</code><code>  </code><code class="c1">// app initialization code...&#13;
</code><code>&#13;
</code><code>  </code><code class="nx">metricsFactory</code><code> </code><code class="o">:=</code><code> </code><code class="nx">prometheus</code><code class="p">.</code><code class="nx">New</code><code class="p">(</code><code class="p">)</code><code> </code><a class="co" href="#callout_application_considerations_CO1-1" id="co_application_considerations_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
&#13;
  </code><code class="nx">cfg</code><code> </code><code class="o">:=</code><code> </code><code class="nx">config</code><code class="p">.</code><code class="nx">Configuration</code><code class="p">{</code><code class="p">}</code><code> </code><a class="co" href="#callout_application_considerations_CO1-2" id="co_application_considerations_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
  </code><code class="nx">tracer</code><code class="p">,</code><code> </code><code class="nx">closer</code><code class="p">,</code><code> </code><code class="nx">err</code><code> </code><code class="o">:=</code><code> </code><code class="nx">cfg</code><code class="p">.</code><code class="nx">NewTracer</code><code class="p">(</code><code> </code><a class="co" href="#callout_application_considerations_CO1-3" id="co_application_considerations_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
    </code><code class="nx">config</code><code class="p">.</code><code class="nx">Metrics</code><code class="p">(</code><code class="nx">metricsFactory</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>    </code><code class="nx">config</code><code class="p">.</code><code class="nx">Logger</code><code class="p">(</code><code class="nx">jaeger</code><code class="p">.</code><code class="nx">StdLogger</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>  </code><code class="p">)</code><code>&#13;
</code><code>  </code><code class="k">if</code><code> </code><code class="nx">err</code><code> </code><code class="o">!=</code><code> </code><code class="kc">nil</code><code> </code><code class="p">{</code><code>&#13;
</code><code>    </code><code class="nx">log</code><code class="p">.</code><code class="nx">Fatalf</code><code class="p">(</code><code class="s">"error initializing tracer: %v"</code><code class="p">,</code><code> </code><code class="nx">err</code><code class="p">)</code><code>&#13;
</code><code>  </code><code class="p">}</code><code>&#13;
</code><code>&#13;
</code><code>  </code><code class="k">defer</code><code> </code><code class="nx">closer</code><code class="p">.</code><code class="nx">Close</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code>  </code><code class="c1">// continue main()...&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_application_considerations_CO1-1" id="callout_application_considerations_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Create a Prometheus metrics factory that Jaeger can use to emit metrics.</p></dd>&#13;
<dt><a class="co" href="#co_application_considerations_CO1-2" id="callout_application_considerations_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Create a default Jaeger configuration with no hardcoded configuration (use environment variables instead).</p></dd>&#13;
<dt><a class="co" href="#co_application_considerations_CO1-3" id="callout_application_considerations_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Create a new tracer from the configuration, and provide the metrics factory and the Go standard library logger.</p></dd>&#13;
</dl>&#13;
&#13;
<p>With the tracer initialized, we can start creating spans in our service.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Creating Spans" data-type="sect2"><div class="sect2" id="idm45611971983720">&#13;
<h2>Creating Spans</h2>&#13;
&#13;
<p>Now that we have a tracer, we can start creating spans within our service.<a data-primary="distributed tracing" data-secondary="instrumenting services for" data-tertiary="creating spans" data-type="indexterm" id="idm45611971982312"/><a data-primary="applications on Kubernetes" data-secondary="instrumenting for distributed tracing" data-tertiary="creating spans" data-type="indexterm" id="idm45611971981096"/> Assuming the service is somewhere in the middle of the request processing flow, the service needs to deserialize the incoming span information from the previous service and create a child span. Our example is an HTTP service, so the span context is propagated via HTTP headers. The following code extracts the context from the headers and creates a new span. Note that the tracer we initialized in the previous section must be in scope:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">package</code><code> </code><code class="nx">main</code><code>&#13;
</code><code>&#13;
</code><code class="kn">import</code><code> </code><code class="p">(</code><code>&#13;
</code><code>	</code><code class="s">"github.com/opentracing/opentracing-go"</code><code>&#13;
</code><code>	</code><code class="s">"github.com/opentracing/opentracing-go/ext"</code><code>&#13;
</code><code>	</code><code class="s">"net/http"</code><code>&#13;
</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="kd">func</code><code> </code><code class="p">(</code><code class="nx">s</code><code> </code><code class="nx">server</code><code class="p">)</code><code> </code><code class="nx">handleListPayments</code><code class="p">(</code><code class="nx">w</code><code> </code><code class="nx">http</code><code class="p">.</code><code class="nx">ResponseWriter</code><code class="p">,</code><code> </code><code class="nx">req</code><code> </code><code class="o">*</code><code class="nx">http</code><code class="p">.</code><code class="nx">Request</code><code class="p">)</code><code> </code><code class="p">{</code><code>&#13;
</code><code>	</code><code class="nx">spanCtx</code><code class="p">,</code><code> </code><code class="nx">err</code><code> </code><code class="o">:=</code><code> </code><code class="nx">s</code><code class="p">.</code><code class="nx">tracer</code><code class="p">.</code><code class="nx">Extract</code><code class="p">(</code><code> </code><a class="co" href="#callout_application_considerations_CO2-1" id="co_application_considerations_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
		</code><code class="nx">opentracing</code><code class="p">.</code><code class="nx">HTTPHeaders</code><code class="p">,</code><code>&#13;
</code><code>		</code><code class="nx">opentracing</code><code class="p">.</code><code class="nx">HTTPHeadersCarrier</code><code class="p">(</code><code class="nx">req</code><code class="p">.</code><code class="nx">Header</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>	</code><code class="p">)</code><code>&#13;
</code><code>	</code><code class="k">if</code><code> </code><code class="nx">err</code><code> </code><code class="o">!=</code><code> </code><code class="kc">nil</code><code> </code><code class="p">{</code><code>&#13;
</code><code>		</code><code class="c1">// handle the error&#13;
</code><code>	</code><code class="p">}</code><code>&#13;
</code><code>&#13;
</code><code>	</code><code class="nx">span</code><code> </code><code class="o">:=</code><code> </code><code class="nx">opentracing</code><code class="p">.</code><code class="nx">StartSpan</code><code class="p">(</code><code> </code><a class="co" href="#callout_application_considerations_CO2-2" id="co_application_considerations_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
		</code><code class="s">"listPayments"</code><code class="p">,</code><code>&#13;
</code><code>		</code><code class="nx">ext</code><code class="p">.</code><code class="nx">RPCServerOption</code><code class="p">(</code><code class="nx">spanCtx</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>	</code><code class="p">)</code><code>&#13;
</code><code>	</code><code class="k">defer</code><code> </code><code class="nx">span</code><code class="p">.</code><code class="nx">Finish</code><code class="p">(</code><code class="p">)</code><code>&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_application_considerations_CO2-1" id="callout_application_considerations_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Extract the context information from the HTTP headers.</p></dd>&#13;
<dt><a class="co" href="#co_application_considerations_CO2-2" id="callout_application_considerations_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Create a new span using the extracted span context.</p></dd>&#13;
</dl>&#13;
&#13;
<p>As the service handles the request, it can add child spans to the span we just created. As an example, let’s assume the service calls a function to perform a SQL query. We can use the following code to create a child span for the function and set the operation name to <code>listPayments</code>:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kd">func</code> <code class="nx">listPayments</code><code class="p">(</code><code class="nx">ctx</code> <code class="nx">context</code><code class="p">.</code><code class="nx">Context</code><code class="p">)</code> <code class="p">([]</code><code class="nx">Payment</code><code class="p">,</code> <code class="kt">error</code><code class="p">)</code> <code class="p">{</code>&#13;
  <code class="nx">span</code><code class="p">,</code> <code class="nx">ctx</code> <code class="o">:=</code> <code class="nx">opentracing</code><code class="p">.</code><code class="nx">StartSpanFromContext</code><code class="p">(</code><code class="nx">ctx</code><code class="p">,</code> <code class="s">"listPayments"</code><code class="p">)</code>&#13;
  <code class="k">defer</code> <code class="nx">span</code><code class="p">.</code><code class="nx">Finish</code><code class="p">()</code>&#13;
&#13;
  <code class="c1">// run sql query</code>&#13;
<code class="p">}</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Propagate Context" data-type="sect2"><div class="sect2" id="idm45611971785064">&#13;
<h2>Propagate Context</h2>&#13;
&#13;
<p>Up to this point, we’ve created spans within the same service or process.<a data-primary="distributed tracing" data-secondary="instrumenting services for" data-tertiary="propagating context" data-type="indexterm" id="idm45611971772792"/><a data-primary="applications on Kubernetes" data-secondary="instrumenting for distributed tracing" data-tertiary="propagating context" data-type="indexterm" id="idm45611971771576"/> When there are other services involved in processing a request, we need to propagate the trace context over the wire for the service on the other end. As discussed in the previous section, you can use HTTP headers to propagate the context.</p>&#13;
&#13;
<p>The OpenTracing <a data-primary="OpenTracing" data-secondary="helper functions inject context into HTTP headers" data-type="indexterm" id="idm45611971769656"/>libraries provide helper functions you can use to inject the context into HTTP headers.<a data-primary="HTTP headers, injecting context into" data-type="indexterm" id="idm45611971768552"/> The following code shows an example that uses the Go standard library HTTP client to create and send the request:</p>&#13;
&#13;
<pre data-code-language="go" data-type="programlisting"><code class="kn">import</code><code> </code><code class="p">(</code><code>&#13;
</code><code>    </code><code class="s">"github.com/opentracing/opentracing-go"</code><code>&#13;
</code><code>  	</code><code class="s">"github.com/opentracing/opentracing-go/ext"</code><code>&#13;
</code><code>&#13;
</code><code>	  </code><code class="s">"net/http"</code><code>&#13;
</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1">// create an HTTP request&#13;
</code><code class="nx">req</code><code class="p">,</code><code> </code><code class="nx">err</code><code> </code><code class="o">:=</code><code> </code><code class="nx">http</code><code class="p">.</code><code class="nx">NewRequest</code><code class="p">(</code><code class="s">"GET"</code><code class="p">,</code><code> </code><code class="nx">serviceURL</code><code class="p">,</code><code> </code><code class="kc">nil</code><code class="p">)</code><code>&#13;
</code><code class="k">if</code><code> </code><code class="nx">err</code><code> </code><code class="o">!=</code><code> </code><code class="kc">nil</code><code> </code><code class="p">{</code><code>&#13;
</code><code>  </code><code class="c1">// handle error&#13;
</code><code class="p">}</code><code>&#13;
</code><code>&#13;
</code><code class="c1">// inject context into the request's HTTP headers&#13;
</code><code class="nx">ext</code><code class="p">.</code><code class="nx">SpanKindRPCClient</code><code class="p">.</code><code class="nx">Set</code><code class="p">(</code><code class="nx">span</code><code class="p">)</code><code> </code><a class="co" href="#callout_application_considerations_CO3-1" id="co_application_considerations_CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="nx">ext</code><code class="p">.</code><code class="nx">HTTPUrl</code><code class="p">.</code><code class="nx">Set</code><code class="p">(</code><code class="nx">span</code><code class="p">,</code><code> </code><code class="nx">url</code><code class="p">)</code><code>&#13;
</code><code class="nx">ext</code><code class="p">.</code><code class="nx">HTTPMethod</code><code class="p">.</code><code class="nx">Set</code><code class="p">(</code><code class="nx">span</code><code class="p">,</code><code> </code><code class="s">"GET"</code><code class="p">)</code><code>&#13;
</code><code class="nx">span</code><code class="p">.</code><code class="nx">Tracer</code><code class="p">(</code><code class="p">)</code><code class="p">.</code><code class="nx">Inject</code><code class="p">(</code><code> </code><a class="co" href="#callout_application_considerations_CO3-2" id="co_application_considerations_CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
  </code><code class="nx">span</code><code class="p">.</code><code class="nx">Context</code><code class="p">(</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code>  </code><code class="nx">opentracing</code><code class="p">.</code><code class="nx">HTTPHeaders</code><code class="p">,</code><code>&#13;
</code><code>  </code><code class="nx">opentracing</code><code class="p">.</code><code class="nx">HTTPHeadersCarrier</code><code class="p">(</code><code class="nx">req</code><code class="p">.</code><code class="nx">Header</code><code class="p">)</code><code class="p">,</code><code>&#13;
</code><code class="p">)</code><code>&#13;
</code><code>&#13;
</code><code class="c1">// send the request&#13;
</code><code class="nx">resp</code><code class="p">,</code><code> </code><code class="nx">err</code><code> </code><code class="o">:=</code><code> </code><code class="nx">http</code><code class="p">.</code><code class="nx">DefaultClient</code><code class="p">.</code><code class="nx">Do</code><code class="p">(</code><code class="nx">req</code><code class="p">)</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_application_considerations_CO3-1" id="callout_application_considerations_CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Adds a tag to mark the span as the client side of a service call.</p></dd>&#13;
<dt><a class="co" href="#co_application_considerations_CO3-2" id="callout_application_considerations_CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Injects the span context into the request’s HTTP headers.</p></dd>&#13;
</dl>&#13;
&#13;
<p>As we’ve discussed through these sections, instrumenting an application for tracing involves initializing a tracer, creating spans within the service, and propagating the span context to other services. There is additional functionality that you should explore, including tagging, logging, and baggage. If the platform team offers tracing as a platform service, now you have an idea of what it takes to take advantage of it.<a data-primary="applications on Kubernetes" data-secondary="instrumenting for distributed tracing" data-startref="ix_appsKDT" data-type="indexterm" id="idm45611971617352"/><a data-primary="distributed tracing" data-secondary="instrumenting services for" data-startref="ix_dsttrcins" data-type="indexterm" id="idm45611971616168"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45611971614856">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>There are multiple things you can do to make your applications run better in Kubernetes. While most require investing time and effort to implement, we find that they are critical to achieve production-grade outcomes for your applications. As you onboard applications to your platform, make sure to consider the guidance provided in this chapter, including injecting configuration and secrets at runtime, specifying resource requests and limits, exposing application health information using probes, and instrumenting applications with logs, metrics, and traces.<a data-primary="applications on Kubernetes" data-startref="ix_appsK" data-type="indexterm" id="idm45611971612984"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>