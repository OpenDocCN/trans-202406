<html><head></head><body><section data-pdf-bookmark="Chapter 3. Monitoring and Logging in Kubernetes" data-type="chapter" epub:type="chapter"><div class="chapter" id="monitoring_and_logging_in_kubernetes">&#13;
<h1><span class="label">Chapter 3. </span>Monitoring and Logging in Kubernetes</h1>&#13;
&#13;
&#13;
<p>In this chapter, we discuss best practices for monitoring and logging&#13;
in Kubernetes. We’ll dive into the details of different monitoring&#13;
patterns, important metrics to collect, and building dashboards from&#13;
these raw metrics. We then wrap up with examples of implementing&#13;
monitoring for your Kubernetes cluster.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics Versus Logs" data-type="sect1"><div class="sect1" id="id189">&#13;
<h1>Metrics Versus Logs</h1>&#13;
&#13;
<p>You first need to understand the difference between log collection&#13;
and metrics collection. They are complementary but serve&#13;
different purposes:</p>&#13;
<dl>&#13;
<dt>Metrics</dt>&#13;
<dd>&#13;
<p>A<a data-primary="metrics" data-secondary="defined" data-type="indexterm" id="id526"/> series of numbers measured over a period of time.</p>&#13;
</dd>&#13;
<dt>Logs</dt>&#13;
<dd>&#13;
<p>Logs keep<a data-primary="logging" data-secondary="defined" data-type="indexterm" id="id527"/> track of what happens while a program is running, including any errors, warnings, or notable events that occur.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>A example of where you would need to use both metrics and logging is&#13;
when an application is performing poorly. Our first indication of the&#13;
issue might be an alert of high latency on the pods hosting the&#13;
application, but the metrics might not give a good indication of the&#13;
issue. We then can look into our logs to investigate&#13;
errors that are being emitted from the application.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring Techniques" data-type="sect1"><div class="sect1" id="id19">&#13;
<h1>Monitoring Techniques</h1>&#13;
&#13;
<p>Closed-box monitoring <a data-primary="monitoring" data-secondary="closed-box monitoring" data-type="indexterm" id="id528"/><a data-primary="closed-box monitoring" data-type="indexterm" id="id529"/>focuses on monitoring from the outside of an&#13;
application and is what’s been used traditionally when monitoring&#13;
systems for components like CPU, memory, storage, and so on. Closed-box&#13;
monitoring can still be useful for monitoring at the infrastructure&#13;
level, but it lacks insights and context into how the application is&#13;
operating. For example, to test whether a cluster is healthy, we might schedule a&#13;
pod, and if it’s successful, we know that the scheduler and service&#13;
discovery are healthy within our cluster, so we can assume the cluster&#13;
components are healthy.</p>&#13;
&#13;
<p>Open-box monitoring <a data-primary="monitoring" data-secondary="open-box monitoring" data-type="indexterm" id="id530"/><a data-primary="open-box monitoring" data-type="indexterm" id="id531"/>focuses on the details in the context of the&#13;
application state, such as total HTTP requests, number of 500 errors,&#13;
latency of requests, and so on. With open-box monitoring, we can begin to&#13;
understand the <em>why</em> of our system state. It allows us to ask, “Why did the disk fill up?” and not just state, “The disk filled&#13;
up.”</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring Patterns" data-type="sect1"><div class="sect1" id="id20">&#13;
<h1>Monitoring Patterns</h1>&#13;
&#13;
<p>You might<a data-primary="monitoring" data-secondary="patterns of" data-type="indexterm" id="monitor-pattern"/> look at monitoring and say, “How difficult can this be? We’ve always&#13;
monitored our systems.” The concept of monitoring isn’t new, and we have many tools at our disposal to help us understand how our systems are performing. But platforms like Kubernetes are much more dynamic and transient, so you’ll need to change your thinking about how to monitor these environments. For example, when monitoring a virtual machine (VM) you expect that VM to be up 24/7 and all its state preserved. In Kubernetes, pods can be very dynamic and short-lived, so you need to have monitoring in place that can handle this dynamic and transient nature.</p>&#13;
&#13;
<p>There are two monitoring patterns to focus on when&#13;
monitoring distributed systems. The <em>USE</em> method,<a data-primary="USE method (monitoring)" data-type="indexterm" id="id532"/> popularized by Brendan Gregg, focuses on the&#13;
following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>U—Utilization</p>&#13;
</li>&#13;
<li>&#13;
<p>S—Saturation</p>&#13;
</li>&#13;
<li>&#13;
<p>E—Errors</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This method is focused on infrastructure monitoring because there are&#13;
limitations on using it for application-level monitoring. The&#13;
USE method is described as “For every resource, check utilization,&#13;
saturation, and error rates.” This method lets you quickly&#13;
identify resource constraints and error rates of your systems. For&#13;
example, to check the health of the network for your nodes in the&#13;
cluster, you will want to monitor the utilization, saturation, and error&#13;
rate to be able to easily identify any network bottlenecks or errors in&#13;
the network stack. The USE method is a tool in a larger toolbox and is&#13;
not the only method you will utilize to monitor your systems.</p>&#13;
&#13;
<p>Another monitoring approach, called<a data-primary="RED method (monitoring)" data-type="indexterm" id="id533"/> the <em>RED</em> method, was popularized&#13;
by Tom Wilkie. The RED method approach is focused on the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>R—Rate</p>&#13;
</li>&#13;
<li>&#13;
<p>E—Errors</p>&#13;
</li>&#13;
<li>&#13;
<p>D—Duration</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The philosophy was taken <a data-primary="Four Golden Signals (Google)" data-type="indexterm" id="id534"/>from Google’s <em>Four Golden Signals</em>:</p>&#13;
<dl>&#13;
<dt>Latency</dt>&#13;
<dd>&#13;
<p>How long it takes to serve a request</p>&#13;
</dd>&#13;
<dt>Traffic</dt>&#13;
<dd>&#13;
<p>How much demand is placed on your system</p>&#13;
</dd>&#13;
<dt>Errors</dt>&#13;
<dd>&#13;
<p>The rate of requests that are failing</p>&#13;
</dd>&#13;
<dt>Saturation</dt>&#13;
<dd>&#13;
<p>How utilized your service is</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As an example, you could use this method to monitor a frontend service&#13;
running in Kubernetes to calculate the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How many requests is my frontend service processing?</p>&#13;
</li>&#13;
<li>&#13;
<p>How many 500 errors are users of the service receiving?</p>&#13;
</li>&#13;
<li>&#13;
<p>Is the service overutilized by requests?</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As you can see from the previous example, this method is more focused on the users’ experience with the service.</p>&#13;
&#13;
<p>The USE and RED methods are complementary given that the USE&#13;
method focuses on the infrastructure components and the RED method&#13;
focuses on monitoring the end-user experience for the <a data-primary="monitoring" data-secondary="patterns of" data-startref="monitor-pattern" data-type="indexterm" id="id535"/>application.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes Metrics Overview" data-type="sect1"><div class="sect1" id="id190">&#13;
<h1>Kubernetes Metrics Overview</h1>&#13;
&#13;
<p>Now that <a data-primary="clusters" data-secondary="components of" data-type="indexterm" id="id536"/>we know the different monitoring techniques and patterns, let’s look at what components you should be monitoring in your Kubernetes cluster. A Kubernetes cluster consists of control-plane components and node components. The control-plane components consist of the API server, etcd, scheduler, and controller manager. The nodes consist of the kubelet, container runtime, kube-proxy, kube-dns, and pods. You need to monitor all these components to ensure a healthy cluster and application.</p>&#13;
&#13;
<p>Kubernetes exposes these metrics in a variety of ways, so let’s look at different components that you can use to collect metrics within your&#13;
cluster.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="cAdvisor" data-type="sect2"><div class="sect2" id="id21">&#13;
<h2>cAdvisor</h2>&#13;
&#13;
<p>Container Advisor, <a data-primary="monitoring" data-secondary="with cAdvisor" data-secondary-sortas="cAdvisor" data-type="indexterm" id="id537"/><a data-primary="cAdvisor" data-type="indexterm" id="id538"/><a data-primary="metrics" data-secondary="with cAdvisor" data-secondary-sortas="cAdvisor" data-type="indexterm" id="id539"/>or cAdvisor, is an open source project that&#13;
collects resources and metrics for containers running on a node.&#13;
cAdvisor is built into the Kubernetes kubelet, which runs on every&#13;
node in the cluster. It collects memory and CPU metrics through&#13;
the Linux control group (cgroup) tree. If you are not familiar with cgroups, it’s a Linux kernel feature that allows isolation of resources for CPU, disk I/O, or <span class="keep-together">network</span> I/O. cAdvisor will also collect disk metrics through statfs, which is built into the Linux kernel. These are implementation details you don’t really need to worry about, but you should understand how these metrics are exposed and the type of information you can collect. You should consider cAdvisor as the source of truth for all container metrics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics Server" data-type="sect2"><div class="sect2" id="id22">&#13;
<h2>Metrics Server</h2>&#13;
&#13;
<p>The<a data-primary="monitoring" data-secondary="with metrics server" data-secondary-sortas="metrics server" data-type="indexterm" id="id540"/><a data-primary="metrics" data-secondary="with metrics server" data-secondary-sortas="metrics server" data-type="indexterm" id="id541"/><a data-primary="Metrics Server API" data-type="indexterm" id="id542"/> Kubernetes metrics server and Metrics Server API replace the deprecated Heapster. Heapster had some architectural disadvantages with how it implemented the data sink, which caused a lot of vendored solutions in the core Heapster code base. This issue was solved by implementing a resource and Custom Metrics API as an aggregated API in Kubernetes. This allows implementations to be switched out without changing the API.</p>&#13;
&#13;
<p>There are two aspects to understand in the Metrics Server API and metrics server.</p>&#13;
&#13;
<p>First, the canonical implementation of the Resource Metrics API is the metrics server. The metrics server gathers resource metrics such as CPU and memory. It gathers these metrics from the kubelet’s API and then stores them in memory. Kubernetes uses these resource metrics in the scheduler, Horizontal Pod Autoscaler (HPA), and Vertical Pod Autoscaler (VPA).</p>&#13;
&#13;
<p>Second, <a data-primary="Custom Metrics API" data-type="indexterm" id="id543"/>the Custom Metrics API allows monitoring systems to collect arbitrary metrics. This allows monitoring solutions to build custom adapters that will allow for extending outside the core resource metrics. For example, Prometheus built one of the first custom metrics adapters, which allows you to use the HPA based on a custom metric. This opens up better scaling based on your use case because now you can bring in metrics like queue size and scale based on a metric that might be external to Kubernetes.</p>&#13;
&#13;
<p>Now that there is a standardized Metrics API, this opens up many possibilities to scale outside the plain old CPU and memory metrics.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="kube-state-metrics" data-type="sect2"><div class="sect2" id="id23">&#13;
<h2>kube-state-metrics</h2>&#13;
&#13;
<p>kube-state-metrics is <a data-primary="monitoring" data-secondary="with kube-state-metrics" data-secondary-sortas="kube-state-metrics" data-type="indexterm" id="monitor-kube-state-metric"/><a data-primary="kube-state-metrics" data-type="indexterm" id="kube-state-metric"/><a data-primary="metrics" data-secondary="with kube-state-metrics" data-secondary-sortas="kube-state-metrics" data-type="indexterm" id="metrics-kube-state-metric"/>a Kubernetes add-on that monitors the object&#13;
stored in Kubernetes. Where cAdvisor and Metrics Server are used to&#13;
provide detailed metrics on resource usage, kube-state-metrics is&#13;
focused on identifying conditions on Kubernetes objects deployed to your&#13;
cluster.</p>&#13;
&#13;
<p class="pagebreak-before">Following are some questions that kube-state-metrics can answer for you:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Pods</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How many pods are deployed to the cluster?</p>&#13;
</li>&#13;
<li>&#13;
<p>How many pods are in a pending state?</p>&#13;
</li>&#13;
<li>&#13;
<p>Are there enough resources to serve a pods request?</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Deployments</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>How many pods are in a running state versus a desired state?</p>&#13;
</li>&#13;
<li>&#13;
<p>How many replicas are available?</p>&#13;
</li>&#13;
<li>&#13;
<p>What deployments have been updated?</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Nodes</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>What’s the status of my nodes?</p>&#13;
</li>&#13;
<li>&#13;
<p>What are the allottable CPU cores in my cluster?</p>&#13;
</li>&#13;
<li>&#13;
<p>Are there any nodes that are unschedulable?</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Jobs</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>When did a job start?</p>&#13;
</li>&#13;
<li>&#13;
<p>When did a job complete?</p>&#13;
</li>&#13;
<li>&#13;
<p>How many jobs failed?</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>As of this writing, kube-state-metrics tracks many object types. These are always expanding, and you can find the documentation <a data-primary="monitoring" data-secondary="with kube-state-metrics" data-secondary-sortas="kube-state-metrics" data-startref="monitor-kube-state-metric" data-type="indexterm" id="id544"/><a data-primary="kube-state-metrics" data-startref="kube-state-metric" data-type="indexterm" id="id545"/><a data-primary="metrics" data-secondary="with kube-state-metrics" data-secondary-sortas="kube-state-metrics" data-startref="metrics-kube-state-metric" data-type="indexterm" id="id546"/>in the <a href="https://oreil.ly/bdTp2">GitHub repository</a>.</p>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Metrics Do I Monitor?" data-type="sect1"><div class="sect1" id="id191">&#13;
<h1>What Metrics Do I Monitor?</h1>&#13;
&#13;
<p>The<a data-primary="metrics" data-secondary="what to monitor" data-type="indexterm" id="metrics-what-to-monitor"/><a data-primary="monitoring" data-secondary="what to monitor" data-type="indexterm" id="monitor-what-to-monitor"/> easy answer is “everything,” but if you try to monitor too much,&#13;
you can create noise that filters out the real signals into which you need to have insight. When we think about monitoring in Kubernetes, we want  a layered approach that takes into account the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Physical or virtual nodes</p>&#13;
</li>&#13;
<li>&#13;
<p>Cluster components</p>&#13;
</li>&#13;
<li>&#13;
<p>Cluster add-ons</p>&#13;
</li>&#13;
<li>&#13;
<p>End-user applications</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p class="pagebreak-before">Using this layered approach to monitoring allows you to more easily identify the correct signals in your monitoring system. It allows you to&#13;
approach issues in a more targeted way. For example, if you have pods going into a pending state, you can start with resource&#13;
utilization of the nodes, and if all is OK, you can target cluster-level components.</p>&#13;
&#13;
<p>Following are metrics you would want to target in your system:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Nodes</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>CPU utilization</p>&#13;
</li>&#13;
<li>&#13;
<p>Memory utilization</p>&#13;
</li>&#13;
<li>&#13;
<p>Network utilization</p>&#13;
</li>&#13;
<li>&#13;
<p>Disk utilization</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Cluster components</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>etcd latency</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Cluster add-ons</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Cluster Autoscaler</p>&#13;
</li>&#13;
<li>&#13;
<p>Ingress controller</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Application</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Container memory utilization and saturation</p>&#13;
</li>&#13;
<li>&#13;
<p>Container CPU utilization</p>&#13;
</li>&#13;
<li>&#13;
<p>Container network utilization and error rate</p>&#13;
</li>&#13;
<li>&#13;
<p>Application framework–specific <a data-primary="metrics" data-secondary="what to monitor" data-startref="metrics-what-to-monitor" data-type="indexterm" id="id547"/><a data-primary="monitoring" data-secondary="what to monitor" data-startref="monitor-what-to-monitor" data-type="indexterm" id="id548"/>metrics</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring Tools" data-type="sect1"><div class="sect1" id="id24">&#13;
<h1>Monitoring Tools</h1>&#13;
&#13;
<p>Many<a data-primary="monitoring" data-secondary="tools for" data-type="indexterm" id="monitor-tools"/> monitoring tools can integrate with Kubernetes,&#13;
and more arrive every day, building on their feature set to better integrate with Kubernetes. Following are a few popular tools that integrate with Kubernetes:</p>&#13;
<dl>&#13;
<dt>Prometheus</dt>&#13;
<dd>&#13;
<p>Prometheus is<a data-primary="Prometheus" data-type="indexterm" id="id549"/> an open source systems monitoring and alerting toolkit originally built at SoundCloud. Since its inception in 2012, many companies and organizations have adopted Prometheus, and the project has a very active developer and user community. It is now a standalone open source project and maintained independent of any company. To emphasize this, and to clarify the project’s governance structure, Prometheus joined the Cloud Native Computing Foundation (CNCF) in 2016 as the second hosted project, after Kubernetes.</p>&#13;
</dd>&#13;
<dt>InfluxDB</dt>&#13;
<dd>&#13;
<p>InfluxDB is<a data-primary="InfluxDB" data-type="indexterm" id="id550"/> a time-series database designed to handle high write and query loads. It is an integral component of the TICK (Telegraf, InfluxDB, Chronograf, and Kapacitor) stack. InfluxDB is meant to be used as a backing store for any use case involving large amounts of timestamped data, including DevOps monitoring, application metrics, IoT sensor data, and real-time analytics.</p>&#13;
</dd>&#13;
<dt>Datadog</dt>&#13;
<dd>&#13;
<p>Datadog <a data-primary="Datadog" data-type="indexterm" id="id551"/>provides a monitoring service for cloud-scale applications, providing monitoring of servers, databases, tools, and services through a SaaS-based data analytics platform.</p>&#13;
</dd>&#13;
<dt>Sysdig</dt>&#13;
<dd>&#13;
<p>Sysdig Monitor <a data-primary="Sysdig Monitor" data-type="indexterm" id="id552"/>is a commercial tool that provides Docker monitoring and Kubernetes monitoring for container-native apps. Sysdig also allows you to collect, correlate, and query Prometheus metrics with direct Kubernetes integration.</p>&#13;
</dd>&#13;
<dt>Cloud provider tools</dt>&#13;
<dd>&#13;
<p>All <a data-primary="cloud provider monitoring tools" data-type="indexterm" id="id553"/>major cloud providers provide monitoring tools for their different solutions. These tools are typically integrated into the cloud provider’s ecosystem and provide a good starting point for monitoring your Kubernetes cluster. Following are some examples of cloud provider tools:</p>&#13;
<dl>&#13;
<dt>GCP Stackdriver</dt>&#13;
<dd>&#13;
<p>Stackdriver Kubernetes Engine Monitoring <a data-primary="GCP Stackdriver" data-type="indexterm" id="id554"/><a data-primary="Stackdriver Kubernetes Engine Monitoring" data-type="indexterm" id="id555"/>is designed to monitor Google Kubernetes Engine (GKE) clusters. It manages monitoring and logging <span class="keep-together">services</span> together and its interface provides a dashboard customized for GKE clusters. Stackdriver Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications. It collects metrics, events, and metadata from Google Cloud Platform (GCP), Amazon Web Services (AWS), hosted uptime probes, and application <span class="keep-together">instrumentation.</span></p>&#13;
</dd>&#13;
<dt>Microsoft Azure Monitor for containers</dt>&#13;
<dd>&#13;
<p>Azure Monitor for containers<a data-primary="Microsoft Azure Monitor" data-type="indexterm" id="id556"/><a data-primary="Azure Monitor" data-type="indexterm" id="id557"/> is a feature designed to monitor the performance of container workloads deployed to either Azure Container Instances or managed Kubernetes clusters hosted on Azure Kubernetes Service. Monitoring your containers is critical, especially when you’re running a production cluster, at scale, with multiple applications. Azure Monitor for containers gives you performance visibility by collecting memory and processor metrics from controllers, nodes, and containers that are available in Kubernetes through the Metrics API. Container logs are also collected. After you enable monitoring from Kubernetes clusters, metrics and logs are automatically collected for you through a containerized version of the Log Analytics agent for Linux.</p>&#13;
</dd>&#13;
<dt>AWS Container Insights</dt>&#13;
<dd>&#13;
<p>If you<a data-primary="AWS Container Insights" data-type="indexterm" id="id558"/><a data-primary="CloudWatch Container Insights" data-type="indexterm" id="id559"/> use Amazon Elastic Container Service (ECS), Amazon Elastic Kubernetes Service, or other Kubernetes platforms on Amazon EC2, you can use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. The metrics include utilization for resources such as CPU, memory, disk, and network. Container Insights also provides diagnostic information, such as container restart failures, to help you isolate issues and resolve them quickly.</p>&#13;
</dd>&#13;
</dl>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>One important aspect when looking at implementing a tool to monitor&#13;
metrics is to look at how the metrics are stored. Tools that provide a time-series database with <span class="keep-together">key/value</span> pairs&#13;
will give you a higher degree of attributes for the metric.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Always evaluate monitoring tools you already have, because taking on&#13;
a new monitoring tool has a learning curve and a cost due to the operational&#13;
implementation of the tool. Many of the monitoring tools now have&#13;
integration into Kubernetes, so evaluate which ones you have today and&#13;
whether they will meet your <a data-primary="monitoring" data-secondary="tools for" data-startref="monitor-tools" data-type="indexterm" id="id560"/>requirements.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring Kubernetes Using Prometheus" data-type="sect1"><div class="sect1" id="id25">&#13;
<h1>Monitoring Kubernetes Using Prometheus</h1>&#13;
&#13;
<p>In this <a data-primary="monitoring" data-secondary="with Prometheus" data-secondary-sortas="Prometheus" data-type="indexterm" id="monitor-prometheus"/><a data-primary="Prometheus" data-type="indexterm" id="prometheus"/>section we focus on monitoring metrics with Prometheus,&#13;
which provides good integrations with Kubernetes labeling, service&#13;
discovery, and metadata. The high-level concepts we implement throughout the chapter will also apply to other monitoring systems.</p>&#13;
&#13;
<p>Prometheus is an open source project hosted by the CNCF. It was originally developed at SoundCloud,&#13;
and a lot of its concepts are based on Google’s internal monitoring&#13;
system, Borgmon. It implements a multidimensional data model with&#13;
keypairs that work much like how the Kubernetes labeling system works.&#13;
Prometheus exposes metrics in a human-readable format, as in the following example:</p>&#13;
<pre># HELP node_cpu_seconds_total Seconds the CPU is spent in each mode.&#13;
# TYPE node_cpu_seconds_total counter&#13;
node_cpu_seconds_total{cpu="0",mode="idle"} 5144.64&#13;
node_cpu_seconds_total{cpu="0",mode="iowait"} 117.98</pre>&#13;
&#13;
<p>To collect metrics, Prometheus uses a pull model in which it scrapes a&#13;
metrics endpoint to collect and ingest the metrics into the Prometheus&#13;
server. Systems like Kubernetes already expose their metrics in a&#13;
Prometheus format, making it simple to collect metrics. Many other Kubernetes ecosystem projects (NGINX, Traefik, Istio, Linkerd, etc.) also expose their metrics in a Prometheus format. Prometheus also can use exporters, which allow you to take emitted metrics from your service and translate them to Prometheus-formatted metrics.</p>&#13;
&#13;
<p>Prometheus has a very simplified architecture, as depicted in <a data-type="xref" href="#prometheus_architecture">Figure 3-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="prometheus_architecture">&#13;
<img alt="Prometheus architecture" src="assets/kbp2_0301.png"/>&#13;
<h6><span class="label">Figure 3-1. </span>Prometheus architecture</h6>&#13;
</div></figure>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>You can install <a data-primary="installing" data-secondary="Prometheus" data-type="indexterm" id="id561"/>Prometheus within the cluster or outside the&#13;
cluster. It’s a good practice to monitor your cluster from a “utility&#13;
cluster” to avoid a production issue also affecting your monitoring&#13;
system. Tools like&#13;
<a href="https://oreil.ly/7e6Wf">Thanos</a> provide high&#13;
availability for Prometheus and allow you to export metrics into an external&#13;
storage <span class="keep-together">system.</span></p>&#13;
</div>&#13;
&#13;
<p>A deep dive into the Prometheus architecture is beyond the scope of&#13;
this book, and you should refer to one of the dedicated books on&#13;
this topic.  <a class="orm:hideurl" href="https://oreil.ly/NewNE"><em>Prometheus: Up &amp; Running</em></a> (O’Reilly) is a good in-depth book to get you started.</p>&#13;
&#13;
<p>So, let’s dive in and get Prometheus set up on our Kubernetes cluster.&#13;
There are many different ways to deploy Prometheus, and the deployment will depend on your specific implementation.&#13;
We will install the Prometheus Operator with Helm:</p>&#13;
<dl>&#13;
<dt>Prometheus server</dt>&#13;
<dd>&#13;
<p>Pulls and stores metrics being collected from&#13;
systems.</p>&#13;
</dd>&#13;
<dt>Prometheus Operator</dt>&#13;
<dd>&#13;
<p>Makes the Prometheus configuration Kubernetes native, and manages and operates Prometheus and Alertmanager clusters. Allows you to create, destroy, and configure Prometheus resources through native Kubernetes resource definitions.</p>&#13;
</dd>&#13;
<dt>Node Exporter</dt>&#13;
<dd>&#13;
<p>Exports host metrics from Kubernetes nodes in the&#13;
cluster.</p>&#13;
</dd>&#13;
<dt>kube-state-metrics</dt>&#13;
<dd>&#13;
<p>Collects Kubernetes-specific metrics.</p>&#13;
</dd>&#13;
<dt>Alertmanager</dt>&#13;
<dd>&#13;
<p>Allows you to configure and forward alerts to external&#13;
systems.</p>&#13;
</dd>&#13;
<dt>Grafana</dt>&#13;
<dd>&#13;
<p>Provides visualization on dashboard capabilities for&#13;
Prometheus.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>First, we’ll start by getting minikube setup to deploy Prometheus to. We are using Macs so<a data-primary="minikube, installing" data-type="indexterm" id="id562"/><a data-primary="installing" data-secondary="minikube" data-type="indexterm" id="id563"/> we’ll use <code>brew</code> to install minikube. You can also install minikube from the <a href="https://oreil.ly/BgFFL">minikube website</a>.</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">brew<code class="w"> </code>install<code class="w"> </code>minikube<code class="w"/></pre>&#13;
&#13;
<p>Now we’ll install kube-prometheus-stack (formerly Prometheus Operator) and prepare our cluster to start monitoring the Kubernetes API server for changes.</p>&#13;
&#13;
<p>Create a namespace for monitoring:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>create<code class="w"> </code>ns<code class="w"> </code>monitoring<code class="w"/></pre>&#13;
&#13;
<p>Add the prometheus-community Helm chart repository:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>repo<code class="w"> </code>add<code class="w"> </code>prometheus-community<code class="w"/>&#13;
<code class="w">    </code>https://prometheus-community.github.io/helm-charts<code class="w"/></pre>&#13;
&#13;
<p>Add the Helm Stable chart repository:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>repo<code class="w"> </code>add<code class="w"> </code>stable<code class="w"> </code>https://charts.helm.sh/stable<code class="w"/></pre>&#13;
&#13;
<p>Update the chart repository:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>repo<code class="w"> </code>update<code class="w"/></pre>&#13;
&#13;
<p>Install <a data-primary="kube-prometheus-stack chart, installing" data-type="indexterm" id="id564"/><a data-primary="installing" data-secondary="kube-prometheus-stack chart" data-type="indexterm" id="id565"/>the kube-prometheus-stack chart:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>install<code class="w"> </code>--namespace<code class="w"> </code>monitoring<code class="w"> </code>prometheus<code class="w"/>&#13;
<code class="w">    </code>prometheus-community/kube-prometheus-stack<code class="w"/></pre>&#13;
&#13;
<p>Let’s check to ensure that all the pods are running:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>get<code class="w"> </code>pods<code class="w"> </code>-n<code class="w"> </code>monitoring<code class="w"/></pre>&#13;
&#13;
<p>If installed correctly you should see the following pods:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>get<code class="w"> </code>pods<code class="w"> </code>-n<code class="w"> </code>monitoring<code class="w"/></pre>&#13;
&#13;
<pre data-type="programlisting">NAME                                               READY  STATUS   RESTARTS  AGE&#13;
alertmanager-prometheus-kube-prometheus-alertm...  2/2    Running  1         79s&#13;
prometheus-grafana-6f7cf9b968-xtnzj                3/3    Running  0         97s&#13;
prometheus-kube-prometheus-operator-7bdb94567b...  1/1    Running  0         97s&#13;
prometheus-kube-state-metrics-6bdd65d76-s5r5j      1/1    Running  0         97s&#13;
prometheus-prometheus-kube-prometheus-promethe...  2/2    Running  0         78s&#13;
prometheus-prometheus-node-exporter-dgrlf          1/1    Running  0         98s</pre>&#13;
&#13;
<p>Now we’ll create a tunnel to the<a data-primary="Grafana" data-type="indexterm" id="grafana"/> Grafana instance that is included with kube-prometheus-stack. This will allow us to connect to Grafana from our local machine.</p>&#13;
&#13;
<p>This creates a tunnel to our localhost on port 3000. Now we can open a web browser and connect to Grafana on <a class="bare" href="http://127.0.0.1:3000"><em class="hyperlink">http://127.0.0.1:3000</em></a>.</p>&#13;
&#13;
<p class="pagebreak-before">We talked earlier in the chapter about employing the USE method, so let’s gather some node metrics on CPU utilization and saturation. Kube-prometheus-stack provides visualizations for these common USE method metrics we want to track. The great thing about the kube-prometheus-stack you installed is that it comes with prebuilt Grafana dashboards you can use.</p>&#13;
&#13;
<p>Now we’ll create a tunnel to the Grafana instance that is included with kube-prometheus-stack. This will allow us to connect to Grafana from our local machine:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>port-forward<code class="w"> </code>-n<code class="w"> </code>monitoring<code class="w"> </code>svc/prometheus-grafana<code class="w"> </code><code class="m">3000</code>:80<code class="w"/></pre>&#13;
&#13;
<p>Point your web browser at <a class="bare" href="http://localhost:3000"><em class="hyperlink">http://localhost:3000</em></a> and log in using the&#13;
following &#13;
<span class="keep-together">credentials:</span></p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Username: admin</p>&#13;
</li>&#13;
<li>&#13;
<p>Password: prom-operator</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Under the Grafana dashboards you’ll find a dashboard called Kubernetes /&#13;
USE Method / Cluster. This dashboard gives you a good overview of&#13;
the utilization and saturation of the Kubernetes cluster, which is at the heart of the USE method. <a data-type="xref" href="#grafana_dashboard">Figure 3-2</a> presents an example of the dashboard.</p>&#13;
&#13;
<figure><div class="figure" id="grafana_dashboard">&#13;
<img alt="A Grafana dashboard" src="assets/kbp2_0302.png"/>&#13;
<h6><span class="label">Figure 3-2. </span>A Grafana dashboard</h6>&#13;
</div></figure>&#13;
&#13;
<p>Go ahead and take some time to explore the different dashboards and&#13;
metrics that you can visualize in <a data-primary="Grafana" data-startref="grafana" data-type="indexterm" id="id566"/>Grafana.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>Avoid creating too many dashboards (aka “The Wall of Graphs”)&#13;
because this can be difficult for engineers to reason with in troubleshooting situations. You might think having more information in a dashboard means better monitoring, but the majority of the time it causes more confusion for a user looking at the dashboard. Focus your dashboard design on outcomes and<a data-primary="monitoring" data-secondary="with Prometheus" data-secondary-sortas="Prometheus" data-startref="monitor-prometheus" data-type="indexterm" id="id567"/><a data-primary="Prometheus" data-startref="prometheus" data-type="indexterm" id="id568"/> time to resolution.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logging Overview" data-type="sect1"><div class="sect1" id="id26">&#13;
<h1>Logging Overview</h1>&#13;
&#13;
<p>Up to <a data-primary="logging" data-secondary="what to log" data-type="indexterm" id="log-what-to-log"/>this point, we have discussed a lot about metrics and Kubernetes,&#13;
but to get the full picture of your environment, you also need to&#13;
collect and centralize logs from the Kubernetes cluster and the&#13;
applications deployed to your cluster. With logging, it might be easy to say, “Let’s just log everything,” but this can cause two issues:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>There is too much noise to find issues quickly.</p>&#13;
</li>&#13;
<li>&#13;
<p>Logs can consume a lot of resources and come with a high cost.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>There is no clear-cut answer to what exactly you should log because debug logs become a necessary evil. Over time you’ll start to understand your environment better and learn what noise you can tune out from the logging system. Also, to address the ever-increasing number of logs stored, you will need<a data-primary="logging" data-secondary="retention/archival process" data-type="indexterm" id="id569"/><a data-primary="retention/archival process for logging" data-type="indexterm" id="id570"/> to implement a retention and archival policy. From an end-user experience, having somewhere between 30 and 45 days’ worth of historical logs is a good fit. This allows for investigation of problems that manifest over a longer period of time but also reduces the amount of resources needed to store logs. If you require longer-term storage for compliance reasons, you’ll want to archive the logs to more cost-effective resources.</p>&#13;
&#13;
<p>In a Kubernetes cluster, there are multiple components to log. Following is a list of components from which you should be collecting metrics:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Node logs</p>&#13;
</li>&#13;
<li>&#13;
<p>Kubernetes control-plane logs</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>API server</p>&#13;
</li>&#13;
<li>&#13;
<p>Controller manager</p>&#13;
</li>&#13;
<li>&#13;
<p>Scheduler</p>&#13;
</li>&#13;
</ul>&#13;
</li>&#13;
<li>&#13;
<p>Kubernetes audit logs</p>&#13;
</li>&#13;
<li>&#13;
<p>Application container logs</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>With <a data-primary="node logs" data-type="indexterm" id="id571"/>node logs, you want to collect events that happen to essential node&#13;
services. For example, you will want to collect logs from the Docker daemon&#13;
running on the nodes. A healthy Docker daemon is essential for&#13;
running containers on the node. Collecting these logs will help&#13;
you diagnose any issues that you might run into with the Docker daemon, and it will&#13;
give you information into any underlying issues with the daemon. There&#13;
are also other essential services that you will want to log from the&#13;
underlying node.</p>&#13;
&#13;
<p>The <a data-primary="Kubernetes control plane logs" data-type="indexterm" id="id572"/>Kubernetes control plane consists of several components from which you’ll need to collect logs to give you more insight into underlying&#13;
issues within it. The Kubernetes control plane is core to a&#13;
healthy cluster, and you’ll want to aggregate the logs that it stores on&#13;
the host in <em>/var/log/kube-APIserver.log</em>, <em>/var/log/kube-scheduler.log</em>, and <em>/var/log/kube-controller-manager.log</em>. The controller manager is responsible for&#13;
creating objects defined by the end user. As an example, as a user you&#13;
create a Kubernetes service with type LoadBalancer and it just sits in a&#13;
pending state; the Kubernetes events might not give all the details to&#13;
diagnose the issue. If you collect the logs in a centralized system, it&#13;
will give you more detail into the underlying issue and a&#13;
quicker way to investigate it.</p>&#13;
&#13;
<p>You can think of Kubernetes audit logs <a data-primary="audit logs" data-type="indexterm" id="id573"/>as security monitoring because they give you insight into who did what within the system. These logs can be very noisy, so you’ll want to tune them for your environment. In many instances these logs can cause a huge spike in your logging system when first initialized, so make sure that you follow the Kubernetes documentation guidance on audit log monitoring.</p>&#13;
&#13;
<p>Application container logs<a data-primary="application container logs" data-type="indexterm" id="id574"/> give you insight into the actual logs&#13;
your application is emitting. You can forward these logs to a central&#13;
repository in multiple ways. The first and recommended way is to&#13;
send all application logs to STDOUT because this gives you a uniform way of&#13;
application logging, and a monitoring daemon set can gather the logs directly from the Docker daemon. The other way is to use a <em>sidecar</em>&#13;
pattern and run a log-forwarding container next to the application&#13;
container in a Kubernetes pod. You might need to use this pattern if your&#13;
application logs to the <span class="keep-together">filesystem.</span></p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>There are many options and configurations for managing Kubernetes audit logs. These audit logs can be very noisy and it can be expensive to log all actions. You should consider looking at the <a href="https://oreil.ly/L84dM">audit logging documentation</a> so that you can fine-tune these logs for your<a data-primary="logging" data-secondary="what to log" data-startref="log-what-to-log" data-type="indexterm" id="id575"/> environment.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tools for Logging" data-type="sect1"><div class="sect1" id="id192">&#13;
<h1>Tools for Logging</h1>&#13;
&#13;
<p>As with <a data-primary="logging" data-secondary="tools for" data-type="indexterm" id="log-tools"/>collecting metrics, there are many tools to collect logs from&#13;
Kubernetes and applications running in the cluster. You might already have&#13;
tooling for this, but be aware of how the tool implements logging. The&#13;
tool should have the capability to run as a Kubernetes DaemonSet, and&#13;
have a solution to run as a sidecar for applications that don’t send logs to STDOUT. An advantage of using an existing tool is that you will already have operational knowledge of the tool.</p>&#13;
&#13;
<p>Some of the more popular tools with Kubernetes integration are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Loki</p>&#13;
</li>&#13;
<li>&#13;
<p>Elastic Stack</p>&#13;
</li>&#13;
<li>&#13;
<p>Datadog</p>&#13;
</li>&#13;
<li>&#13;
<p>Sumo Logic</p>&#13;
</li>&#13;
<li>&#13;
<p>Sysdig</p>&#13;
</li>&#13;
<li>&#13;
<p>Cloud provider services (GCP Stackdriver, Azure Monitor for containers,&#13;
and Amazon CloudWatch)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>When looking for a tool to centralize logs, hosted solutions can provide&#13;
a lot of value because they offload a lot of the operational cost. Hosting&#13;
your own logging solution seems great on day <em>N</em>, but as the environment&#13;
grows, it can be very time consuming to maintain the solution.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logging by Using a Loki-Stack" data-type="sect1"><div class="sect1" id="id27">&#13;
<h1>Logging by Using a Loki-Stack</h1>&#13;
&#13;
<p>For the <a data-primary="logging" data-secondary="with Loki-Stack" data-secondary-sortas="Loki-Stack" data-type="indexterm" id="log-loki-stack"/><a data-primary="Loki-Stack" data-type="indexterm" id="Loki-Stack"/>purposes of this book, we use a Loki-Stack with prom-tail for&#13;
logging for our cluster. Implementing a Loki-Stack can be a good way&#13;
to get started, but at some point you’ll probably ask yourself, “Is it&#13;
really worth managing my own logging platform?” Typically, it’s not worth the effort because self-hosted logging solutions are great at first, but become overly complex with time. Self-hosted logging solutions become more operationally complex as your environment scales. There is no one correct answer, so evaluate whether your business requirements need you to host your own solution. There is also a hosted Loki solution (provided by Grafana), so you can always move pretty easily if you choose not to host it yourself.</p>&#13;
&#13;
<p>We will use the following for the logging stack:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Loki</p>&#13;
</li>&#13;
<li>&#13;
<p>prom-tail</p>&#13;
</li>&#13;
<li>&#13;
<p>Grafana</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Deploy Loki-Stack with Helm to your Kubernetes cluster with the following steps.</p>&#13;
&#13;
<p>Add Loki-Stack Helm repo:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>repo<code class="w"> </code>add<code class="w"> </code>grafana<code class="w"> </code>https://grafana.github.io/helm-charts<code class="w"/></pre>&#13;
&#13;
<p>Update Helm repo:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>repo<code class="w"> </code>update<code class="w"/></pre>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">helm<code class="w"> </code>upgrade<code class="w"> </code>--install<code class="w"> </code>loki<code class="w"> </code>--namespace<code class="o">=</code>monitoring<code class="w"> </code>grafana/loki-stack<code class="w"/></pre>&#13;
&#13;
<p>This deploys Loki with prom-tail, which will allow us to forward logs&#13;
to Loki and visualize the logs using Grafana.</p>&#13;
&#13;
<p>You should see the following pods deployed to your cluster:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>get<code class="w"> </code>pods<code class="w"> </code>-n<code class="w"> </code>monitoring<code class="w"/></pre>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">NAME<code class="w">                 </code>READY<code class="w">  </code>STATUS<code class="w">   </code>RESTARTS<code class="w">  </code>AGE<code class="w"/>&#13;
loki-0<code class="w">               </code><code class="m">1</code>/1<code class="w">    </code>Running<code class="w">  </code><code class="m">0</code><code class="w">         </code>93s<code class="w"/>&#13;
loki-promtail-x7nw8<code class="w">  </code><code class="m">1</code>/1<code class="w">    </code>Running<code class="w">  </code><code class="m">0</code><code class="w">         </code>93s<code class="w"/></pre>&#13;
&#13;
<p>After all pods are “Running,” go ahead and connect to Grafana&#13;
through port forwarding to our localhost:</p>&#13;
&#13;
<pre data-code-language="bash" data-type="programlisting">kubectl<code class="w"> </code>port-forward<code class="w"> </code>-n<code class="w"> </code>monitoring<code class="w"> </code>svc/prometheus-grafana<code class="w"> </code><code class="m">3000</code>:80<code class="w"/></pre>&#13;
&#13;
<p>Now, point your web browser at <a class="bare" href="http://localhost:3000"><em class="hyperlink">http://localhost:3000</em></a> and log in using the&#13;
following credentials:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Username: admin</p>&#13;
</li>&#13;
<li>&#13;
<p>Password: prom-operator</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Under the Grafana configuration you’ll find data sources, as shown in <a data-type="xref" href="#grafana_datasource">Figure 3-3</a>. We’ll then add Loki as a <code>Data Source</code>.</p>&#13;
&#13;
<figure class="width-65"><div class="figure" id="grafana_datasource">&#13;
<img alt="The Grafana datasource" src="assets/kbp2_0303.png"/>&#13;
<h6><span class="label">Figure 3-3. </span>The Grafana data source</h6>&#13;
</div></figure>&#13;
&#13;
<p>We will then add a new data source and add Loki as the data source (see <a data-type="xref" href="#loki_datasource">Figure 3-4</a>).</p>&#13;
&#13;
<figure><div class="figure" id="loki_datasource">&#13;
<img alt="Loki datasource" src="assets/kbp2_0304.png"/>&#13;
<h6><span class="label">Figure 3-4. </span>Loki datasource</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the Loki settings page (<a data-type="xref" href="#loki_configuration">Figure 3-5</a>), fill in the URL with http://loki:3100, then click the Save &amp; Test button.</p>&#13;
&#13;
<figure><div class="figure" id="loki_configuration">&#13;
<img alt="Loki configuration" src="assets/kbp2_0305.png"/>&#13;
<h6><span class="label">Figure 3-5. </span>Loki configuration</h6>&#13;
</div></figure>&#13;
&#13;
<p>In Grafana, you can perform ad hoc queries on the logs, and you can build&#13;
out dashboards to give you an overview of the environment.</p>&#13;
&#13;
<p>To explore the logs that the Loki-Stack has collected we can use the <em>Explore</em> function in Grafana, as shown in <a data-type="xref" href="#loki_explore">Figure 3-6</a>. This will allow us to run a query against the logs that have been collected.</p>&#13;
&#13;
<figure><div class="figure" id="loki_explore">&#13;
<img alt="Explore Loki logs" src="assets/kbp2_0306.png"/>&#13;
<h6><span class="label">Figure 3-6. </span>Explore Loki logs</h6>&#13;
</div></figure>&#13;
&#13;
<p>For the label filter you will need the following filter:</p>&#13;
&#13;
<pre data-type="programlisting">namespace = kube-system</pre>&#13;
&#13;
<p>Go ahead and take some time to explore the different logs that you can visualize from Loki and <a data-primary="logging" data-secondary="with Loki-Stack" data-secondary-sortas="Loki-Stack" data-startref="log-loki-stack" data-type="indexterm" id="id576"/><a data-primary="Loki-Stack" data-startref="Loki-Stack" data-type="indexterm" id="id577"/>Grafana.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Alerting" data-type="sect1"><div class="sect1" id="id28">&#13;
<h1>Alerting</h1>&#13;
&#13;
<p>Alerting<a data-primary="alerting" data-secondary="when to alert" data-type="indexterm" id="alert-when-to-alert"/> is a double-edged sword, and you need to strike a balance between what&#13;
you alert on versus what should just be monitored. Alerting on too much&#13;
causes alert fatigue, and important events will be lost in all the&#13;
noise. An example would be generating an alert any time a pod fails. You might be&#13;
asking, “Why wouldn’t I want to monitor for a pod failure?” Well, the&#13;
beauty of Kubernetes is that it provides features to automatically check the&#13;
health of a container and restart the container automatically. You&#13;
really want to focus alerting on events that affect your<a data-primary="Service-Level Objectives (SLOs)" data-type="indexterm" id="id578"/><a data-primary="SLOs (Service-Level Objectives)" data-type="indexterm" id="id579"/> Service-Level Objectives (SLOs). SLOs are specific measurable characteristics such as availability, throughput, frequency, and response time that you agree upon with the end user of your service. Setting SLOs sets expectations with your end users and provides clarity on how the system should behave. Without an SLO, users can form their opinion, which might be an unrealistic expectation of the service. Alerting in a system like Kubernetes needs an entirely new approach from what we are typically accustomed to and needs to focus on how the end user is experiencing the service. For example, if your SLO for a frontend service is a 20-ms response time and you are seeing higher&#13;
latency than average, you want to be alerted on the problem.</p>&#13;
&#13;
<p>You need to decide what alerts are good and require intervention. In typical monitoring, you might be accustomed to alerting on high CPU usage, memory usage, or processes not responding. These might seem like good alerts, but they probably don’t indicate an issue that someone needs to take immediate action on and that requires notifying an on-call engineer. An alert to an on-call engineer should be an issue that needs immediate human attention and is affecting the UX of the application. If you have ever experienced a “that issue resolved itself” scenario, then that is a good indication that the alert did not need to contact an on-call engineer.</p>&#13;
&#13;
<p>One way to handle alerts that don’t need immediate action is to focus on automating the remediation of the cause. For example, when a disk fills up, you could automate the deletion of logs to free up space on the disk. Also, utilizing Kubernetes <em>liveness probes</em> in your app deployment can help auto-remediate issues with a process that is not responding in the application.</p>&#13;
&#13;
<p>When building alerts, you also need to <a data-primary="alert thresholds" data-type="indexterm" id="id580"/>consider <em>alert thresholds</em>; if you set thresholds too short, then you can get a lot of false positives with your alerts. It’s generally recommended to set a threshold of at least five minutes to help eliminate false positives. Coming up with standard thresholds can help define a standard and avoid micromanaging many different thresholds. For example, you might want to follow a specific pattern of 5 minutes, 10 minutes, 30 minutes, 1 hour, and so on.</p>&#13;
&#13;
<p>When building notifications for alerts, you want to ensure that you provide relevant information in the notification. For example, you might provide a link to a “playbook” that gives troubleshooting or other helpful information on resolving the issue. You should also include information on the datacenter, region, app owner, and affected system in notifications. Providing all this information will allow engineers to quickly formulate a theory around the issue.</p>&#13;
&#13;
<p>You also need to build notification channels to route alerts that are fired. When thinking about “Who do I notify when an alert is triggered?” you should ensure that notifications are not just sent to a distribution list or team emails. What tends to happen if alerts are sent to larger groups is that they end up getting filtered out because users see these as noise. You should route notifications to the user who is going to take responsibility for the issue.</p>&#13;
&#13;
<p>With alerting, you’ll never get it perfect on day one, and we could argue it&#13;
might never be perfect. You just want to make sure that you incrementally&#13;
improve on alerting to preclude alert fatigue, which can cause many issues with staff burnout&#13;
and your systems.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>For further insight on how to approach alerting on and managing systems, read <a href="https://oreil.ly/YPxju">“My Philosophy on Alerting”</a> by Rob Ewaschuk, which is based on Rob’s observations as a site reliability engineer (SRE) at <a data-primary="alerting" data-secondary="when to alert" data-startref="alert-when-to-alert" data-type="indexterm" id="id581"/>Google.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Best Practices for Monitoring, Logging, and Alerting" data-type="sect1"><div class="sect1" id="id347">&#13;
<h1>Best Practices for Monitoring, Logging, and Alerting</h1>&#13;
&#13;
<p>Following are the best practices that you should adopt regarding monitoring, logging, and alerting.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring" data-type="sect2"><div class="sect2" id="id193">&#13;
<h2>Monitoring</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Monitor<a data-primary="monitoring" data-secondary="best practices" data-type="indexterm" id="id582"/><a data-primary="best practices" data-secondary="monitoring" data-type="indexterm" id="id583"/> nodes and all Kubernetes components for utilization,&#13;
saturation, and error rates, and monitor applications for rate, errors, and duration.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use closed-box monitoring to monitor for symptoms and not predictive&#13;
health of a system.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use open-box monitoring to inspect the system and its internals with&#13;
<span class="keep-together">instrumentation.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Implement time-series-based metrics to gain high-precision metrics that also allow you to have insight into the behavior of your application.</p>&#13;
</li>&#13;
<li>&#13;
<p>Utilize monitoring systems like Prometheus that provide key labeling&#13;
for high dimensionality; this will give a better signal to symptoms of an impacting issue.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use average metrics to visualize subtotals and metrics based on factual&#13;
data. Utilize sum metrics to visualize the distribution across a specific metric.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Logging" data-type="sect2"><div class="sect2" id="id194">&#13;
<h2>Logging</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You <a data-primary="logging" data-secondary="best practices" data-type="indexterm" id="id584"/><a data-primary="best practices" data-secondary="logging" data-type="indexterm" id="id585"/>should use logging in combination with metrics monitoring to get&#13;
the full picture of how your environment is operating.</p>&#13;
</li>&#13;
<li>&#13;
<p>Be cautious of storing logs for more than 30 to 45 days; if needed, use&#13;
cheaper resources for long-term archiving.</p>&#13;
</li>&#13;
<li>&#13;
<p>Limit usage of log forwarders in a sidecar pattern, as they&#13;
will utilize a lot more resources. Opt for using a DaemonSet for the&#13;
log forwarder and sending logs to STDOUT.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Alerting" data-type="sect2"><div class="sect2" id="id195">&#13;
<h2>Alerting</h2>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Be <a data-primary="best practices" data-secondary="alerting" data-type="indexterm" id="id586"/><a data-primary="alerting" data-secondary="best practices" data-type="indexterm" id="id587"/>cautious of alert fatigue because it can lead to bad behaviors in&#13;
people and processes.</p>&#13;
</li>&#13;
<li>&#13;
<p>Always look at incrementally improving on alerting and accept that it&#13;
will not always be perfect.</p>&#13;
</li>&#13;
<li>&#13;
<p>Alert for symptoms that affect your SLOs and customers and not for transient issues that don’t need immediate human attention.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="id348">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter we discussed the patterns, techniques, and tools that&#13;
can be used for monitoring our systems with metrics and log collection.&#13;
The most important piece to take away from this chapter is that you need&#13;
to rethink how you perform monitoring and do it from the outset. Too many times&#13;
we see this implemented after the fact, and it can get you into a very bad&#13;
place in understanding your system. Monitoring is all about having&#13;
better insight into a system and being able to provide better&#13;
resiliency, which in turn provides a better end-user experience for your&#13;
application. Monitoring distributed applications and distributed&#13;
systems like Kubernetes requires a lot of work, so you must be ready&#13;
for it at the beginning of your journey.</p>&#13;
</div></section>&#13;
</div></section></body></html>