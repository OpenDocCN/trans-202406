["```\n    $ mkdir cert && cd cert\n    ```", "```\n    $ openssl genrsa -out johndoe.key 2048\n    Generating RSA private key, 2048 bit long modulus\n    ..............................+\n    ..+\n    e is 65537 (0x10001)\n    $ ls\n    johndoe.key\n    ```", "```\n    $ openssl req -new -key johndoe.key -out johndoe.csr -subj \\\n      \"/CN=johndoe/O=cka-study-guide\"\n    $ ls\n    johndoe.csr johndoe.key\n    ```", "```\n    $ openssl x509 -req -in johndoe.csr -CA [/.minikube/ca.crt -CAkey] \\\n      /.minikube/ca.key -CAcreateserial -out johndoe.crt -days 364\n    Signature ok\n    subject=/CN=johndoe/O=cka-study-guide\n    Getting CA Private Key\n\n    ```", "```\n    $ kubectl config set-credentials johndoe \\\n      --client-certificate=johndoe.crt --client-key=johndoe.key\n    User \"johndoe\" set.\n    $ kubectl config set-context johndoe-context --cluster=minikube \\\n      --user=johndoe\n    Context \"johndoe-context\" modified.\n    ```", "```\n    $ kubectl config use-context johndoe-context\n    Switched to context \"johndoe-context\".\n    $ kubectl config current-context\n    johndoe-context\n    ```", "```\n$ kubectl create serviceaccount build-bot\nserviceaccount/build-bot created\n```", "```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: build-bot\n```", "```\n$ kubectl get serviceaccounts\nNAME        SECRETS   AGE\nbuild-bot   1         78s\ndefault     1         93d\n```", "```\n$ kubectl describe serviceaccount build-bot\nName:                build-bot\nNamespace:           default\nLabels:              <none>\nAnnotations:         <none>\nImage pull secrets:  <none>\nMountable secrets:   build-bot-token-rvjnz\nTokens:              build-bot-token-rvjnz\nEvents:              <none>\n```", "```\n$ kubectl get secrets\nNAME                    TYPE                                  DATA   AGE\nbuild-bot-token-rvjnz   kubernetes.io/service-account-token   3      20m\ndefault-token-qgh5n     kubernetes.io/service-account-token   3      93d\n```", "```\n$ kubectl run build-observer --image=alpine --restart=Never \\\n  --serviceaccount=build-bot\npod/build-observer created\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n  name: build-observer\nspec:\n  serviceAccountName: build-bot\n...\n```", "```\n$ kubectl create role read-only --verb=list,get,watch \\\n  --resource=pods,deployments,services\nrole.rbac.authorization.k8s.io/read-only created\n```", "```\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: read-only\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  - services\n  verbs:\n  - list\n  - get\n  - watch\n- apiGroups:\n  - apps\n  resources:\n  - deployments\n  verbs:\n  - list\n  - get\n  - watch\n```", "```\n$ kubectl get roles\nNAME        CREATED AT\nread-only   2021-06-23T19:46:48Z\n```", "```\n$ kubectl describe role read-only\nName:         read-only\nLabels:       <none>\nAnnotations:  <none>\nPolicyRule:\n  Resources         Non-Resource URLs  Resource Names  Verbs\n  ---------         -----------------  --------------  -----\n  pods              []                 []              [list get watch]\n  services          []                 []              [list get watch]\n  deployments.apps  []                 []              [list get watch]\n```", "```\n$ kubectl create rolebinding read-only-binding --role=read-only --user=johndoe\nrolebinding.rbac.authorization.k8s.io/read-only-binding created\n```", "```\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-only-binding\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: read-only\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: User\n  name: johndoe\n```", "```\n$ kubectl get rolebindings\nNAME                ROLE             AGE\nread-only-binding   Role/read-only   24h\n```", "```\n$ kubectl describe rolebinding read-only-binding\nName:         read-only-binding\nLabels:       <none>\nAnnotations:  <none>\nRole:\n  Kind:  Role\n  Name:  read-only\nSubjects:\n  Kind  Name     Namespace\n  ----  ----     ---------\n  User  johndoe\n```", "```\n$ kubectl config current-context\nminikube\n$ kubectl create deployment myapp --image=nginx --port=80 --replicas=2\ndeployment.apps/myapp created\n```", "```\n$ kubectl config use-context johndoe-context\nSwitched to context \"johndoe-context\".\n```", "```\n$ kubectl get deployments\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nmyapp   2/2     2            2           8s\n```", "```\n$ kubectl get replicasets\nError from server (Forbidden): replicasets.apps is forbidden: User \"johndoe\" \\\ncannot list resource \"replicasets\" in API group \"apps\" in the namespace \"default\"\n```", "```\n$ kubectl delete deployment myapp\nError from server (Forbidden): deployments.apps \"myapp\" is forbidden: User \\\n\"johndoe\" cannot delete resource \"deployments\" in API group \"apps\" in the \\\nnamespace \"default\"\n```", "```\n$ kubectl auth can-i --list --as johndoe\nResources          Non-Resource URLs   Resource Names   Verbs\n...\npods               []                  []               [list get watch]\nservices           []                  []               [list get watch]\ndeployments.apps   []                  []               [list get watch]\n$ kubectl auth can-i list pods --as johndoe\nyes\n```", "```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: list-pods\n  namespace: rbac-example\n  labels:\n    rbac-pod-list: \"true\"\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - pods\n  verbs:\n  - list\n```", "```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: delete-services\n  namespace: rbac-example\n  labels:\n    rbac-service-delete: \"true\"\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  verbs:\n  - delete\n```", "```\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: pods-services-aggregation-rules\n  namespace: rbac-example\naggregationRule:\n  clusterRoleSelectors:\n  - matchLabels:\n      rbac-pod-list: \"true\"\n  - matchLabels:\n      rbac-service-delete: \"true\"\nrules: []\n```", "```\n$ kubectl describe clusterroles pods-services-aggregation-rules -n rbac-example\nName:         pods-services-aggregation-rules\nLabels:       <none>\nAnnotations:  <none>\nPolicyRule:\n  Resources  Non-Resource URLs  Resource Names  Verbs\n  ---------  -----------------  --------------  -----\n  services   []                 []              [delete]\n  pods       []                 []              [list]\n```", "```\n$ ssh kube-control-plane\nWelcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)\n...\n```", "```\n$ sudo kubeadm init --pod-network-cidr 172.18.0.0/16 \\\n  --apiserver-advertise-address 10.8.8.10\n...\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on \\\neach as root:\n\nkubeadm join 10.8.8.10:6443 --token fi8io0.dtkzsy9kws56dmsp \\\n    --discovery-token-ca-cert-hash \\\n    sha256:cc89ea1f82d5ec460e21b69476e0c052d691d0c52cce83fbd7e403559c1ebdac\n```", "```\n$ mkdir -p $HOME/.kube\n$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n$ sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```", "```\n$ kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version= \\\n  $(kubectl version | base64 | tr -d '\\n')\"\nserviceaccount/weave-net created\nclusterrole.rbac.authorization.k8s.io/weave-net created\nclusterrolebinding.rbac.authorization.k8s.io/weave-net created\nrole.rbac.authorization.k8s.io/weave-net created\nrolebinding.rbac.authorization.k8s.io/weave-net created\ndaemonset.apps/weave-net created\n\n```", "```\n$ kubectl get nodes\nNAME                 STATUS   ROLES                  AGE   VERSION\nkube-control-plane   Ready    control-plane,master   24m   v1.21.2\n```", "```\n$ exit\nlogout\n...\n```", "```\n$ ssh kube-worker-1\nWelcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)\n...\n```", "```\n$ sudo kubeadm join 10.8.8.10:6443 --token fi8io0.dtkzsy9kws56dmsp \\\n  --discovery-token-ca-cert-hash \\\n  sha256:cc89ea1f82d5ec460e21b69476e0c052d691d0c52cce83fbd7e403559c1ebdac\n[preflight] Running pre-flight checks\n[preflight] Reading configuration from the cluster...\n[preflight] FYI: You can look at this config file with \\\n'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[kubelet-start] Writing kubelet configuration to file \\\n\"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Writing kubelet environment file with \\\nflags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Starting the kubelet\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n\nThis node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the control plane to see this node join the cluster.\n\n```", "```\n$ ssh kube-control-plane\nWelcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)\n...\n$ kubectl get nodes\nNAME                 STATUS   ROLES                  AGE     VERSION\nkube-control-plane   Ready    control-plane,master   5h49m   v1.21.2\nkube-worker-1        Ready    <none>                 15m     v1.21.2\n```", "```\n$ ssh kube-control-plane\nWelcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)\n...\n```", "```\n$ kubectl get nodes\nNAME                 STATUS   ROLES    AGE     VERSION\nkube-control-plane   Ready    master   4m54s   v1.18.0\nkube-worker-1        Ready    <none>   3m18s   v1.18.0\n```", "```\n$ sudo apt update\n...\n$ sudo apt-cache madison kubeadm\n   kubeadm |  1.21.2-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.21.1-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.21.0-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.8-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.7-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.6-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.5-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.4-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.2-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.1-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n   kubeadm |  1.20.0-00 | http://apt.kubernetes.io kubernetes-xenial/main \\\n   amd64 Packages\n...\n```", "```\n$ sudo apt-mark unhold kubeadm && sudo apt-get update && sudo apt-get install \\\n  -y kubeadm=1.19.0-00 && sudo apt-mark hold kubeadm\nCanceled hold on kubeadm.\n...\nUnpacking kubeadm (1.19.0-00) over (1.18.0-00) ...\nSetting up kubeadm (1.19.0-00) ...\nkubeadm set on hold.\n$ sudo apt-get update && sudo apt-get install -y --allow-change-held-packages \\\n  kubeadm=1.19.0-00\n...\nkubeadm is already the newest version (1.19.0-00).\n0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n$ kubeadm version\nkubeadm version: &version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", \\\nGitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", \\\nBuildDate:\"2020-08-26T14:28:32Z\", GoVersion:\"go1.15\", Compiler:\"gc\", \\\nPlatform:\"linux/amd64\"}\n```", "```\n$ sudo kubeadm upgrade plan\n...\n[upgrade] Fetching available versions to upgrade to\n[upgrade/versions] Cluster version: v1.18.20\n[upgrade/versions] kubeadm version: v1.19.0\nI0708 17:32:53.037895   17430 version.go:252] remote version is much newer: \\\nv1.21.2; falling back to: stable-1.19\n[upgrade/versions] Latest stable version: v1.19.12\n[upgrade/versions] Latest version in the v1.18 series: v1.18.20\n...\nYou can now apply the upgrade by executing the following command:\n\n\tkubeadm upgrade apply v1.19.12\n\nNote: Before you can perform this upgrade, you have to update kubeadm to v1.19.12.\n...\n```", "```\n$ sudo kubeadm upgrade apply v1.19.0\n...\n[upgrade/version] You have chosen to change the cluster version to \"v1.19.0\"\n[upgrade/versions] Cluster version: v1.18.20\n[upgrade/versions] kubeadm version: v1.19.0\n...\n[upgrade/successful] SUCCESS! Your cluster was upgraded to \"v1.19.0\". Enjoy!\n\n[upgrade/kubelet] Now that your control plane is upgraded, please proceed \\\nwith upgrading your kubelets if you haven't already done so.\n```", "```\n$ kubectl drain kube-control-plane --ignore-daemonsets\nnode/kube-control-plane cordoned\nWARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-qndb9, \\\nkube-system/kube-proxy-vpvms\nevicting pod kube-system/calico-kube-controllers-65f8bc95db-krp72\nevicting pod kube-system/coredns-f9fd979d6-2brkq\npod/calico-kube-controllers-65f8bc95db-krp72 evicted\npod/coredns-f9fd979d6-2brkq evicted\nnode/kube-control-plane evicted\n```", "```\n$ sudo apt-mark unhold kubelet kubectl && sudo apt-get update && sudo \\\n  apt-get install -y kubelet=1.19.0-00 kubectl=1.19.0-00 && sudo apt-mark \\\n  hold kubelet kubectl\n...\nSetting up kubelet (1.19.0-00) ...\nSetting up kubectl (1.19.0-00) ...\nkubelet set on hold.\nkubectl set on hold.\n```", "```\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart kubelet\n```", "```\n$ kubectl uncordon kube-control-plane\nnode/kube-control-plane uncordoned\n```", "```\n$ kubectl get nodes\nNAME                 STATUS   ROLES    AGE   VERSION\nkube-control-plane   Ready    master   21h   v1.19.0\nkube-worker-1        Ready    <none>   21h   v1.18.0\n```", "```\n$ exit\nlogout\n...\n```", "```\n$ ssh kube-worker-1\nWelcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)\n...\n```", "```\n$ sudo apt-mark unhold kubeadm && sudo apt-get update && sudo apt-get install \\\n  -y kubeadm=1.19.0-00 && sudo apt-mark hold kubeadm\nCanceled hold on kubeadm.\n...\nUnpacking kubeadm (1.19.0-00) over (1.18.0-00) ...\nSetting up kubeadm (1.19.0-00) ...\nkubeadm set on hold.\n$ kubeadm version\nkubeadm version: &version.Info{Major:\"1\", Minor:\"19\", GitVersion:\"v1.19.0\", \\\nGitCommit:\"e19964183377d0ec2052d1f1fa930c4d7575bd50\", GitTreeState:\"clean\", \\\nBuildDate:\"2020-08-26T14:28:32Z\", GoVersion:\"go1.15\", Compiler:\"gc\", \\\nPlatform:\"linux/amd64\"}\n```", "```\n$ sudo kubeadm upgrade node\n[upgrade] Reading configuration from the cluster...\n[upgrade] FYI: You can look at this config file with 'kubectl -n kube-system \\\nget cm kubeadm-config -o yaml'\n[preflight] Running pre-flight checks\n[preflight] Skipping prepull. Not a control plane node.\n[upgrade] Skipping phase. Not a control plane node.\n[kubelet-start] Writing kubelet configuration to file \\\n\"/var/lib/kubelet/config.yaml\"\n[upgrade] The configuration for this node was successfully updated!\n[upgrade] Now you should go ahead and upgrade the kubelet package using your \\\npackage manager.\n\n```", "```\n$ kubectl drain kube-worker-1 --ignore-daemonsets\nnode/kube-worker-1 cordoned\nWARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-2hrxg, \\\nkube-system/kube-proxy-qf6nl\nevicting pod kube-system/calico-kube-controllers-65f8bc95db-kggbr\nevicting pod kube-system/coredns-f9fd979d6-7zm4q\nevicting pod kube-system/coredns-f9fd979d6-tlmhq\npod/calico-kube-controllers-65f8bc95db-kggbr evicted\npod/coredns-f9fd979d6-7zm4q evicted\npod/coredns-f9fd979d6-tlmhq evicted\nnode/kube-worker-1 evicted\n```", "```\n$ sudo apt-mark unhold kubelet kubectl && sudo apt-get update && sudo apt-get \\\ninstall -y kubelet=1.19.0-00 kubectl=1.19.0-00 && sudo apt-mark hold kubelet \\\nkubectl\n...\nSetting up kubelet (1.19.0-00) ...\nSetting up kubectl (1.19.0-00) ...\nkubelet set on hold.\nkubectl set on hold.\n```", "```\n$ sudo systemctl daemon-reload\n$ sudo systemctl restart kubelet\n```", "```\n$ kubectl uncordon kube-worker-1\nnode/kube-worker-1 uncordoned\n```", "```\n$ kubectl get nodes\nNAME                 STATUS   ROLES    AGE   VERSION\nkube-control-plane   Ready    master   24h   v1.19.0\nkube-worker-1        Ready    <none>   24h   v1.19.0\n```", "```\n$ exit\nlogout\n...\n```", "```\n$ ssh kube-control-plane\nWelcome to Ubuntu 18.04.5 LTS (GNU/Linux 4.15.0-132-generic x86_64)\n...\n```", "```\n$ etcdctl version\netcdctl version: 3.4.14\nAPI version: 3.4\n```", "```\n$ kubectl get pods -n kube-system\nNAME                                       READY   STATUS    RESTARTS   AGE\n...\netcd-kube-control-plane                    1/1     Running   0          33m\n...\n$ kubectl describe pod etcd-kube-control-plane -n kube-system\n...\nContainers:\n  etcd:\n    Container ID:  docker://28325c63233edaa94e16691e8082e8d86f5e7da58c0fb54 \\\n    d95d68dec6e80cf54\n    Image:         k8s.gcr.io/etcd:3.4.3-0\n    Image ID:      docker-pullable://k8s.gcr.io/etcd@sha256:4afb99b4690b418 \\\n    ffc2ceb67e1a17376457e441c1f09ab55447f0aaf992fa646\n...\n```", "```\n$ kubectl describe pod etcd-kube-control-plane -n kube-system\n...\nContainers:\n  etcd:\n    ...\n    Command:\n      etcd\n      ...\n      --cert-file=/etc/kubernetes/pki/etcd/server.crt\n      --key-file=/etc/kubernetes/pki/etcd/server.key\n      --listen-client-urls=/etc/kubernetes/pki/etcd/server.key\n      --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt\n...\n```", "```\n$ sudo ETCDCTL_API=3 etcdctl --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n  --key=/etc/kubernetes/pki/etcd/server.key \\\n  snapshot save /opt/etcd-backup.db\n{\"level\":\"info\",\"ts\":1625860312.3468597, \\\n\"caller\":\"snapshot/v3_snapshot.go:119\", \\\n\"msg\":\"created temporary db file\",\"path\":\"/opt/etcd-backup.db.part\"}\n{\"level\":\"info\",\"ts\":\"2021-07-09T19:51:52.356Z\", \\\n\"caller\":\"clientv3/maintenance.go:200\", \\\n\"msg\":\"opened snapshot stream; downloading\"}\n{\"level\":\"info\",\"ts\":1625860312.358686, \\\n\"caller\":\"snapshot/v3_snapshot.go:127\", \\\n\"msg\":\"fetching snapshot\",\"endpoint\":\"127.0.0.1:2379\"}\n{\"level\":\"info\",\"ts\":\"2021-07-09T19:51:52.389Z\", \\\n\"caller\":\"clientv3/maintenance.go:208\", \\\n\"msg\":\"completed snapshot read; closing\"}\n{\"level\":\"info\",\"ts\":1625860312.392891, \\\n\"caller\":\"snapshot/v3_snapshot.go:142\", \\\n\"msg\":\"fetched snapshot\",\"endpoint\":\"127.0.0.1:2379\", \\\n\"size\":\"2.3 MB\",\"took\":0.045987318}\n{\"level\":\"info\",\"ts\":1625860312.3930364, \\\n\"caller\":\"snapshot/v3_snapshot.go:152\", \\\n\"msg\":\"saved\",\"path\":\"/opt/etcd-backup.db\"}\nSnapshot saved at /opt/etcd-backup.db\n```", "```\n$ exit\nlogout\n...\n```", "```\n$ sudo ETCDCTL_API=3 etcdctl --data-dir=/var/lib/from-backup snapshot restore \\\n  /opt/etcd-backup.db\n{\"level\":\"info\",\"ts\":1625861500.5752304, \\\n\"caller\":\"snapshot/v3_snapshot.go:296\", \\\n\"msg\":\"restoring snapshot\",\"path\":\"/opt/etcd-backup.db\", \\\n\"wal-dir\":\"/var/lib/from-backup/member/wal\", \\\n\"data-dir\":\"/var/lib/from-backup\", \\\n\"snap-dir\":\"/var/lib/from-backup/member/snap\"}\n{\"level\":\"info\",\"ts\":1625861500.6146874, \\\n\"caller\":\"membership/cluster.go:392\", \\\n\"msg\":\"added member\",\"cluster-id\":\"cdf818194e3a8c32\", \\\n\"local-member-id\":\"0\", \\\n\"added-peer-id\":\"8e9e05c52164694d\", \\\n\"added-peer-peer-urls\":[\"http://localhost:2380\"]}\n{\"level\":\"info\",\"ts\":1625861500.6350253, \\\n\"caller\":\"snapshot/v3_snapshot.go:309\", \\\n\"msg\":\"restored snapshot\",\"path\":\"/opt/etcd-backup.db\", \\\n\"wal-dir\":\"/var/lib/from-backup/member/wal\", \\\n\"data-dir\":\"/var/lib/from-backup\", \\\n\"snap-dir\":\"/var/lib/from-backup/member/snap\"}\n$ sudo ls /var/lib/from-backup\nmember\n```", "```\n$ cd /etc/kubernetes/manifests/\n$ sudo vim etcd.yaml\n...\nspec:\n  volumes:\n  ...\n  - hostPath:\n      path: /var/lib/from-backup\n      type: DirectoryOrCreate\n    name: etcd-data\n...\n```", "```\n$ kubectl get pod etcd-kube-control-plane -n kube-system\nNAME                      READY   STATUS    RESTARTS   AGE\netcd-kube-control-plane   1/1     Running   0          5m1s\n```", "```\n$ exit\nlogout\n...\n```"]