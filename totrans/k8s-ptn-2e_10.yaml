- en: Chapter 7\. Batch Job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *Batch Job* pattern is suited for managing isolated atomic units of work.
    It is based on the Job resource, which runs short-lived Pods reliably until completion
    on a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main primitive in Kubernetes for managing and running containers is the
    Pod. There are different ways of creating Pods with varying characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Bare Pod
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to create a Pod manually to run containers. However, when the
    node such a Pod is running on fails, the Pod is not restarted. Running Pods this
    way is discouraged except for development or testing purposes. This mechanism
    is also known as *unmanaged* or *naked Pods*.
  prefs: []
  type: TYPE_NORMAL
- en: ReplicaSet
  prefs: []
  type: TYPE_NORMAL
- en: This controller is used for creating and managing the lifecycle of Pods expected
    to run continuously (e.g., to run a web server container). It maintains a stable
    set of replica Pods running at any given time and guarantees the availability
    of a specified number of identical Pods. ReplicaSets are described in detail in
    [Chapter 11, “Stateless Service”](ch11.html#StatelessService).
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  prefs: []
  type: TYPE_NORMAL
- en: This controller runs a single Pod on every node and is used for managing platform
    capabilities such as monitoring, log aggregation, storage containers, and others.
    See [Chapter 9, “Daemon Service”](ch09.html#DaemonService), for a more detailed
    discussion.
  prefs: []
  type: TYPE_NORMAL
- en: A common aspect of these Pods is that they represent long-running processes
    that are not meant to stop after a certain time. However, in some cases there
    is a need to perform a predefined finite unit of work reliably and then shut down
    the container. For this task, Kubernetes provides the Job resource.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Kubernetes Job is similar to a ReplicaSet as it creates one or more Pods and
    ensures they run successfully. However, the difference is that, once the expected
    number of Pods terminate successfully, the Job is considered complete, and no
    additional Pods are started. A Job definition looks like [Example 7-1](#ex-job).
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-1\. A Job specification
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_batch_job_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Job should run five Pods to completion, which all must succeed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_batch_job_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Two Pods can run in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_batch_job_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Keep Pods for five minutes (300 seconds) before garbage-collecting them.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_batch_job_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Specifying the `restartPolicy` is mandatory for a Job. The possible values are
    `OnFailure` or `Never`.
  prefs: []
  type: TYPE_NORMAL
- en: One crucial difference between the Job and the ReplicaSet definition is the
    `.spec.template.spec.restartPolicy`. The default value for a ReplicaSet is `Always`,
    which makes sense for long-running processes that must always be kept running.
    The value `Always` is not allowed for a Job, and the only possible options are
    `OnFailure` or `Never`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So why bother creating a Job to run a Pod only once instead of using bare Pods?
    Using Jobs provides many reliability and scalability benefits that make them the
    preferred option:'
  prefs: []
  type: TYPE_NORMAL
- en: A Job is not an ephemeral in-memory task but a persisted one that survives cluster
    restarts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When a Job is completed, it is not deleted but is kept for tracking purposes.
    The Pods that are created as part of the Job are also not deleted but are available
    for examination (e.g., to check the container logs). This is also true for bare
    Pods but only for `restartPolicy: OnFailure`. You can still remove the Pods of
    a Job after a certain time by specifying `.spec.ttlSecondsAfterFinished`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Job may need to be performed multiple times. Using the `.spec.completions`
    field, it is possible to specify how many times a Pod should complete successfully
    before the Job itself is done.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a Job has to be completed multiple times, it can also be scaled and executed
    by starting multiple Pods at the same time. That can be done by specifying the
    `.spec.parallelism` field.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Job can be suspended by setting the field `.spec.suspend` to `true`. In this
    case, all active Pods are deleted and restarted if the Job is resumed (i.e., `.spec.suspend`
    set to `false` by the user).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the node fails or when the Pod is evicted for some reason while still running,
    the scheduler places the Pod on a new healthy node and reruns it. Bare Pods would
    remain in a failed state as existing Pods are never moved to other nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of this makes the Job primitive attractive for scenarios requiring some
    guarantees for the completion of a unit of work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following two fields play major roles in the behavior of a Job:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.spec.completions`'
  prefs: []
  type: TYPE_NORMAL
- en: Specifies how many Pods should run to complete a Job.
  prefs: []
  type: TYPE_NORMAL
- en: '`.spec.parallelism`'
  prefs: []
  type: TYPE_NORMAL
- en: Specifies how many Pod replicas could run in parallel. Setting a high number
    does not guarantee a high level of parallelism, and the actual number of Pods
    may still be fewer (and in some corner cases, more) than the desired number (e.g.,
    because of throttling, resource quotas, not enough completions left, and other
    reasons). Setting this field to 0 effectively pauses the Job.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 7-1](#img-job) shows how the Job defined in [Example 7-1](#ex-job)
    with a completion count of 5 and a parallelism of 2 is processed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Parallel Batch Job with a fixed completion count](assets/kup2_0701.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7-1\. Parallel Batch Job with a fixed completion count
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Based on these two parameters, there are the following types of Jobs:'
  prefs: []
  type: TYPE_NORMAL
- en: Single Pod Jobs
  prefs: []
  type: TYPE_NORMAL
- en: This type is selected when you leave out both `.spec.completions` and `.spec.parallelism`
    or set them to their default values of 1\. Such a Job starts only one Pod and
    is completed as soon as the single Pod terminates successfully (with exit code
    0).
  prefs: []
  type: TYPE_NORMAL
- en: Fixed completion count Jobs
  prefs: []
  type: TYPE_NORMAL
- en: For a fixed completion count Job, you should set `.spec.completions` to the
    number of completions needed. You can set `.spec.parallelism`, or leave it unset
    and it will default to 1\. Such a Job is considered completed after the `.spec.completions`
    number of Pods has completed successfully. [Example 7-1](#ex-job) shows this mode
    in action and is the best choice when we know the number of work items in advance
    and the processing cost of a single work item justifies the use of a dedicated
    Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Work queue Jobs
  prefs: []
  type: TYPE_NORMAL
- en: For a work queue Job, you need to leave `.spec.completions` unset, and set `.spec.parallelism`
    to a number greater than one. A work queue Job is considered completed when at
    least one Pod has terminated successfully and all other Pods have terminated too.
    This setup requires the Pods to coordinate among themselves and determine what
    each one is working on so that they can finish in a coordinated fashion. For example,
    when a fixed but unknown number of work items is stored in a queue, parallel Pods
    can pick these up one by one to work on them. The first Pod that detects that
    the queue is empty and exits with success indicates the completion of the Job.
    The Job controller waits for all other Pods to terminate too. Since one Pod processes
    multiple work items, this Job type is an excellent choice for granular work items—when
    the overhead for one Pod per work item is not justified.
  prefs: []
  type: TYPE_NORMAL
- en: Indexed Jobs
  prefs: []
  type: TYPE_NORMAL
- en: Similar to *Work queue Jobs*, you can distribute work items to individual Jobs
    without needing an external work queue. When using a fixed completion count and
    setting the completion mode `.spec.completionMode` to `Indexed`, every Pod of
    the Job gets an associated index ranging from 0 to `.spec.completions` - 1\. The
    assigned index is available to the containers through the Pod annotation `batch.kubernetes.io/job-completion-index`
    (see [Chapter 14, “Self Awareness”](ch14.html#SelfAwareness), to learn how this
    annotation can be accessed from your code) or directly via the environment variable
    `JOB_COMPLETION_INDEX` that is set to the index associated with this Pod. With
    this index at hand, the application can pick the associated work item without
    any external synchronization. [Example 7-2](#ex-job-indexed) shows a Job that
    processes the lines of a single file individually by separate Pods. A more realistic
    example would be an indexed Job used for video processing, where parallel Pods
    are processing a certain frame range calculated from the index.
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-2\. An indexed Job selecting its work items based on a job index
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_batch_job_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Enable an indexed completion mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_batch_job_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Run five Pods in parallel to completion.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_batch_job_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Execute a shell script that prints out a range of lines from a given file */logs/random.log*.
    This file is expected to have 50,000 lines of data.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_batch_job_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate start and end line numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_batch_job_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Use `awk` to print out a range of line numbers (`NR` is the `awk`-internal line
    number when iterating over the file).
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_batch_job_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Mount the input data from an external volume. The volume is not shown here;
    you can find the full working definition in the [example repository](https://oreil.ly/PkVF0).
  prefs: []
  type: TYPE_NORMAL
- en: If you have an unlimited stream of work items to process, other controllers
    like ReplicaSet are the better choice for managing the Pods processing these work
    items.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Job abstraction is a pretty basic but also fundamental primitive that other
    primitives such as CronJobs are based on. Jobs help turn isolated work units into
    a reliable and scalable unit of execution. However, a Job doesn’t dictate how
    you should map individually processable work items into Jobs or Pods. That is
    something you have to determine after considering the pros and cons of each option:'
  prefs: []
  type: TYPE_NORMAL
- en: One Job per work item
  prefs: []
  type: TYPE_NORMAL
- en: This option has the overhead of creating Kubernetes Jobs and also means the
    platform has to manage a large number of Jobs that are consuming resources. This
    option is useful when each work item is a complex task that has to be recorded,
    tracked, or scaled independently.
  prefs: []
  type: TYPE_NORMAL
- en: One Job for all work items
  prefs: []
  type: TYPE_NORMAL
- en: This option is right for a large number of work items that do not have to be
    independently tracked and managed by the platform. In this scenario, the work
    items have to be managed from within the application via a batch framework.
  prefs: []
  type: TYPE_NORMAL
- en: The Job primitive provides only the very minimum basics for scheduling work
    items. Any complex implementation has to combine the Job primitive with a batch
    application framework (e.g., in the Java ecosystem, we have Spring Batch and JBeret
    as standard implementations) to achieve the desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Not all services must run all the time. Some services must run on demand, some
    at a specific time, and some periodically. Using Jobs can run Pods only when needed
    and only for the duration of the task execution. Jobs are scheduled on nodes that
    have the required capacity, satisfy Pod placement policies, and take into account
    other container dependency considerations. Using Jobs for short-lived tasks rather
    than using long-running abstractions (such as ReplicaSet) saves resources for
    other workloads on the platform. All of that makes Jobs a unique primitive, and
    Kubernetes a platform supporting diverse workloads.
  prefs: []
  type: TYPE_NORMAL
- en: More Information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Batch Job Example](https://oreil.ly/PkVF0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Jobs](https://oreil.ly/I2Xum)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Parallel Processing Using Expansions](https://oreil.ly/mNmhN)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Coarse Parallel Processing Using a Work Queue](https://oreil.ly/W5aqH)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Fine Parallel Processing Using a Work Queue](https://oreil.ly/-8FBt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Indexed Job for Parallel Processing with Static Work Assignment](https://oreil.ly/2B2Nn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Spring Batch on Kubernetes: Efficient Batch Processing at Scale](https://oreil.ly/8dLDo)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[JBeret Introduction](https://oreil.ly/YyYxy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
