<html><head></head><body><section data-pdf-bookmark="Chapter 2. Deployment Models" data-type="chapter" epub:type="chapter"><div class="chapter" id="deployment_models">&#13;
<h1><span class="label">Chapter 2. </span>Deployment Models</h1>&#13;
&#13;
&#13;
<p>The first step to using Kubernetes in production is obvious: make Kubernetes exist.  <a data-primary="deployment models" data-type="indexterm" id="ix_dplym"/>This includes installing systems to provision Kubernetes clusters and to manage future upgrades.  Being that Kubernetes is a distributed software system, deploying Kubernetes largely boils down to a software installation exercise.  The important difference compared with most other software installs is that Kubernetes is intrinsically tied to the infrastructure.  As such, the software installation and the infrastructure it’s being installed on need to be simultaneously solved for.</p>&#13;
&#13;
<p>In this chapter we will first address preliminary questions around deploying Kubernetes clusters and how much you should leverage managed services and existing products or projects.  For those that heavily leverage existing services, products, and projects, most of this chapter may not be of interest because about 90% of the content in this chapter covers how to approach custom automation.<a data-primary="custom automation" data-type="indexterm" id="idm45612001105880"/>  This chapter can still be of interest if you are evaluating tools for deploying Kubernetes so that you can reason about the different approaches available.  For those in the uncommon position of having to build custom automation for deploying Kubernetes, we will address overarching architectural concerns, including special considerations for etcd as well as how to manage the various clusters under management.  We will also look at useful patterns for managing the various software installations as well as the infrastructure dependencies and will break down the various cluster components and demystify how they fit together.  We’ll also look at ways to manage the add-ons you install to the base Kubernetes cluster as well as strategies for upgrading Kubernetes and the add-on components that make up your application platform.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Managed Service Versus Roll Your Own" data-type="sect1"><div class="sect1" id="idm45612001104072">&#13;
<h1>Managed Service Versus Roll Your Own</h1>&#13;
&#13;
<p>Before we get further into the topic of deployment models for Kubernetes, we should address the <a data-primary="deployment models" data-secondary="managed services versus roll your own" data-type="indexterm" id="ix_dplymmvr"/>idea of whether you should even <em>have</em> a full deployment model for Kubernetes.  Cloud providers offer managed Kubernetes services that mostly alleviate the deployment concerns.  You should still develop reliable, declarative systems for provisioning these managed Kubernetes clusters, but it may be advantageous to abstract away most of the details of <em>how</em> the cluster is brought up.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Managed Services" data-type="sect2"><div class="sect2" id="idm45612001099464">&#13;
<h2>Managed Services</h2>&#13;
&#13;
<p>The case for using managed Kubernetes services boils down to savings in engineering effort.<a data-primary="deployment models" data-secondary="managed services versus roll your own" data-tertiary="managed Kubernetes services" data-type="indexterm" id="idm45612001098024"/><a data-primary="managed Kubernetes services, using for deployments" data-type="indexterm" id="idm45612001096696"/>  There is considerable technical design and implementation in properly managing the deployment and life cycle of Kubernetes.  And remember, Kubernetes is just one component of your application platform—the container orchestrator.</p>&#13;
&#13;
<p>In essence, with a managed service you get a Kubernetes control plane that you can attach worker nodes to at will.  The obligation to scale, ensure availability, and manage the control plane is alleviated.  These are each significant concerns.  Furthermore, if you already use a cloud provider’s existing services you get a leg up.  For example, if you are in Amazon Web Services (AWS) and already use Fargate for serverless compute, Identity and Access Management (IAM) for role-based access control, and CloudWatch for observability, you can leverage these with their Elastic Kubernetes Service (EKS) and solve for several concerns in your app platform.</p>&#13;
&#13;
<p>It is not unlike using a managed database service. If your core concern is an application that serves your business needs, and that app requires a relational database, but you cannot justify having a dedicated database admin on staff, paying a cloud provider to supply you with a database can be a huge boost.  You can get up and running faster. The managed service provider will manage availability, take backups, and perform upgrades on your behalf.  In many cases this is a clear benefit. But, as always, there is a trade-off.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Roll Your Own" data-type="sect2"><div class="sect2" id="idm45612001093400">&#13;
<h2>Roll Your Own</h2>&#13;
&#13;
<p>The savings available in using a managed Kubernetes service come with a price tag.<a data-primary="deployment models" data-secondary="managed services versus roll your own" data-tertiary="roll your own" data-type="indexterm" id="idm45612001091832"/><a data-primary="roll your own deployments" data-type="indexterm" id="idm45612001090568"/>  You pay with a lack of flexibility and freedom.  Part of this is the threat of vendor lock-in.  The managed services are generally offered by cloud infrastructure providers.  If you invest heavily in using a particular vendor for your infrastructure, it is highly likely that you will design systems and leverage services that will not be vendor neutral.  The concern is that if they raise their prices or let their service quality slip in the future, you may find yourself painted into a corner.  Those experts you paid to handle concerns you didn’t have time for may now wield dangerous power over your destiny.</p>&#13;
&#13;
<p>Of course, you can diversify by using managed services from multiple providers, but there will be deltas between the way they expose features of Kubernetes, and which features are exposed could become an awkward inconsistency to overcome.</p>&#13;
&#13;
<p>For this reason, you may prefer to roll your own Kubernetes.  There is a vast array of knobs and levers to adjust on Kubernetes.  This configurability makes it wonderfully flexible and powerful.  If you invest in understanding and managing Kubernetes itself, the app platform world is your oyster.  There will be no feature you cannot implement, no requirement you cannot meet.  And you will be able to implement that seamlessly across infrastructure providers, whether they be public cloud providers, or your own servers in a private datacenter.  Once the different infrastructure inconsistencies are accounted for, the Kubernetes features that are exposed in your platform will be consistent.  And the developers that use your platform will not care—and may not even know—who is providing the underlying infrastructure.</p>&#13;
&#13;
<p>Just keep in mind that developers will care only about the features of the platform, not the underlying infrastructure or who provides it.  If you are in control of the features available, and the features you deliver are consistent across infrastructure providers, you have the freedom to deliver a superior experience to your devs.  You will have control of the Kubernetes version you use.  You will have access to all the flags and features of the control plane components.  You will have access to the underlying machines and the software that is installed on them as well as the static Pod manifests that are written to disk there.  You will have a powerful and dangerous tool to use in the effort to win over your developers.  But never ignore the obligation you have to learn the tool well.  A failure to do so risks injuring yourself and others with it.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Making the Decision" data-type="sect2"><div class="sect2" id="idm45612001085784">&#13;
<h2>Making the Decision</h2>&#13;
&#13;
<p>The path to glory is rarely clear when you begin the journey.<a data-primary="deployment models" data-secondary="managed services versus roll your own" data-tertiary="deciding between" data-type="indexterm" id="idm45612001084376"/>  If you are deciding between a managed Kubernetes service or rolling your own clusters, you are much closer to the beginning of your journey with Kubernetes than the glorious final conclusion.  And the decision of managed service versus roll your own is fundamental enough that it will have long-lasting implications for your business.  So here are some guiding principles to aid the process.</p>&#13;
&#13;
<p>You should lean toward a managed service if:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The idea of understanding Kubernetes sounds terribly arduous</p>&#13;
</li>&#13;
<li>&#13;
<p>The responsibility for managing a distributed software system that is critical to the success of your business sounds dangerous</p>&#13;
</li>&#13;
<li>&#13;
<p>The inconveniences of restrictions imposed by vendor-provided features seem manageable</p>&#13;
</li>&#13;
<li>&#13;
<p>You have faith in your managed service vendor to respond to your needs and be a good business partner</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>You should lean toward rolling your own Kubernetes if:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The vendor-imposed restrictions make you uneasy</p>&#13;
</li>&#13;
<li>&#13;
<p>You have little or no faith in the corporate behemoths that provide cloud compute infrastructure</p>&#13;
</li>&#13;
<li>&#13;
<p>You are excited by the power of the platform you can build around Kubernetes</p>&#13;
</li>&#13;
<li>&#13;
<p>You relish the opportunity to leverage this amazing container orchestrator to provide a delightful experience to your devs</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>If you decide to use a managed service, consider skipping most of the remainder of this chapter. <a data-type="xref" href="#addons">“Add-ons”</a> and <a data-type="xref" href="#triggering_mechanisms">“Triggering Mechanisms”</a> are still applicable to your use case, but the other sections in this chapter will not apply.  If, on the other hand, you are looking to manage your own clusters, read on!  Next we’ll dig more into the deployment models and tools you should consider.<a data-primary="deployment models" data-secondary="managed services versus roll your own" data-startref="ix_dplymmvr" data-type="indexterm" id="idm45612001070984"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Automation" data-type="sect1"><div class="sect1" id="idm45612001069624">&#13;
<h1>Automation</h1>&#13;
&#13;
<p>If you are to undertake designing a deployment model for your Kubernetes clusters, the topic of automation is of the utmost importance.<a data-primary="deployment models" data-secondary="automation in" data-type="indexterm" id="ix_dplymau"/><a data-primary="automation" data-secondary="in deployments" data-secondary-sortas="deployments" data-type="indexterm" id="ix_autodep"/>  Any deployment model will need to keep this as a guiding principle.  Removing human toil is critical to reduce cost and improve stability.  Humans are costly.  Paying the salary for engineers to execute routine, tedious operations is money not spent on innovation.  Furthermore, humans are unreliable.  They make mistakes.  Just one error in a series of steps may introduce instability or even prevent the system from working at all.  The upfront engineering investment to automate deployments using software systems will pay dividends in saved toil and troubleshooting in the future.</p>&#13;
&#13;
<p>If you decide to manage your own cluster life cycle, you must formulate your strategy for doing this.  You have a choice between using a prebuilt Kubernetes installer or developing your own custom automation from the ground up.  This decision has parallels with the decision between managed services versus roll your own.  One path gives you great power, control, and flexibility but at the cost of engineering effort.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Prebuilt Installer" data-type="sect2"><div class="sect2" id="idm45612001063688">&#13;
<h2>Prebuilt Installer</h2>&#13;
&#13;
<p>There are now countless open source and enterprise-supported Kubernetes installers available.<a data-primary="deployment models" data-secondary="automation in" data-tertiary="prebuilt installers" data-type="indexterm" id="idm45612001062248"/><a data-primary="prebuilt installers" data-type="indexterm" id="idm45612001061000"/><a data-primary="installers, prebuilt" data-type="indexterm" id="idm45612001060328"/>  Case in point: there are currently 180 Kubernetes Certified Service Providers listed on the <a href="https://www.cncf.io/certification/kcsp">CNCF’s website</a>.  Some you will need to pay money for and will be accompanied by experienced field engineers to help get you up and running as well as support staff you can call on in times of need.  Others will require research and &#13;
<span class="keep-together">experimentation</span> to understand and use.  Some installers—usually the ones you pay money for—will get you from zero to Kubernetes with the push of a button.  If you fit the prescriptions provided and options available, and your budget can accommodate the expense, this installer method could be a great fit.  At the time of this writing, using prebuilt installers is the approach we see most commonly in the field.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Custom Automation" data-type="sect2"><div class="sect2" id="idm45612001057192">&#13;
<h2>Custom Automation</h2>&#13;
&#13;
<p>Some amount of custom automation is commonly required even if using a prebuilt installer.<a data-primary="custom automation" data-secondary="building to install and manage Kubernetes" data-type="indexterm" id="idm45612001055512"/><a data-primary="deployment models" data-secondary="automation in" data-tertiary="custom automation" data-type="indexterm" id="idm45612001054568"/>  This is usually in the form of integration with a team’s existing systems.  However, in this section we’re talking about developing a custom Kubernetes installer.</p>&#13;
&#13;
<p>If you are beginning your journey with Kubernetes or changing direction with your Kubernetes strategy, the homegrown automation route is likely your choice only if all of the following apply:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You have more than just one or two engineers to devote to the effort</p>&#13;
</li>&#13;
<li>&#13;
<p>You have engineers on staff with deep Kubernetes experience</p>&#13;
</li>&#13;
<li>&#13;
<p>You have specialized requirements that no managed service or prebuilt installer satisfies well</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Most of the remainder of this chapter is for you if one of the following applies:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>You fit the use case for building custom automation</p>&#13;
</li>&#13;
<li>&#13;
<p>You are evaluating installers and want to gain a deeper insight into what good patterns look like</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This brings us to details of building custom automation to install and manage Kubernetes clusters.  Underpinning all these concerns should be a clear understanding of your platform requirements.  These should be driven primarily by the requirements of your app development teams, particularly those that will be the earliest adopters.  <a data-primary="testing" data-secondary="early prerelease versions of platform" data-type="indexterm" id="idm45612001045816"/>Do not fall into the trap of building a platform in a vacuum without close collaboration with the consumers of the platform.  Make early prerelease versions of your platform available to dev teams for testing.  Cultivate a productive feedback loop for fixing bugs and adding features.  The successful adoption of your platform depends upon this.</p>&#13;
&#13;
<p>Next we will cover architecture concerns that should be considered before any implementation begins.  This includes deployment models for etcd, separating deployment environments into tiers, tackling challenges with managing large numbers of clusters, and what types of node pools you might use to host your workloads.  After that, we’ll get into Kubernetes installation details, first for the infrastructure dependencies, then, the software that is installed on the clusters’ virtual or physical machines, and finally for the containerized components that constitute the control plane of a Kubernetes &#13;
<span class="keep-together">cluster</span>.<a data-primary="automation" data-secondary="in deployments" data-secondary-sortas="deployments" data-startref="ix_autodep" data-type="indexterm" id="idm45612001042632"/><a data-primary="deployment models" data-secondary="automation in" data-startref="ix_dplymau" data-type="indexterm" id="idm45612001041080"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Architecture and Topology" data-type="sect1"><div class="sect1" id="arch_and_topology">&#13;
<h1>Architecture and Topology</h1>&#13;
&#13;
<p>This section covers the architectural decisions that have broad implications for the systems<a data-primary="deployment models" data-secondary="architecture and topology" data-type="indexterm" id="ix_dplymat"/><a data-primary="architecture and topology" data-type="indexterm" id="ix_arctop"/> you use to provision and manage your Kubernetes clusters.  They include the deployment model for etcd and the unique considerations you must take into account for that component of the platform.  Among these topics is how you organize the various clusters under management into tiers based on the service level objectives (SLOs) for them.  We will also look at the concept of node pools and how they can be used for different purposes within a given cluster.  And, lastly, we will address the methods you can use for federated management of your clusters and the software you deploy to them.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="etcd Deployment Models" data-type="sect2"><div class="sect2" id="idm45612001034920">&#13;
<h2>etcd Deployment Models</h2>&#13;
&#13;
<p>As the database for the objects in a Kubernetes cluster, etcd deserves special consideration.<a data-primary="deployment models" data-secondary="architecture and topology" data-tertiary="etcd deployments" data-type="indexterm" id="idm45612001033272"/><a data-primary="architecture and topology" data-secondary="etcd deployment models" data-type="indexterm" id="idm45612001032008"/><a data-primary="etcd" data-secondary="deployment models" data-type="indexterm" id="idm45612001031048"/>  etcd is a distributed data store that uses a consensus algorithm to maintain a copy of the your cluster’s state on multiple machines.  This introduces network considerations for the nodes in an etcd cluster so they can reliably maintain that consensus over their network connections.  It has unique network latency requirements that we need to design for when considering network topology.  We’ll cover that topic in this section and also look at the two primary architectural choices to make in the deployment model for etcd: dedicated versus colocated and whether to run in a container or install directly on the host.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Network considerations" data-type="sect3"><div class="sect3" id="network_considerations">&#13;
<h3>Network considerations</h3>&#13;
&#13;
<p>The default settings in etcd are designed for the latency in a single datacenter.  <a data-primary="etcd" data-secondary="deployment models" data-tertiary="network considerations" data-type="indexterm" id="idm45612001027640"/><a data-primary="networks" data-secondary="considerations in etcd deployment" data-type="indexterm" id="idm45612001026392"/>If you distribute etcd across multiple datacenters, you should test the average round-trip between members and tune the heartbeat interval and election timeout for etcd if need be.  We strongly discourage the use of etcd clusters distributed across different regions.  If using multiple datacenters for improved availability, they should at least be in close proximity within a region.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dedicated versus colocated" data-type="sect3"><div class="sect3" id="idm45612001024648">&#13;
<h3>Dedicated versus colocated</h3>&#13;
&#13;
<p>A very common question we get about how to deploy is whether to give etcd its own dedicated machines or to colocate <a data-primary="etcd" data-secondary="deployment models" data-tertiary="dedicated versus colocated" data-type="indexterm" id="idm45612001023256"/>them on the control plane machines with the API server, scheduler, controller manager, etc.  The first thing to consider is the size of clusters you will be managing, i.e., the number of worker nodes you will run per cluster.  <a data-primary="clusters" data-secondary="size of in etcd deployments" data-type="indexterm" id="idm45612001021576"/>The trade-offs around cluster sizes will be discussed later in the chapter.  Where you land on that subject will largely inform whether you dedicate machines to etcd.  Obviously etcd is crucial.  <a data-primary="performance" data-secondary="etcd deployments and" data-type="indexterm" id="idm45612001020280"/>If etcd performance is compromised, your ability to control the resources in your cluster will be compromised.  As long as your workloads don’t have dependencies on the Kubernetes API, they should not suffer, but keeping your control plane healthy is still very important.</p>&#13;
&#13;
<p>If you are driving a car down the street and the steering wheel stops working, it is little comfort that the car is still driving down the road.  In fact, it may be terribly dangerous.  For this reason, if you are going to be placing the read and write demands on etcd that come with larger clusters, it is wise to dedicate machines to them to eliminate resource contention with other control plane components.  In this context, a “large” cluster is dependent upon the size of the control plane machines in use but should be at least a topic of consideration with anything above 50 worker nodes.  If planning for clusters with over 200 workers, it’s best to just plan for dedicated etcd clusters.  If you do plan smaller clusters, save yourself the management overhead and infrastructure costs—go with colocated etcd.<a data-primary="kubeadm" data-type="indexterm" id="idm45612001017688"/>  Kubeadm is a popular Kubernetes bootstrapping tool that you will likely be using; it supports this model and will take care of the associated concerns.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Containerized versus on host" data-type="sect3"><div class="sect3" id="idm45612001016424">&#13;
<h3>Containerized versus on host</h3>&#13;
&#13;
<p>The next common question revolves around whether to install etcd on the machine or to run it in a container.<a data-primary="etcd" data-secondary="deployment models" data-tertiary="containerized versus on host" data-type="indexterm" id="idm45612001015032"/><a data-primary="containers" data-secondary="containerized versus on host etcd deployments" data-type="indexterm" id="idm45612001013720"/>  Let’s tackle the easy answer first:  if you’re running etcd in a colocated manner, run it in a container.  When leveraging kubeadm for Kubernetes bootstrapping, this configuration is supported and well tested.  It is your best option.  If, on the other hand, you opt for running etcd on dedicated machines your options are as follows:  you can install etcd on the host, which gives you the opportunity to bake it into machine images and eliminate the additional concerns of having a container runtime on the host.  Alternatively, if you run in a container, the most useful pattern is to install a container runtime and kubelet on the machines and use a static manifest to spin up etcd.  This has the advantage of following the same patterns and install methods as the other control plane components.  Using repeated patterns in complex systems is useful, but this question is largely a question of preference.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Tiers" data-type="sect2"><div class="sect2" id="idm45612001011496">&#13;
<h2>Cluster Tiers</h2>&#13;
&#13;
<p>Organizing your clusters according to tiers is an almost universal pattern we see in the field.<a data-primary="architecture and topology" data-secondary="cluster tiers" data-type="indexterm" id="idm45612001010200"/><a data-primary="deployment models" data-secondary="architecture and topology" data-tertiary="cluster tiers" data-type="indexterm" id="idm45612001009208"/><a data-primary="cluster tiers" data-type="indexterm" id="idm45612001007976"/><a data-primary="testing" data-secondary="cluster tier for" data-type="indexterm" id="idm45612001007304"/>  These tiers often include testing, development, staging, and production.  Some teams refer to these as different “environments.”  However, this is a broad term that can have different meanings and implications.<a data-primary="tiers (cluster)" data-type="indexterm" id="idm45612001005992"/>  We will use the term <em>tier</em> here to specifically address the different types of clusters.  In particular, we’re talking about the SLOs and SLAs that may be associated with the cluster, as well as the purpose for the cluster, and where the cluster sits in the path to production for an application, if at all.  What exactly these tiers will look like for different organizations varies, but there are common themes and we will describe what each of these four tiers commonly mean.  Use the same cluster deployment and life cycle management systems across all tiers.  The heavy use of these systems in the lower tiers will help ensure they will work as expected when applied to critical production clusters:</p>&#13;
<dl>&#13;
<dt>Testing</dt>&#13;
<dd>&#13;
<p>Clusters in the testing tier are single-tenant, ephemeral clusters that often have a time-to-live (TTL) applied such that they are automatically destroyed after a specified amount of time, usually less than a week.  These are spun up very commonly by platform engineers for the purpose of testing particular components or platform features they are developing.  Testing tiers may also be used by developers when a local cluster is inadequate for local development, or as a subsequent step to testing on a local cluster.  This is more common when an app dev team is initially containerizing and testing their application on Kubernetes.  There is no SLO or SLA for these clusters.  These clusters would use the latest version of a platform, or perhaps optionally a pre-alpha release.</p>&#13;
</dd>&#13;
<dt>Development</dt>&#13;
<dd>&#13;
<p>Development tier clusters are generally “permanent” clusters without a TTL.<a data-primary="development clusters" data-type="indexterm" id="idm45612001000232"/>  They are multitenant (where applicable) and have all the features of a production cluster.<a data-primary="integration testing" data-secondary="use of development tiers" data-type="indexterm" id="idm45612000999288"/>  They are used for the first round of integration tests for applications and are used to test the compatibility of application workloads with alpha versions of the platform. Development tiers are also used for general testing and development for the app dev teams.  These clusters normally have an SLO but not a formal agreement associated with them.  The availability objectives will often be near production-level, at least during business hours, because outages will impact developer productivity.  In contrast, the applications have zero SLO or SLA when running on dev clusters and are very frequently updated and in constant flux.  These clusters will run the officially released alpha and/or beta version of the platform.</p>&#13;
</dd>&#13;
<dt>Staging</dt>&#13;
<dd>&#13;
<p>Like those in the development tier, clusters in the staging tier are also permanent clusters and are commonly used by multiple tenants.<a data-primary="staging clusters" data-type="indexterm" id="idm45612000996072"/>  They are used for final integration testing and approval before rolling out to live production.  They are used by stakeholders that are not actively developing the software running there.  This would include project managers, product owners, and executives.  This may also include customers or external stakeholders who need access to prerelease versions of software.  They will often have a similar SLO to development clusters.  Staging tiers may have a formal SLA associated with them if external stakeholders or paying customers are accessing workloads on the cluster.  These clusters will run the officially released beta version of the platform if strict backward compatibility is followed by the platform team.  If backward compatibility cannot be guaranteed, the staging cluster should run the same stable release of the platform as used in production.</p>&#13;
</dd>&#13;
<dt>Production</dt>&#13;
<dd>&#13;
<p>Production tier clusters are the money-makers.<a data-primary="production clusters" data-type="indexterm" id="idm45612000993080"/>  These are used for customer-facing, revenue-producing applications and websites.  Only approved, production-ready, stable releases of software are run here.  And only the fully tested and approved stable release of the platform is used.  Detailed well-defined SLOs are used and tracked.  Often, legally binding SLAs apply.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Node Pools" data-type="sect2"><div class="sect2" id="idm45612000991528">&#13;
<h2>Node Pools</h2>&#13;
&#13;
<p>Node pools are a way to group together types of nodes within a single Kubernetes cluster.  <a data-primary="deployment models" data-secondary="architecture and topology" data-tertiary="node pools" data-type="indexterm" id="idm45612000989880"/><a data-primary="architecture and topology" data-secondary="node pools" data-type="indexterm" id="idm45612000988664"/><a data-primary="node pools" data-type="indexterm" id="idm45612000987704"/>These types of nodes may be grouped together by way of their unique characteristics or by way of the role they play.  It’s important to understand the trade-offs of using node pools before we get into details.  The trade-off often revolves around the choice between using multiple node pools within a single cluster versus provisioning separate, distinct clusters.  If you use node pools, you will need to use Node selectors on your workloads to make sure they end up in the appropriate node pool.  You will also likely need to use Node taints to prevent workloads without Node selectors from inadvertently landing where they shouldn’t.  Additionally, the scaling of nodes within your cluster becomes more complicated because your systems have to monitor distinct pools and scale each separately.  If, on the other hand, you use distinct clusters you displace these concerns into cluster management and software federation concerns.  You will need more clusters.  And you will need to properly target your workloads to the right clusters. <a data-type="xref" href="#node_pool_pros_and_cons">Table 2-1</a> summarizes these pros and cons of using node pools.</p>&#13;
<table id="node_pool_pros_and_cons">&#13;
<caption><span class="label">Table 2-1. </span>Node pool pros and cons</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Pros</th>&#13;
<th>Cons</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Reduced number of clusters under management</p></td>&#13;
<td><p>Node selectors for workloads will often be needed</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Smaller number of target clusters for workloads</p></td>&#13;
<td><p>Node taints will need to be applied and managed</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td/>&#13;
<td><p>More complicated cluster scaling operations</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>A characteristic-based node pool is <a data-primary="characteristic-based node pools" data-type="indexterm" id="idm45612000976520"/>one that consists of nodes that have components or attributes that are required by, or suited to, some particular workloads.  An example of this is the presence of a specialized device like a graphics processing unit (GPU).  Another example of a characteristic may be the type of network interface it uses.  One more could be the ratio of memory to CPU on the machine.  We will discuss the reasons you may use nodes with different ratios of these resources in more depth later on in <a data-type="xref" href="#infrastructure">“Infrastructure”</a>.  Suffice to say for now, all these characteristics lend themselves to different types of workloads, and if you run them collectively in the same cluster, you’ll need to group them into pools to manage where different Pods land.</p>&#13;
&#13;
<p>A role-based node pool is one that has a particular function and that you often want to insulate from resource contention.<a data-primary="role-based node pools" data-type="indexterm" id="idm45612000973560"/> The nodes sliced out into a role-based pool don’t necessarily have peculiar characteristics, just a different function.  A common example is to dedicate a node pool to the ingress layer in your cluster.  In the example of an ingress pool, the dedicated pool not only insulates the workloads from resource contention (particularly important in this case since resource requests and limits are not currently available for network usage) but also simplifies the networking model and the specific nodes that are exposed to traffic from sources outside the cluster.  In contrast to the characteristic-based node pool, these roles are often not a concern you can displace into distinct clusters because the machines play an important role in the function of a particular cluster.  That said, do ensure you are slicing off nodes into a pool for good reason.  Don’t create pools indiscriminately.  Kubernetes clusters are complex enough.  Don’t complicate your life more than you need to.</p>&#13;
&#13;
<p>Keep in mind that you will most likely need to solve the multicluster management problems that many distinct clusters bring, regardless of whether you use node pools.  There are very few enterprises that use Kubernetes that don’t accumulate a large number of distinct clusters.  There are a large variety of reasons for this.  So if you are tempted to introduce characteristic-based node pools, consider investing the engineering effort into developing and refining your multicluster management.  Then you unlock the opportunity to seamlessly use distinct clusters for the different machine characteristics you need to provide.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Federation" data-type="sect2"><div class="sect2" id="idm45612000970568">&#13;
<h2>Cluster Federation</h2>&#13;
&#13;
<p>Cluster federation broadly refers to how to centrally manage all the clusters under your control.<a data-primary="deployment models" data-secondary="architecture and topology" data-tertiary="cluster federation" data-type="indexterm" id="ix_dplymcf"/><a data-primary="architecture and topology" data-secondary="cluster federation" data-type="indexterm" id="ix_arctopcf"/><a data-primary="federation" data-secondary="cluster" data-type="indexterm" id="ix_fedcls"/><a data-primary="cluster federation" data-type="indexterm" id="ix_clsfed"/>  Kubernetes is like a guilty pleasure.  When you discover how much you enjoy it, you can’t have just one.  But, similarly, if you don’t keep that habit under control, it can become messy.<a data-primary="dependencies" data-secondary="software, federation strategies and" data-type="indexterm" id="idm45612000963672"/>  Federation strategies are ways for enterprises to manage their software dependencies so they don’t spiral into costly, destructive addictions.</p>&#13;
&#13;
<p>A common, useful approach is to federate regionally and then globally.  This lessens the blast radius of, and reduces the computational load for, these federation clusters.  When you first begin federation efforts, you may not have the global presence or volume of infrastructure to justify a multilevel federation approach, but keep it in mind as a design principle in case it becomes a future requirement.</p>&#13;
&#13;
<p>Let’s discuss some important related subjects in this area.  In this section, we’ll look at how management clusters can help with consolidating and centralizing regional services.  We’ll consider how we can consolidate the metrics for workloads in various clusters.  And we’ll discuss how this impacts the managing workloads that are deployed across different clusters in a centrally managed way.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Management clusters" data-type="sect3"><div class="sect3" id="management_clusters">&#13;
<h3>Management clusters</h3>&#13;
&#13;
<p>Management clusters are what they sound like: Kubernetes clusters that manage other clusters.<a data-primary="architecture and topology" data-secondary="cluster federation" data-tertiary="management clusters" data-type="indexterm" id="idm45612000958712"/><a data-primary="cluster federation" data-secondary="management clusters" data-type="indexterm" id="idm45612000957448"/><a data-primary="management clusters" data-type="indexterm" id="idm45612000956504"/>  Organizations are finding that, as their usage expands and as the number of clusters under management increases, they need to leverage software systems for smooth operation.  And, as you might expect, they often use Kubernetes-based platforms to run this software.  <a href="https://cluster-api.sigs.k8s.io">Cluster API</a> has become a popular project for accomplishing this.<a data-primary="Cluster API" data-type="indexterm" id="idm45612000954744"/>  It is a set of Kubernetes operators that use custom resources such as Cluster and Machine resources to represent other Kubernetes clusters and their components.  A common pattern used is to deploy the Cluster API components to a management cluster for deploying and managing the infrastructure for other workload clusters.</p>&#13;
&#13;
<p>Using a management cluster in this manner does have flaws, however.  It is usually prudent to strictly separate concerns between your production tier and other tiers.  Therefore, organizations will often have a management cluster dedicated to production.  This further increases the management cluster overhead.  <a data-primary="clusters" data-secondary="autoscaling" data-type="indexterm" id="idm45612000952856"/><a data-primary="autoscaling" data-secondary="cluster" data-type="indexterm" id="idm45612000951880"/>Another problem is with cluster autoscaling, which is a method of adding and removing worker nodes in response to the scaling of workloads.  The Cluster Autoscaler typically runs in the cluster that it scales so as to watch for conditions that require scaling events.  But the management cluster contains the controller that manages the provisioning and decommissioning of those worker nodes.  This introduces an external dependency on the management cluster for any workload cluster that uses Cluster Autoscaler, as illustrated in <a data-type="xref" href="#cluster_autoscaler_accessing_a_management_cluster_to_trigger_scaling_events">Figure 2-1</a>.  What if the management cluster becomes unavailable at a busy time that your cluster needs to scale out to meet demand?</p>&#13;
&#13;
<figure><div class="figure" id="cluster_autoscaler_accessing_a_management_cluster_to_trigger_scaling_events">&#13;
<img alt="prku 0201" src="assets/prku_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>Cluster Autoscaler accessing a management cluster to trigger scaling events.</h6>&#13;
</div></figure>&#13;
&#13;
<p>One strategy to remedy this is to run the Cluster API components in the workload cluster in a self-contained manner.  In this case, the Cluster and Machine resources will also live there in the workload cluster.  You can still use the management cluster for creation and deletion of clusters, but the workload cluster becomes largely autonomous and free from the external dependency on the management cluster for routine operations, such as autoscaling, as shown in <a data-type="xref" href="#cluster_autoscaler_accessing_a_local_cluster_api_component_to_perform_scaling_events">Figure 2-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="cluster_autoscaler_accessing_a_local_cluster_api_component_to_perform_scaling_events">&#13;
<img alt="prku 0202" src="assets/prku_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>Cluster Autoscaler accessing a local Cluster API component to perform scaling events.</h6>&#13;
</div></figure>&#13;
&#13;
<p>This pattern also has the distinct advantage that if any other controllers or workloads in the cluster have a need for metadata or attributes contained in the Cluster API custom resources, they can access them by reading the resource through the local API.  There is no need to access the management cluster API.  For example, if you have a Namespace controller that changes its behavior based on whether it is in a development or production cluster, that is information that can already be contained in the Cluster resource that represents the cluster in which it lives.</p>&#13;
&#13;
<p>Additionally, management clusters also often host shared or regional services that are accessed by systems in various other clusters.  These are not so much <em>management</em> functions.  Management clusters are often just a logical place to run these shared services.  Examples of these shared services include CI/CD systems and container &#13;
<span class="keep-together">registries</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Observability" data-type="sect3"><div class="sect3" id="idm45612000940712">&#13;
<h3>Observability</h3>&#13;
&#13;
<p>When managing a large number of clusters, one of the challenges that arises is the collection of metrics from across this infrastructure and bringing them—or a subset thereof—into a central location.  <a data-primary="architecture and topology" data-secondary="cluster federation" data-tertiary="observability" data-type="indexterm" id="idm45612000939016"/><a data-primary="cluster federation" data-secondary="observability" data-type="indexterm" id="idm45612000937800"/><a data-primary="observability" data-secondary="in cluster federation" data-secondary-sortas="cluster" data-type="indexterm" id="idm45612000936856"/>High-level measurable data points that give you a clear picture of the health of the clusters and workloads under management is a critical concern of cluster federation.  <a href="https://prometheus.io">Prometheus</a> is a mature open source project that many organizations use to gather and store metrics.<a data-primary="Prometheus" data-type="indexterm" id="idm45612000934552"/>  Whether you use it or not, the model it uses for federation is very useful and worth looking at so as to replicate with the tools you use, if possible.  It supports the regional approach to federation by allowing federated Prometheus servers to scrape subsets of metrics from other, lower-level Prometheus servers.  So it will accommodate any federation strategy you employ. <a data-type="xref" href="ch09.html#observability_chapter">Chapter 9</a> explores this topic in more depth.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Federated software deployment" data-type="sect3"><div class="sect3" id="idm45612000932392">&#13;
<h3>Federated software deployment</h3>&#13;
&#13;
<p>Another important concern when managing various, remote clusters is how to manage deployment of software to those clusters.<a data-primary="federated software deployment" data-type="indexterm" id="idm45612000930984"/><a data-primary="cluster federation" data-secondary="federated software deployment" data-type="indexterm" id="idm45612000930216"/><a data-primary="architecture and topology" data-secondary="cluster federation" data-tertiary="federated software deployment" data-type="indexterm" id="idm45612000929256"/>  It’s one thing to be able to manage the clusters themselves, but it’s another entirely to organize the deployment of end-user workloads to these clusters.  These are, after all, the point of having all these clusters.  Perhaps you have critical, high-value workloads that must be deployed to multiple regions for availability purposes.  Or maybe you just need to organize where workloads get deployed based on characteristics of different clusters.  How you make these determinations is a challenging problem, as evidenced by the relative lack of consensus around a good solution to the problem.</p>&#13;
&#13;
<p>The Kubernetes community has attempted to tackle this problem in a way that is broadly applicable for some time.  The most recent incarnation is <a href="https://github.com/kubernetes-sigs/kubefed">KubeFed</a>.  <a data-primary="KubeFed" data-type="indexterm" id="idm45612000925832"/>It also addresses cluster configurations, but here we’re concerned more with the definitions of workloads that are destined for multiple clusters.  One of the useful design concepts that has emerged is the ability to federate any API type that is used in Kubernetes.  For example, you can use federated versions of Namespace and Deployment types and for declaring how resources should be applied to multiple clusters.  This is a powerful notion in that you can centrally create a FederatedDeployment resource in one management cluster and have that manifest as multiple remote Deployment objects being created in other clusters.  However, we expect to see more advances in this area in the future.  At this time, the most common way we still see in the field to manage this concern is with CI/CD tools that are configured to target different clusters when workloads are deployed.</p>&#13;
&#13;
<p>Now that we’ve covered the broad architectural concerns that will frame how your fleet of clusters is organized and managed, let’s dig into the infrastructure concerns in detail.<a data-primary="deployment models" data-secondary="architecture and topology" data-startref="ix_dplymcf" data-tertiary="cluster federation" data-type="indexterm" id="idm45612000923496"/><a data-primary="cluster federation" data-startref="ix_clsfed" data-type="indexterm" id="idm45612000921960"/><a data-primary="architecture and topology" data-secondary="cluster federation" data-startref="ix_arctopcf" data-type="indexterm" id="idm45612000921016"/><a data-primary="deployment models" data-secondary="architecture and topology" data-startref="ix_dplymat" data-type="indexterm" id="idm45612000919784"/><a data-primary="federation" data-secondary="cluster" data-startref="ix_fedcls" data-type="indexterm" id="idm45612000918552"/><a data-primary="architecture and topology" data-startref="ix_arctop" data-type="indexterm" id="idm45612000917336"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Infrastructure" data-type="sect1"><div class="sect1" id="infrastructure">&#13;
<h1>Infrastructure</h1>&#13;
&#13;
<p>Kubernetes deployment is a software installation process with a dependency on IT infrastructure.<a data-primary="infrastructure" data-type="indexterm" id="ix_infr"/><a data-primary="deployment models" data-secondary="infrastructure" data-type="indexterm" id="ix_dplyminf"/>  A Kubernetes cluster can be spun up on one’s laptop using virtual machines or Docker containers.  But this is merely a simulation for testing purposes.  For production use, various infrastructure components need to be present, and they are often provisioned as a part of the Kubernetes deployment itself.</p>&#13;
&#13;
<p>A useful production-ready Kubernetes cluster needs some number of computers connected to a network to run on.<a data-primary="machines" data-type="indexterm" id="idm45612000911000"/>  To keep our terminology consistent, we’ll use the term <em>machines</em> for these computers.  Those machines may be virtual or physical.  The important issue is that you are able to provision these machines, and a primary concern is the method used to bring them online.</p>&#13;
&#13;
<p>You may have to purchase hardware and install them in a datacenter.  Or you may be able to simply request the needed resources from a cloud provider to spin up machines as needed.  Whatever the process, you need machines as well as properly configured networking, and this needs to be accounted for in your deployment model.</p>&#13;
&#13;
<p>As an important part of your automation efforts, give careful consideration to the automation of infrastructure management.<a data-primary="automation" data-secondary="of infrastructure management" data-secondary-sortas="infrastructure" data-type="indexterm" id="idm45612000908264"/>  Lean away from manual operations such as clicking through forms in an online wizard.  Lean toward using declarative systems that instead call an API to bring about the same result.  This automation model requires the ability to provision servers, networking, and related resources on demand, as with a cloud provider such as Amazon Web Services, Microsoft Azure, or Google Cloud Platform.  However, not all environments have an API or web user interface to spin up infrastructure.  Vast production workloads run in datacenters filled with servers that are purchased and installed by the company that will use them.  This needs to happen well before the Kubernetes software components are installed and run.  It’s important we draw this distinction and identify the models and patterns that apply usefully in each case.</p>&#13;
&#13;
<p>The next section will address the challenges of running Kubernetes on bare metal in contrast to using virtual machines for the nodes in your Kubernetes clusters.  We will then discuss cluster sizing trade-offs and the implications that has for your cluster life cycle management.  Subsequently, we will go over the concerns you should take into account for the compute and networking infrastructure.  And, finally, this will lead us to some specific strategies for automating the infrastructure management for your Kubernetes clusters.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Bare Metal Versus Virtualized" data-type="sect2"><div class="sect2" id="idm45612000905032">&#13;
<h2>Bare Metal Versus Virtualized</h2>&#13;
&#13;
<p>When exploring Kubernetes, many ponder whether the relevance of the virtual machine layer is necessary.<a data-primary="virtualization" data-secondary="bare metal versus virtualized machines" data-type="indexterm" id="idm45612000903224"/><a data-primary="bare metal versus virtualized machines" data-type="indexterm" id="idm45612000902280"/><a data-primary="infrastructure" data-secondary="bare metal versus virtualized machines" data-type="indexterm" id="idm45612000901592"/><a data-primary="deployment models" data-secondary="infrastructure" data-tertiary="bare metal versus virtualized machines" data-type="indexterm" id="idm45612000900632"/><a data-primary="machines" data-secondary="bare metal versus virtualized" data-type="indexterm" id="idm45612000899400"/> Don’t containers largely do the same thing? Would you essentially be running two layers of virtualization? The answer is, not necessarily. Kubernetes initiatives can be wildly successful atop bare metal or virtualized environments. Choosing the right medium to deploy to is critical and should be done through the lens of problems solved by various technologies and your team’s maturity in these technologies.</p>&#13;
&#13;
<p>The virtualization revolution changed how the world provisions and manages infrastructure. Historically, infrastructure teams used methodologies such as PXE booting hosts, managing server configurations, and making ancillary hardware, such as storage, available to servers. Modern virtualized environments abstract all of this behind APIs, where resources can be provisioned, mutated, and deleted at will without knowing what the underlying hardware looks like. This model has been proven throughout datacenters with vendors such as VMware and in the cloud where the majority of compute is running atop some sort of hypervisor.<a data-primary="hypervisors" data-secondary="compute running atop in the cloud" data-type="indexterm" id="idm45612000896856"/> Thanks to these advancements, many newcomers operating infrastructure in the cloud native world may never know about some of those underlying hardware concerns.  The diagram in <a data-type="xref" href="#comparison_of_administrator_interactions_when_provisioning_and_managing_bare_metal">Figure 2-3</a> is not an exhaustive representation of the difference between virtualization and bare metal, but more so how the interaction points tend to differ.</p>&#13;
&#13;
<figure><div class="figure" id="comparison_of_administrator_interactions_when_provisioning_and_managing_bare_metal">&#13;
<img alt="prku 0203" src="assets/prku_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Comparison of administrator interactions when provisioning and managing bare metal compute infrastructure versus virtual machines.</h6>&#13;
</div></figure>&#13;
&#13;
<p>The benefits of the virtualized models go far beyond having a unified API to interact with. In virtualized environments, we have the benefit of building many virtual servers within our hardware server, enabling us to slice each computer into fully isolated machines where we can:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Easily create and clone machines and machine images</p>&#13;
</li>&#13;
<li>&#13;
<p>Run many operating systems on the same server</p>&#13;
</li>&#13;
<li>&#13;
<p>Optimize server usage by dedicating variant amounts of resources based on application needs</p>&#13;
</li>&#13;
<li>&#13;
<p>Change resource settings without disrupting the server</p>&#13;
</li>&#13;
<li>&#13;
<p>Programmatically control what hardware servers have access to, e.g., NICs</p>&#13;
</li>&#13;
<li>&#13;
<p>Run unique networking and routing configurations per server</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This flexibility also enables us to scope operational concerns on a smaller basis. For example, we can now upgrade one host without impacting all others running on the hardware. Additionally, with many of the mechanics available in virtualized environments, the creating and destroying of servers is typically more efficient. Virtualization has its own set of trade-offs.<a data-primary="virtualization" data-secondary="trade-offs with" data-type="indexterm" id="idm45612000884872"/> There is, typically, overhead incurred when running further away from the metal. Many hyper-performance–sensitive applications, such as trading applications, may prefer running on bare metal.<a data-primary="performance" data-secondary="virtualization and" data-type="indexterm" id="idm45612000883560"/> There is also overhead in running the virtualization stack itself. In edge computing, for cases such as telcos running their 5G networks, they may desire running against the hardware.</p>&#13;
&#13;
<p>Now that we’ve completed a brief review of the virtualization revolution, let’s examine how this has impacted us when using Kubernetes and container abstractions because these force our point of interaction even higher up the stack. <a data-type="xref" href="#operator_interactions_when_using_kubernetes">Figure 2-4</a> illustrates what this looks like through an operator’s eyes at the Kubernetes layer.  The underlying computers are viewed as a “sea of compute” where workloads can define what resources they need and will be scheduled appropriately.</p>&#13;
&#13;
<figure><div class="figure" id="operator_interactions_when_using_kubernetes">&#13;
<img alt="prku 0204" src="assets/prku_0204.png"/>&#13;
<h6><span class="label">Figure 2-4. </span>Operator interactions when using Kubernetes.</h6>&#13;
</div></figure>&#13;
&#13;
<p>It’s important to note that Kubernetes clusters have several integration points with the underlying infrastructure. For example, many use CSI-drivers to integrate with storage providers. There are multiple infra management projects that enable requesting new hosts from the provider and joining the cluster. And, most commonly, organizations rely on Cloud Provider Integrations (CPIs), which do additional work, such as provisioning load balancers outside of the cluster to route traffic within.</p>&#13;
&#13;
<p>In essence, there are a lot of conveniences infrastructure teams lose when leaving virtualization behind—things that Kubernetes <em>does not</em> inherently solve. However, there are several projects and integration points with bare metal that make this space evermore promising. Bare metal options are becoming available through major cloud providers, and bare metal–exclusive IaaS services like Packet (recently acquired by  <a href="https://metal.equinix.com">Equinix Metal</a>) are gaining market share. Success with bare metal is not without its challenges, including:</p>&#13;
<dl>&#13;
<dt>Significantly larger nodes</dt>&#13;
<dd>&#13;
<p>Larger nodes cause (typically) more Pods per node. When thousands of Pods per node are needed to make good use of your hardware, operations can become more complicated. For example, when doing in-place upgrades, needing to drain a node to upgrade it means you may trigger 1000+ rescheduling events.</p>&#13;
</dd>&#13;
<dt>Dynamic scaling</dt>&#13;
<dd>&#13;
<p>How to get new nodes up quickly based on workload or traffic needs.</p>&#13;
</dd>&#13;
<dt>Image provisioning</dt>&#13;
<dd>&#13;
<p>Quickly baking and distributing machine images to keep cluster nodes as immutable as possible.</p>&#13;
</dd>&#13;
<dt>Lack of load balancer API</dt>&#13;
<dd>&#13;
<p>Need to provide ingress routing from outside of the cluster to the Pod network within.</p>&#13;
</dd>&#13;
<dt>Less sophisticated storage integration</dt>&#13;
<dd>&#13;
<p>Solving for getting network-attached storage to Pods.</p>&#13;
</dd>&#13;
<dt>Multitenant security concerns</dt>&#13;
<dd>&#13;
<p>When hypervisors are in play, we have the luxury of ensuring security-sensitive containers run on dedicated hypervisors.<a data-primary="hypervisors" data-secondary="multitenancy security and" data-type="indexterm" id="idm45612000866408"/> Specifically we can slice up a physical server in any arbitrary way and make container scheduling decisions based on that.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These challenges are absolutely solvable. For example, the lack of load balancer integration can be solved with projects like <a href="https://kube-vip.io">kube-vip</a> or <a href="https://metallb.universe.tf">metallb</a>. Storage integration can be solved by integrating with a ceph cluster. However, the key is that containers aren’t a new-age virtualization technology. Under the hood, containers are (in most implementations) using Linux kernel primitives to make processes feel isolated from others on a host. There’s an endless number of trade-offs to continue unpacking, but in essence, our guidance when choosing between cloud providers (virtualization), on-prem virtualization, and bare metal is to consider what option makes the most sense based on your technical requirements and your organization’s operational experience. If Kubernetes is being considered a replacement for a virtualization stack, reconsider exactly what Kubernetes solves for. Remember that learning to operate Kubernetes and enabling teams to operate Kubernetes is already an undertaking. Adding the complexity of completely changing how you manage your infrastructure underneath it significantly grows your engineering effort and risk.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster Sizing" data-type="sect2"><div class="sect2" id="idm45612000904408">&#13;
<h2>Cluster Sizing</h2>&#13;
&#13;
<p>Integral to the design of your Kubernetes deployment model and the planning for infrastructure is the cluster sizes you plan to use.<a data-primary="clusters" data-secondary="sizing" data-type="indexterm" id="idm45612000860440"/><a data-primary="infrastructure" data-secondary="cluster sizing" data-type="indexterm" id="idm45612000859464"/><a data-primary="deployment models" data-secondary="infrastructure" data-tertiary="cluster sizing" data-type="indexterm" id="idm45612000858520"/> We’re often asked, “How many worker nodes should be in production clusters?”  This is a distinct question from, “How many worker nodes are needed to satisfy workloads?”  If you plan to use one, single production cluster to rule them all, the answer to both questions will be the same.  However, that is a unicorn we never see in the wild.  Just as a Kubernetes cluster allows you to treat server machines as cattle, modern Kubernetes installation methods and cloud providers allow you to treat the clusters themselves as cattle.  And every enterprise that uses Kubernetes has at least a small herd.</p>&#13;
&#13;
<p>Larger clusters<a data-primary="larger clusters, benefits of" data-type="indexterm" id="idm45612000856168"/> offer the following benefits:</p>&#13;
<dl>&#13;
<dt>Better resource utilization</dt>&#13;
<dd>&#13;
<p>Each cluster comes with a control plane overhead cost.  This includes etcd and components such as the API server.<a data-primary="API server" data-secondary="in larger clusters" data-secondary-sortas="larger" data-type="indexterm" id="idm45612000853544"/>  Additionally, you’ll run a variety of platform services in each cluster; for example, proxies via Ingress controllers. These components add overhead.  A larger cluster minimizes replication of these services.</p>&#13;
</dd>&#13;
<dt>Fewer cluster deployments</dt>&#13;
<dd>&#13;
<p>If you run your own bare metal compute infrastructure, as opposed to provisioning it on-demand from a cloud provider or on-prem virtualization platform, spinning clusters up and down as needed, scaling those clusters as demands dictate becomes less feasible.  Your cluster deployment strategy can afford to be less automated if you execute that deployment strategy less often.  It is entirely possible the engineering effort to fully automate cluster deployments would be greater than the engineering effort to manage a less automated strategy.</p>&#13;
</dd>&#13;
<dt>Simpler cluster and workload management profile</dt>&#13;
<dd>&#13;
<p>If you have fewer production clusters, the systems you use to allocate, federate, and manage these concerns need not be as streamlined and sophisticated.  Federated cluster and workload management across fleets of clusters is complex and challenging.  The community has been working on this.  Large teams at enormous enterprises have invested heavily in bespoke systems for this.  And these efforts have enjoyed limited success thus far.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Smaller clusters offer the following benefits:</p>&#13;
<dl>&#13;
<dt>Smaller blast radius</dt>&#13;
<dd>&#13;
<p>Cluster failures will impact fewer workloads.</p>&#13;
</dd>&#13;
<dt>Tenancy flexibility</dt>&#13;
<dd>&#13;
<p>Kubernetes provides all the <a data-primary="smaller clusters, benefits of" data-type="indexterm" id="idm45612000844488"/>mechanisms needed to build a multitenant platform.  However, in some cases you will spend far less engineering effort by provisioning a new cluster for a particular tenant.  For example, if one tenant needs access to a cluster-wide resource like Custom Resource Definitions, and another tenant needs stringent guarantees of isolation for security and/or compliance, it may be justified to dedicate clusters to such teams, especially if their workloads demand significant compute resources.</p>&#13;
</dd>&#13;
<dt>Less tuning for scale</dt>&#13;
<dd>&#13;
<p>As clusters scale into several hundred workers, we often encounter issues of scale that need to be solved for.<a data-primary="scaling" data-secondary="less tuning for scale with smaller clusters" data-type="indexterm" id="idm45612000841720"/>  These issues vary case to case, but bottlenecks in your control plane can occur. Engineering effort will need to be expended in &#13;
<span class="keep-together">troubleshooting</span> and tuning clusters.  Smaller clusters considerably reduce this &#13;
<span class="keep-together">expenditure</span>.</p>&#13;
</dd>&#13;
<dt>Upgrade options</dt>&#13;
<dd>&#13;
<p>Using smaller clusters lends itself more readily to replacing clusters in order to upgrade them.<a data-primary="upgrades" data-secondary="benefits of smaller clusters for" data-type="indexterm" id="idm45612000837480"/>  Cluster replacements certainly come with their own challenges, and these are covered later in this chapter in <a data-type="xref" href="#upgrades">“Upgrades”</a>, but this replacement strategy is an attractive upgrade option in many cases, and operating smaller clusters can make it even more attractive.</p>&#13;
</dd>&#13;
<dt>Node pool alternative</dt>&#13;
<dd>&#13;
<p>If you have workloads with specialized concerns such as GPUs or memory optimized nodes, and <a data-primary="node pools" data-secondary="smaller clusters as alternative to" data-type="indexterm" id="idm45612000833880"/>your systems readily accommodate lots of smaller clusters, it will be trivial to run dedicated clusters to accommodate these kinds of specialized concerns.  This alleviates the complexity of managing multiple node pools as discussed earlier in this chapter.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Compute Infrastructure" data-type="sect2"><div class="sect2" id="idm45612000847000">&#13;
<h2>Compute Infrastructure</h2>&#13;
&#13;
<p>To state the obvious, a Kubernetes cluster needs machines.<a data-primary="compute infrastructure" data-type="indexterm" id="idm45612000831064"/><a data-primary="infrastructure" data-secondary="compute" data-type="indexterm" id="idm45612000830360"/><a data-primary="deployment models" data-secondary="infrastructure" data-tertiary="compute infrastructure" data-type="indexterm" id="idm45612000829416"/>  Managing pools of these machines is the core purpose, after all.<a data-primary="machines" data-secondary="compute infrastructure" data-type="indexterm" id="idm45612000828072"/>  An early consideration is what types of machines you should choose.  How many cores?  How much memory?  How much onboard storage?  What grade of network interface?  Do you need any specialized devices such as GPUs?  These are all concerns that are driven by the demands of the software you plan to run.  Are the workloads compute intensive?  Or are they memory hungry?  Are you running machine learning or AI workloads that necessitate GPUs?  If your use case is very typical in that your workloads fit general-purpose machines’ compute-to-memory ratio well, and if your workloads don’t vary greatly in their resource consumption profile, this will be a relatively simple exercise.  However, if you have less typical and more diverse software to run, this will be a little more involved.  Let’s consider the different types of machines to consider for your cluster:</p>&#13;
<dl>&#13;
<dt>etcd machines (optional)</dt>&#13;
<dd>&#13;
<p>This is an optional machine type that is only applicable if you run a dedicated etcd clusters for your Kubernetes clusters.<a data-primary="etcd" data-secondary="etcd machines" data-type="indexterm" id="idm45612000824136"/>  We covered the trade-offs with this option in an earlier section.  These machines should prioritize disk read/write performance, so never use old spinning disk hard drives.<a data-primary="performance" data-secondary="disk read/write, prioritization by etcd machines" data-type="indexterm" id="idm45612000822840"/>  Also consider dedicating a storage disk to etcd, even if running etcd on dedicated machines, so that etcd suffers no contention with the OS or other programs for use of the disk.  Also consider network performance, including proximity on the network, to reduce network latency between machines in a given etcd cluster.</p>&#13;
</dd>&#13;
<dt>Control plane nodes (required)</dt>&#13;
<dd>&#13;
<p>These machines will be dedicated to running control plane components for the cluster.<a data-primary="control plane nodes" data-type="indexterm" id="idm45612000820152"/>  They should be general-purpose machines that are sized and numbered according to the anticipated size of the cluster as well as failure tolerance requirements.  In a larger cluster, the API server will have more clients and manage more traffic.  This can be accommodated with more compute resources per machine, or more machines.  <a data-primary="API server" data-secondary="and different types of machines in a cluster" data-type="indexterm" id="idm45612000818872"/>However, components like the scheduler and controller-manager have only one active leader at any given time.  Increasing capacity for these cannot be achieved with more replicas the way it can with the API server.  Scaling vertically with more compute resources per machine must be used if these components become starved for resources.  Additionally, if you are colocating etcd on these control plane machines, the same considerations for etcd machines noted above also apply.<a data-primary="worker nodes" data-type="indexterm" id="idm45612000817272"/></p>&#13;
</dd>&#13;
<dt>Worker Nodes (required)</dt>&#13;
<dd>&#13;
<p>These are general-purpose machines that host non–control plane workloads.</p>&#13;
</dd>&#13;
<dt>Memory optimized Nodes (optional)</dt>&#13;
<dd>&#13;
<p>If you have workloads that have a memory profile that doesn’t make them a good fit for <a data-primary="memory-optimized nodes" data-type="indexterm" id="idm45612000813832"/>general-purpose worker nodes, you should consider a node pool that is memory optimized.  For example, if you are using AWS general-purpose M5 instance types for worker nodes that have a CPU:memory ratio of 1CPU:4GiB, but you have a workload that consumes resources at a ratio of 1CPU:8GiB, these workloads will leave unused CPU when resources are requested (reserved) in your cluster at this ratio.  This inefficiency can be overcome by using memory-optimized nodes such as the R5 instance types on AWS, which have a ratio of 1CPU:8GiB.</p>&#13;
</dd>&#13;
<dt>Compute optimized Nodes (optional)</dt>&#13;
<dd>&#13;
<p>Alternatively, if <a data-primary="compute-optimized nodes" data-type="indexterm" id="idm45612000811144"/>you have workloads that fit the profile of a compute-optimized node such as the C5 instance type in AWS with 1CPU:2GiB, you should consider adding a node pool with these machine types for improved efficiency.</p>&#13;
</dd>&#13;
<dt>Specialized hardware Nodes (optional)</dt>&#13;
<dd>&#13;
<p>A common hardware ask is GPUs.<a data-primary="hardware nodes, specialized" data-type="indexterm" id="idm45612000808712"/>  If you have workloads (e.g., machine learning) requiring specialized hardware, adding a node pool in your cluster and then targeting those nodes for the appropriate workloads will work well.</p>&#13;
</dd>&#13;
</dl>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Networking Infrastructure" data-type="sect2"><div class="sect2" id="idm45612000807352">&#13;
<h2>Networking Infrastructure</h2>&#13;
&#13;
<p>Networking is easy to brush off as an implementation detail, but it can have important impacts on your deployment models.<a data-primary="deployment models" data-secondary="infrastructure" data-tertiary="networking" data-type="indexterm" id="idm45612000805704"/><a data-primary="infrastructure" data-secondary="networking" data-type="indexterm" id="idm45612000804456"/><a data-primary="networks" data-secondary="networking infrastructure" data-type="indexterm" id="idm45612000803512"/>  First, let’s examine the elements that you will need to consider and design for.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Routability" data-type="sect3"><div class="sect3" id="idm45612000802248">&#13;
<h3>Routability</h3>&#13;
&#13;
<p>You almost certainly do not want your cluster nodes exposed to the public internet.<a data-primary="routability" data-type="indexterm" id="idm45612000800952"/><a data-primary="networks" data-secondary="networking infrastructure" data-tertiary="routability" data-type="indexterm" id="idm45612000800152"/>  The convenience of being able to connect to those nodes from anywhere almost never justifies the threat exposure.  You will need to solve for gaining access to those nodes should you need to connect to them, but a bastion host or jump box that is well secured and that will allow SSH access, and in turn allow you to connect to cluster nodes, is a low barrier to hop.</p>&#13;
&#13;
<p>However, there are more nuanced questions to answer, such as network access on your private network.  There will be services on your network that will need connectivity to and from your cluster.  For example, it is common to need connectivity with storage arrays, internal container registries, CI/CD systems, internal DNS, private NTP servers, etc.  Your cluster will also usually need access to public resources such as public container registries, even if via an outbound proxy.</p>&#13;
&#13;
<p>If outbound public internet access is out of the question, those resources such as open source container images and system packages will need to be made available in some other way.  Lean toward simpler systems that are consistent and effective.  Lean away from, if possible, mindless mandates and human approval for infrastructure needed for cluster deployments.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Redundancy" data-type="sect3"><div class="sect3" id="idm45612000796472">&#13;
<h3>Redundancy</h3>&#13;
&#13;
<p>Use availability zones (AZs) to help maintain uptime where possible.  For clarity, an availability zone is a <a data-primary="redundancy in networking infrastructure" data-type="indexterm" id="idm45612000795016"/><a data-primary="availability zones (AZs)" data-type="indexterm" id="idm45612000794296"/><a data-primary="networks" data-secondary="networking infrastructure" data-tertiary="redundancy" data-type="indexterm" id="idm45612000793608"/>datacenter that has a distinct power source and backup as well as a distinct connection to the public internet.  Two subnets in a datacenter with a shared power supply do not constitute two availability zones.  However, two distinct datacenters that are in relatively close proximity to one another and have a low-latency, high-bandwidth network connection between them do constitute a pair of availability zones.  Two AZs is good.  Three is better.  More than that depends of the level of catastrophe you need to prepare for.  Datacenters have been known to go down.  For multiple datacenters in a region to suffer simultaneous outages is possible, but rare and would often indicate a kind of disaster that will require you to consider how critical your workloads are.  Are you running workloads necessary to national defense, or an online store?  Also consider where you need redundancy.  Are you building redundancy for your workloads?  Or the control plane of the cluster itself?  In our experience it is acceptable to run etcd across AZs but, if doing so, revisit <a data-type="xref" href="#network_considerations">“Network considerations”</a>.  Keep in mind that distributing your control plane across AZs gives redundant <em>control</em> over the cluster.  Unless your workloads depend on the cluster control plane (which should be avoided), your workload availability will not be affected by a control plane outage.  What will be affected is your ability to make any changes to your running software.  A control plane outage is not trivial.  It is a high-priority emergency.  But it is not the same as an outage for user-facing workloads.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Load balancing" data-type="sect3"><div class="sect3" id="idm45612000789112">&#13;
<h3>Load balancing</h3>&#13;
&#13;
<p>You will need a load balancer for the Kubernetes API servers.<a data-primary="API server" data-secondary="load balancing" data-type="indexterm" id="idm45612000787784"/><a data-primary="load balancing" data-type="indexterm" id="idm45612000786808"/><a data-primary="networks" data-secondary="networking infrastructure" data-tertiary="load balancing" data-type="indexterm" id="idm45612000786136"/>  Can you programmatically provision a load balancer in your environment?  If so, you will be able to spin up and configure it as a part of the deployment of your cluster’s control plane.  Think through the access policies to your cluster’s API and, subsequently, what firewalls your load balancer will sit behind.  You almost certainly will not make this accessible from the public internet.  Remote access to your cluster’s control plane is far more commonly done so via a VPN that provides access to the local network that your cluster resides on.  On the other hand, if you have workloads that are publicly exposed, you will need a separate and distinct load balancer that connects to your cluster’s ingress.  In most cases this load balancer will serve all incoming requests to the various workloads in your cluster.  There is little value in deploying a load balancer and cluster ingress for each workload that is exposed to requests from outside the cluster.  If running a dedicated etcd cluster, do not put a load balancer between the Kubernetes API and etcd.  The etcd client that the API uses will handle the connections to etcd without the need for a load balancer.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Automation Strategies" data-type="sect2"><div class="sect2" id="idm45612000783320">&#13;
<h2>Automation Strategies</h2>&#13;
&#13;
<p>In automating the infrastructure components for your Kubernetes clusters, you have some strategic decisions to make.<a data-primary="automation" data-secondary="infrastructure" data-type="indexterm" id="ix_autoinf"/><a data-primary="infrastructure" data-secondary="automation strategies for" data-type="indexterm" id="ix_infrauto"/>  We’ll break this into two categories, the first being the tools that exist today that you can leverage.  Then, we’ll talk about how Kubernetes operators can be used in this regard.  Recognizing that automation capabilities will look very different for bare metal installations, we will start from the assumption that you have an API with which to provision machines or include them in a pool for Kubernetes deployment.  If that is not the case, you will need to fill in the gaps with manual operations up to the point where you do have programmatic access and control.  Let’s start with some of the tools you may have at your disposal.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Infra management tools" data-type="sect3"><div class="sect3" id="idm45612000778616">&#13;
<h3>Infra management tools</h3>&#13;
&#13;
<p>Tools such as <a href="https://www.terraform.io">Terraform</a> and <a href="https://aws.amazon.com/cloudformation">CloudFormation for AWS</a> allow you to declare the desired state for your compute and networking infrastructure and then apply that state.<a data-primary="infrastructure" data-secondary="automation strategies for" data-tertiary="infra management tools" data-type="indexterm" id="idm45612000775480"/><a data-primary="Terraform" data-type="indexterm" id="idm45612000774216"/><a data-primary="CloudFormation for AWS" data-type="indexterm" id="idm45612000773544"/>  They use data formats or configuration languages that allow you to define the outcome you require in text files and then tell a piece of software to satisfy the desired state declared in those text files.</p>&#13;
&#13;
<p>They are advantageous in that they use tooling that engineers can readily adopt and get outcomes with.  They are good at simplifying the process of relatively complex infrastructure provisioning.  They excel when you have a prescribed set of infrastructure that needs to be stamped out repeatedly and there is not a lot of variance between instances of the infrastructure.  It greatly lends itself to the principle of immutable infrastructure because the repeatability is reliable, and infrastructure <em>replacement</em> as opposed to <em>mutation</em> becomes quite manageable.</p>&#13;
&#13;
<p>These tools begin to decline in value when the infrastructure requirements become significantly complex, dynamic, and dependent on variable conditions.  For example, if you are designing Kubernetes deployment systems across multiple cloud providers, these tools will become cumbersome.  Data formats like JSON and configuration languages are not good at expressing conditional statements and looping functions.  This is where general-purpose programming languages shine.</p>&#13;
&#13;
<p>In development stages, infra management tools are very commonly used successfully.  They are indeed used to manage production environments in certain shops, too.  But they become cumbersome to work with over time and often take on a kind of technical debt that is almost impossible to pay down.  For these reasons, strongly consider using or developing Kubernetes operators for this purpose.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kubernetes operators" data-type="sect3"><div class="sect3" id="idm45612000768920">&#13;
<h3>Kubernetes operators</h3>&#13;
&#13;
<p>If infra management tools impose limitations that warrant writing software using general-purpose programming languages, what form should that software take?<a data-primary="operators (Kubernetes)" data-type="indexterm" id="idm45612000767336"/><a data-primary="infrastructure" data-secondary="automation strategies for" data-tertiary="Kubernetes operators" data-type="indexterm" id="idm45612000766632"/>  You could write a web application to manage your Kubernetes infrastructure.  Or a command-line tool.  If considering custom software development for this purpose, strongly consider Kubernetes operators.</p>&#13;
&#13;
<p>In the context of Kubernetes, operators use custom resources and custom-built Kubernetes controllers to manage systems.  Controllers use a method of managing state that is powerful and reliable.  When you create an instance of a Kubernetes resource, the controller responsible for that resource kind is notified by the API server via its watch mechanism and then uses the declared desired state in the resource spec as instructions to fulfill the desired state.  So extending the Kubernetes API with new resource kinds that represent infrastructure concerns, and developing Kubernetes operators to manage the state of these infrastructure resources, is very powerful.  The topic of Kubernetes operators is covered in more depth in <a data-type="xref" href="ch11.html#building_platform_services">Chapter 11</a>.</p>&#13;
&#13;
<p>This is exactly what the Cluster API project is.<a data-primary="Cluster API" data-secondary="Kubernetes operators" data-type="indexterm" id="idm45612000762600"/>  It is a collection of Kubernetes operators that can be used to manage the infrastructure for Kubernetes clusters.  And you can certainly leverage that open source project for your purposes.  In fact, we would recommend you examine this project to see if it may fit your needs before starting a similar project from scratch.  And if it doesn’t fulfill your requirements, could your team get involved in contributing to that project to help develop the features and/or supported infrastructure providers that you require?</p>&#13;
&#13;
<p>Kubernetes offers excellent options for automating the management of containerized software deployments.  Similarly, it offers considerable benefits for cluster infrastructure automation strategies through the use of Kubernetes operators.  Strongly consider using and, where possible, contributing to projects such as Cluster API.  If you have custom requirements and prefer to use infrastructure management tools, you can certainly be successful with this option.  However, your solutions will have less flexibility and more workarounds due to the limitations of using configuration languages and formats rather than full-featured programming languages.<a data-primary="automation" data-secondary="infrastructure" data-startref="ix_autoinf" data-type="indexterm" id="idm45612000759896"/><a data-primary="infrastructure" data-secondary="automation strategies for" data-startref="ix_infrauto" data-type="indexterm" id="idm45612000758648"/><a data-primary="infrastructure" data-startref="ix_infr" data-type="indexterm" id="idm45612000757416"/><a data-primary="deployment models" data-secondary="infrastructure" data-startref="ix_dplyminf" data-type="indexterm" id="idm45612000756472"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Machine Installations" data-type="sect1"><div class="sect1" id="idm45612000915528">&#13;
<h1>Machine Installations</h1>&#13;
&#13;
<p>When the machines for your cluster are spun up, they will need an operating system, certain packages installed, and configurations written.<a data-primary="machines" data-secondary="installations" data-type="indexterm" id="ix_mchins"/><a data-primary="deployment models" data-secondary="machine installations" data-type="indexterm" id="ix_dplymmi"/>  You will also need some utility or program to determine environmental and other variable values, apply them, and coordinate the process of starting the Kubernetes containerized components.</p>&#13;
&#13;
<p>There are two broad strategies commonly used here:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Configuration management tools</p>&#13;
</li>&#13;
<li>&#13;
<p>Machine images</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Configuration Management" data-type="sect2"><div class="sect2" id="idm45612000747944">&#13;
<h2>Configuration Management</h2>&#13;
&#13;
<p>Configuration management tools<a data-primary="machines" data-secondary="installations" data-tertiary="using config managment tools" data-type="indexterm" id="idm45612000746568"/><a data-primary="configuration management tools" data-type="indexterm" id="idm45612000745256"/> such as Ansible, Chef, Puppet, and Salt gained popularity in a world where software was installed on virtual machines and run directly on the host.  These tools are quite magnificent for automating the configuration of multitudes of remote machines.  They follow varying models but, in general, administrators can declaratively prescribe how a machine must look and apply that prescription in an automated fashion.</p>&#13;
&#13;
<p>These config management tools are excellent in that they allow you to reliably establish machine consistency.  Each machine can get an effectively identical set of software and configurations installed.  And it is normally done with declarative recipes or playbooks that are checked into version control.  These all make them a positive solution.</p>&#13;
&#13;
<p>Where they fall short in a Kubernetes world is the speed and reliability with which you can bring cluster nodes online.  If the process you use to join a new worker node to a cluster includes a config management tool performing installations of packages that pull assets over network connections, you are adding significant time to the join process for that cluster node.  Furthermore, errors occur during configuration and installation.  Everything from temporarily unavailable package repositories to missing or incorrect variables can cause a config management process to fail.  This interrupts the cluster node join altogether.  And if you’re relying on that node to join an autoscaled cluster that is resource constrained, you may well invoke or prolong an availability problem.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Machine Images" data-type="sect2"><div class="sect2" id="idm45612000741784">&#13;
<h2>Machine Images</h2>&#13;
&#13;
<p>Using machine images is a superior alternative.<a data-primary="machines" data-secondary="installations" data-tertiary="machine images" data-type="indexterm" id="idm45612000740216"/>  If you use machine images with all required packages installed, the software is ready to run as soon as the machine boots up.  There is no package install that depends on the network and an available package repo.  Machine images improve the reliability of the node joining the cluster and considerably shorten the lead time for the node to be ready to accept traffic.</p>&#13;
&#13;
<p>The added beauty of this method is that you can often use the config management tools you are familiar with to build the machine images.  For example, using <a href="https://www.packer.io">Packer</a> from HashiCorp you can employ Ansible to build an Amazon Machine Image and have that prebuilt image ready to apply to your instances whenever they are needed.  An error running an Ansible playbook to build a machine image is not a big deal.  In contrast, having a playbook error occur that interrupts a worker node joining a cluster could induce a significant production incident.</p>&#13;
&#13;
<p>You can—and should—still keep the assets used for builds in version control, and all aspects of the installations can remain declarative and clear to anyone that inspects the repository.  Anytime upgrades or security patches need to occur, the assets can be updated, committed and, ideally, run automatically according to a pipeline once merged.</p>&#13;
&#13;
<p>Some decisions involve difficult trade-offs.  Some are dead obvious.  This is one of those.  Use prebuilt machine images.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What to Install" data-type="sect2"><div class="sect2" id="idm45612000735336">&#13;
<h2>What to Install</h2>&#13;
&#13;
<p>So what do you need to install on the machine?<a data-primary="machines" data-secondary="installations" data-tertiary="what to install" data-type="indexterm" id="idm45612000733608"/></p>&#13;
&#13;
<p>To start with the most obvious, you need an operating system.<a data-primary="Linux" data-secondary="distributions used with Kubernetes" data-type="indexterm" id="idm45612000731976"/>  A Linux distribution that Kubernetes is commonly used and tested with is the safe bet.  RHEL/CentOS or Ubuntu are easy choices.  If you have enterprise support for, or if you are passionate about, another distro and you’re willing to invest a little extra time in testing and development, that is fine, too.<a data-primary="Flatcar Container Linux" data-type="indexterm" id="idm45612000730584"/>  Extra points if you opt for a distribution designed for containers such as <a href="https://www.flatcar-linux.org">Flatcar Container Linux</a>.</p>&#13;
&#13;
<p>To continue in order of obviousness, you will need a container runtime such as docker or containerd.<a data-primary="container runtimes" data-type="indexterm" id="idm45612000728552"/>  When running containers, one must have a container runtime.</p>&#13;
&#13;
<p>Next is the kubelet.<a data-primary="kubelet" data-type="indexterm" id="idm45612000727336"/>  This is the interface between Kubernetes and the containers it orchestrates.<a data-primary="containers" data-secondary="runtime" data-type="indexterm" id="idm45612000726472"/>  This is the component that is installed on the machine that coordinates the containers.  Kubernetes is a containerized world.  Modern conventions follow that Kubernetes itself runs in containers.  With that said, the kubelet is one of the components that runs as a regular binary or process on the host. There have been attempts to run the kubelet as a container, but that just complicates things. Don’t do that. Install the kubelet on the host and run the rest of Kubernetes in containers.  The mental model is clear and the practicalities hold true.</p>&#13;
&#13;
<p>So far we have a Linux OS, a container runtime to run containers, and an interface between Kubernetes and the container runtime.  Now we need something that can bootstrap the Kubernetes control plane.<a data-primary="bootstrapping Kubernetes control plane" data-type="indexterm" id="idm45612000724216"/>  The kubelet can get containers running, but without a control plane it doesn’t yet know what Kubernetes Pods to spin up.<a data-primary="kubeadm" data-type="indexterm" id="idm45612000723224"/>  This is where kubeadm and static Pods come in.<a data-primary="Pods" data-secondary="kubeadm and static Pods" data-type="indexterm" id="idm45612000722360"/></p>&#13;
&#13;
<p>Kubeadm is far from the only tool that can perform this bootstrapping.  But it has gained wide adoption in the community and is used successfully in many enterprise production systems.  It is a command-line program that will, in part, stamp out the static Pod manifests needed to get Kubernetes up and running.  The kubelet can be configured to watch a directory on the host and run Pods for any Pod manifest it finds there.  Kubeadm will configure the kubelet appropriately and deposit the manifests as needed.  This will bootstrap the core, essential Kubernetes control plane components, notably etcd, kube-apiserver, kube-scheduler, and kube-controller-manager.</p>&#13;
&#13;
<p>Thereafter, the kubelet will get all further instructions to create Pods from manifests submitted to the Kubernetes API.  Additionally, kubeadm will generate bootstrap tokens you can use to securely join other nodes to your shiny new cluster.</p>&#13;
&#13;
<p>Lastly, you will need some kind of <em>bootstrap utility</em>.<a data-primary="bootstrap utilities" data-type="indexterm" id="idm45612000718616"/>  The Cluster API project uses Kubernetes custom resources and controllers for this.  But a command-line program installed on the host also works well.  The primary function of this utility is to call kubeadm and manage runtime configurations.  When the machine boots, arguments provided to the utility allow it to configure the bootstrapping of Kubernetes.  For example, in AWS you can leverage user data to run your bootstrap utility and pass arguments to it that will inform which flags should be added to the kubeadm command or what to include in a kubeadm config file.  Minimally, it will include a runtime config that tells the bootstrap utility whether to create a new cluster with <code>kubeadm init</code> or join the machine to an existing cluster with <code>kubeadm join</code>.  It should also include a secure location to store the bootstrap token if initializing, or to retrieve the bootstrap token if joining.  These tokens ensure only approved machines are attached to your cluster, so treat them with care.  To gain a clear idea of what runtime configs you will need to provide to your bootstrap utility, run through a manual install of Kubernetes using kubeadm, which is well documented in the official docs.  As you run through those steps it will become apparent what will be needed to meet your requirements in your environment. <a data-type="xref" href="#bootstrapping_a_machine_to_initialize_kubernetes">Figure 2-5</a> illustrates the steps involved in bringing up a new machine to create the first control plane node in a new Kubernetes cluster.</p>&#13;
&#13;
<figure><div class="figure" id="bootstrapping_a_machine_to_initialize_kubernetes">&#13;
<img alt="prku 0205" src="assets/prku_0205.png"/>&#13;
<h6><span class="label">Figure 2-5. </span>Bootstrapping a machine to initialize Kubernetes.</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now that we’ve covered what to install on the machines that are used as part of a Kubernetes cluster, let’s move on to the software that runs in containers to form the control plane for Kubernetes.<a data-primary="deployment models" data-secondary="machine installations" data-startref="ix_dplymmi" data-type="indexterm" id="idm45612000712056"/><a data-primary="machines" data-secondary="installations" data-startref="ix_mchins" data-type="indexterm" id="idm45612000710808"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Containerized Components" data-type="sect1"><div class="sect1" id="idm45612000734712">&#13;
<h1>Containerized Components</h1>&#13;
&#13;
<p>The static manifests used to spin up a cluster should include those essential components of the control plane: etcd, kube-apiserver, kube-scheduler, and kube-controller-manager.<a data-primary="deployment models" data-secondary="containerized components" data-type="indexterm" id="idm45612000707896"/><a data-primary="kube-controller-manager" data-type="indexterm" id="idm45612000706856"/><a data-primary="kube-scheduler" data-type="indexterm" id="idm45612000706184"/><a data-primary="kube-apiserver" data-type="indexterm" id="idm45612000705512"/><a data-primary="etcd" data-type="indexterm" id="idm45612000704840"/><a data-primary="control plane" data-secondary="components of" data-type="indexterm" id="idm45612000704168"/>  You can provide additional custom Pod manifests as needed, but strictly limit them to Pods that absolutely need to run before the Kubernetes API is available or registered into a federated system.  If a workload can be installed by way of the API server later on, do so.  Any Pods created with static manifests can be managed only by editing those static manifests on the machine’s disk, which is much less accessible and prone to automation.</p>&#13;
&#13;
<p>If using kubeadm, which is strongly <a data-primary="kubeadm" data-secondary="creating static manifests for control plane components" data-type="indexterm" id="idm45612000702248"/>recommended, the static manifests for your control plane, including etcd, will be created when a control plane node is initialized with <code>kubeadm init</code>.  Any flag specifications you need for these components can be passed to kubeadm using the kubeadm config file.  The bootstrap utility that we discussed in the previous section that calls kubeadm can write a templated kubeadm config file, for example.</p>&#13;
&#13;
<p>Avoid customizing the static Pod manifests directly with your bootstrap utility.  If really necessary, you can perform separate static manifest creation and cluster initialization steps with kubeadm that will give you the opportunity to inject customization if needed, but only do so if it’s important and cannot be achieved via the kubeadm config.  A simpler, less complicated bootstrapping of the Kubernetes control plane will be more robust, faster, and will be far less likely to break with Kubernetes version upgrades.</p>&#13;
&#13;
<p>Kubeadm will also generate self-signed TLS assets that are needed to securely connect components of your control plane.  Again, avoid tinkering with this.  If you have security requirements that demand using your corporate CA as a source of trust, then you can do so.  If this is a requirement, it’s important to be able to automate the acquisition of the intermediate authority.  And keep in mind that if your cluster bootstrapping systems are secure, the trust of the self-signed CA used by the control plane will be secure and will be valid only for the control plane of a single cluster.</p>&#13;
&#13;
<p>Now that we’ve covered the nuts and bolts of installing Kubernetes, let’s dive into the immediate concerns that come up once you have a running cluster.  We’ll begin with approaches for getting the essential add-ons installed onto Kubernetes.  These add-ons constitute the components you need to have in addition to Kubernetes to deliver a production-ready application platform.  Then we’ll get into the concerns and strategies for carrying out upgrades to your platform.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Add-ons" data-type="sect1"><div class="sect1" id="addons">&#13;
<h1>Add-ons</h1>&#13;
&#13;
<p>Cluster add-ons broadly cover those additions of platform services layered onto a Kubernetes cluster.<a data-primary="add-ons (cluster)" data-type="indexterm" id="ix_addcl"/><a data-primary="deployment models" data-secondary="add-ons" data-type="indexterm" id="ix_dplymadd"/>  We will not cover <em>what</em> to install as a cluster add-on in this section.  That is essentially the topic of the rest of the chapters in this book.  Rather, this is a look at <em>how</em> to go about installing the components that will turn your raw Kubernetes cluster into a production-ready, developer-friendly platform.</p>&#13;
&#13;
<p>The add-ons that you add to a cluster should be considered as a part of the deployment model.  Add-on installation will usually constitute the final phase of a cluster deployment.  These add-ons should be managed and versioned in combination with the Kubernetes cluster itself.  It is useful to consider Kubernetes and the add-ons that comprise the platform as a package that is tested and released together since there will inevitably be version and configuration dependencies between certain platform &#13;
<span class="keep-together">components</span>.</p>&#13;
&#13;
<p>Kubeadm installs “required” add-ons that<a data-primary="kubeadm" data-secondary="installation of cluster add-ons" data-type="indexterm" id="idm45612000689624"/> are necessary to pass the Kubernetes project’s conformance tests, including cluster DNS and kube-proxy, which implements Kubernetes Service resources.  However, there are many more, similarly critical components that will need to be applied after kubeadm has finished its work.  The most glaring example is a container network interface plug-in.<a data-primary="networks" data-secondary="container network interface plug-in" data-type="indexterm" id="idm45612000688136"/>  Your cluster will not be good for much without a Pod network.  Suffice to say you will end up with a significant list of components that need to be added to your cluster, usually in the form of DaemonSets, Deployments, or StatefulSets that will add functionality to the platform you’re building on Kubernetes.</p>&#13;
&#13;
<p>Earlier, in <a data-type="xref" href="#arch_and_topology">“Architecture and Topology”</a>, we discussed cluster federation and the registration of new clusters into that system.  That is usually a precursor to add-on installation because the systems and definitions for installation often live in a management cluster.</p>&#13;
&#13;
<p>Whatever the architecture used, once registration is achieved, the installation of cluster add-ons can be triggered.  <a data-primary="triggering mechanisms for automated installations and management" data-type="indexterm" id="idm45612000684712"/><a data-primary="API server" data-secondary="installation of cluster add-ons" data-type="indexterm" id="idm45612000684040"/>This installation process will be a series of calls to the cluster’s API server to create the Kubernetes resources needed for each component.  Those calls can come from a system outside the cluster or inside.</p>&#13;
&#13;
<p>One approach to installing add-ons is to use a continuous delivery pipeline using existing tools such as Jenkins.<a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="using pipeline to install cluster add-ons" data-type="indexterm" id="idm45612000682216"/>  The “continuous” part is irrelevant in this context since the trigger is not a software update but rather a new target for installation.  The “continuous” part of CI and CD usually refers to automated rollouts of software once new changes have been merged into a branch of version-controlled source code.  Triggering installations of cluster add-on software into a newly deployed cluster is an entirely different mechanism but is useful in that the pipeline generally contains the capabilities needed for the installations.  All that is needed to implement is the call to run a pipeline in response to the creation of a new cluster along with any variables to perform proper installation.</p>&#13;
&#13;
<p>Another approach that is more native to Kubernetes is to use a Kubernetes operator for the task.<a data-primary="operators (Kubernetes)" data-secondary="using to define cluster add-ons" data-type="indexterm" id="idm45612000679832"/>  This more advanced approach involves extending the Kubernetes API with one or more custom resources that allow you to define the add-on components for the cluster and their versions.  It also involves writing the controller logic that can execute the proper installation of the add-on components given the defined spec in the custom resource.</p>&#13;
&#13;
<p>This approach is useful in that it provides a central, clear source of truth for what the add-ons are for a cluster.  But more importantly, it offers the opportunity to programmatically manage the ongoing life cycle of these add-ons.  The drawback is the complexity of developing and maintaining more complex software. If you take on this complexity, it should be because you will implement those day-2 upgrades and ongoing management that will greatly reduce future human toil.  If you stop at day-1 installation and do not develop the logic and functionality to manage upgrades, you will be taking on a significant software engineering cost for little ongoing benefit.  Kubernetes operators offer the most value in ongoing operational management with their watch functionality of the custom resources that represent desired state.</p>&#13;
&#13;
<p>To be clear, the add-on operator concept isn’t necessarily entirely independent from external systems such as a CI/CD.<a data-primary="CI/CD (continuous integration/continuous delivery)" data-secondary="add-on operators and" data-type="indexterm" id="idm45612000676552"/>  In reality, they are far more likely to be used in conjunction.  For example, you may use a CD pipeline to install the operator and add-on custom resources and then let the operator take over.  Also, the operator will likely need to fetch manifests for installation, perhaps from a code repository that contains templated Kubernetes manifests for the add-ons.</p>&#13;
&#13;
<p>Using an operator in this manner reduces external dependencies, which drives improved reliability.  However, external dependencies cannot be eliminated altogether.  Using an operator to solve add-ons should be undertaken only when you have engineers that know the Kubernetes operator pattern well and have experience leveraging it.  Otherwise, stick with tools and systems that your team knows well while you advance the knowledge and experience of your team in this domain.</p>&#13;
&#13;
<p>That brings us to the conclusion of the “day 1” concerns: the systems to install a Kubernetes cluster and its add-ons.  Now we will turn to the “day 2” concern of upgrades.<a data-primary="add-ons (cluster)" data-startref="ix_addcl" data-type="indexterm" id="idm45612000673544"/><a data-primary="deployment models" data-secondary="add-ons" data-startref="ix_dplymadd" data-type="indexterm" id="idm45612000672568"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Upgrades" data-type="sect1"><div class="sect1" id="upgrades">&#13;
<h1>Upgrades</h1>&#13;
&#13;
<p>Cluster life cycle management is closely related to cluster deployment.<a data-primary="deployment models" data-secondary="upgrades" data-type="indexterm" id="ix_dplymup"/><a data-primary="upgrades" data-type="indexterm" id="ix_upgr"/>  A cluster deployment system doesn’t necessarily need to account for future upgrades; however, there are enough overlapping concerns to make it advisable.  At the very least, your upgrade strategy needs to be solved before going to production.  Being able to deploy the platform without the ability to upgrade and maintain it is hazardous at best.  When you see production workloads running on versions of Kubernetes that are way behind the latest release, you are looking at the outcome of developing a cluster deployment system that has been deployed to production before upgrade capabilities were added to the system.  When you first go to production with revenue-producing workloads running, considerable engineering budget will be spent attending to features you find missing or to sharp edges you find your team cutting themselves on.  As time goes by, those features will be added and the sharp edges removed, but the point is they will naturally take priority while the upgrade strategy sits in the backlog getting stale.  Budget early for those day-2 concerns.  Your future self will thank you.</p>&#13;
&#13;
<p>In addressing this subject of upgrades we will first look at versioning your platform to help ensure dependencies are well understood for the platform itself and for the workloads that will use it.  We will also address how to approach planning for rollbacks in the event something goes wrong and the testing to verify that everything has gone according to plan.  Finally, we will compare and contrast specific strategies for upgrading Kubernetes.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Platform Versioning" data-type="sect2"><div class="sect2" id="idm45612000665128">&#13;
<h2>Platform Versioning</h2>&#13;
&#13;
<p>First of all, version your platform and document the versions of all software used in that platform.<a data-primary="upgrades" data-secondary="platform versioning" data-type="indexterm" id="idm45612000663688"/><a data-primary="versioning (platform)" data-type="indexterm" id="idm45612000662712"/>  That includes the operating system version of the machines and all packages installed on them, such as the container runtime.  It obviously includes the version of Kubernetes in use.  And it should also include the version of each add-on that is added to make up your application platform.  It is somewhat common for teams to adopt the Kubernetes version for their platform so that everyone knows that version 1.18 of the application platform uses Kubernetes version 1.18 without any mental overhead or lookup.  This is of trivial importance compared to the fact of just doing the versioning and documenting it.  Use whatever system your team prefers.  But have the system, document the system, and use it religiously.  My only objection to pinning your platform version to any component of that system is that it may occasionally induce confusion.  For example, if you need to update your container runtime’s version due to a security vulnerability, you should reflect that in the version of your platform.  If using semantic versioning conventions, that would probably look like a change to the bugfix version number.  That may be confused with a version change in Kubernetes itself, i.e., v1.18.5 → 1.18.6.  Consider giving your platform its own independent version numbers, especially if using semantic versioning that follows the major/minor/bugfix convention.  It’s almost universal that software has its own independent version with dependencies on other software and their versions.  If your platform follows those same conventions, the meaning will be immediately clear to all engineers.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Plan to Fail" data-type="sect2"><div class="sect2" id="idm45612000660040">&#13;
<h2>Plan to Fail</h2>&#13;
&#13;
<p>Start from the premise that something will go wrong during the upgrade process.  Imagine yourself<a data-primary="upgrades" data-secondary="planning for failures" data-type="indexterm" id="idm45612000658632"/> in the situation of having to recover from a catastrophic failure, and use that fear and anguish as motivation to prepare thoroughly for that outcome.  Build automation to take and restore backups for your Kubernetes resources—both with direct etcd snapshots as well as Velero backups taken through the API.  Do the same for the persistent data used by your applications.  And address disaster recovery for your critical applications and their dependencies directly.  For complex, stateful, distributed applications it will likely not be enough to merely restore the application’s state and Kubernetes resources without regard to order and dependencies.  Brainstorm all the possible failure modes, and develop automated recovery systems to remedy and then test them.  For the most critical workloads and their dependencies, consider having standby clusters ready to fail over to—and then automate and test those fail-overs where possible.</p>&#13;
&#13;
<p>Consider your rollback paths carefully.  If an upgrade induces errors or outages that you cannot immediately diagnose, having rollback options is good insurance.  Complex distributed systems can take time to troubleshoot, and that time can be extended by the stress and distraction of production outages.  Predetermined playbooks and automation to fall back on are more important than ever when dealing with complex Kubernetes-based platforms.  But be practical and realistic.  In the real world, rollbacks are not always a good option.  For example, if you’re far enough along in an upgrade process, rolling all earlier changes back may be a terrible idea.  Think that through ahead of time, know where your points of no return are, and strategize before you execute those operations live.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Integration Testing" data-type="sect2"><div class="sect2" id="idm45612000655112">&#13;
<h2>Integration Testing</h2>&#13;
&#13;
<p>Having a well-documented versioning system that includes all component versions is one thing, but how you manage these versions is another.<a data-primary="upgrades" data-secondary="integration testing" data-type="indexterm" id="idm45612000653624"/><a data-primary="testing" data-secondary="integration testing of platform upgrades" data-type="indexterm" id="idm45612000652648"/><a data-primary="integration testing" data-secondary="of platform upgrades" data-type="indexterm" id="idm45612000651736"/>  In systems as complex as Kubernetes-based platforms, it is a considerable challenge to ensure everything integrates and works together as expected every time.  Not only is compatibility between all components of the platform critical, but compatibility between the workloads that run on the platform and the platform itself must also be tested and confirmed.  Lean toward platform agnosticism for your applications to reduce possible problems with platform compatibility, but there are many instances when application workloads yield tremendous value when leveraging platform features.</p>&#13;
&#13;
<p>Unit testing for all platform components is important, along with all other sound software engineering practices.  But integration testing is equally vital, even if considerably more challenging.  An excellent tool to aid in this effort is the Sonobuoy conformance test utility.  It is most commonly used to run the upstream Kubernetes end-to-end tests to ensure you have a properly running cluster; i.e., all the cluster’s components are working together as expected.  Often teams will run a Sonobuoy scan after a new cluster is provisioned to automate what would normally be a manual process of examining control plane Pods and deploying test workloads to ensure the cluster is properly operational.  However, I suggest taking this a couple of steps further.  Develop your own plug-ins that test the specific functionality and features of your platform.  Test the operations that are critical to your organization’s requirements.  And run these scans routinely.  Use a Kubernetes CronJob to run at least a subset of plug-ins, if not the full suite of tests.  This is not exactly available out of the box today but can be achieved with a little engineering and is well worth the effort: expose the scan results as metrics that can be displayed in dashboards and alerted upon.  These conformance scans can essentially test that the various parts of a distributed system are working together to produce the functionality and features you expect to be there and constitute a very capable automated integration testing approach.</p>&#13;
&#13;
<p>Again, integration testing must be extended to the applications that run on the platform.  Different integration testing strategies will be employed by different app dev teams, and this may be largely out of the platform team’s hands, but advocate strongly for it.  Run the integrations tests on a cluster that closely resembles the production environment, but more on that shortly.  This will be more critical for workloads that leverage platform features.  Kubernetes operators are a compelling example of this.  These extend the Kubernetes API and are naturally deeply integrated with the platform.  And if you’re using an operator to deploy and manage the life cycle for any of your organization’s software systems, it is imperative that you perform integration tests across versions of your platform, especially when Kubernetes version upgrades are involved.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Strategies" data-type="sect2"><div class="sect2" id="idm45612000646568">&#13;
<h2>Strategies</h2>&#13;
&#13;
<p>We’re going to look at three <a data-primary="deployment models" data-secondary="upgrades" data-tertiary="strategies for" data-type="indexterm" id="idm45612000645000"/><a data-primary="upgrades" data-secondary="strategies for" data-type="indexterm" id="idm45612000643752"/>strategies for upgrading your Kubernetes-based &#13;
<span class="keep-together">platforms</span>:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Cluster replacement</p>&#13;
</li>&#13;
<li>&#13;
<p>Node replacement</p>&#13;
</li>&#13;
<li>&#13;
<p>In-place upgrades</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We’re going to address them in order of highest cost with lowest risk to lowest cost with highest risk.  As with most things, there is a trade-off that eliminates the opportunity for a one-size-fits-all, universally ideal solution.  The costs and benefits need to be considered to find the right solution for your requirements, budget, and risk tolerance.  Furthermore, within each strategy, there are degrees of automation and testing that, again, will depend on factors such as engineering budget, risk tolerance, and upgrade frequency.</p>&#13;
&#13;
<p>Keep in mind, these strategies are not mutually exclusive.  You can use combinations.  For example, you could perform in-place upgrades for a dedicated etcd cluster and then use node replacements for the rest of the Kubernetes cluster.  You <em>can</em> also use different strategies in different tiers where the risk tolerances are different.  However, it is advisable to use the same strategy everywhere so that the methods you employ in production have first been thoroughly tested in development and staging.</p>&#13;
&#13;
<p>Whichever strategy you employ, a few principles remain constant: test thoroughly and automate as much as is practical.  If you build automation to perform actions and test that automation thoroughly in testing, development, and staging clusters, your production upgrades will be far less likely to produce issues for end users and far less likely to invoke stress in your platform operations team.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cluster replacement" data-type="sect3"><div class="sect3" id="idm45612000635464">&#13;
<h3>Cluster replacement</h3>&#13;
&#13;
<p>Cluster replacement is the highest cost, lowest risk solution.<a data-primary="deployment models" data-secondary="upgrades" data-tertiary="cluster replacement" data-type="indexterm" id="idm45612000633896"/><a data-primary="upgrades" data-secondary="strategies for" data-tertiary="cluster replacement" data-type="indexterm" id="idm45612000632648"/><a data-primary="clusters" data-secondary="replacement of" data-type="indexterm" id="idm45612000631432"/>  It is low risk in that it follows immutable infrastructure principles applied to the entire cluster.  An upgrade is performed by deploying an entirely new cluster alongside the old.  Workloads are migrated from the old cluster to the new.  The new, upgraded cluster is scaled out as needed as workloads are migrated on.  The old cluster’s worker nodes are scaled in as workloads are moved off.  But throughout the upgrade process there is an addition of an entirely distinct new cluster and the costs associated with it.  The scaling out of the new and scaling in of the old mitigates this cost, which is to say that if you are upgrading a 300-node production cluster, you do not need to provision a new cluster with 300 nodes at the outset.  You would provision a cluster with, say, 20 nodes.  And when the first few workloads have been migrated, you can scale in the old cluster that has reduced usage and scale out the new to accommodate other incoming workloads.  The use of cluster autoscaling and cluster overprovisioning can make this quite seamless, but upgrades alone are unlikely to be a sound justification for using those technologies.  There are two common challenges when tackling a cluster replacement.</p>&#13;
&#13;
<p>The first is managing ingress traffic.  As workloads are migrated from one cluster to the next, traffic will need to be rerouted to the new, upgraded cluster.  This implies that DNS for the publicly exposed workloads does not resolve to the cluster ingress, but rather to a global service load balancer (GSLB) or reverse proxy that, in turn, routes traffic to the cluster ingress.  This gives you a point from which to manage traffic routing into multiple clusters.</p>&#13;
&#13;
<p>The other is persistent storage availability.<a data-primary="storage" data-secondary="persistent storage availability in cluster replacement" data-type="indexterm" id="idm45612000627848"/>  If using a storage service or appliance, the same storage needs to be accessible from both clusters.  If using a managed service such as a database service from a public cloud provider, you must ensure the same service is available from both clusters.  In a private datacenter this could be a networking and firewalling question.  In the public cloud it will be a question of networking and availability zones; for example, AWS EBS volumes are available from specific availability zones.  And managed services in AWS often have specific Virtual Private Clouds (VPCs) associated.<a data-primary="Virtual Private Clouds (VPCs)" data-type="indexterm" id="idm45612000626120"/>  You may consider using a single VPC for multiple clusters for this reason.  Oftentimes Kubernetes installers assume a VPC per cluster, but this isn’t always the best model.</p>&#13;
&#13;
<p>Next, you will concern yourself with workload migrations.<a data-primary="workloads" data-secondary="migrating between clusters" data-type="indexterm" id="idm45612000624680"/>  Primarily, we’re talking about the Kubernetes resources themselves—the Deployments, Services, ConfigMaps, etc.  You can do this workload migration in one of two ways:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Redeploy from a declared source of truth</p>&#13;
</li>&#13;
<li>&#13;
<p>Copy the existing resources over from the old cluster</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>The first option would likely involve pointing your deployment pipeline at the new cluster and having it redeploy the same resource to the new cluster.  This assumes the source of truth for your resource definitions that you have in version control is reliable and that no in-place changes have taken place.  In reality, this is quite uncommon.  Usually, humans, controllers, and other systems have made in-place changes and adjustments.  If this is the case, you will need go with option 2 and make a copy of the existing resources and deploy them to the new cluster.  This is where a tool like Velero can be extremely valuable.  Velero is more commonly touted as a backup tool, but its value as a migration tool is as high or possibly even higher.  Velero can take a snapshot of all resources in your cluster, or a subset.  So if you migrate workloads one Namespace at a time, you can take snapshots of each Namespace at the time of migration and restore those snapshots into the new cluster in a highly reliable manner.  It takes these snapshots not directly from the etcd data store, but rather through the Kubernetes API, so as long as you can provide access to Velero to the API server for both clusters, this method can be very useful. <a data-type="xref" href="#migrating_workloads_between_clusters_with_a_backup">Figure 2-6</a> illustrates this approach.</p>&#13;
&#13;
<figure><div class="figure" id="migrating_workloads_between_clusters_with_a_backup">&#13;
<img alt="prku 0206" src="assets/prku_0206.png"/>&#13;
<h6><span class="label">Figure 2-6. </span>Migrating workloads between clusters with a backup and restore using Velero.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Node replacement" data-type="sect3"><div class="sect3" id="idm45612000616456">&#13;
<h3>Node replacement</h3>&#13;
&#13;
<p>The node replacement option represents a middle ground for cost and risk.  It is a common approach and is supported by Cluster API.<a data-primary="deployment models" data-secondary="upgrades" data-tertiary="node replacement" data-type="indexterm" id="idm45612000614744"/><a data-primary="upgrades" data-secondary="strategies for" data-tertiary="node replacement" data-type="indexterm" id="idm45612000613496"/><a data-primary="nodes" data-secondary="replacement of" data-type="indexterm" id="idm45612000612280"/>  It is a palatable option if you’re managing larger clusters and compatibility concerns are well understood. Those compatibility concerns represent one of the biggest risks for this method because you are upgrading the control plane in-place as far as your cluster services and workloads are concerned.  If you upgrade Kubernetes in-place and an API version that one of your workloads is using is no longer present, your workload could suffer an outage.  There are several ways to mitigate this:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Read the Kubernetes release notes. Before rolling out a new version of your platform that includes a Kubernetes version upgrade, read the CHANGELOG thoroughly.  Any API deprecations or removals are well documented there, so you will have plenty of advance notice.</p>&#13;
</li>&#13;
<li>&#13;
<p>Test thoroughly before production. Run new versions of your platform extensively in development and staging clusters before rolling out to production.  Get the latest version of Kubernetes running in dev shortly after it is released and you will be able to thoroughly test and still have recent releases of Kubernetes running in production.</p>&#13;
</li>&#13;
<li>&#13;
<p>Avoid tight coupling with the API. This doesn’t apply to platform services that run in your cluster.  Those, by their nature, need to integrate closely with Kubernetes.  But keep your end user, production workloads as platform-agnostic as possible.  Don’t have the Kubernetes API as a dependency.  For example, your application should know nothing of Kubernetes Secrets.  It should simply consume an environment variable or read a file that is exposed to it.  That way, as long as the manifests used to deploy your app are upgraded, the application workload itself will continue to run happily, regardless of API changes.  If you find that you want to leverage Kubernetes features in your workloads, consider using a Kubernetes operator.  An operator outage should not affect the availability of your application.  An operator outage will be an urgent problem to fix, but it will not be one your customers or end users should see, which is a world of &#13;
<span class="keep-together">difference</span>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The node replacement option can be very beneficial when you build machine images ahead of time that are well tested and verified.  Then you can bring up new machines and readily join them to the cluster.  The process will be rapid because all updated software, including operating system and packages, are already installed and the processes to deploy those new machines can use much the same process as original deployment.</p>&#13;
&#13;
<p>When replacing nodes for your cluster, start with the control plane.  If you’re running a dedicated etcd cluster, start there.<a data-primary="control plane nodes" data-secondary="replacing" data-type="indexterm" id="idm45612000603752"/>  The persistent data for your cluster is critical and must be treated carefully.  If you encounter a problem upgrading your first etcd node, if you are properly prepared, it will be relatively trivial to abort the upgrade.  If you upgrade all your worker nodes and the Kubernetes control plane, then find yourself with issues upgrading etcd, you are in a situation where rolling back the entire upgrade is not practical—you need to remedy the live problem as a priority.  You have lost the opportunity to abort the entire process, regroup, retest, and resume later.  You need to solve that problem or at the very least diligently ensure that you can leave the existing versions as-is safely for a time.</p>&#13;
&#13;
<p>For a<a data-primary="etcd" data-secondary="node replacements in dedicated cluster" data-type="indexterm" id="idm45612000601544"/> dedicated etcd cluster, consider replacing nodes subtractively; i.e., remove a node and then add in the upgraded replacement, as opposed to first adding a node to the cluster and then removing the old.  This method gives you the opportunity to leave the member list for each etcd node unchanged.  Adding a fourth member to a three-node etcd cluster, for example, will require an update to all etcd nodes’ member list, which will require a restart.  It will be far less disruptive to drop a member and replace it with a new one that has the same IP address as the old, if possible.  The etcd documentation on upgrades is excellent and may lead you to consider doing in-place upgrades for etcd.  This will necessitate in-place upgrades to OS and packages on the machine as applicable, but this will often be quite palatable and perfectly safe.</p>&#13;
&#13;
<p>For the control plane nodes, they can be replaced additively.<a data-primary="kubeadm" data-secondary="node replacements with" data-type="indexterm" id="idm45612000599144"/>  Using <code>kubeadm join</code> with the <code>--control-plane</code> flag on new machines that have the upgraded Kubernetes binaries—kubeadm, kubectl, kubelet—installed.  As each of the control plane nodes comes online and is confirmed operational, one old-versioned node can be drained and then deleted.  If you are running etcd colocated on the control plane nodes, include etcd checks when confirming operationality and etcdctl to manage the members of the cluster as needed.</p>&#13;
&#13;
<p>Then you can proceed to replace the worker nodes.<a data-primary="worker nodes" data-secondary="replacing" data-type="indexterm" id="idm45612000596248"/>  These can be done additively or subtractively—one at a time or several at a time.  A primary concern here is cluster utilization.  If your cluster is highly utilized, you will want to add new worker nodes before draining and removing existing nodes to ensure you have sufficient compute resources for the displaced Pods.  Again, a good pattern is to use machine images that have all the updated software installed that are brought online and use <code>kubeadm join</code> to be added to the cluster.  And, again, this could be implemented using many of the same mechanisms as used in cluster deployment. <a data-type="xref" href="#performing_upgrades_by_replacing_nodes_in_a_cluster">Figure 2-7</a> illustrates this operation of replacing control plane nodes one at a time and worker nodes in batches.</p>&#13;
&#13;
<figure><div class="figure" id="performing_upgrades_by_replacing_nodes_in_a_cluster">&#13;
<img alt="prku 0207" src="assets/prku_0207.png"/>&#13;
<h6><span class="label">Figure 2-7. </span>Performing upgrades by replacing nodes in a cluster.</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="In-place upgrades" data-type="sect3"><div class="sect3" id="idm45612000591080">&#13;
<h3>In-place upgrades</h3>&#13;
&#13;
<p>In-place upgrades are appropriate in resource-constrained environments where replacing nodes is not practical.<a data-primary="deployment models" data-secondary="upgrades" data-tertiary="in-place" data-type="indexterm" id="idm45612000589384"/><a data-primary="upgrades" data-secondary="in-place" data-type="indexterm" id="idm45612000588136"/><a data-primary="in-place upgrades" data-type="indexterm" id="idm45612000587192"/>  The rollback path is more difficult and, hence, the risk is higher.  But this can and should be mitigated with comprehensive testing.  Keep in mind as well that Kubernetes in production configurations is a highly available system.<a data-primary="nodes" data-secondary="in-place upgrades" data-type="indexterm" id="idm45612000586136"/>  If in-place upgrades are done one node at a time, the risk is reduced.  So, if using a config management tool such as Ansible to execute the steps of this upgrade operation, resist the temptation to hit all nodes at once in production.</p>&#13;
&#13;
<p>For etcd nodes, following the documentation for<a data-primary="etcd" data-secondary="in-place upgrades of nodes" data-type="indexterm" id="idm45612000584424"/> that project, you will simply take each node offline, one at a time, performing the upgrade for OS,  etcd, and other packages, and then bringing it back online.  If running etcd in a container, consider pre-pulling the image in question prior to bringing the member offline to minimize downtime.</p>&#13;
&#13;
<p>For the Kubernetes control plane and worker nodes, if kubeadm was used for initializing the cluster, that tool should also be used for upgrades.<a data-primary="control plane nodes" data-secondary="in-place upgrades" data-type="indexterm" id="idm45612000582488"/><a data-primary="worker nodes" data-secondary="in-place upgrades" data-type="indexterm" id="idm45612000581512"/>  The upstream docs have detailed instructions on how to perform this process for each minor version upgrade from 1.13 forward.  At the risk of sounding like a broken record, as always, plan for failure, automate as much as possible, and test extensively.</p>&#13;
&#13;
<p>That brings us to end of upgrade options.  Now, let’s circle back around to the beginning of the story—what mechanisms you use to trigger these cluster provisioning and upgrade options.  We’re tackling this topic last because it requires the context of everything we’ve covered so far in this chapter.<a data-primary="deployment models" data-secondary="upgrades" data-startref="ix_dplymup" data-type="indexterm" id="idm45612000579464"/><a data-primary="upgrades" data-startref="ix_upgr" data-type="indexterm" id="idm45612000578216"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Triggering Mechanisms" data-type="sect1"><div class="sect1" id="triggering_mechanisms">&#13;
<h1>Triggering Mechanisms</h1>&#13;
&#13;
<p>Now that we’ve looked at all the concerns to<a data-primary="deployment models" data-secondary="triggering mechanisms" data-type="indexterm" id="idm45612000575336"/><a data-primary="triggering mechanisms for automated installations and management" data-type="indexterm" id="idm45612000574360"/><a data-primary="automation" data-secondary="triggering mechanisms for" data-type="indexterm" id="idm45612000573640"/> solve for in your Kubernetes deployment model, it’s useful to consider the triggering mechanisms that fire off the automation for installation and management, whatever form that takes.  Whether using a Kubernetes managed service, a prebuilt installer, or your own custom automation built from the ground up, how you fire off cluster builds, cluster scaling, and cluster upgrades is important.<a data-primary="clusters" data-secondary="triggering mechanisms for builds, scaling, and upgrades" data-type="indexterm" id="idm45612000572184"/></p>&#13;
&#13;
<p>Kubernetes installers generally have a CLI tool that can be used to initiate the installation process.  However, using that tool in isolation leaves you without a single source of truth or cluster inventory record.  Managing your cluster inventory is difficult when you don’t have a list of that inventory.</p>&#13;
&#13;
<p>A GitOps approach has become popular in recent years.  In this case the source of truth is a code repository that contains the configurations for the clusters under management.  When configurations for a new cluster are committed, automation is triggered to provision a new cluster.  When existing configurations are updated, automation is triggered to update the cluster, perhaps to scale the number of worker nodes or perform an upgrade of Kubernetes and the cluster add-ons.</p>&#13;
&#13;
<p>Another approach that is more Kubernetes-native is to represent clusters and their dependencies in Kubernetes custom resources and then use Kubernetes operators to respond to the declared state in those custom resources by provisioning clusters.<a data-primary="operators (Kubernetes)" data-secondary="using for triggering mechanisms" data-type="indexterm" id="idm45612000568856"/>  This is the approach taken by projects like Cluster API.  The sources of truth in this case are the Kubernetes resources stored in etcd in the management cluster.  However, multiple management clusters for different regions or tiers are commonly employed.  Here, the GitOps approach can be used in conjunction whereby the cluster resource manifests are stored in source control and the pipeline submits the manifests to the appropriate management cluster.  In this way, you get the best of both the GitOps and Kubernetes-native worlds.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45612000567096">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>When developing a deployment model for Kubernetes, consider carefully what managed services or existing Kubernetes installers (free and licensed) you may leverage.  Keep automation as a guiding principle for all the systems you build.  Wrap your wits around all the architecture and topology concerns, particularly if you have uncommon requirements that need to be met.  Think through the infrastructure dependencies and how to integrate them into your deployment process.  Consider carefully how to manage the machines that will comprise your clusters.  Understand the containerized components that will form the control plane of your cluster.  Develop consistent patterns for installing the cluster add-ons that will provide the essential features of your app platform.  Version your platform and get your day-2 management and upgrade paths in place before you put production workloads on your clusters.<a data-primary="deployment models" data-startref="ix_dplym" data-type="indexterm" id="idm45612000564872"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>