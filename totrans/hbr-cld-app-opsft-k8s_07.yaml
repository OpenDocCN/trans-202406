- en: 'Chapter 6\. Multicluster Fleets: Provision and Upgrade Life Cycles'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章。多集群舰队：供应和升级生命周期
- en: 'The terms *multicluster* and *multicloud* have become common in today’s landscape.
    For the purposes of this discussion, we will define these terms as follows:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*多集群* 和 *多云* 这些术语在当今的景观中变得普遍。对于本讨论的目的，我们将定义这些术语如下：'
- en: Multicluster
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 多集群
- en: Refers to scenarios where more than a single cluster is under management or
    an application is made up of parts that are hosted on more than one cluster
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 指的是管理超过一个集群或应用程序由部分托管在多个集群上的场景
- en: Multicloud
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 多云
- en: Refers to scenarios where the multiple clusters in use also span infrastructure
    substrates, which might include a private datacenter and a single public cloud
    provider or multiple public cloud providers
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 涉及使用多个集群的场景，这些集群也跨越基础设施基板，可能包括私有数据中心和单个或多个公共云提供商
- en: The differences here are more academic; the reality is that you are more likely
    than not to have to manage many clusters just as your organization has had to
    manage multiple VMware ESXi hosts that run VMs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的差异更多是学术性的；实际情况是，你很可能需要管理多个集群，就像你的组织不得不管理多个运行 VM 的 VMware ESXi 主机一样。
- en: The differences will matter when your container orchestration platform has variation
    across infrastructure substrates. We’ll talk about some of the places where this
    currently comes up and may affect some of your management techniques or application
    architectures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的容器编排平台在基础设施基板上有变化时，这些差异就会有所影响。我们将讨论当前一些可能出现这种情况的地方，以及可能影响一些你的管理技术或应用架构的地方。
- en: Why Multicluster?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么要多集群？
- en: Let’s discuss the use cases that lead to multiple clusters under management.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论导致管理多个集群的使用案例。
- en: 'Use Case: Using Multiple Clusters to Provide Regional Availability for Your
    Applications'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：使用多个集群为你的应用程序提供区域性可用性
- en: As discussed in [Chapter 4](ch04.html#single_cluster_availability), a single
    cluster can span multiple availability zones. Each availability zone has independent
    failure characteristics. A failure in the power supply, network provider, and
    even physical space (e.g., the datacenter building) should be isolated to one
    availability zone. Typically, the network links across availability zones still
    provide for significant throughput and low latency, allowing the etcd cluster
    for the Kubernetes API server to span hosts running in different availability
    zones. However, your application may need to tolerate an outage that affects more
    than two availability zones within a region or tolerate an outage of the entire
    region.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如同 [第 4 章](ch04.html#single_cluster_availability) 中所讨论的，一个单一集群可以跨越多个可用区。每个可用区具有独立的故障特性。例如，电源供应、网络提供商，甚至物理空间（例如数据中心建筑）的故障应该被隔离到一个可用区内。通常情况下，跨可用区的网络连接仍提供显著的吞吐量和低延迟，允许
    Kubernetes API 服务器的 etcd 集群跨越运行在不同可用区主机上的情况。然而，你的应用程序可能需要容忍影响同一区域内超过两个可用区或整个区域的停机情况。
- en: So perhaps one of the most easily understood use cases is to create more than
    one multiavailability zone cluster in two or more regions. You will commonly find
    applications that are federated across two “swim lanes,” sometimes referred to
    as a [*blue-green architecture*](https://oreil.ly/82hDU). The “blue-green” pairing
    pattern can often be found within the same region, with alternate blue-green pairs
    in other regions. You may choose to bring that same architecture to OpenShift
    where you run two separate clusters that host the same set of components for the
    application, effectively running two complete end-to-end environments, either
    of which can support most of the load of your users. Additional issues concerning
    load balancing and data management arise around architectural patterns required
    to support cross-region deployments and will be covered in [Chapter 8](ch08.html#working_example_of_multicluster_applicat).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，可能最容易理解的一个使用案例是在两个或更多区域创建超过一个多可用区集群。你通常会发现跨两个“游泳道”联合的应用程序，有时被称为 [*蓝绿架构*](https://oreil.ly/82hDU)。在同一区域内经常可以找到“蓝绿”配对模式，在其他区域中也有交替的蓝绿配对。你可以选择将同样的架构带到
    OpenShift 中，运行两个分开的集群，为应用程序托管相同的组件集，有效地运行两个完整的端到端环境，其中任何一个都可以支持大多数用户的负载。围绕支持跨区域部署所需的架构模式，涉及负载均衡和数据管理的其他问题将在
    [第 8 章](ch08.html#working_example_of_multicluster_applicat) 中进行讨论。
- en: 'Use Case: Using Multiple Clusters for Multiple Tenants'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：使用多个集群为多个租户提供服务
- en: The Kubernetes community boundary for tenancy is a single cluster. In general,
    the API constructs within Kubernetes focus on dividing the compute resources of
    the cluster into namespaces (also called *projects* in OpenShift). Users are then
    assigned roles or `ClusterRole`s to access their namespaces. However, cluster-scoped
    resources like `ClusterRole`s, CRDs, namespaces/projects, webhook configurations,
    and so on really cannot be managed by independent parties. Each API resource must
    have a unique name within the collection of the same kind of API resources. If
    there were true multitenancy within a cluster, then some API concept (like a tenant)
    would group things like `ClusterRole`s, CRDs, and webhook configurations and prevent
    collisions (in naming or behavior) across each tenant, much like projects do for
    applications (e.g., deployments, services, and `PersistentVolumeClaim`s can duplicate
    names or behavior across different namespaces/projects).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes社区对租户的界限是一个单独的集群。一般来说，Kubernetes内的API结构侧重于将集群的计算资源划分为命名空间（在OpenShift中也称为*项目*）。然后，用户被分配角色或`ClusterRole`来访问他们的命名空间。然而，像`ClusterRole`、CRD、命名空间/项目、Webhook配置等集群范围的资源确实不能由独立方管理。每个API资源在同类API资源集合中必须有唯一的名称。如果集群内真正有多租户，那么某种API概念（比如租户）将会组织`ClusterRole`、CRD和Webhook配置等事物，并防止跨每个租户中的命名或行为发生冲突，就像应用程序的项目（例如，部署、服务和`PersistentVolumeClaim`在不同命名空间/项目中可以重复命名或行为一样）。
- en: So Kubernetes is easiest to consume when you can assign a cluster to a tenant.
    A tenant might be a line of business or a functional team within your organization
    (e.g., quality engineering or performance and scale test). Then, a set of cluster-admins
    or similarly elevated `ClusterRole`s can be assigned to the owners of the cluster.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当您可以为租户分配一个集群时，Kubernetes最容易被消费。租户可以是组织内的业务线或功能团队（例如，质量工程或性能和规模测试）。然后，一组集群管理员或类似的`ClusterRole`可以分配给集群的所有者。
- en: Hence, an emerging pattern is that platform teams that manage OpenShift clusters
    will define a process where a consumer may request a cluster for their purposes.
    As a result, multiple clusters now require consistent governance and policy management.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个新兴的模式是OpenShift集群管理平台团队将定义一个流程，消费者可以请求一个集群来满足其目的。因此，多个集群现在需要一致的治理和策略管理。
- en: 'Use Case: Supporting Far-Edge Use Cases Where Clusters Do Not Run in Traditional
    Datacenters or Clouds'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用案例：支持远边使用案例，在这些案例中，集群不运行在传统数据中心或云中。
- en: There are some great examples of how technology is being applied to a variety
    of use cases where computing power is coupled with sensor data from cameras, audio
    sensors, or environmental sensors and machine learning or AI to improve efficiency,
    provide greater safety, or create novel consumer interactions.^([1](ch06.html#ch01fn36))
    These use cases are often referred to generically as *edge computing* because
    the computing power is outside the datacenter and closer to the “edge” of the
    consumer experience.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 技术正在被应用到多种使用案例中，这些案例中，计算能力与来自摄像头、音频传感器或环境传感器的传感器数据以及机器学习或人工智能结合，以提高效率、提供更大的安全性，或创建新型消费者互动[^1]。这些使用案例通常被泛称为*边缘计算*，因为计算能力位于数据中心之外，更接近消费者体验的“边缘”。
- en: The introduction of high-bandwidth capabilities with 5G also creates scenarios
    where an edge-computing solution can use a localized 5G network within a space
    like a manufacturing plant and where edge-computing applications help track product
    assembly, automate safety controls for employees, or protect sensitive machinery.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 高带宽能力与5G的引入也创造了场景，边缘计算解决方案可以在类似制造厂的空间内使用本地化的5G网络，边缘计算应用程序帮助追踪产品组装，自动化员工安全控制，或保护敏感机器设备。
- en: Just as containers provide a discrete package for enterprise web-based applications,
    there are significant benefits of using containers in edge-based applications.
    Similarly, the automated recovery of services by your container orchestration
    is also beneficial, even more so when the computing source is not easily accessible
    within your datacenter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 就像容器为企业基于Web的应用程序提供了一个独立的包装一样，在基于边缘的应用程序中使用容器也有显著的好处。同样，你的容器编排的自动服务恢复在计算资源不易访问的数据中心内更为有益。
- en: Architectural Characteristics
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构特征
- en: Now that we have seen some of the reasons why you may use multiple clusters
    or clouds to support your needs, let’s take a look at some of the architectural
    benefits and challenges of such an approach.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到一些使用多个集群或云来支持您需求的原因，让我们来看看这种方法的一些架构优势和挑战。
- en: Region availability versus availability zones
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 区域可用性与可用区可用性
- en: With multiple clusters hosting the application, you can spread instances of
    the application across multiple cloud regions. Each cluster within a region will
    still spread compute capacity across multiple availability zones. See [Figure 6-1](#cluster_region_availability_allows_multi)
    for a visual representation of this topology.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多个托管应用程序的集群，您可以在多个云区域中分布应用程序的实例。每个区域内的每个集群仍将计算容量分布在多个可用区中。请参见[图 6-1](#cluster_region_availability_allows_multi)以查看此拓扑结构的可视化表示。
- en: '![](assets/hcok_0601.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0601.png)'
- en: Figure 6-1\. Cluster region availability allows multiple clusters to run across
    independent cloud regions
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 集群区域可用性允许在独立云区域中运行多个集群
- en: Under this style of architecture, each cluster can tolerate the total loss of
    any one availability zone (AZ1, AZ2, or AZ3 could become unavailable but not more
    than one), and the workload will continue to run and serve requests. As a result
    of two availability zone failures, the etcd cluster would lose quorum and the
    control plane would become unavailable.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种架构风格下，每个集群可以容忍任何一个可用区的完全丧失（例如 AZ1、AZ2 或 AZ3 可能变得不可用，但不会超过一个），并且工作负载将继续运行和提供请求。由于两个可用区的故障，etcd
    集群将丧失法定人数，并且控制平面将变得不可用。
- en: The reason that a Kubernetes control plane becomes inoperable with more than
    a single availability zone failure is because of quorum requirements for etcd.
    Typically, etcd will have three replicas that are maintained in the control plane,
    each replica supported by exactly one availability zone. If a single availability
    zone is lost, there are still two out of three replicas present and distributed
    writes can still be sure that the write transaction is accepted. If two availability
    zones fail, then write attempts will be rejected. Pods running on worker nodes
    in the cluster may still be able to serve traffic, but no updates related to the
    Kubernetes API will be accepted or take place. However, the independent cluster
    running in one of the other regions could continue to respond to user requests.
    See [Chapter 4](ch04.html#single_cluster_availability) for a deeper analysis of
    how this process works.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面在超过单个可用区故障时无法运行的原因是由于 etcd 的法定人数要求。通常情况下，etcd 将在控制平面中维护三个副本，每个副本由正好一个可用区支持。如果单个可用区丢失，仍然存在三个副本中的两个，并且分布式写入仍然可以确保写入事务被接受。如果两个可用区失败，则写入尝试将被拒绝。在集群中运行在工作节点上的
    Pod 可能仍然能够提供流量服务，但不会接受或执行与 Kubernetes API 相关的任何更新。然而，在其他区域的独立集群仍然可以继续响应用户请求。详细分析此过程的内容请参见[第
    4 章](ch04.html#single_cluster_availability)。
- en: Mitigating latency for users based on geography
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根据地理位置减少用户的延迟
- en: If you have users in different locations, using more than one cloud region can
    also improve response times for your users. When a user attempts to access the
    web user experience of your application or an API exposed by your workload, their
    request can be routed to the nearest available instance of your application. Typically,
    a *Global Server Load Balancer* (GSLB) is used to efficiently route traffic in
    these scenarios. When the user attempts to access the service, a DNS lookup will
    be delegated to the nameservers hosted by your GSLB. Then, based on a heuristic
    of where the request originated, the nameserver will respond with the IP address
    of the nearest hosted instance of your application. You can see a visual representation
    of this in [Figure 6-2](#requests_to_resolve_the_address_of_a_glo).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的用户分布在不同地区，使用多个云区域还可以改善用户的响应时间。当用户尝试访问您应用的 Web 用户体验或工作负载提供的 API 时，他们的请求可以被路由到最近可用的应用实例。通常情况下，使用*全球服务器负载均衡器*（GSLB）来有效地处理这些流量路由。当用户尝试访问服务时，DNS
    查询将委托给由您的 GSLB 托管的名称服务器。然后，根据请求的来源启发式算法，名称服务器将返回最近托管实例的 IP 地址。您可以在[图 6-2](#requests_to_resolve_the_address_of_a_glo)中看到此过程的可视化表示。
- en: '![](assets/hcok_0602.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0602.png)'
- en: Figure 6-2\. Requests to resolve the address of a global service using a GSLB
    will return the closest instance based on proximity to the request originator
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 使用 GSLB 解析全局服务地址的请求将返回最接近请求发起者的实例，基于其与请求发起者的距离。
- en: Consistency of your platform (managed Kubernetes versus OpenShift plus cloud
    identity providers)
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 您的平台一致性（托管 Kubernetes 与 OpenShift 加云身份提供商）
- en: One of the major benefits of the OpenShift Container Platform is that it deploys
    and runs consistently across all cloud providers and substrates like VMware and
    bare metal. When you consider whether to consume a managed Kubernetes provider
    or OpenShift, be aware that each distribution of Kubernetes makes various architectural
    decisions that can require greater awareness of your application to ensure cross-provider
    portability.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: OpenShift 容器平台的一个主要优势是，它在所有云提供商和基础设施基板（如 VMware 和裸金属）上都能部署和运行一致。当您考虑是选择消费托管
    Kubernetes 提供商还是 OpenShift 时，请注意每个 Kubernetes 分发版都会做出各种架构决策，这可能需要更多关注您的应用程序，以确保跨提供商的可移植性。
- en: Provisioning Across Clouds
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跨云资源配置
- en: Choosing a Kubernetes strategy affords a great way to simplify how applications
    consume elastic cloud-based infrastructure. To some extent, the problem of how
    you consume a cloud’s resources shifts from solving the details for every application
    to one platform—namely, how your organization will adopt and manage Kubernetes
    across infrastructure substrates.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个 Kubernetes 策略提供了简化应用程序如何消费弹性云基础设施的一种有效方式。在某种程度上，解决如何消费云资源的问题从为每个应用程序解决细节转变为一个平台，即您的组织将如何在基础设施基板上采用和管理
    Kubernetes。
- en: There are several ways to provision Kubernetes from community-supported projects.
    For the purposes of this section, we’ll focus on how to provision Kubernetes using
    the Red Hat OpenShift Container Platform. Then we’ll discuss how you could alternatively
    consume managed OpenShift or managed Kubernetes services as part of your provisioning
    life cycle.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方式可以从社区支持的项目中为 Kubernetes 提供资源。在本节中，我们将专注于如何使用 Red Hat OpenShift 容器平台来进行
    Kubernetes 的资源配置。然后，我们将讨论如何在资源配置生命周期中作为一部分使用托管的 OpenShift 或托管的 Kubernetes 服务。
- en: User-Managed OpenShift
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户管理的 OpenShift
- en: When you provision an OpenShift Container Platform 4.x cluster, you have two
    options for how infrastructure resources are created. *User-provisioned infrastructure*
    (UPI) allows you more control to spin up VMs, network resources, and storage and
    then give these details to the install process and allow them to be bootstrapped
    into a running cluster. Alternatively, you can rely on the more automated approach
    of *installer-provisioned infrastructure* (IPI). Using IPI, the installer accepts
    cloud credentials with the appropriate privileges to create the required infrastructure
    resources. The IPI process will typically define a *virtual private cloud* (VPC).
    Note that you can specify the VPC as an input parameter if your organization has
    its own conventions for how these resources are created and managed. Within the
    VPC, resources, including network load balancers, object store buckets, virtual
    computing resources, elastic IP addresses, and so forth, are all created and managed
    by the install process.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当您配置 OpenShift 容器平台 4.x 集群时，您有两种选项来创建基础设施资源。*用户提供的基础设施*（UPI）允许您更多控制权，可以启动虚拟机、网络资源和存储，然后将这些详细信息提供给安装过程，并允许它们引导到一个运行中的集群。或者，您可以依赖更自动化的*安装程序提供的基础设施*（IPI）方法。使用
    IPI，安装程序接受具有适当特权的云凭证来创建所需的基础设施资源。IPI 过程通常会定义一个*虚拟私有云*（VPC）。请注意，如果您的组织有自己的约定方式来创建和管理这些资源，您可以将
    VPC 指定为输入参数。在 VPC 内，资源包括网络负载均衡器、对象存储桶、虚拟计算资源、弹性 IP 地址等，所有这些资源都由安装过程创建和管理。
- en: 'Let’s take a look at provisioning an OpenShift cluster across three cloud providers:
    AWS, Microsoft Azure, and Google Cloud Platform. For this discussion, we will
    review how the install process makes use of declarative configuration (just as
    Kubernetes does in general) and how this relates to the `ClusterVersionOperator`
    (CVO), which manages the life cycle of the OpenShift cluster itself.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下如何在三个云提供商（AWS、Microsoft Azure 和 Google Cloud Platform）上配置 OpenShift 集群。在本讨论中，我们将回顾安装过程如何使用声明性配置（与
    Kubernetes 一般的做法相同），以及这与管理 OpenShift 集群生命周期的 `ClusterVersionOperator`（CVO）的关系。
- en: First, you will need to download the openshift-installer binary for your appropriate
    version. Visit [Red Hat](https://cloud.redhat.com), create an account, and follow
    the steps to Create Cluster and download the binary for local use. Specific details
    about the options available for installation are available in the [product documentation](https://oreil.ly/HerIm).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要根据适当的版本下载openshift-installer二进制文件。访问[Red Hat](https://cloud.redhat.com)，创建帐户，并按照创建集群和下载本地使用二进制文件的步骤进行操作。有关安装可用选项的具体详细信息，请参阅[产品文档](https://oreil.ly/HerIm)。
- en: Let’s demonstrate how this approach works by looking at a few example configuration
    files for the openshift-installer binary. The full breadth of options for installing
    and configuring OpenShift is beyond the scope of this book. See the [OpenShift
    Container Platform documentation](https://oreil.ly/wOJ3P) for a thorough reference
    of all supported options. The following examples will highlight how the declarative
    nature of the OpenShift 4.x install methodology simplifies provisioning clusters
    across multiple substrates. Further, a walk-through example of the `MachineSet`
    API will demonstrate how operators continue to manage the life cycle and health
    of the cluster after provisioning.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过查看一些openshift-installer二进制文件的示例配置文件来演示此方法的工作原理。OpenShift 4.x安装方法的所有支持选项超出了本书的范围。请参阅[OpenShift
    Container Platform文档](https://oreil.ly/wOJ3P)了解所有支持选项的详细参考。以下示例将突出显示OpenShift
    4.x安装方法的声明性特性如何简化在多个基础设施上配置集群的过程。此外，通过`MachineSet` API的演练示例将演示在配置集群后如何继续管理集群的生命周期和健康状态。
- en: '[Example 6-1](#an_example_install_configdotyaml_to_pro) defines a set of options
    for provisioning an OpenShift cluster on AWS. [Example 6-2](#an_example_install_configdotyaml_to_pr)
    defines how to provision an OpenShift cluster on Microsoft Azure, while [Example 6-3](#an_example_install_configdotyaml_to_p)
    defines the equivalent configuration for Google Cloud Platform. [Example 6-4](#an_example_install_configdotyaml_to)—you
    guessed it!—provides an example configuration for VMware vSphere. With the exception
    of the VMware vSphere example (which is more sensitive to your own environment),
    you can use these examples to provision your own clusters with minimal updates.
    Refer to the OpenShift Container Platform product documentation for a full examination
    of install methods.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-1](#an_example_install_configdotyaml_to_pro) 定义了一组选项，用于在AWS上配置OpenShift集群。[示例 6-2](#an_example_install_configdotyaml_to_pr)
    定义了如何在Microsoft Azure上配置OpenShift集群，而[示例 6-3](#an_example_install_configdotyaml_to_p)
    则为Google Cloud Platform提供了等效的配置。[示例 6-4](#an_example_install_configdotyaml_to)——你猜对了！—为VMware
    vSphere提供了一个配置示例。除了VMware vSphere示例（对您自己的环境更敏感外），您可以使用这些示例来最小化更新以配置您自己的集群。请参阅OpenShift
    Container Platform产品文档，详细了解安装方法。'
- en: Example 6-1\. An example *install-config.yaml* to provision an OpenShift cluster
    on AWS
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-1\. 一个用于在AWS上配置OpenShift集群的示例*install-config.yaml*。
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Example 6-2\. An example *install-config.yaml* to provision an OpenShift cluster
    on Microsoft Azure
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-2\. 一个用于在Microsoft Azure上配置OpenShift集群的示例*install-config.yaml*。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Example 6-3\. An example *install-config.yaml* to provision an OpenShift cluster
    on Google Cloud Platform
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-3\. 一个用于在Google Cloud Platform上配置OpenShift集群的示例*install-config.yaml*。
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example 6-4\. An example *install-config.yaml* to provision an OpenShift cluster
    on VMware vSphere
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 6-4\. 一个用于在VMware vSphere上配置OpenShift集群的示例*install-config.yaml*。
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Any one of these *install-config.yaml* files can be used to provision your
    cluster using the following command:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这些*install-config.yaml*文件中的任何一个都可以用来使用以下命令来配置您的集群：
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note how each example shares some of the same options, notably the `clusterName`
    and `baseDomain` that will be used to derive the default network address of the
    cluster (applications will be hosted by default at `https://*.apps.clusterName.baseDomain`
    and the API endpoint for OpenShift will be available at `https://api.clusterName.baseDomain:6443`).
    When the openshift-installer runs, DNS entries on the cloud provider (e.g., Route
    53 in the case of AWS) will be created and linked to the appropriate network load
    balancers (also created by the install process), that in turn resolve to IP addresses
    running within the VPC.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，每个示例都共享一些相同的选项，特别是`clusterName`和`baseDomain`，这些将用于推导集群的默认网络地址（应用程序默认托管在`https://*.apps.clusterName.baseDomain`，而OpenShift的API端点将可用于`https://api.clusterName.baseDomain:6443`）。当openshift-installer运行时，云提供商上的DNS条目（例如AWS的Route
    53）将被创建并链接到适当的网络负载均衡器（也由安装过程创建），这些负载均衡器进而解析为运行在VPC内的IP地址。
- en: Each example defines sections for the `controlPlane` and `compute` that correspond
    to `MachineSet`s that will be created and managed. We’ll talk about how these
    relate to operators within the cluster shortly. More than one `MachinePool` within
    the `compute` section can be specified. Both the `controlPlane` and `compute`
    sections provide configurability for the compute hosts and can customize which
    availability zones are used. Settings including the type (or size) for each host
    and the options for what kind of storage is attached to the hosts are also available,
    but reasonable defaults will be chosen if omitted.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 每个示例定义了与将创建和管理的 `MachineSet` 相对应的 `controlPlane` 和 `compute` 部分。我们将很快讨论这些与集群中操作员的关系。`compute`
    部分可以指定多个 `MachinePool`。`controlPlane` 和 `compute` 部分均提供了对计算主机的配置能力，并且可以自定义使用的可用性区域。如果省略，将选择合理的默认设置，包括每个主机的类型（或大小）以及附加到主机的存储类型选项。
- en: Now, if we compare where the *install-config.yaml* properties vary for each
    substrate, we will find cloud-specific options within the `platform` sections.
    There is a global `platform` to specify which region the cluster should be created
    within, as well as `platform` sections under each of the `controlPlane` and `compute`
    sections to override settings for each provisioned host.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们比较每个基板的 *install-config.yaml* 属性的差异，我们将在 `platform` 部分中找到特定于云的选项。全局 `platform`
    指定集群应在哪个地区创建，每个 `controlPlane` 和 `compute` 部分下的 `platform` 部分可以覆盖为每个创建的主机设置。
- en: As introduced in [Chapter 5](ch05.html#continuous_delivery_across_clusters),
    the [Open Cluster Management](https://oreil.ly/D1UvC) project is a new approach
    to managing the multicluster challenges that most cluster maintainers encounter.
    [Chapter 5](ch05.html#continuous_delivery_across_clusters) discussed how applications
    can be distributed easily across clusters. Now let’s look at how the cluster provisioning,
    upgrade, and decommissioning process can be driven using Open Cluster Management.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第 5 章](ch05.html#continuous_delivery_across_clusters)介绍的，[Open Cluster Management](https://oreil.ly/D1UvC)
    项目是管理大多数集群维护者遇到的多集群挑战的新方法。[第 5 章](ch05.html#continuous_delivery_across_clusters)讨论了如何轻松地在集群之间分发应用程序。现在让我们看看如何使用
    Open Cluster Management 驱动集群的配置、升级和退役过程。
- en: In the following example, we will walk through creating a new cluster on a cloud
    provider. The underlying behavior is leveraging the same openshift-install process
    that we just discussed. Once provisioned, the Open Cluster Management framework
    will install an agent that runs as a set of pods on the new cluster. We refer
    to this agent as a `klusterlet`, mimicking the naming of the `kubelet` process
    that runs on nodes that are part of a Kubernetes cluster.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将演示如何在云服务提供商上创建新的集群。其底层行为利用了我们刚讨论过的 openshift-install 进程。一旦创建，Open
    Cluster Management 框架将安装一个作为一组 pod 运行的代理。我们称这个代理为 `klusterlet`，模仿运行在 Kubernetes
    集群节点上的 `kubelet` 进程的命名。
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following assumes the user has already set up the Open Cluster Management
    project or RHACM for Kubernetes as described in [Chapter 5](ch05.html#continuous_delivery_across_clusters).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户已经按照[第 5 章](ch05.html#continuous_delivery_across_clusters)中描述的方式设置了 Open
    Cluster Management 项目或 RHACM 用于 Kubernetes。
- en: From the RHACM for Kubernetes web console, open the Automate Infrastructure
    > Clusters page and click on the action to “Create cluster” as shown in [Figure 6-3](#the_cluster_overview_page_allows_you_to).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从 RHACM for Kubernetes Web 控制台中，打开“自动化基础设施 > 集群”页面，并点击“创建集群”操作，如[图 6-3](#the_cluster_overview_page_allows_you_to)所示。
- en: '![](assets/hcok_0603.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0603.png)'
- en: Figure 6-3\. The cluster overview page allows you to provision new clusters
    from the console
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-3\. 集群概览页面允许您从控制台为新的集群提供资源。
- en: The “Create cluster” action shown in [Figure 6-4](#cluster_creation_form_via_rhacm_for_kube)
    opens to a form where you provide a name and select one of the available cloud
    providers.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 6-4](#cluster_creation_form_via_rhacm_for_kube)中显示的“创建集群”操作打开一个表单，您可以在其中提供名称并选择一个可用的云服务提供商。
- en: '![](assets/hcok_0604.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0604.png)'
- en: Figure 6-4\. Cluster creation form via RHACM for Kubernetes
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Figure 6-4\. 通过 RHACM for Kubernetes 的集群创建表单
- en: 'Next, select a version of OpenShift to provision. The available list maps directly
    to the `ClusterImageSet`s available on the hub cluster. You can introspect these
    images with the following command:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，选择要部署的 OpenShift 版本。可用列表直接映射到 hub 集群上的 `ClusterImageSet`。您可以使用以下命令检查这些镜像：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Further down the page, as shown in [Figure 6-5](#select_your_release_image_left_parenthes),
    you will also need to specify a provider connection. In the case of AWS, you will
    need to provide the Access ID and Secret Key to allow API access by the installation
    process with your AWS account.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 页面底部，如 [图 6-5](#select_your_release_image_left_parenthes) 所示，您还需要指定一个提供者连接。在
    AWS 的情况下，您需要提供访问 ID 和 Secret Key，以允许安装过程通过您的 AWS 帐户进行 API 访问。
- en: '![](assets/hcok_0605.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0605.png)'
- en: Figure 6-5\. Select your release image (the version to provision) and your provider
    connection
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 选择您的发布镜像（要供应的版本）和您的提供者连接
- en: At this point, you can simply click Create and the cluster will be provisioned.
    However, let’s walk through how the `MachinePool` operator allows you to manage
    `MachineSet`s within the cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您只需单击“创建”即可开始供应集群。然而，让我们浏览一下 `MachinePool` 运算符如何允许您在集群内管理 `MachineSet`。
- en: Customize the “Worker pool1” `NodePool` for your desired region and availability
    zones. See Figures [6-6](#customizing_the_region_and_zones_for_the) and [6-7](#customizing_the_availability_zone)
    for an example of what this will look like in the form. You can amend these options
    after the cluster is provisioned as well.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义“Worker pool1” `NodePool` 以满足您期望的区域和可用区。参见图 [6-6](#customizing_the_region_and_zones_for_the)
    和 [6-7](#customizing_the_availability_zone) 中表单中的示例。集群供应后，您还可以修改这些选项。
- en: '![](assets/hcok_0606.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0606.png)'
- en: Figure 6-6\. Customizing the region and zones for the cluster workers
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 自定义集群工作节点的区域和区域
- en: '![](assets/hcok_0607.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0607.png)'
- en: Figure 6-7\. Customize the availability zones within the region that are valid
    to host workers
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 自定义在区域内有效托管工作节点的可用区
- en: A final summary of your confirmed choices is presented for review in [Figure 6-8](#confirmed_options_in_form).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最终确认您的选择的摘要在 [图 6-8](#confirmed_options_in_form) 中供查看。
- en: '![](assets/hcok_0608.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0608.png)'
- en: Figure 6-8\. Confirmed options selected in the form
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 在表单中确认的选项
- en: Once you have made your final customizations, click Create to begin the provisioning
    process as shown in [Figure 6-9](#rhacm_web_console_view_that_includes_lin). The
    web console provides a view that includes links to the provisioning logs for the
    cluster. If the cluster fails to provision (e.g., due to a quota restriction in
    your cloud account), the provisioning logs provide clues on what to troubleshoot.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 完成最终定制后，单击“创建”以开始供应过程，如 [图 6-9](#rhacm_web_console_view_that_includes_lin) 所示。Web
    控制台提供了一个视图，其中包含指向集群供应日志的链接。如果集群无法供应（例如，由于云账户的配额限制），供应日志提供了需要排除故障的线索。
- en: '![](assets/hcok_0609.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0609.png)'
- en: Figure 6-9\. RHACM web console view that includes links to the provisioning
    logs for the cluster
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. RHACM Web 控制台视图，包含指向集群供应日志的链接
- en: Behind the form editor, a number of Kubernetes API objects are created. A small
    number of these API objects are cluster scoped (`ManagedCluster` in particular).
    The `ManagedCluster` controller will ensure that a project (namespace) exists
    that maps to the cluster name. Other controllers, including the controller that
    begins the provisioning process, will use the `cluster` project (namespace) to
    store resources that provide an API control surface for provisioning. Let’s take
    a look at a subset of these that you should become familiar with.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在表单编辑器后面，创建了许多 Kubernetes API 对象。其中一小部分是集群范围的（特别是`ManagedCluster`）。`ManagedCluster`
    控制器将确保存在一个映射到集群名称的项目（命名空间）。其他控制器，包括开始供应过程的控制器，将使用 `cluster` 项目（命名空间）来存储提供供应的 API
    控制面资源。让我们来看看你应该熟悉的其中一部分。
- en: ManagedCluster
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ManagedCluster
- en: '`ManagedCluster` (API group: cluster.open-cluster-management.io/v1; cluster
    scoped) recognizes that a remote cluster is under the control of the hub cluster.
    The agent that runs on the remote cluster will attempt to create `ManagedCluster`
    if it does not exist on the hub, and it must be accepted by a user identity on
    the hub with appropriate permissions. You can see the example created in [Example 6-5](#example_of_the_managedcluster_api_object).
    Note that labels for this object will drive placement decisions later in this
    chapter.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`ManagedCluster`（API 组：cluster.open-cluster-management.io/v1；集群范围）承认远程集群受控于中心集群。在远程集群上运行的代理将尝试在中心集群上创建
    `ManagedCluster` 如果它在中心集群上不存在，并且必须由具有适当权限的用户身份在中心集群上接受。您可以查看在 [示例 6-5](#example_of_the_managedcluster_api_object)
    中创建的示例。请注意，此对象的标签将在本章后面驱动放置决策。'
- en: Example 6-5\. Example of the `ManagedCluster` API object
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-5\. `ManagedCluster` API 对象的示例
- en: '[PRE6]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ClusterDeployment
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterDeployment
- en: '`ClusterDeployment` (API group: hive.openshift.io/v1; namespace scoped) controls
    the provisioning and decommissioning phases of the cluster. A controller on the
    hub takes care of running the openshift-installer on your behalf. If the cluster
    creation process fails for any reason (e.g., you encounter a quota limit within
    your cloud account), the cloud resources will be destroyed and another attempt
    will be made after a waiting period to reattempt successful creation of the cluster.
    Unlike traditional automation methods that “try once” and require user intervention
    upon failure, the Kubernetes reconcile loop for this API kind will continue to
    attempt to create the cluster (with appropriate waiting periods in between) as
    shown in [Example 6-6](#example_clusterdeployment_created_by_the). You can also
    create these resources directly through the `oc` or `kubectl` like any Kubernetes
    native resource.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterDeployment`（API 组：hive.openshift.io/v1；命名空间范围）控制集群的供应和解除阶段。中央控制器负责代表您运行
    openshift-installer。如果由于任何原因（例如，在您的云帐户中遇到配额限制）集群创建过程失败，云资源将被销毁，并在等待一段时间后再次尝试成功创建集群。与传统的“尝试一次”并在失败时需要用户干预的自动化方法不同，此
    API 种类的 Kubernetes 协调循环将继续尝试创建集群（在之间适当的等待期间）如 [示例 6-6](#example_clusterdeployment_created_by_the)
    所示。您还可以像任何 Kubernetes 本机资源一样直接通过 `oc` 或 `kubectl` 创建这些资源。'
- en: Example 6-6\. Example `ClusterDeployment` created by the form
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 通过表单创建的示例 `ClusterDeployment`
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: KlusterletAddonConfig
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: KlusterletAddonConfig
- en: '`KlusterletAddonConfig` (API group: agent.open-cluster-management.io/v1; namespace
    scoped) represents the capabilities that should be provided on the remote agent
    that manages the cluster. In [Example 6-7](#an_example_of_the_klusterletaddonconfig),
    the Open Cluster Management project refers to the remote agent as a `klusterlet`,
    mirroring the language of `kubelet`.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`KlusterletAddonConfig`（API 组：agent.open-cluster-management.io/v1；命名空间范围）代表应在管理集群的远程代理上提供的功能。在
    [示例 6-7](#an_example_of_the_klusterletaddonconfig) 中，Open Cluster Management 项目将远程代理称为
    `klusterlet`，与 `kubelet` 的语言相呼应。'
- en: Example 6-7\. An example of the `KlusterletAddonConfig` API object
  id: totrans-95
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. `KlusterletAddonConfig` API 对象的示例
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: MachinePool
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MachinePool
- en: '`MachinePool` (API group: hive.openshift.io/v1; namespace -scoped) allows you
    to create a collection of hosts that work together and share characteristics.
    You might use a `MachinePool` to group a set of compute capacity that supports
    a specific team or line of business. As we will see in the next section, `MachinePool`
    also allows you to dynamically size your cluster. Finally, the status provides
    a view into the `MachineSet`s that are available on `ManagedCluster`. See [Example 6-8](#the_machinepool_api_object_provides_a_co)
    for the example `MachinePool` created earlier, which provides a control surface
    to scale the number of replicas up or down within the pool and status about the
    `MachineSet`s under management on the remote cluster.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '`MachinePool`（API 组：hive.openshift.io/v1；命名空间范围）允许您创建一组共同工作并共享特征的主机集合。您可以使用
    `MachinePool` 来将支持特定团队或业务线的一组计算能力分组。正如我们将在下一节中看到的，`MachinePool` 还允许您动态调整集群的大小。最后，状态提供了一个查看在远程集群上管理的
    `MachineSet` 的视图。查看 [示例 6-8](#the_machinepool_api_object_provides_a_co) 以获取先前创建的示例
    `MachinePool`，它提供了一个控制界面，用于在池中增加或减少副本数量，并提供有关在远程集群上管理的 `MachineSet` 的状态。'
- en: Example 6-8\. The example `MachinePool` API object
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-8\. 示例 `MachinePool` API 对象
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once provisioned, the address for the Kubernetes API server and OpenShift web
    console will be available from the cluster details page. You can use these coordinates
    to open your web browser and authenticate with the new cluster as the kubeadmin
    user. You can also access the `KUBECONFIG` certificates that allow you command-line
    access to the cluster.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦提供，Kubernetes API 服务器和 OpenShift Web 控制台的地址将在集群详细信息页面中可用。您可以使用这些坐标打开您的 Web
    浏览器，并作为 kubeadmin 用户对新集群进行身份验证。您还可以访问 `KUBECONFIG` 证书，以便您可以通过命令行访问集群。
- en: You can download the `KUBECONFIG` authorization as shown in [Figure 6-10](#downloading_the_kubeconfig_authorization)
    for the new cluster from the RHACM web console under the cluster overview page
    or access it from the command line. From the web console, click on the cluster
    name in the list of clusters to view an overview of that cluster. Once the provisioning
    process has completed, you will be able to download the `KUBECONFIG` file that
    will allow you command-line access to the cluster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照[图 6-10](#downloading_the_kubeconfig_authorization)所示从 RHACM Web 控制台下载新集群的`KUBECONFIG`授权，位于集群概述页面下方或从命令行访问。从
    Web 控制台，点击集群列表中的集群名称以查看该集群的概述。一旦完成配置过程，您将能够下载`KUBECONFIG`文件，从而允许您通过命令行访问集群。
- en: '![](assets/hcok_0610.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![示例 6-10](assets/hcok_0610.png)'
- en: Figure 6-10\. Downloading the kubeconfig authorization from the RHACM web console
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. 从 RHACM Web 控制台下载 kubeconfig 授权
- en: From the command line, you can retrieve the information stored in a secret under
    the cluster project (namespace) as in [Example 6-9](#output_of_the_cluster_kubeconfig_filedot).
    Save the file contents and configure your `KUBECONFIG` environment variable to
    point to the location of the file. Then `oc` will be able to run commands against
    the remote cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从命令行可以检索存储在集群项目（命名空间）下的秘密信息，例如[示例 6-9](#output_of_the_cluster_kubeconfig_filedot)。保存文件内容并配置您的`KUBECONFIG`环境变量指向文件的位置。然后`oc`将能够对远程集群运行命令。
- en: Example 6-9\. Output of the cluster `KUBECONFIG` file
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-9\. 集群`KUBECONFIG`文件的输出
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that our cluster is up and running, let’s walk through how we can scale
    the cluster. We will review this concept from the context of the oc CLI.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的集群已经启动并运行，让我们来看看如何扩展集群。我们将从 oc CLI 的上下文中审视这个概念。
- en: First, open two terminals and configure the `KUBECONFIG` or context for each
    of the hub clusters and our newly minted `mycluster`. See Examples [6-10](#example_six_onezero_an_example_of_what_t)
    and [6-11](#an_example_of_how_terminal_two_will_look) for examples of what each
    of the two separate terminals will look like after you run these commands. Note
    the tip to override your PS1 shell prompt temporarily to avoid confusion when
    running commands on each cluster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，打开两个终端并为每个 hub 集群和我们新创建的`mycluster`配置`KUBECONFIG`或上下文。查看示例[6-10](#example_six_onezero_an_example_of_what_t)和[6-11](#an_example_of_how_terminal_two_will_look)，了解在运行这些命令后每个独立终端的外观示例。请注意，临时覆盖您的
    PS1 shell 提示，以避免在每个集群上运行命令时混淆。
- en: Example 6-10\. An example of what Terminal 1 will look like
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-10\. 终端 1 的示例外观
- en: '[PRE11]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Example 6-11\. An example of how Terminal 2 will look
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-11\. 终端 2 的示例外观
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now you should have Terminal 1 with a prompt including `hubcluster` and Terminal
    2 with a prompt including `mycluster`. We will refer to these terminals by the
    appropriate names through the rest of the example.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该在终端 1 中有一个包含`hubcluster`的提示，而在终端 2 中有一个包含`mycluster`的提示。在后续示例中，我们将根据相应的名称引用这些终端。
- en: In the following walk-through, we will review the `MachineSet` API, which underpins
    how an OpenShift cluster understands compute capacity. We will then scale the
    size of our managed cluster from the hub using the `MachinePool` API that we saw
    earlier.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，我们将回顾`MachineSet` API，这是 OpenShift 集群理解计算容量的基础。然后，我们将使用之前见过的`MachinePool`
    API来调整我们管理的集群大小。
- en: 'In the `mycluster` terminal, review the `MachineSet`s for your cluster:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mycluster`终端中，查看您集群的`MachineSet`：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Each `MachineSet` will have a name following the pattern: `<clusterName>-<five-character
    identifier>-<machinePoolName>-<availabilityZone>`. In your cluster, you should
    see counts for the desired number of machines per `MachineSet`, the current number
    of machines that are available, and the current number that are considered `Ready`
    to be integrated as nodes into the OpenShift cluster. Note that these three counts
    should generally be equivalent and should only vary when the cluster is in a transition
    state (adding or removing machines) or when an underlying availability problem
    in the cluster causes one or more machines to be considered unhealthy. For example,
    when you edit a `MachineSet` to increase the desired replicas, you will see the
    `Desired` count increment by one for that `MachineSet`. As the machine is provisioned
    and proceeds to boot and configure the `kubelet`, the `Current` count will increment
    by one. Finally, as the `kubelet` registers with the Kubernetes API control plane
    and marks the node as `Ready`, the `Ready` count will increment by one. If at
    any point the machine becomes unhealthy, the `Ready` count may decrease. Similarly,
    if you reduced the `Desired` count by one, you would see the same staggered reduction
    in counts as the machine proceeds through various life-cycle states until it is
    removed.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, in the hub terminal, review the `worker MachinePool` defined for the
    managed cluster:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We will increase the size of the managed cluster `mycluster` by one node:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The size of the worker node will be determined by the values set in the `MachinePool
    mycluster-worker`. The availability zone of the new node will be determined by
    the `MachinePool` controller, where nodes are distributed across availability
    zones as evenly as possible.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Immediately after you have patched the `MachinePool` to increase the number
    of desired replicas, rerun the command to view the `MachineSet` on your managed
    cluster:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Over the course of a few minutes, you should see the new node on the managed
    cluster transition from `Desired` to `Current` to `Ready`, with a final result
    that looks like the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Let’s recap what we’ve just seen. First, we used a declarative method (*install-config.yaml*)
    to provision our first cluster, called a hub. Next, we used the hub to provision
    the first managed cluster in our fleet. That managed cluster was created under
    the covers using the same IPI method but with the aid of Kubernetes API and continuous
    reconcilers that ensure that the running cluster matches the `Desired` state.
    One of the APIs that governs the `Desired` state is the `MachinePool` API on the
    hub cluster. Because our first fleet member, `mycluster`, was created from the
    hub cluster, we can use the `MachinePool API` to govern how `mycluster` adds or
    removes nodes. Or indeed, we can create additional `MachinePool`s that add capacity
    to the cluster.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the process, the underlying infrastructure substrate was completely
    managed through operators. The `MachineSet` operator on the managed cluster was
    given updated instructions by the `MachinePool` operator on the hub to grow the
    number of machines available in one of the `MachineSet`s supporting `mycluster`.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中，底层的基础设施底层完全通过运算符进行管理。在管理的集群上，`MachineSet`运算符通过中心的`MachinePool`运算符获得更新指令，以增加支持`mycluster`中一个`MachineSet`中的可用机器数量。
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We will use the term *infrastructure substrate* as a catchall term to refer
    to the compute, network, and storage resources provided by bare metal virtualization
    within your datacenter or virtualization offered by a public cloud provider.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用术语*基础设施底层*来作为一个总称术语，用于指代您数据中心内提供的裸金属虚拟化或公共云提供的虚拟化中所提供的计算、网络和存储资源。
- en: Upgrading Your Clusters to the Latest Version of Kubernetes
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将您的集群升级到最新版本的Kubernetes
- en: 'As we saw with `MachinePool`s and `MachineSet`s, operators provide a powerful
    way to abstract the differences across infrastructure substrates, allowing an
    administrator to declaratively specify the desired outcome. An OpenShift cluster
    is managed by the CVO, which acts as an “operator of operators” pattern to manage
    operators for each dimension of the cluster’s configuration (authentication, networking,
    machine creation, bootstrapping, and removal, and so on). Every cluster will have
    a `Cluster​Ver⁠sion` API object named `version`. You can retrieve the details
    for this object with the command:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如同我们在`MachinePool`和`MachineSet`中所见，运算符提供了一种强大的方式来抽象基础设施底层的差异，允许管理员以声明方式指定期望的结果。OpenShift集群由CVO管理，其作为“运算符的运算符”模式来管理集群配置的各个方面（身份验证、网络、机器创建、引导和移除等）。每个集群都将有一个名为`version`的`ClusterVersion`
    API对象。您可以使用以下命令检索此对象的详细信息：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`ClusterVersion` specifies a “channel” to seek available versions for the cluster
    and a desired version from that channel. Think of a channel as an ongoing list
    of available versions (e.g., 4.5.1, 4.5.2, 4.5.7, and so on). There are channels
    for “fast” adoption of new versions, as well as “stable” versions. The fast channels
    produce new versions quickly. Coupled with connected telemetry data from a broad
    source of OpenShift clusters running across infrastructure substrates and industries,
    fast channels allow the delivery and validation of new releases very quickly (on
    order of weeks or days). As releases in fast channels have enough supporting evidence
    that they are broadly acceptable across the global fleet of OpenShift clusters,
    versions are promoted to stable channels. Hence, the list of versions within a
    channel is not always consecutive. An example `ClusterVersion` API object is represented
    in [Example 6-12](#an_example_clusterversion_api_object_tha).'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClusterVersion`指定了一个“通道”来查找集群的可用版本和从该通道中选择的期望版本。将通道视为可用版本的持续列表（例如4.5.1、4.5.2、4.5.7等）。有用于快速采纳新版本的通道，也有用于“稳定”版本的通道。快速通道快速生成新版本。与来自运行在不同基础设施底层和行业的广泛OpenShift集群的连接的遥测数据结合使用，快速通道允许快速交付和验证新发布的版本（一般在几周或几天内）。由于快速通道中的发布版本有足够的支持证据表明它们在全球OpenShift集群范围内被广泛接受，因此这些版本会晋升为稳定通道。因此，通道中版本的列表并非总是连续的。一个示例`ClusterVersion`
    API对象在[示例 6-12](#an_example_clusterversion_api_object_tha)中表示。'
- en: Example 6-12\. An example `ClusterVersion` API object that records the version
    history for the cluster and the desired version—changing the desired version will
    cause the operator to begin applying updates to achieve the goal
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-12\. 记录集群版本历史和期望版本的`ClusterVersion` API对象示例—更改期望版本将导致运算符开始应用更新以实现目标
- en: '[PRE19]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Upgrading a version of Kubernetes along with all other supporting APIs and infrastructure
    around it can be a daunting task. The operator that controls the life cycle of
    all of the container images is known informally as [“Cincinnati”](https://oreil.ly/6HIZd)
    and formally as the *OpenShift Update Service* (OSUS). OSUS (or Cincinnati) maintains
    a connected graph of versions and tracks which “walks” or “routes” within the
    graph are known as good upgrade paths. For example, an issue may be detected in
    early release channels that indicates that the upgrade from 4.4.23 to 4.5.0 to
    4.5.18 may be associated with a specific problem. A fix can be released to create
    a new release 4.4.24 that then allows a successful and predictable upgrade from
    4.4.23 to 4.4.24 to 4.5.0 to 4.5.18\. The graph records the successive nodes that
    must be walked to ensure success.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Kubernetes 版本以及其周围所有其他支持 API 和基础设施一起升级可能是一项令人生畏的任务。控制所有容器镜像生命周期的操作器非正式称为[“辛辛那提”](https://oreil.ly/6HIZd)，正式称为*OpenShift
    更新服务*（OSUS）。OSUS（或辛辛那提）维护版本的连接图，并跟踪图中称为良好升级路径的“行走”或“路由”。例如，在早期发布通道可能检测到的问题表明，从
    4.4.23 升级到 4.5.0 再到 4.5.18 可能会与特定问题相关联。可以发布修复版本 4.4.24，然后可以成功和可预测地从 4.4.23 升级到
    4.4.24，再到 4.5.0，最后到 4.5.18。该图记录了必须行走的连续节点，以确保成功。
- en: However, the OSUS operator removes the guesswork, allowing the cluster administrator
    to specify the desired version from the channel. From there, the CVO will carry
    out the following tasks:^([2](ch06.html#ch01fn37))
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，OSUS 操作器消除了猜测工作，允许集群管理员从通道中指定所需版本。从那里开始，CVO 将执行以下任务：^([2](ch06.html#ch01fn37))
- en: Upgrade the Kubernetes and OpenShift control plane pods, including etcd.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 和 OpenShift 控制平面的 pod，包括 etcd。
- en: Upgrade the operating system for the nodes running the control plane pods.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级运行控制平面 pod 的节点操作系统。
- en: Upgrade the cluster operators controlling aspects like authentication, networking,
    storage, and so on.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 升级控制认证、网络、存储等方面的集群运算符。
- en: For nodes managed by the `MachineConfigOperator`, upgrade the operating system
    for the nodes running the data plane pods (user workload).
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对由 `MachineConfigOperator` 管理的节点，升级运行数据平面 pod（用户工作负载）的操作系统。
- en: The upgrade takes place in a rolling fashion, avoiding bursting the size of
    the cluster or taking out too much capacity at the same time. Because the control
    plane is spread across three machines, as each machine undergoes an operating
    system update and reboot, the other two nodes maintain the availability of the
    Kubernetes control plane, including the datastore (etcd), the scheduler, the controller,
    the Kubernetes API server, and the network ingress controller.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 升级以滚动方式进行，避免使集群的规模突然增加或同时占用太多容量。因为控制平面分布在三台机器上，每台机器进行操作系统更新和重启时，其他两个节点会保持 Kubernetes
    控制平面的可用性，包括数据存储（etcd）、调度器、控制器、Kubernetes API 服务器和网络入口控制器。
- en: When the data plane is upgraded, the upgrade process will respect `PodDisruptionBudget`s
    and look for feedback about the health of OpenShift and user workloads running
    on each node by means of liveness and readiness probes.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据平面升级时，升级过程将尊重 `PodDisruptionBudget` 并通过生存探针和就绪探针寻找有关 OpenShift 和每个节点上运行的用户工作负载健康状况的反馈。
- en: Note
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Sometimes the group of clusters under management is referred to as a *fleet*.
    Individual clusters under management may be referred to as *fleet members*, primarily
    to distinguish them from the hub cluster that is responsible for management.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，管理下的集群组被称为*舰队*。个别管理的集群可能被称为*舰队成员*，主要是为了区分它们与负责管理的中心集群。
- en: From the RHACM web console, you can manipulate the desired version of a managed
    cluster for a single fleet member or the entire fleet. From the console, choose
    the “Upgrade cluster” action for any cluster that shows “Upgrade available.” Recall
    from the discussion around channels that not every channel may have an upgrade
    currently available. Additionally, the list of versions may not be consecutive.
    Figures [6-11](#actions_permitted_on_a_cluster_allow_a_f), [6-12](#the_list_of_available_versions_is_provid),
    and [6-13](#multiple_clusters_may_be_selected_for_up) provide examples of what
    this actually looks like for a specific cluster or for multiple clusters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 RHACM Web 控制台，您可以为单个联合成员或整个联合提供的版本管理群集。从控制台选择“升级集群”操作，以升级显示“可升级”的任何群集。回顾通道周围的讨论，不是每个通道都可以当前提供升级。此外，版本列表可能不是连续的。图
    [6-11](#actions_permitted_on_a_cluster_allow_a_f)、[6-12](#the_list_of_available_versions_is_provid)
    和 [6-13](#multiple_clusters_may_be_selected_for_up) 提供了特定群集或多个群集的实际示例。
- en: '![](assets/hcok_0611.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0611.png)'
- en: Figure 6-11\. Actions permitted on a cluster allow a fleet manager to upgrade
    the desired version of a cluster
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-11\. 允许对群集执行的操作允许联合管理者升级群集的期望版本
- en: '![](assets/hcok_0612.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0612.png)'
- en: Figure 6-12\. The list of available versions is provided for user selection
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-12\. 用户可选择的可用版本列表。
- en: '![](assets/hcok_0613.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/hcok_0613.png)'
- en: Figure 6-13\. Multiple clusters may be selected for upgrade, and the versions
    available will vary based on the cluster’s attached channel configuration in the
    `ClusterVersion` object
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-13\. 可选择多个集群进行升级，可用版本会根据`ClusterVersion`对象附加的通道配置而变化。
- en: A core topic for this book is how to manage your clusters as a fleet, and for
    that, we will rely on policies. The preceding discussion should provide a foundation
    for you to understand the moving parts and see that you can explicitly trigger
    upgrade behavior across the fleet. In [Chapter 7](ch07.html#multicluster_policy_configuration),
    we will discuss how we can control upgrade behavior across the fleet by policy.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的核心主题之一是如何将您的群集作为一个联合管理，并为此，我们将依赖策略。前面的讨论应为您提供了理解各个组成部分并看到您可以通过策略显式触发整个联合中的升级行为的基础。在
    [第 7 章](ch07.html#multicluster_policy_configuration) 中，我们将讨论如何通过策略控制整个联合的升级行为。
- en: Summary of Multicloud Cluster Provisioning
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多云集群供应概述
- en: Throughout our example, the specific infrastructure substrate showed up in a
    few declarative APIs, specifically represented by the *install-config.yaml* for
    the hub cluster and as part of the secrets referenced by the `ClusterDeployment`
    API object for the managed cluster. However, the action of provisioning a new
    cluster and adding or removing nodes for that fleet member was completely driven
    through Kubernetes API objects.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，具体的基础设施基质出现在几个声明性 API 中，特别是 *install-config.yaml* 用于中心群集和作为 `ClusterDeployment`
    API 对象的受管群集的一部分的秘密引用。然而，通过 Kubernetes API 对象完全驱动了为该联合成员的新群集进行供应以及添加或移除节点的操作。
- en: In addition, the upgrade life cycle managed through the CVO is consistent across
    supported infrastructure substrates. Hence, regardless if you provision an OpenShift
    cluster on a public cloud service or in your datacenter, you can still declaratively
    manage the upgrade process. The powerful realization you should now understand
    is that managing the infrastructure substrate for OpenShift clusters in multicloud
    scenarios can be completely abstracted away from many basic cluster-provisioning
    life-cycle operations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过 CVO 管理的升级生命周期在受支持的基础设施基质中保持一致。因此，无论您在公共云服务上还是在您的数据中心中提供 OpenShift 群集，您仍然可以声明式地管理升级过程。您现在应该明白的强大实现是，在多云场景中管理
    OpenShift 群集的基础设施基质可以完全从许多基本的群集供应生命周期操作中抽象出来。
- en: Beyond controlling the capacity of your fleet from the hub, you can assign policies
    with Open Cluster Management and drive behavior like fleet upgrades. We will see
    an example of fleet upgrade by policy in [Chapter 7](ch07.html#multicluster_policy_configuration).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从中心控制您的联合成员的能力，您还可以使用 Open Cluster Management 分配策略并驱动像联合升级这样的行为。我们将在 [第 7
    章](ch07.html#multicluster_policy_configuration) 中看到通过策略进行联合升级的示例。
- en: OpenShift as a Service
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenShift 作为一种服务
- en: The previous section described how you can abstract the provisioning and life
    cycle of OpenShift across multiple infrastructure substrates. Under the model
    we’ve outlined, you are responsible for the availability of your clusters. For
    budget or organizational reasons, you may choose to consider a managed service
    for OpenShift or Kubernetes. Using a vendor-provided “OpenShift as a Service”
    or “Kubernetes as a Service” can change how you interact with some dimensions,
    including cluster creation or decommissioning. However, your applications will
    run consistently regardless of whether the vendor manages the underlying infrastructure
    or you manage it.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们描述了如何在多个基础设施基板上抽象出 OpenShift 的供应和生命周期。根据我们所提出的模型，您负责集群的可用性。出于预算或组织原因，您可能选择考虑使用
    OpenShift 或 Kubernetes 的托管服务。使用供应商提供的“OpenShift即服务”或“Kubernetes即服务”可以改变您与某些维度的交互方式，包括集群的创建或退役。但是，无论供应商是否管理基础设施，您的应用程序都会始终稳定运行。
- en: Azure Red Hat OpenShift
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure Red Hat OpenShift
- en: '[Azure Red Hat OpenShift](https://oreil.ly/0OWW7) is integrated into the Microsoft
    Azure ecosystem, including Azure billing. Other aspects, including single sign-on,
    are automatically configured with Azure Active Directory, simplifying how you
    expose capabilities to your organization, particularly if you are already consuming
    other services on Azure. The underlying service is maintained by a partnership
    between Microsoft and Red Hat.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[Azure Red Hat OpenShift](https://oreil.ly/0OWW7) 整合到 Microsoft Azure 生态系统中，包括
    Azure 计费。其他方面，包括单点登录，都通过 Azure Active Directory 自动配置，简化了向您的组织展示功能的方式，特别是如果您已经在
    Azure 上消费其他服务的情况下。底层服务由 Microsoft 和 Red Hat 合作维护。'
- en: Red Hat OpenShift on AWS
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS 上的 Red Hat OpenShift
- en: '[Red Hat OpenShift on AWS](https://oreil.ly/fX0a2) was announced at the end
    of 2020 with planned availability in 2021\. It integrates OpenShift into the Amazon
    ecosystem, allowing for access and creation through the Amazon cloud console and
    consistent billing with the rest of your Amazon account. The underlying service
    is maintained by a partnership between Amazon and Red Hat.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[AWS 上的 Red Hat OpenShift](https://oreil.ly/fX0a2) 在 2020 年末宣布，并计划在 2021 年提供。它将
    OpenShift 整合到亚马逊生态系统中，允许通过亚马逊云控制台访问和创建，并与您的亚马逊帐户的其余部分进行一致的计费。底层服务由亚马逊和 Red Hat
    合作维护。'
- en: Red Hat OpenShift on IBM Cloud
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IBM 云上的 Red Hat OpenShift
- en: '[Red Hat OpenShift on IBM Cloud](https://oreil.ly/gZBHU) integrates OpenShift
    consumption into the IBM Cloud ecosystem, including integration with IBM Cloud
    single sign-on and billing. In addition, IBM Cloud APIs are provided to manage
    cluster provisioning, worker nodes, and the upgrade process. These APIs allow
    separate access controls via IBM Cloud Identity and Access Management for management
    of the cluster versus the access controls used for managing resources in the cluster.
    The underlying service is maintained by IBM.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[IBM 云上的 Red Hat OpenShift](https://oreil.ly/gZBHU) 将 OpenShift 消费整合到 IBM 云生态系统中，包括与
    IBM 云单点登录和计费的集成。此外，提供了 IBM 云 API 来管理集群的供应、工作节点和升级过程。这些 API 允许通过 IBM 云身份和访问管理来进行集群管理与管理集群中资源的访问控制分离。底层服务由
    IBM 维护。'
- en: OpenShift Dedicated
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenShift Dedicated
- en: '[OpenShift Dedicated](https://cloud.redhat.com) is a managed OpenShift as a
    Service offering provided by Red Hat. OpenShift clusters can be created across
    a variety of clouds from this service, in some cases under your own preexisting
    cloud account. Availability and maintenance of the cluster are handled by the
    Red Hat SRE team. The underlying service is maintained by Red Hat with options
    to bring your own cloud accounts on some supported infrastructure providers like
    AWS.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[OpenShift Dedicated](https://cloud.redhat.com) 是由 Red Hat 提供的托管 OpenShift
    即服务。可以通过此服务在各种云中创建 OpenShift 集群，在某些情况下可以在您自己的现有云帐户下进行。集群的可用性和维护由 Red Hat SRE 团队处理。底层服务由
    Red Hat 维护，并且在某些支持的基础设施提供商如 AWS 上有带来自己的云帐户的选项。'
- en: Kubernetes as a Service
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 即服务
- en: 'In addition to vendor-managed OpenShift as a Service, many vendors offer managed
    Kubernetes as a Service distributions. These are typically where the vendor adopts
    Kubernetes and integrates it into its ecosystem. The following are some examples
    of these services:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了供应商管理的 OpenShift 即服务外，许多供应商还提供托管的 Kubernetes 即服务发行版。这些通常是供应商采用 Kubernetes
    并将其集成到其生态系统中的情况。以下是一些这些服务的例子：
- en: Amazon Elastic Kubernetes Service
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊弹性 Kubernetes 服务
- en: Azure Kubernetes Service
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure Kubernetes 服务
- en: Google Kubernetes Engine
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Kubernetes Engine
- en: IBM Cloud Kubernetes Service
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM 云 Kubernetes 服务
- en: 'Because the Kubernetes community leaves some decisions up to vendors or users
    who assemble their own distributions, each of these managed services can introduce
    some variations that you should be aware of when adopting them as part of a larger
    multicloud strategy. In particular, several specific areas in Kubernetes have
    been evolving quickly:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 Kubernetes 社区将一些决策留给供应商或用户自行组装其分发版，每个托管服务都可能引入一些变化，这些变化在你采用它们作为更大的多云策略的一部分时需要注意。特别是，在
    Kubernetes 的几个特定领域中，发展迅速：
- en: Cluster creation
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群创建
- en: User identity and access management
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户身份和访问管理
- en: Network routing
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络路由
- en: Pod security management
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 安全管理
- en: Role-based access control
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于角色的访问控制
- en: Value-added admission controllers
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增值准入控制器
- en: Operating system management of the worker nodes
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点的操作系统管理
- en: Different security apparatus to manage compliance
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的安全工具来管理合规性
- en: Across each dimension, a vendor providing a managed Kubernetes service must
    decide how to best integrate that aspect of Kubernetes into the cloud provider’s
    ecosystem. The core API should respond consistently by way of the [CNCF Kubernetes
    Certification process](https://oreil.ly/sWAXA). In practice, differences tend
    to arise where Kubernetes is integrated into a particular cloud ecosystem.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个维度上，提供托管 Kubernetes 服务的供应商必须决定如何最好地将 Kubernetes 的这一方面整合到云提供商的生态系统中。核心 API
    应该通过 [CNCF Kubernetes 认证流程](https://oreil.ly/sWAXA) 保持一致响应。实际上，差异通常出现在 Kubernetes
    集成到特定云生态系统中的地方。
- en: For instance, in some cases a managed Kubernetes service will come with Kubernetes
    RBAC deployed and configured out of the box. Other vendors may leave it to the
    cluster creator to configure RBAC for Kubernetes. Across vendors that automatically
    configure the Kubernetes with RBAC, the set of out-of-the-box `ClusterRole`s and
    roles can vary.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在某些情况下，托管的 Kubernetes 服务将会预先配置和部署 Kubernetes 的 RBAC。其他供应商可能会让集群创建者为 Kubernetes
    配置 RBAC。对于自动配置了 Kubernetes RBAC 的供应商，预设的`ClusterRole`和角色集可能会有所不同。
- en: 'In other cases, the network ingress for a Kubernetes cluster can vary from
    cloud-specific extensions to use of the community network ingress controller.
    Hence, your application may need to provide alternative network ingress behavior
    based on the cloud providers that you choose to provide Kubernetes. When using
    Ingress (API group: networking.k8s.io/v1) on a particular cloud vendor–managed
    Kubernetes, the set of respected annotations can vary across providers, requiring
    additional validation for apps that must tolerate different managed Kubernetes
    services. With an OpenShift cluster (managed by a vendor or by you), all applications
    define the standard Ingress API with a fixed set of annotations or Route (API
    group: route.openshift.io/v1) API, which will be correctly exposed into the specific
    infrastructure substrate.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，Kubernetes 集群的网络入口可能会因云特定的扩展或使用社区网络入口控制器而有所不同。因此，你的应用可能需要根据你选择提供 Kubernetes
    的云服务商提供的情况，提供替代的网络入口行为。在特定云供应商管理的 Kubernetes 上使用 Ingress（API 组：networking.k8s.io/v1）时，跨供应商的注释集可能会有所不同，需要额外的验证以容忍不同的托管
    Kubernetes 服务。在 OpenShift 集群（由供应商管理或你自己管理）中，所有应用程序都使用标准的 Ingress API，并具有固定的注释集或
    Route API（API 组：route.openshift.io/v1），这将正确地暴露到特定的基础设施底层。
- en: The variation that you must address in your application architectures and multicloud
    management strategies is not insurmountable. However, be aware of these aspects
    as you plan your adoption strategy. Whether you adopt an OpenShift as a Service
    provider or run OpenShift within your own cloud accounts, all of the API-facing
    applications, including RBAC and networking, will behave the same.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在你规划采用策略时，你必须解决应用架构和多云管理策略中的这些变化。无论你选择将 OpenShift 作为服务提供商采用，还是在自己的云账户中运行 OpenShift，所有面向
    API 的应用程序，包括 RBAC 和网络，都会表现相同。
- en: Operating System Currency for Nodes
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点的操作系统版本
- en: 'As your consumption of an OpenShift cluster grows, practical concerns around
    security and operating system currency must be addressed. With an OpenShift 4.x
    cluster, the control plane hosts are configured with Red Hat CoreOS as the operating
    system. When upgrades occur for the cluster, the operating system of the control
    plane nodes are also upgraded. The CoreOS package manager uses a novel approach
    to applying updates: updates are packaged into containers and applied transactionally.
    Either the entire update succeeds or fails. When managing the update of an OpenShift
    control plane, the result of this approach limits the potential for partially
    completed or failed installs from the interaction of unknown or untested configurations
    within the operating system. By default, the operating system provisioned for
    workers will also use Red Hat CoreOS, allowing the data plane of your cluster
    the same transactional update benefits.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OpenShift群集的使用量增加，必须解决围绕安全性和操作系统当前性的实际问题。在OpenShift 4.x群集中，控制平面主机配置了Red Hat
    CoreOS作为操作系统。当对群集进行升级时，控制平面节点的操作系统也会升级。CoreOS软件包管理器采用新颖的方法来应用更新：更新被打包为容器并以事务方式应用。整个更新要么成功要么失败。在管理OpenShift控制平面的更新时，此方法的结果限制了由操作系统中未知或未经测试配置的交互部分完成或失败的潜力。默认情况下，为工作节点提供的操作系统也将使用Red
    Hat CoreOS，使得群集的数据平面享有同样的事务性更新优势。
- en: It is possible to add workers to an OpenShift cluster configured with RHEL.
    The process to add a RHEL worker node is covered in the product documentation
    and is beyond the scope of this book.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 可以向配置了RHEL的OpenShift群集添加工作节点。添加RHEL工作节点的过程在产品文档中有所涵盖，但超出了本书的范围。
- en: 'If you integrate a managed Kubernetes service into your multicluster strategy,
    pay attention to the division of responsibilities between your vendor and your
    teams: who owns the currency/compliance status of the worker nodes in the cluster?
    Virtually all of the managed Kubernetes service providers manage the operating
    system of the control plane nodes. However, there is variance across the major
    cloud vendors on who is responsible for the operating system of the worker nodes.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将托管的Kubernetes服务集成到您的多集群策略中，请注意供应商和团队之间责任划分：谁负责群集中工作节点的当前/合规状态？几乎所有托管的Kubernetes服务提供商管理控制平面节点的操作系统。但是，主要云供应商在工作节点操作系统的责任划分上存在差异。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We have covered quite a bit in this chapter. By now, you should understand how
    IPI provides a consistent abstraction of many infrastructure substrates. Once
    provisioned, operators within OpenShift manage the life-cycle operations for key
    functions of the cluster, including machine management, authentication, networking,
    and storage. We can also use the API exposed by these operators to request and
    drive upgrade operations against the control plane of the cluster and the operating
    system of the supporting nodes.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们覆盖了很多内容。现在，您应该了解IPI如何提供多个基础设施底层的一致抽象。一旦部署，OpenShift内的操作员将管理群集的关键功能的生命周期操作，包括机器管理、认证、网络和存储。我们还可以使用这些操作员提供的API请求和驱动控制平面和支持节点操作系统的升级操作。
- en: We also introduced cluster life-cycle management from [Open Cluster Management](https://oreil.ly/3J1SW)
    using a supporting offering, Red Hat Advanced Cluster Management. Using RHACM,
    we saw how to trigger the upgrade behavior for user-managed clusters on any infrastructure
    substrate.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了通过[Open Cluster Management](https://oreil.ly/3J1SW)使用支持提供的集群生命周期管理，Red
    Hat Advanced Cluster Management。使用RHACM，我们看到了如何触发任何基础设施底层的用户管理群集的升级行为。
- en: In the next chapter, we will continue to leverage cluster operators to configure
    and drive cluster behavior by defining Open Cluster Management policies that we
    can apply to one or more clusters under management.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续利用群集操作员通过定义可应用于一个或多个管理中的群集的Open Cluster Management策略来配置和驱动群集行为。
- en: ^([1](ch06.html#ch01fn36-marker)) Ted Dunning, “Using Data Fabric and Kubernetes
    in Edge Computing,” The Newstack (May 21, 2020), [*https://oreil.ly/W3J7f*](https://oreil.ly/W3J7f);
    “Edge Computing at Chick-fil-A,” Chick-fil-A Tech Blog (July 30, 2018), [*https://oreil.ly/HcJqZ*](https://oreil.ly/HcJqZ).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.html#ch01fn36-marker)) Ted Dunning，《在边缘计算中使用数据布局和Kubernetes》，The
    Newstack（2020年5月21日），[*https://oreil.ly/W3J7f*](https://oreil.ly/W3J7f)；《Chick-fil-A的边缘计算》，Chick-fil-A
    技术博客（2018年7月30日），[*https://oreil.ly/HcJqZ*](https://oreil.ly/HcJqZ)。
- en: ^([2](ch06.html#ch01fn37-marker)) Rob Szumski, “The Ultimate Guide to OpenShift
    Release and Upgrade Process for Cluster Administrators,” Red Hat OpenShift Blog
    (November 9, 2020), [*https://oreil.ly/hKCex*](https://oreil.ly/hKCex).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch06.html#ch01fn37-marker)) Rob Szumski，“集群管理员的 OpenShift 发布和升级流程终极指南”，红帽
    OpenShift 博客（2020年11月9日），[*https://oreil.ly/hKCex*](https://oreil.ly/hKCex)。
